<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí∏ üë©üèæ‚Äçüíª üëÜüèª Comment AWS fabrique ses services r√©silients. Mise √† l'√©chelle du r√©seau üò∑ üë®üèº‚Äç‚úàÔ∏è üë®üèº‚Äçüé®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le r√©seau Amazon Web Services compte 69 sites dans le monde dans 22 r√©gions: √âtats-Unis, Europe, Asie, Afrique et Australie. Dans chaque zone, il y a ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment AWS fabrique ses services r√©silients. Mise √† l'√©chelle du r√©seau</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/471688/"> Le r√©seau Amazon Web Services compte 69 sites dans le monde dans 22 r√©gions: √âtats-Unis, Europe, Asie, Afrique et Australie.  Dans chaque zone, il y a jusqu'√† 8 centres de donn√©es - centres de traitement des donn√©es.  Chaque centre de donn√©es poss√®de des milliers ou des centaines de milliers de serveurs.  Le r√©seau est construit de telle mani√®re que tous les sc√©narios d'interruption improbables soient pris en compte.  Par exemple, toutes les r√©gions sont isol√©es les unes des autres et les zones d'acc√®s sont espac√©es de plusieurs kilom√®tres.  M√™me si vous coupez le c√¢ble, le syst√®me passera aux canaux de sauvegarde et la perte d'informations √©quivaudra √† des unit√©s de paquets de donn√©es.  A propos de quels autres principes le r√©seau est construit et comment il est construit, dira Vasily Pantyukhin. <br><br><img src="https://habrastorage.org/webt/5p/_u/v_/5p_uv_g6etdeiay-nwb0r6v8ns0.png"><br><br>  <b>Vasily Pantyukhin a</b> commenc√© en tant qu‚Äôadministrateur Unix dans des soci√©t√©s .ru, a pass√© 6 ans dans les grandes glandes de Sun Microsystem et pendant 11 ans, il a pr√™ch√© la concentration sur les donn√©es du monde chez EMC.  Naturellement, il est devenu un cloud priv√©, puis est devenu public.  D√©sormais, en tant qu'architecte d'Amazon Web Services, les conseils techniques vous aident √† vivre et √† grandir dans le cloud AWS. <br><br>  Dans la partie pr√©c√©dente de la trilogie des appareils AWS, Vasily s'est plong√© dans l'appareil des serveurs physiques et de la mise √† l'√©chelle de la base de donn√©es.  Nitro-cards, hyperviseur personnalis√© bas√© sur KVM, base de donn√©es Amazon Aurora - tout cela dans l'article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment AWS" cuisine "ses services √©lastiques.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Mise √† l'√©chelle du serveur et de la base de donn√©es</a> . ‚Äù  Lisez pour plonger dans le contexte ou regardez une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">vid√©o de la</a> pr√©sentation. <br><br>  Dans cette partie, nous nous concentrerons sur la mise √† l'√©chelle du r√©seau - l'un des syst√®mes les plus complexes d'AWS.  L'√©volution d'un r√©seau plat vers le cloud priv√© virtuel et son appareil, les services internes Blackfoot et HyperPlane, le probl√®me d'un voisin bruyant, et √† la fin - l'√©chelle du r√©seau, de la dorsale et des c√¢bles physiques.  √Ä propos de tout cela sous la coupe. <br><br>  <i>Avis de non-responsabilit√©: tout ce qui suit est l'opinion personnelle de Vasily, et cela peut ne pas co√Øncider avec la position d'Amazon Web Services.</i> <br><a name="habracut"></a><br><h2>  Mise √† l'√©chelle du r√©seau </h2><br>  AWS Cloud a √©t√© lanc√© en 2006.  Son r√©seau √©tait assez primitif - avec une structure plate.  La plage d'adresses priv√©es √©tait commune √† tous les locataires du cloud.  Lorsque vous d√©marrez une nouvelle machine virtuelle, vous avez accidentellement re√ßu une adresse IP disponible de cette plage. <br><br><img src="https://habrastorage.org/webt/kj/u9/jw/kju9jwz69aeoldlbn6yn_vmtzom.jpeg"><br><br>  Cette approche √©tait facile √† mettre en ≈ìuvre, mais limitait fondamentalement l'utilisation du cloud.  En particulier, il √©tait assez difficile de d√©velopper des solutions hybrides combinant des r√©seaux priv√©s sur le terrain et dans AWS.  Le probl√®me le plus courant √©tait l'intersection de plages d'adresses IP. <br><br><img src="https://habrastorage.org/webt/lw/fu/tg/lwfutg75jtwalgbliyv-rmdhfbc.jpeg"><br><br><h3>  Cloud priv√© virtuel </h3><br>  Le cloud √©tait en demande.  Il est temps de r√©fl√©chir √† l'√©volutivit√© et √† la possibilit√© de son utilisation par des dizaines de millions de locataires.  Le r√©seau plat est devenu un obstacle majeur.  Par cons√©quent, nous avons r√©fl√©chi √† la fa√ßon d'isoler les utilisateurs les uns des autres au niveau du r√©seau afin qu'ils puissent s√©lectionner ind√©pendamment les plages IP. <br><br><img src="https://habrastorage.org/webt/fl/tw/co/fltwcomu7sr802932c3wyyaucve.jpeg"><br><br>  Qu'est-ce qui vous vient √† l'esprit en premier lorsque vous pensez √† l'isolement du r√©seau?  Bien s√ªr, le <b>VLAN</b> et le <b>VRF sont le routage et le transfert virtuels</b> . <br><br>  Malheureusement, cela n'a pas fonctionn√©.  L'ID VLAN n'est que de 12 bits, ce qui nous donne seulement 4096 segments isol√©s.  M√™me dans les plus grands commutateurs, vous pouvez utiliser un maximum de 1 √† 2 000 VRF.  L'utilisation combin√©e de VRF et de VLAN ne nous donne que quelques millions de sous-r√©seaux.  Ce n'est certainement pas suffisant pour des dizaines de millions de locataires, chacun devant pouvoir utiliser plusieurs sous-r√©seaux. <br><br>  Pourtant, nous ne pouvons tout simplement pas nous permettre d'acheter le nombre requis de grandes bo√Ætes, par exemple, aupr√®s de Cisco ou de Juniper.  Il y a deux raisons: cela co√ªte furieusement cher, et nous ne voulons pas devenir d√©pendants de leurs politiques de d√©veloppement et de correctifs. <br><br><blockquote>  Il n'y a qu'une seule conclusion - pour pr√©parer votre propre d√©cision. </blockquote><br>  En 2009, nous avons annonc√© <b>VPC</b> - <b>Virtual Private Cloud</b> .  Le nom a pris racine et maintenant de nombreux fournisseurs de cloud l'utilisent √©galement. <br><br>  VPC est un r√©seau virtuel <b>SDN</b> (Software Defined Network).  Nous avons d√©cid√© de ne pas inventer de protocoles sp√©ciaux aux niveaux L2 et L3.  Le r√©seau fonctionne sur Ethernet et IP standard.  Pour la transmission sur un r√©seau, le trafic des machines virtuelles est encapsul√© dans un wrapper de notre propre protocole.  Il indique l'ID qui appartient au VPC du locataire. <br><br><img src="https://habrastorage.org/webt/x7/yh/nc/x7yhncwzn9xfi677tpy65s3td18.jpeg"><br><br>  Cela semble facile.  Cependant, il est n√©cessaire de r√©soudre plusieurs probl√®mes techniques graves.  Par exemple, o√π et comment stocker les donn√©es de mappage pour les adresses MAC / IP virtuelles, les ID VPC et les adresses MAC / IP physiques correspondantes.  √Ä l'√©chelle AWS, il s'agit d'une √©norme table qui devrait fonctionner avec une latence minimale.  Le <b>service de cartographie</b> , qui est recouvert d'une couche mince sur tout le r√©seau, en est responsable. <br><br>  Dans les machines des nouvelles g√©n√©rations, l'encapsulation est r√©alis√©e par des cartes Nitro au niveau du fer.  Dans les cas plus anciens, encapsulation et d√©capsulation de logiciels. <br><br><img src="https://habrastorage.org/webt/6q/pt/zr/6qptzrmyqowtaimwlbu6mqobr44.jpeg"><br><br>  Voyons comment cela fonctionne en termes g√©n√©raux.  Commen√ßons par le niveau L2.  Supposons que nous ayons une machine virtuelle avec IP 10.0.0.2 sur un serveur physique 192.168.0.3.  Il envoie des donn√©es √† une machine virtuelle 10.0.0.3 qui vit sur 192.168.1.4.  Une requ√™te ARP est g√©n√©r√©e, qui tombe sur la carte r√©seau Nitro.  Pour simplifier, nous pensons que les deux machines virtuelles vivent dans le m√™me VPC ¬´bleu¬ª. <br><br><img src="https://habrastorage.org/webt/vl/u5/hv/vlu5hvvmaufe2e2jirp0mz14ugg.png"><br><br>  La carte remplace l'adresse source par la sienne et envoie la trame ARP au service de mappage. <br><br><img src="https://habrastorage.org/webt/0j/d9/lx/0jd9lx41a5744ptq64tdrmgzxya.png"><br><br>  Le service de mappage renvoie les informations n√©cessaires √† la transmission sur le r√©seau physique L2. <br><br><img src="https://habrastorage.org/webt/3e/ir/nt/3eirntgsopwiccgdaaneovvgqxe.png"><br><br>  La carte nitro dans la r√©ponse ARP remplace le MAC dans le r√©seau physique par l'adresse dans le VPC. <br><br><img src="https://habrastorage.org/webt/fv/uo/kh/fvuokhk6mswvl5i8ifguxgxrhee.png"><br><br>  Lors du transfert de donn√©es, nous enveloppons le MAC et l'IP logiques dans un wrapper VPC.  Tout cela est transmis sur le r√©seau physique √† l'aide des cartes IP Nitro appropri√©es de la source et de la destination. <br><br><img src="https://habrastorage.org/webt/4k/4i/u-/4k4iu-ei5cp9cdk-vfqmmf8vtig.png"><br><br>  La machine physique sur laquelle le package est destin√© √† effectuer des v√©rifications.  C'est pour √©viter la possibilit√© d'usurpation.  La machine envoie une demande sp√©ciale au service de mappage et demande: ¬´De la machine physique 192.168.0.3, j'ai re√ßu un paquet con√ßu pour 10.0.0.3 dans le VPC bleu.  Est-il l√©gitime? " <br><br><img src="https://habrastorage.org/webt/vv/qn/0e/vvqn0ecobvoxvxw8-1u3hgc8k48.png"><br><br>  Le service de mappage v√©rifie sa table d'allocation de ressources et autorise ou refuse le passage du paquet.  Dans toutes les nouvelles instances, une validation suppl√©mentaire est cousue sur des cartes Nitro.  Il est impossible de se d√©placer m√™me th√©oriquement.  Par cons√©quent, l'usurpation d'identit√© vers des ressources dans un autre VPC ne fonctionnera pas. <br><br><img src="https://habrastorage.org/webt/xx/tr/wa/xxtrwaqlrsbhdledrligrt0mfjc.png"><br><br>  Les donn√©es sont ensuite envoy√©es √† la machine virtuelle √† laquelle elles sont destin√©es. <br><br><img src="https://habrastorage.org/webt/1i/hb/py/1ihbpywnhbngzsuzs8376qf0i8g.png"><br><br>  Le service de mappage fonctionne √©galement comme un routeur logique pour transf√©rer des donn√©es entre des machines virtuelles sur diff√©rents sous-r√©seaux.  Tout y est conceptuellement simple, je ne vais pas l'analyser en d√©tail. <br><br><img src="https://habrastorage.org/webt/rr/rj/9n/rrrj9nvl-jwgk54pzowtmqm6ynm.png"><br><br>  Il s'av√®re que lors de la transmission de chaque paquet, les serveurs acc√®dent au service de mappage.  Comment faire face aux retards in√©vitables?  <b>La mise en cache</b> , bien s√ªr. <br><br>  Tout le charme est que vous n'avez pas besoin de mettre en cache toute la grande table.  Les machines virtuelles d'un nombre relativement faible de VPC vivent sur un serveur physique.  Les informations doivent √™tre mises en cache uniquement sur ces VPC.  Le transfert de donn√©es vers d'autres VPC dans la configuration "par d√©faut" n'est toujours pas l√©gitime.  Si des fonctionnalit√©s telles que l'appairage de VPC sont utilis√©es, des informations sur les VPC correspondants sont en outre charg√©es dans le cache. <br><br><img src="https://habrastorage.org/webt/cj/vu/uh/cjvuuhb_xbbsrhjdzxpf0x7lnck.jpeg"><br><br>  Avec le transfert de donn√©es vers le VPC compris. <br><br><h3>  Pieds-noirs </h3><br>  Que faire dans les cas o√π le trafic doit √™tre transmis √† l'ext√©rieur, par exemple sur Internet ou via un VPN au sol?  C'est l√† que <b>Blackfoot</b> , le service interne AWS, nous aide.  Il est con√ßu par notre √©quipe sud-africaine.  Par cons√©quent, le service porte le nom du pingouin qui vit en Afrique du Sud. <br><br><img src="https://habrastorage.org/webt/af/7s/gf/af7sgf9jhvniudixr94rqhwgdy0.jpeg"><br><br>  Blackfoot d√©capsule le trafic et en fait ce dont il a besoin.  Les donn√©es Internet sont envoy√©es telles quelles. <br><br><img src="https://habrastorage.org/webt/fi/93/02/fi9302-pumvdpx70gqpo7ufaaqw.png"><br><br>  Les donn√©es sont d√©capsul√©es et envelopp√©es √† nouveau dans un wrapper IPsec lors de l'utilisation d'un VPN. <br><br><img src="https://habrastorage.org/webt/jk/ag/iu/jkagiufrbqprn50vjuxqy_hwbe8.png"><br><br>  Lorsque vous utilisez Direct Connect, le trafic est √©tiquet√© et transmis au VLAN correspondant. <br><br><img src="https://habrastorage.org/webt/yj/xj/i0/yjxji0wexg4cugs-cztvnv_wvxs.png"><br><br><h3>  HyperPlane </h3><br>  Il s'agit d'un service de contr√¥le de flux interne.  De nombreux services r√©seau n√©cessitent de surveiller l' <b>√©tat du flux de donn√©es</b> .  Par exemple, lors de l'utilisation de NAT, le contr√¥le de flux doit garantir que chaque paire ¬´IP: port de destination¬ª poss√®de un port sortant unique.  Dans le cas de l'√©quilibreur <b>NLB</b> - √©quilibreur de <b>charge r√©seau</b> , le flux de donn√©es doit toujours √™tre dirig√© vers la m√™me machine virtuelle cible.  Les groupes de s√©curit√© sont un pare-feu dynamique.  Il surveille le trafic entrant et ouvre implicitement les ports pour le flux de paquets sortant. <br><br><img src="https://habrastorage.org/webt/wq/pa/kk/wqpakkyp8v_rdhyuclzte2e2y6w.jpeg"><br><br>  Dans le cloud AWS, les exigences de latence de transmission sont extr√™mement √©lev√©es.  Par cons√©quent, <b>HyperPlane est</b> essentiel √† l' <b>int√©grit√©</b> de l'ensemble du r√©seau. <br><br><img src="https://habrastorage.org/webt/ov/cr/wm/ovcrwmosat9fxborz1tmkxkjt_4.jpeg"><br><br>  Hyperplane est construit sur des machines virtuelles EC2.  Il n'y a pas de magie ici, seulement de la ruse.  L'astuce est que ce sont des machines virtuelles avec une grande RAM.  Les transactions sont transactionnelles et effectu√©es exclusivement en m√©moire.  Cela permet des retards de seulement quelques dizaines de microsecondes.  Travailler avec un disque tuerait toutes les performances. <br><br>  Hyperplane est un syst√®me distribu√© √† partir d'un grand nombre de ces machines EC2.  Chaque machine virtuelle a une bande passante de 5 Go / s.  Sur l'ensemble du r√©seau r√©gional, cela donne des t√©rabits de bande passante sauvages et vous permet de traiter des <b>millions de connexions par seconde</b> . <br><br>  HyperPlane ne fonctionne qu'avec des threads.  L'encapsulation des paquets VPC lui est totalement transparente.  La vuln√©rabilit√© potentielle de ce service interne ne permettra toujours pas de rompre l'isolement du VPC.  Pour la s√©curit√©, les niveaux ci-dessous sont responsables. <br><br><h3>  Voisin bruyant </h3><br>  Il y a aussi le <b>probl√®me du</b> <b>voisin bruyant</b> .  Supposons que nous ayons 8 n≈ìuds.  Ces n≈ìuds traitent les threads de tous les utilisateurs du cloud.  Tout semble aller bien et la charge devrait √™tre r√©partie uniform√©ment sur tous les n≈ìuds.  Les n≈ìuds sont tr√®s puissants et difficiles √† surcharger. <br><br>  Mais nous construisons notre architecture sur la base de sc√©narios m√™me improbables. <br><br><blockquote>  Une faible probabilit√© ne signifie pas une impossibilit√©. </blockquote><br>  Nous pouvons imaginer une situation dans laquelle un ou plusieurs utilisateurs g√©n√©reront trop de charge.  Tous les n≈ìuds HyperPlane sont impliqu√©s dans le traitement de cette charge, et d'autres utilisateurs peuvent potentiellement ressentir une sorte de d√©gradation des performances.  Cela d√©truit le concept du cloud, dans lequel les locataires n'ont aucun moyen de s'influencer mutuellement. <br><br><img src="https://habrastorage.org/webt/gc/ni/_l/gcni_lqe59zlatesmuodjxmcxcm.png"><br><br>  Comment r√©soudre le probl√®me d'un voisin bruyant?  La premi√®re chose qui me vient √† l'esprit est le partage.  Nos 8 n≈ìuds sont logiquement divis√©s en 4 fragments avec 2 n≈ìuds chacun.  Maintenant, un voisin bruyant ne sera g√™n√© que par un quart de tous les utilisateurs, mais beaucoup plus. <br><br><img src="https://habrastorage.org/webt/e7/vz/-v/e7vz-vablrrhawvz2xbvb1psfxu.png"><br><br>  Faisons-le diff√©remment.  Chaque utilisateur ne dispose que de 3 n≈ìuds. <br><br><img src="https://habrastorage.org/webt/md/su/px/mdsupxahouehxh6y-jffnyhjpyy.png"><br><br>  L'astuce consiste √† affecter des n≈ìuds √† diff√©rents utilisateurs de mani√®re al√©atoire.  Dans l'image ci-dessous, l'utilisateur bleu coupe les n≈ìuds avec l'un des deux autres utilisateurs - vert et orange. <br><br><img src="https://habrastorage.org/webt/xw/ee/yz/xweeyztmogrwbeomfqi_sbec0u0.png"><br><br>  Avec 8 n≈ìuds et 3 utilisateurs, la probabilit√© de passage d'un voisin bruyant avec l'un des utilisateurs est de 54%.  C'est avec cette probabilit√© que l'utilisateur bleu affectera les autres locataires.  De plus, seule une partie de sa charge.  Dans notre exemple, cette influence sera au moins en quelque sorte perceptible par tout le monde, mais seulement un tiers de tous les utilisateurs.  C'est d√©j√† un bon r√©sultat. <br><div class="scrollable-table"><table><tbody><tr><td>  Le nombre d'utilisateurs qui se croisent <br></td><td>  Probabilit√© en pourcentage <br></td></tr><tr><td>  0 <br></td><td>  18% <br></td></tr><tr><td>  1 <br></td><td>  54% <br></td></tr><tr><td>  2 <br></td><td>  26% <br></td></tr><tr><td>  3 <br></td><td>  2% <br></td></tr></tbody></table></div><br>  Ramenons la situation plus proche de la r√©alit√© - prenez 100 n≈ìuds et 5 utilisateurs sur 5 n≈ìuds.  Dans ce cas, aucun des n≈ìuds ne se croise avec une probabilit√© de 77%. <br><div class="scrollable-table"><table><tbody><tr><td>  Le nombre d'utilisateurs qui se croisent <br></td><td>  Probabilit√© en pourcentage <br></td></tr><tr><td>  0 <br></td><td>  77% <br></td></tr><tr><td>  1 <br></td><td>  21% <br></td></tr><tr><td>  2 <br></td><td>  1,8% <br></td></tr><tr><td>  3 <br></td><td>  0,06% <br></td></tr><tr><td>  4 <br></td><td>  0,0006% <br></td></tr><tr><td>  5 <br></td><td>  0.00000013% <br></td></tr></tbody></table></div><br>  Dans une situation r√©elle avec un grand nombre de n≈ìuds et d'utilisateurs HyperPlane, l'impact potentiel d'un voisin bruyant sur les autres utilisateurs est minime.  Cette m√©thode est appel√©e <b>shuffle sharding</b> .  Il minimise l'effet n√©gatif de la d√©faillance du n≈ìud. <br><br>  De nombreux services sont construits sur la base d'HyperPlane: Network Load Balancer, NAT Gateway, Amazon EFS, AWS PrivateLink, AWS Transit Gateway. <br><br><h3>  √âchelle du r√©seau </h3><br>  Parlons maintenant de l'√©chelle du r√©seau lui-m√™me.  Pour octobre 2019, AWS propose ses services dans <b>22 r√©gions</b> , et 9 autres sont pr√©vus. <br><br><ul><li>  Chaque r√©gion contient plusieurs zones de disponibilit√©.  Il y en a 69 dans le monde. <br></li><li>  Chaque AZ se compose de centres de traitement des donn√©es.  Il n'y en a pas plus de 8. <br></li><li>  Dans le centre de donn√©es, il y a un grand nombre de serveurs, certains jusqu'√† 300 000. <br></li></ul><br>  Maintenant, tout cela en moyenne, multipli√© et obtenir un chiffre impressionnant qui affiche l' <b>√©chelle du cloud Amazon</b> . <br><br>  Entre les zones d'acc√®s et le datacenter, de nombreux canaux optiques sont pos√©s.  Dans l'une de nos plus grandes r√©gions, seuls 388 canaux ont √©t√© pos√©s pour la communication d'AZ entre eux et les centres de communication avec les autres r√©gions (Transit Centers).  Au total, cela donne un <b>5000 Tbit</b> fou. <br><br><img src="https://habrastorage.org/webt/zi/nj/bj/zinjbjkov6298ccr_kuiux7z89k.jpeg"><br><br>  Backbone AWS est con√ßu sp√©cifiquement pour le cloud et optimis√© pour fonctionner avec lui.  Nous le construisons sur des canaux √† <b>100 Go / s</b> .  Nous les contr√¥lons enti√®rement, √† l'exception des r√©gions de Chine.  Le trafic n'est pas partag√© avec les charges des autres soci√©t√©s. <br><br><img src="https://habrastorage.org/webt/e-/ko/hs/e-kohsjs0wax_tmb0zd3l5xh5ys.png"><br><br>  Bien s√ªr, nous ne sommes pas le seul fournisseur de cloud avec un r√©seau f√©d√©rateur priv√©.  De plus en plus de grandes entreprises vont dans ce sens.  Cela est confirm√© par des chercheurs ind√©pendants, par exemple, de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Telegeography</a> . <br><br><img src="https://habrastorage.org/webt/my/en/ad/myenadxrsqlm4a58uyc6a_us7kq.jpeg"><br><br>  Le graphique montre que la part des fournisseurs de contenu et des fournisseurs de cloud augmente.  Pour cette raison, la proportion du trafic Internet provenant des fournisseurs de dorsale est en constante diminution. <br><br>  Je vais expliquer pourquoi cela se produit.  Auparavant, la plupart des services Web √©taient disponibles et consomm√©s directement depuis Internet.  D√©sormais, de plus en plus de serveurs sont situ√©s dans le cloud et sont disponibles via le <b>CDN</b> - <b>Content Distribution Network</b> .  Pour acc√©der √† la ressource, l'utilisateur ne passe par Internet qu'au <b>point de pr√©sence</b> CDN PoP le plus proche.  Le plus souvent, c'est quelque part √† proximit√©.  Il quitte ensuite l'Internet public et vole √† travers l'Atlantique via un r√©seau priv√©, par exemple, et se rend directement √† la ressource. <br><br>  Je me demande comment Internet va changer dans 10 ans si cette tendance se poursuit? <br><br><h3>  Canaux physiques </h3><br>  Les scientifiques n'ont pas encore compris comment augmenter la vitesse de la lumi√®re dans l'univers, mais ont fait de grands progr√®s dans les m√©thodes de transmission √† travers la fibre optique.  Nous utilisons actuellement 6912 c√¢bles en fibre.  Cela permet d'optimiser consid√©rablement le co√ªt de leur installation. <br><br>  Dans certaines r√©gions, nous devons utiliser des c√¢bles sp√©ciaux.  Par exemple, dans la r√©gion de Sydney, nous utilisons des c√¢bles avec un rev√™tement sp√©cial contre les termites. <br><br><img src="https://habrastorage.org/webt/qc/vn/wh/qcvnwhnrrbnil48u0qqgblljgna.jpeg"><br><br>  Personne n'est √† l'abri des probl√®mes et parfois nos canaux sont endommag√©s.  La photo de droite montre des c√¢bles optiques dans l'une des r√©gions am√©ricaines d√©chir√©es par les constructeurs.  √Ä la suite de l'accident, seulement 13 paquets de donn√©es ont √©t√© perdus, ce qui est surprenant.  Encore une fois - seulement 13!  Le syst√®me est litt√©ralement pass√© instantan√©ment aux canaux de sauvegarde - la balance fonctionne. <br><br>  Nous avons galop√© sur certains services et technologies cloud d'Amazon.  J'esp√®re que vous avez au moins une id√©e de l'ampleur des t√¢ches que nos ing√©nieurs doivent r√©soudre.  Personnellement, cela m'int√©resse beaucoup. <br><br><blockquote>  Ceci est la derni√®re partie de la trilogie de Vasily Pantyukhin sur l'appareil AWS.  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">premi√®re</a> partie d√©crit l'optimisation du serveur et la mise √† l'√©chelle de la base de donn√©es, et la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">seconde</a> d√©crit les fonctions sans serveur et Firecracker. <br><br>  √Ä <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HighLoad ++</a> en novembre, Vasily Pantyukhin partagera les nouveaux d√©tails des appareils Amazon.  Il <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">parlera</a> des causes des pannes et de la conception des syst√®mes distribu√©s chez Amazon.  Le 24 octobre, vous pouvez toujours <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©server un</a> billet √† un bon prix et payer plus tard.  Nous vous attendons √† HighLoad ++, venez parler! </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr471688/">https://habr.com/ru/post/fr471688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr471670/index.html">Essayer Jetpack Compose au combat?</a></li>
<li><a href="../fr471676/index.html">Escrocs au t√©l√©phone. La deuxi√®me action, dans laquelle je tombe en panne et me dirige vers le distributeur de billets le plus proche</a></li>
<li><a href="../fr471678/index.html">Services aux ours sur demande</a></li>
<li><a href="../fr471684/index.html">Pourquoi vous devez cr√©er des modules pour Nginx</a></li>
<li><a href="../fr471686/index.html">Comment AWS fabrique ses services r√©silients. Mise √† l'√©chelle du serveur et de la base de donn√©es</a></li>
<li><a href="../fr471700/index.html">Comment j'ai choisi une pile technologique avec une base pour l'avenir</a></li>
<li><a href="../fr471702/index.html">Applications Web cyber-am√©lior√©es</a></li>
<li><a href="../fr471704/index.html">Le livre ¬´Mitochondries √©go√Østes. Comment maintenir la sant√© et d√©placer la vieillesse "</a></li>
<li><a href="../fr471706/index.html">9 probl√®mes de r√©seau typiques qui peuvent √™tre d√©tect√©s en utilisant l'analyse NetFlow (en utilisant Flowmon comme exemple)</a></li>
<li><a href="../fr471708/index.html">Les points de stockage sont dangereux pour le d√©veloppement d'applications client-serveur</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>