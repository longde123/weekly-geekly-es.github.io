<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí™üèª üßîüèæ ü§∞ Google Actualit√©s et Leo Tolstoy: visualisation des incorporations de mots Word2Vec √† l'aide de t-SNE ‚ôèÔ∏è üõèÔ∏è ‚ñ´Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tout le monde per√ßoit uniquement les textes, que cette personne lise des nouvelles sur Internet ou des romans classiques de renomm√©e mondiale. Cela s'...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Google Actualit√©s et Leo Tolstoy: visualisation des incorporations de mots Word2Vec √† l'aide de t-SNE</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/449984/"><img src="https://habrastorage.org/webt/6c/ux/7m/6cux7mvmp3phb8d8efjwmqrb_yc.gif"><br>  Tout le monde per√ßoit uniquement les textes, que cette personne lise des nouvelles sur Internet ou des romans classiques de renomm√©e mondiale.  Cela s'applique √©galement √† une vari√©t√© d'algorithmes et de techniques d'apprentissage automatique, qui comprennent les textes de mani√®re plus math√©matique, √† savoir en utilisant un espace vectoriel de grande dimension. <br><br>  Cet article est consacr√© √† la visualisation des incorporations de mots Word2Vec √† haute dimension √† l'aide de t-SNE.  La visualisation peut √™tre utile pour comprendre comment Word2Vec fonctionne et comment interpr√©ter les relations entre les vecteurs captur√©s √† partir de vos textes avant de les utiliser dans des r√©seaux de neurones ou d'autres algorithmes d'apprentissage automatique.  Comme donn√©es de formation, nous utiliserons des articles de Google Actualit√©s et des ≈ìuvres litt√©raires classiques de Leo Tolsto√Ø, l'√©crivain russe qui est consid√©r√© comme l'un des plus grands auteurs de tous les temps. <br><br>  Nous passons par le bref aper√ßu de l'algorithme t-SNE, puis passons au calcul des incorporations de mots √† l'aide de Word2Vec, et enfin, passons √† la visualisation des vecteurs de mots avec t-SNE dans l'espace 2D et 3D.  Nous allons √©crire nos scripts en Python en utilisant Jupyter Notebook. <br><br><a name="habracut"></a><br><h1>  Int√©gration de voisin stochastique distribu√© en T </h1><br>  T-SNE est un algorithme d'apprentissage automatique pour la visualisation de donn√©es, qui est bas√© sur une technique de r√©duction de dimensionnalit√© non lin√©aire.  L'id√©e de base du t-SNE est de r√©duire l'espace dimensionnel en maintenant une distance relative par paire entre les points.  En d'autres termes, l'algorithme mappe des donn√©es multidimensionnelles √† deux dimensions ou plus, o√π les points qui √©taient initialement √©loign√©s l'un de l'autre sont √©galement situ√©s loin, et les points proches sont √©galement convertis en points proches.  On peut dire que t-SNE cherche une nouvelle repr√©sentation des donn√©es o√π les relations de voisinage sont pr√©serv√©es.  La description d√©taill√©e de l'ensemble de la logique t-SNE se trouve dans l'article d'origine [1]. <br><br><h1>  Le mod√®le Word2Vec </h1><br>  Pour commencer, nous devons obtenir des repr√©sentations vectorielles des mots.  √Ä cette fin, j'ai s√©lectionn√© Word2vec [2], c'est-√†-dire un mod√®le pr√©dictif efficace en termes de calcul pour l'apprentissage des incorporations de mots multidimensionnelles √† partir de donn√©es textuelles brutes.  Le concept cl√© de Word2Vec est de localiser des mots, qui partagent des contextes communs dans le corpus d'apprentissage, √† proximit√© imm√©diate de l'espace vectoriel par rapport aux autres. <br><br>  Comme donn√©es d'entr√©e pour la visualisation, nous utiliserons des articles de Google News et quelques romans de Leo Tolstoy.  Des vecteurs pr√©-form√©s form√©s sur une partie de l'ensemble de donn√©es Google Actualit√©s (environ 100 milliards de mots) ont √©t√© publi√©s par Google sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la page officielle</a> , nous allons donc l'utiliser. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim model = gensim.models.KeyedVectors.load_word2vec_format(<span class="hljs-string"><span class="hljs-string">'GoogleNews-vectors-negative300.bin'</span></span>, binary=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  En plus du mod√®le pr√©-form√©, nous formerons un autre mod√®le sur les romans de Tolsto√Ø en utilisant la biblioth√®que Gensim [3].  Word2Vec prend des phrases comme donn√©es d'entr√©e et produit des vecteurs de mots comme sortie.  Tout d'abord, il est n√©cessaire de t√©l√©charger le Punkt Phrase Tokenizer pr√©-form√©, qui divise un texte en une liste de phrases en tenant compte des mots d'abr√©viation, des collocations et des mots, qui indiquent probablement un d√©but ou une fin de phrases.  Par d√©faut, le package de donn√©es NLTK n'inclut pas de jeton Punkt pr√©-form√© pour le russe, nous utiliserons donc des mod√®les tiers de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">github.com/mhq/train_punkt</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecs <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">prepare_for_w2v</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename_from, filename_to, lang)</span></span></span><span class="hljs-function">:</span></span> raw_text = codecs.open(filename_from, <span class="hljs-string"><span class="hljs-string">"r"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'windows-1251'</span></span>).read() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename_to, <span class="hljs-string"><span class="hljs-string">'w'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sentence <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.sent_tokenize(raw_text, lang): print(preprocess_text(sentence.lower()), file=f)</code> </pre><br><br>  Lors de la formation Word2Vec, les hyperparam√®tres suivants ont √©t√© utilis√©s: <br><br><ul><li>  La dimensionnalit√© du vecteur caract√©ristique est de 200. </li><li>  La distance maximale entre les mots analys√©s dans une phrase est de 5. </li><li>  Ignore tous les mots dont la fr√©quence totale est inf√©rieure √† 5 par corpus. </li></ul><br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_word2vec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> data = gensim.models.word2vec.LineSentence(filename) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Word2Vec(data, size=<span class="hljs-number"><span class="hljs-number">200</span></span>, window=<span class="hljs-number"><span class="hljs-number">5</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">5</span></span>, workers=multiprocessing.cpu_count())</code> </pre><br><br><h1>  Visualisation des incorporations de mots √† l'aide de t-SNE </h1><br>  T-SNE est tr√®s utile dans le cas o√π il est n√©cessaire de visualiser la similitude entre des objets situ√©s dans un espace multidimensionnel.  Avec un grand ensemble de donn√©es, il devient de plus en plus difficile de cr√©er un trac√© t-SNE facile √† lire, il est donc courant de visualiser des groupes de mots les plus similaires. <br>  Choisissons quelques mots dans le vocabulaire du mod√®le Google News pr√©-form√© et pr√©parons des vecteurs de mots pour la visualisation. <br><br><pre> <code class="python hljs">keys = [<span class="hljs-string"><span class="hljs-string">'Paris'</span></span>, <span class="hljs-string"><span class="hljs-string">'Python'</span></span>, <span class="hljs-string"><span class="hljs-string">'Sunday'</span></span>, <span class="hljs-string"><span class="hljs-string">'Tolstoy'</span></span>, <span class="hljs-string"><span class="hljs-string">'Twitter'</span></span>, <span class="hljs-string"><span class="hljs-string">'bachelor'</span></span>, <span class="hljs-string"><span class="hljs-string">'delivery'</span></span>, <span class="hljs-string"><span class="hljs-string">'election'</span></span>, <span class="hljs-string"><span class="hljs-string">'expensive'</span></span>, <span class="hljs-string"><span class="hljs-string">'experience'</span></span>, <span class="hljs-string"><span class="hljs-string">'financial'</span></span>, <span class="hljs-string"><span class="hljs-string">'food'</span></span>, <span class="hljs-string"><span class="hljs-string">'iOS'</span></span>, <span class="hljs-string"><span class="hljs-string">'peace'</span></span>, <span class="hljs-string"><span class="hljs-string">'release'</span></span>, <span class="hljs-string"><span class="hljs-string">'war'</span></span>] embedding_clusters = [] word_clusters = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> keys: embeddings = [] words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> similar_word, _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> model.most_similar(word, topn=<span class="hljs-number"><span class="hljs-number">30</span></span>): words.append(similar_word) embeddings.append(model[similar_word]) embedding_clusters.append(embeddings) word_clusters.append(words)</code> </pre> <br><img src="https://habrastorage.org/webt/uc/1k/o6/uc1ko6efgx_d-wnbwolgdabifl8.gif"><br>  <i>Fig.</i>  <i>1. L'effet de diverses valeurs de perplexit√© sur la forme des groupes de mots.</i> <br><br>  Ensuite, nous passons √† la partie fascinante de cet article, la configuration de t-SNE.  Dans cette section, nous devons pr√™ter notre attention aux hyperparam√®tres suivants. <br><br><ul><li>  <i>Le nombre de composants</i> , c'est-√†-dire la dimension de l'espace de sortie. </li><li>  <i>La valeur de perplexit√©</i> , qui, dans le contexte du t-SNE, peut √™tre consid√©r√©e comme une mesure fluide du nombre effectif de voisins.  Il est li√© au nombre de voisins les plus proches qui sont employ√©s dans de nombreux autres apprenants (voir l'image ci-dessus).  Selon [1], il est recommand√© de s√©lectionner une valeur entre 5 et 50. </li><li>  <i>Type d'initialisation initiale</i> pour les incorporations. </li></ul><br><br><pre> <code class="python hljs">tsne_model_en_2d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embedding_clusters = np.array(embedding_clusters) n, m, k = embedding_clusters.shape embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, <span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br><br>  Il convient de mentionner que le t-SNE a une fonction objectif non convexe, qui est minimis√©e en utilisant une optimisation de descente de gradient avec une initiation al√©atoire, de sorte que diff√©rents essais produisent des r√©sultats l√©g√®rement diff√©rents. <br><br>  Ci-dessous, vous trouverez un script pour cr√©er un nuage de points 2D √† l'aide de Matplotlib, l'une des biblioth√®ques les plus populaires pour la visualisation de donn√©es en Python. <br><br><img src="https://habrastorage.org/webt/34/9y/7h/349y7hxuanvvttqfxkpb48j4n-q.png"><br>  <i>Fig.</i>  <i>2. Clusters de mots similaires de Google Actualit√©s (pr√©-complexit√© = 15).</i> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.manifold <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TSNE <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np % matplotlib inline <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_similar_words</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, embedding_clusters, word_clusters, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.7</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, len(labels))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label, embeddings, words, color <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(labels, embedding_clusters, word_clusters, colors): x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=color, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">8</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"f/.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_similar_words(keys, embeddings_en_2d, word_clusters)</code> </pre> <br><br>  Dans certains cas, il peut √™tre utile de tracer tous les vecteurs de mots √† la fois afin de voir l'image enti√®re.  Analysons maintenant Anna Karenina, un roman √©pique de passion, d'intrigue, de trag√©die et de r√©demption. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/Anna Karenina by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_ak = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>) words = [] embeddings = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_ak.wv.vocab): embeddings.append(model_ak.wv[word]) words.append(word) tsne_ak_2d = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(label, embeddings, words=[], a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=colors, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.3</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">10</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"hhh.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_2d(<span class="hljs-string"><span class="hljs-string">'Anna Karenina by Leo Tolstoy'</span></span>, embeddings_ak_2d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/j5/hn/82/j5hn8285nih2kwlop_badd2lzlk.png"><br><br><img src="https://habrastorage.org/webt/x6/jc/i7/x6jci7vka7-efczouqxpiqueisq.png"><br>  <i>Fig.</i>  <i>3. Visualisation du mod√®le Word2Vec form√© sur Anna Karenina.</i> <br><br>  L'image enti√®re peut √™tre encore plus informative si nous cartographions les plongements initiaux dans l'espace 3D.  En ce moment, jetons un ≈ìil √† Guerre et Paix, l'un des romans vitaux de la litt√©rature mondiale et l'une des plus grandes r√©alisations litt√©raires de Tolsto√Ø. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/War and Peace by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_wp = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>) words_wp = [] embeddings_wp = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_wp.wv.vocab): embeddings_wp.append(model_wp.wv[word]) words_wp.append(word) tsne_wp_3d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">30</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">3</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">12</span></span>) embeddings_wp_3d = tsne_wp_3d.fit_transform(embeddings_wp)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_3d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(title, label, embeddings, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure() ax = Axes3D(fig) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) plt.scatter(embeddings[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">2</span></span>], c=colors, alpha=a, label=label) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.title(title) plt.show() tsne_plot_3d(<span class="hljs-string"><span class="hljs-string">'Visualizing Embeddings using t-SNE'</span></span>, <span class="hljs-string"><span class="hljs-string">'War and Peace'</span></span>, embeddings_wp_3d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/ch/jm/os/chjmos082qn6ktw9afdeavbz6_a.png"><br>  <i>Fig.</i>  <i>4. Visualisation du mod√®le Word2Vec form√© sur la guerre et la paix.</i> <br><br><h1>  Les r√©sultats </h1><br>  Voil√† √† quoi ressemblent les textes issus de la prospective Word2Vec et t-SNE.  Nous avons trac√© un tableau assez informatif pour des mots similaires de Google Actualit√©s et deux diagrammes pour les romans de Tolsto√Ø.  De plus, encore une chose, les GIF!  Les GIF sont impressionnants, mais tracer des GIF est presque le m√™me que tracer des graphiques r√©guliers.  J'ai donc d√©cid√© de ne pas les mentionner dans l'article, mais vous pouvez trouver le code pour la g√©n√©ration d'animations dans les sources. <br><br>  Le code source est disponible sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Github</a> . <br><br>  L'article a √©t√© initialement publi√© dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Towards Data Science</a> . <br><br><h1>  Les r√©f√©rences </h1><br><ol><li>  L. Maate et G. Hinton, ¬´Visualizing data using t-SNE¬ª, Journal of Machine Learning Research, vol.  9, pp.  2579-2605, 2008. </li><li>  T. Mikolov, I. Sutskever, K. Chen, G. Corrado et J. Dean, ¬´Repr√©sentations distribu√©es des mots et des phrases et leur compositionnalit√©¬ª, Advances in Neural Information Processing Systems, pp.  3111-3119, 2013. </li><li>  R. Rehurek et P. Sojka, ¬´Software Framework for Topic Modeling with Large Corpora¬ª, Actes de l'atelier LREC 2010 sur les nouveaux d√©fis pour les cadres PNL, 2010. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr449984/">https://habr.com/ru/post/fr449984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr449970/index.html">Publier sous le drapeau noir, ou comme je n'ai pas mis votre cours vid√©o sur le tracker</a></li>
<li><a href="../fr449972/index.html">Comment injecter rapidement des piscines en amont?</a></li>
<li><a href="../fr449974/index.html">Netramesh - solution de maillage de service l√©ger</a></li>
<li><a href="../fr449976/index.html">Conteneurs associatifs multithread en C ++. Rapport Yandex</a></li>
<li><a href="../fr449978/index.html">Igor Antarov de Moscou Tesla Club se d√©bat avec 20 mythes sur Tesla et les voitures √©lectriques</a></li>
<li><a href="../fr449986/index.html">Blockchain: que devons-nous construire un bo√Ætier?</a></li>
<li><a href="../fr449990/index.html">Comment se faire des amis latex, formules et Habr?</a></li>
<li><a href="../fr449992/index.html">Pr√©sentation du mod√®le de pilote simple (SDM) NodeMCU: interface utilisateur dynamique</a></li>
<li><a href="../fr449994/index.html">Les huit r√®gles d'or de Schneiderman vous aideront √† cr√©er une meilleure interface</a></li>
<li><a href="../fr449996/index.html">Comprendre l'algorithme FFT</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>