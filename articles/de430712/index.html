<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕰️ 🧗🏽 🏸 NeurIPS: Wie man die beste ML-Konferenz erobert 🤼 🧑🏻 👨‍👦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="NeurIPS - eine Konferenz, die derzeit als die Top-Veranstaltung in der Welt des maschinellen Lernens gilt. Heute erzähle ich Ihnen von meinen Erfahrun...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>NeurIPS: Wie man die beste ML-Konferenz erobert</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/430712/"><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NeurIPS</a> - eine Konferenz, die derzeit als die Top-Veranstaltung in der Welt des maschinellen Lernens gilt.  Heute erzähle ich Ihnen von meinen Erfahrungen bei der Teilnahme an NeurIPS-Wettbewerben: Wie kann man sich mit den besten Akademikern der Welt messen, einen Preis gewinnen und einen Artikel veröffentlichen? </p><br><img src="https://habrastorage.org/webt/hb/kq/-v/hbkq-vnd_xgxhvcixlo-u8b_pmk.jpeg"><a name="habracut"></a><br><hr><br><h1 id="v-chem-sut-konferencii">  Was ist das Wesentliche der Konferenz? </h1><br><p>  NeurIPS unterstützt die Einführung von Methoden des maschinellen Lernens in verschiedenen wissenschaftlichen Disziplinen.  Jährlich werden etwa 10 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Titel veröffentlicht</a> , um drängende Probleme der akademischen Welt zu lösen.  Nach den Ergebnissen des Wettbewerbs sprechen die Gewinner auf der Konferenz mit Berichten, neuen Entwicklungen und Algorithmen.  Vor allem beschäftige ich mich leidenschaftlich mit verstärktem Lernen (Reinforcement Learning oder RL). Deshalb nehme ich bereits seit zwei Jahren an RL-Wettbewerben teil, die NeurIPS gewidmet sind. </p><br><h1 id="pochemu-neurips">  Warum NeurIPS? </h1><br><img src="https://habrastorage.org/webt/ei/c2/us/eic2usvfs-brxmsjczvkvygpfwq.png"><br><br>  NeurIPS konzentriert sich hauptsächlich auf die Wissenschaft, nicht auf Geld.  Durch die Teilnahme an Wettbewerben tun Sie etwas wirklich Wichtiges und beschäftigen sich mit dringenden Problemen. <br><p>  Zweitens ist diese Konferenz eine globale Veranstaltung, bei der sich Wissenschaftler aus verschiedenen Ländern an einem Ort versammeln, mit denen Sie jeweils sprechen können. </p><br><p>  Darüber hinaus ist die gesamte Konferenz mit den neuesten wissenschaftlichen Erkenntnissen und den neuesten Ergebnissen gefüllt. Für Menschen aus dem Bereich der Datenwissenschaft ist es äußerst wichtig, diese zu kennen und zu überwachen. </p><br><h1 id="kak-nachat">  Wie fange ich an? </h1><br><p>  Die Teilnahme an solchen Wettbewerben ist ganz einfach.  Wenn Sie DL so gut verstehen, dass Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ResNet trainieren können</a> - das ist genug: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Melden</a> Sie sich an und gehen Sie.  Es gibt immer eine öffentliche Rangliste, auf der Sie Ihr Niveau im Vergleich zu anderen Teilnehmern nüchtern beurteilen können.  Und wenn etwas nicht klar ist - es gibt immer Kanäle in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Slack</a> / Discord / Gitter / etc, um alle aufkommenden Probleme zu diskutieren.  Wenn das Thema wirklich „Ihnen“ gehört, hindert Sie nichts daran, das geschätzte Ergebnis zu erhalten - bei allen Wettbewerben, an denen ich teilgenommen habe, wurden alle Ansätze und Lösungen direkt im Verlauf des Wettbewerbs untersucht und umgesetzt. </p><br><h1 id="neurips-na-primere-konkretnogo-keysa-learning-to-run">  NeurIPS-Fallstudie: Laufen lernen </h1><br><img src="https://habrastorage.org/webt/hu/ws/d5/huwsd5weqocxiuqv3hugygmfqea.jpeg"><br><br><h3 id="problematika">  Problem </h3><br><p>  Der Gang einer Person ist das Ergebnis des Zusammenspiels von Muskeln, Knochen, Sehorganen und dem Innenohr.  Im Falle einer Störung des Zentralnervensystems können bestimmte motorische Störungen auftreten, einschließlich Gangstörungen - Abasie. <br>  Forscher des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stanford Laboratory of Neuromuscular Biomechanics</a> beschlossen, maschinelles Lernen mit dem Behandlungsproblem zu verbinden, um ihre Theorien an einem virtuellen Modell des Skeletts und nicht an lebenden Menschen experimentieren und testen zu können. </p><br><h3 id="postanovka-zadachi">  Erklärung des Problems </h3><br><p>  Die Teilnehmer erhielten ein virtuelles menschliches Skelett (im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenSim-</a> Simulator), das anstelle eines Beines eine Prothese hatte.  Die Aufgabe bestand darin, dem Skelett beizubringen, sich mit einer bestimmten Geschwindigkeit in eine bestimmte Richtung zu bewegen.  Während der Simulation können sich Richtung und Geschwindigkeit ändern. </p><br><img src="https://habrastorage.org/webt/od/vj/np/odvjnpxb7xogj5h_5ll85iokhp0.jpeg"><br><br>  Um ein virtuelles Skelettkontrollmodell zu erhalten, wurde vorgeschlagen, das Verstärkungslernen zu verwenden.  Der Simulator gab uns einen Zustand des Skeletts S (ein Vektor von ~ 400 Zahlen).  Es musste vorhergesagt werden, welche Aktion A ausgeführt werden muss (die Aktivierungskräfte der Beinmuskulatur sind ein Vektor von 19 Zahlen).  Im Verlauf der Simulation erhielt das Skelett eine R-Auszeichnung - als eine Art Konstante abzüglich einer Strafe für die Abweichung von einer bestimmten Geschwindigkeit und Richtung. <br><div class="spoiler">  <b class="spoiler_title">Über Verstärkungstraining</b> <div class="spoiler_text"><p>  Reinforcement Learning (RL) ist ein Bereich, der sich mit Entscheidungstheorie und der Suche nach optimalen Verhaltensrichtlinien befasst. </p><br><p>  Erinnern Sie sich, wie sie unterrichten <del>  Katze </del>  Hund neue Tricks.  Wiederholen Sie eine Aktion, geben Sie einen Leckerbissen für die Ausführung eines Tricks und geben Sie nicht für die Nichterfüllung.  Der Hund sollte dies alles verstehen und eine Verhaltensstrategie finden („Richtlinie“ oder „Richtlinie“ in Bezug auf RL), die die Anzahl der erhaltenen Süßigkeiten maximiert. </p><br><p>  Formal haben wir einen Agenten (Hund), der in der Geschichte der Interaktionen mit der Umwelt (Person) geschult ist.  Gleichzeitig bietet ihm die Umgebung, die die Aktionen des Agenten bewertet, eine Belohnung (lecker) - je besser sich der Agent verhält, desto höher ist die Belohnung.  Dementsprechend besteht die Aufgabe des Agenten darin, eine Richtlinie zu finden, die die Belohnung für die gesamte Zeit der Interaktion mit der Umgebung gut maximiert. </p><br><p>  Weiterentwicklung dieses Themas, regelbasierte Lösungen - Software 1.0, wenn alle Regeln vom Entwickler festgelegt wurden, überwachtes Lernen - genau Software 2.0, wenn das System anhand der verfügbaren Beispiele selbst lernt und Datenabhängigkeiten findet, ist das verstärkte Lernen ein Schritt weiter, wenn das System selbst lernt zu recherchieren, zu experimentieren und die erforderlichen Abhängigkeiten in seinen Entscheidungen zu finden.  Je weiter wir gehen, desto besser versuchen wir zu wiederholen, wie eine Person lernt. </p></div></div><br><h3 id="osobennosti-zadachi">  Aufgabenfunktionen </h3><br><p>  Die Aufgabe sieht aus wie ein typischer Vertreter des verstärkten Lernens für Aufgaben mit kontinuierlichem Aktionsraum (RL für kontinuierlichen Aktionsraum).  Es unterscheidet sich von gewöhnlichem RL darin, dass anstelle einer bestimmten Aktion (Drücken der Joystick-Taste) diese Aktion zur genauen Vorhersage erforderlich ist (und es gibt unendlich viele Möglichkeiten). </p><br><p>  Der grundlegende Lösungsansatz ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Deterministic Policy Gradient</a> ) wurde bereits 2015 erfunden. Nach den Standards von DL entwickelt sich die Region seit langem aktiv in Anwendungen für Robotik und reale RL-Anwendungen.  Es gibt etwas zu verbessern: robuste Ansätze (um einen echten Roboter nicht zu beschädigen), Probeneffizienz (um monatelang keine Daten von echten Robotern zu sammeln) und andere RL-Probleme (Kompromiss zwischen Exploration und Ausbeutung usw.).  In diesem Wettbewerb gaben sie uns keinen echten Roboter - nur eine Simulation, aber der Simulator selbst war 2000-mal langsamer als die Open Source-Gegenstücke (bei denen jeder seine RL-Algorithmen überprüft) und brachte daher das Problem der Probeneffizienz auf ein neues Niveau. </p><br><h3 id="etapy-sorevnovaniya">  Wettbewerbsphasen </h3><br><p>  Der Wettbewerb selbst fand in drei Phasen statt, in denen sich Aufgabe und Bedingungen etwas änderten. </p><br><ul><li>  Stufe 1: Das Skelett lernte, mit einer Geschwindigkeit von 3 Metern pro Sekunde geradeaus zu laufen.  Die Aufgabe wurde als erledigt angesehen, wenn der Agent 300 Schritte durchlaufen hatte. </li><li>  Stufe 2: Geschwindigkeit und Richtung werden regelmäßig geändert.  Die Länge der Entfernung wurde auf 1000 Schritte erhöht. </li><li>  Stufe 3: Die endgültige Lösung musste in ein Docker-Image verpackt und zur Überprüfung gesendet werden.  Insgesamt konnten 10 Pakete hergestellt werden. </li></ul><br><p>  Die Hauptqualitätsmetrik wurde als Gesamtbelohnung für die Simulation angesehen, die zeigte, wie gut das Skelett über die gesamte Distanz an einer bestimmten Richtung und Geschwindigkeit festhielt. </p><br><p>  Während der 1. und 2. Phase wurde der Fortschritt jedes Teilnehmers in der Rangliste angezeigt.  Die endgültige Lösung musste als Docker-Image gesendet werden.  Es sah Beschränkungen der Arbeitszeiten und Ressourcen vor. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: öffentliche Rangliste und RL</b> <div class="spoiler_text"><p>  Aufgrund der Verfügbarkeit der Rangliste zeigt niemand sein bestes Modell, um in der Endrunde „etwas mehr als sonst“ zu geben und Rivalen zu überraschen. </p></div></div><br><h6 id="pochemu-tak-vazhny-docker-obrazy">  Warum Docker-Bilder so wichtig sind </h6><br><p>  Letztes Jahr ereignete sich ein kleiner Vorfall bei der Bewertung von Entscheidungen in der ersten Runde.  Zu diesem Zeitpunkt durchlief die Überprüfung eine http-Interaktion mit der Plattform, und es wurde ein Gesicht der Testbedingungen gefunden.  Man konnte herausfinden, in welchen besonderen Situationen der Agent bewertet wurde, und ihn nur unter diesen Bedingungen neu trainieren.  Was natürlich das eigentliche Problem nicht löste.  Aus diesem Grund haben sie beschlossen, das System der Einreichungen auf Docker-Images zu übertragen und auf den Remote-Servern der Organisatoren zu starten.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dbrain</a> verwendet dasselbe System, um das Ergebnis von Wettbewerben aus denselben Gründen genau zu berechnen. </p><br><h1 id="klyuchevye-momenty">  Wichtige Punkte </h1><br><h3 id="komanda">  Das Team </h3><br><img src="https://habrastorage.org/webt/ty/ur/gp/tyurgpqbzb2zl2wimtzri0mnpwk.jpeg"><br><br>  Das erste, was für den Erfolg des gesamten Unternehmens wichtig ist, ist das Team.  Egal wie gut Sie sind (und wie stark Ihre Pfoten sind) - die Teilnahme am Team erhöht die Erfolgschancen erheblich.  Der Grund ist einfach: eine Vielzahl von Meinungen und Ansätzen, die Überprüfung von Hypothesen, die Fähigkeit, die Arbeit zu parallelisieren und mehr Experimente durchzuführen.  All dies ist äußerst wichtig, wenn Sie neue Probleme lösen, mit denen Sie konfrontiert sind. <br><p>  Idealerweise sollten Ihre Kenntnisse und Fähigkeiten auf dem gleichen Niveau sein und sich gegenseitig ergänzen.  So habe ich beispielsweise dieses Jahr unser Team auf PyTorch eingestellt und erste Ideen zur Implementierung eines verteilten Agententrainingsystems erhalten. </p><br><p>  Wie finde ich ein Team?  Erstens können Sie sich den Reihen der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ods anschließen</a> und dort nach Gleichgesinnten suchen.  Zweitens gibt es für RL-Stipendiaten einen separaten Chatraum in einem Telegramm - den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RL-Club</a> .  Drittens können Sie einen wunderbaren Kurs von ShAD - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Practical RL</a> belegen, nach dem Sie sicherlich ein paar Bekanntschaften machen werden. </p><br><p>  Es lohnt sich jedoch, sich an die Politik der "Unterwerfung - oder war es nicht" zu erinnern.  Wenn Sie sich vereinen möchten, treffen Sie zuerst Ihre Entscheidung, senden Sie sie ab, erscheinen Sie in der Rangliste und zeigen Sie Ihr Level.  Wie die Praxis zeigt, sind solche Teams viel ausgeglichener. </p><br><h3 id="motivaciya">  Motivation </h3><br><p>  Wie ich bereits schrieb, wird Sie nichts aufhalten, wenn das Thema „Ihr“ ist.  Das bedeutet, dass die Region Sie nicht nur mag, sondern inspiriert - Sie verbrennen es, Sie wollen der Beste darin werden. <br>  Ich habe RL vor 4 Jahren kennengelernt - während der Passage des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Berkeley 188x - Einführung in die KI</a> - und kann immer noch nicht aufhören, mich über die Fortschritte in diesem Bereich zu wundern. </p><br><h3 id="sistematichnost">  Systematisch </h3><br><p>  Drittens, aber genauso wichtig - Sie müssen in der Lage sein, das zu tun, was Sie versprochen haben, jeden Tag in den Wettbewerb zu investieren und ihn einfach zu lösen.  Täglich.  Kein angeborenes Talent kann mit der Fähigkeit verglichen werden, etwas zu tun, auch nicht ein bisschen, aber jeden Tag.  Dafür ist Motivation gefragt.  Um erfolgreich zu sein, empfehle ich, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepWork</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AMA ternaus zu lesen</a> . </p><br><h3 id="time-management">  Zeitmanagement </h3><br><p>  Eine weitere äußerst wichtige Fähigkeit ist die Fähigkeit, die eigene Kraft zu verteilen und die Freizeit richtig zu nutzen.  Die Kombination von Vollzeitarbeit und Teilnahme an Wettbewerben ist keine triviale Aufgabe.  Das Wichtigste unter diesen Bedingungen ist, nicht auszubrennen und der gesamten Last standzuhalten.  Dazu müssen Sie Ihre Zeit richtig verwalten, Ihre Kraft nüchtern einschätzen und nicht vergessen, sich rechtzeitig zu entspannen. </p><br><h3 id="overwork">  Überarbeitung </h3><br><p>  In der letzten Phase des Wettbewerbs tritt normalerweise eine Situation auf, in der Sie buchstäblich in einer Woche nicht nur viel, sondern SEHR viel tun müssen.  Um das beste Ergebnis zu erzielen, müssen Sie sich zwingen können, sich zu setzen und den letzten Schub zum begehrten Preis zu machen. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: Frist für Frist</b> <div class="spoiler_text"><p>  Aus welchen Gründen kann es im Allgemeinen erforderlich sein, zum Nutzen des Wettbewerbs zu recyceln?  Die Antwort ist ganz einfach - Fristübertragung.  Bei solchen Wettbewerben können die Organisatoren oft nicht alles vorhersagen, weshalb es am einfachsten ist, den Teilnehmern mehr Zeit zu geben.  In diesem Jahr wurde der Wettbewerb dreimal verlängert: zuerst um einen Monat, dann um eine Woche und im allerletzten Moment (24 Stunden vor Ablauf der Frist) - um weitere zwei Tage.  Und wenn Sie während der ersten beiden Transfers nur die Verlängerung richtig organisieren mussten, mussten Sie in den letzten zwei Tagen nur pflügen. </p></div></div><br><h3 id="theory">  Theorie </h3><br><img src="https://habrastorage.org/webt/gf/rg/9q/gfrg9ql1ukvjmlbglwcizlfcpto.png"><br><p>  Vergessen Sie unter anderem nicht die Theorie - sich bewusst zu sein, was auf dem Gebiet passiert, und die relevanten zu notieren.  Um beispielsweise im letzten Jahr eine Lösung zu finden, hat sich unser Team von den folgenden Artikeln verabschiedet: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kontinuierliche Kontrolle mit tiefem Verstärkungslernen</a> ist ein grundlegender Artikel über tiefes Verstärkungslernen für Aufgaben mit kontinuierlichem Aktionsraum. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Parameter Space Noise for Exploration</a> - eine Studie zum Hinzufügen von Rauschen zu Agentengewichten für eine bessere Untersuchung der Umgebung.  Aus Erfahrung - eine der besten Erkundungstechniken in RL. </li></ul><br><p>  In diesem Jahr wurden einige weitere hinzugefügt: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine Verteilungsperspektive zum Reinforcement Learning</a> - Ein neuer Blick auf die Vorhersagen einer möglichen Belohnung.  Anstatt nur den Durchschnitt vorherzusagen, wird die Verteilung zukünftiger Belohnungen berechnet. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Das Lernen zur Verstärkung der Verteilung mit quantitativer Regression</a> ist eine Fortsetzung der vorherigen Arbeit, jedoch mit der „Quantisierung“ der Verteilung. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Distributed Prioritized Experience Replay</a> - Arbeiten Sie aus der Richtung des vertieften Lernens in großem Maßstab.  Informationen zur ordnungsgemäßen Organisation der Experimentarchitektur, um die Nutzung der verfügbaren Ressourcen zu maximieren und die Geschwindigkeit der Schulungsagenten zu erhöhen. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verteilte verteilungsdeterministische Richtlinienverläufe</a> - eine Kombination der drei vorherigen Artikel für Aufgaben mit einem kontinuierlichen Aktionsraum. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adressierungsfunktionsnäherungsfehler bei akteurkritischen Methoden</a> - hervorragende Arbeit zur Erhöhung der Robustheit von RL-Agenten.  Ich empfehle es zu lesen. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dateneffizientes hierarchisches Verstärkungslernen</a> ist eine Weiterentwicklung eines früheren Artikels im Bereich des hierarchischen Verstärkungslernens (HRL). </li></ul><br><div class="spoiler">  <b class="spoiler_title">Zusätzliche Lektüre</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Soft Actor-Critic: Off-Policy-Lernen mit maximaler Entropie und tiefem Verstärkungslernen mit einem stochastischen Akteur</a> - Die Autoren schlugen eine Methode zum Trainieren stochastischer Richtlinien mit nicht-politischem Verstärkungslernen vor.  Dank dieses Artikels wurde es möglich, nicht deterministische Politiker auch bei Aufgaben mit kontinuierlichem Handlungsspielraum auszubilden. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Latent Space Policies für hierarchisches Reinforcement Learning</a> ist eine Fortsetzung eines früheren HRL-Artikels mit mehrstufigen stochastischen Richtlinien. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vielfalt ist alles, was Sie brauchen: Lernfähigkeiten ohne Belohnungsfunktion</a> - Dieser Artikel enthält einen Ansatz zum Erlernen vieler zufälliger stochastischer Richtlinien auf niedriger Ebene ohne Belohnung durch die Umwelt.  Anschließend, wenn wir die Belohnungsfunktion festgelegt haben, kann die am meisten mit der Auszeichnung korrelierte verwendet werden, um hochrangige Politik darüber zu unterrichten. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Reinforcement Learning and Control als probabilistische Folgerung: Tutorial und Review</a> - Ein Überblick über alle Arten von Lernmethoden zur Verstärkung der maximalen Entropie von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sergey Levine</a> . </li></ul><br><p>  Ich empfehle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI auch eine Auswahl von Artikeln zum</a> Thema Verstärkungslernen und dessen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Version für Mendeley</a> .  Und wenn Sie sich für das Thema Verstärkungstraining interessieren, treten Sie dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RL-Club</a> und den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RL-Papieren bei</a> . </p></div></div><br><h3 id="practice">  Übe </h3><br><img src="https://habrastorage.org/webt/xp/g7/it/xpg7itebqdpi3cwex33uzrzkidg.jpeg"><br><br>  Die Theorie allein zu kennen, reicht nicht aus - es ist wichtig, all diese Ansätze in die Praxis umsetzen und das richtige Validierungssystem für die Bewertung von Entscheidungen etablieren zu können.  Zum Beispiel haben wir in diesem Jahr erfahren, dass unser Agent nur zwei Tage vor dem Ende des Wettbewerbs mit einigen regionalen Fällen schlecht zurechtkommt.  Aus diesem Grund hatten wir keine Zeit, unser Modell vollständig zu reparieren, und konnten nicht buchstäblich ein paar Punkte auf den begehrten zweiten Platz bringen.  Wenn wir dies sogar in einer Woche finden würden, könnte das Ergebnis besser sein. <br><div class="spoiler">  <b class="spoiler_title">Coolstory: Folge III</b> <div class="spoiler_text"><p>  Die durchschnittliche Auszeichnung für 10 Testepisoden diente als endgültige Bewertung der Lösung. </p><br><img src="https://habrastorage.org/webt/jq/bj/yc/jqbjyctkjettuu2bqd19xcahssk.png"><br><p>  Die Grafik zeigt die Ergebnisse des Testens unseres Agenten: 9 von 10 Episoden, unser Skelett lief gut (Durchschnitt - 9955,66), aber eine Episode ... Episode 3 wurde ihm nicht gegeben (Belohnung 9870).  Es war dieser Fehler, der zum Abfall der Endgeschwindigkeit auf 9947 (-8 Punkte) führte. </p></div></div><br><h3 id="udacha">  Viel Glück </h3><br><p>  Und zum Schluss - vergessen Sie nicht das banale Glück.  Denken Sie nicht, dass dies ein kontroverser Punkt ist.  Im Gegenteil, ein wenig Glück trägt wesentlich zur ständigen Arbeit an sich selbst bei: Selbst wenn die Wahrscheinlichkeit des Glücks nur 10% beträgt, wird eine Person, die 100 Mal versucht hat, am Wettbewerb teilzunehmen, viel mehr Erfolg haben als jemand, der nur 1 Mal versucht hat und die Idee aufgegeben hat. </p><br><h1 id="tuda-i-obratno-reshenie-proshlogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2017-learning-to-runwinners">  Rundreise: letztjährige Entscheidung - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dritter Platz</a> </h1><br><img src="https://habrastorage.org/webt/mq/lx/i_/mqlxi_alc8pt0acnzwoi8twb8oo.jpeg"><br><br>  Letztes Jahr hat unser Team - Mikhail Pavlov und ich - zum ersten Mal an den NeurIPS-Wettbewerben teilgenommen. Die Hauptmotivation bestand darin, einfach am ersten NeurIPS-Wettbewerb zum Thema „Lernen zur Stärkung“ teilzunehmen.  Dann habe ich gerade den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">praktischen RL-</a> Kurs im SHAD beendet und wollte die erworbenen Fähigkeiten testen.  Infolgedessen haben wir einen ehrenvollen dritten Platz belegt und nur gegen Nnaisene (Schmidhuber) und das Universitäts-Team aus China verloren.  Zu dieser Zeit war unsere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lösung</a> „ziemlich einfach“ und basierte auf verteiltem DDPG mit Parameterrauschen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Veröffentlichung</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Präsentation auf ml</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Trainings</a> ). <br><h1 id="reshenie-etogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2018-ai-for-prosthetics-challengeleaderboards">  Die diesjährige Entscheidung ist der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dritte Platz</a> </h1><br><img src="https://habrastorage.org/webt/gf/qq/to/gfqqtoneh51dn47m3f7oicyqixk.jpeg"><br><p>  In diesem Jahr gab es einige Änderungen.  Erstens gab es keinen Wunsch, nur an diesem Wettbewerb teilzunehmen, ich wollte ihn gewinnen.  Zweitens hat sich auch die Zusammensetzung des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teams</a> geändert: Alexey Grinchuk, Anton Pechenko und ich.  Nehmen und gewinnen - hat nicht funktioniert, aber wir haben wieder den 3. Platz belegt. <br>  Unsere Lösung wird offiziell auf der NeurIPS vorgestellt, und jetzt beschränken wir uns auf eine kleine Anzahl von Details.  Basierend auf der Entscheidung des letzten Jahres und dem Erfolg des Off-Policy-Enforcement-Lernens dieses Jahres (Artikel oben) haben wir eine Reihe unserer eigenen Entwicklungen hinzugefügt, über die wir bei NeurIPS sprechen werden, und den Distributed Quantile Ensemble Critic erhalten, mit dem wir den dritten Platz belegten. </p><br><p>  Alle unsere Best Practices - ein verteiltes Lernsystem, Algorithmen usw. - werden nach NeurIPS in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Catalyst.RL</a> veröffentlicht und verfügbar sein. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: große Jungs - große Waffen</b> <div class="spoiler_text"><p>  Unser Team belegte während des gesamten Wettbewerbs zuversichtlich den 1. Platz.  Die großen Jungs hatten jedoch andere Pläne - 2 große Spieler nahmen 2 Wochen vor Ende des Wettbewerbs am Wettbewerb teil: FireWork (Baidu) und nnaisense (Schmidhuber).  Und wenn mit dem chinesischen Google nichts getan werden konnte, dann konnten wir mit dem Schmidhuber-Team eine ganze Weile ehrlich um den zweiten Platz kämpfen und nur mit einem minimalen Vorsprung verlieren.  Es scheint mir ziemlich gut für Liebhaber. </p></div></div><br><h1 id="zachem-eto-vse">  Warum ist das alles? </h1><br><ul><li>  Kommunikation.  Zu der Konferenz kommen Spitzenforscher, mit denen Sie live chatten können und die keine E-Mail-Korrespondenz führen. </li><li>  Veröffentlichung  Wenn die Lösung den Preis erhält, wird das Team zur Konferenz (oder möglicherweise zu mehreren) eingeladen, um seine Entscheidung zu präsentieren und den Artikel zu veröffentlichen. </li><li>  Stellenangebot und Promotion.  Die Veröffentlichung und ein Preis in einer solchen Konferenz erhöhen Ihre Chancen auf eine Position in führenden Unternehmen wie OpenAI, DeepMind, Google, Facebook und Microsoft erheblich. </li><li>  Wert der realen Welt.  NeurIPS wird durchgeführt, um drängende Probleme der akademischen und realen Welt zu lösen.  Sie können sicher sein, dass die Ergebnisse nicht auf den Tisch kommen, sondern wirklich gefragt sind und zur Verbesserung der Welt beitragen. </li><li>  Fahren  Solche Wettbewerbe zu lösen ... einfach interessant.  In einem Wettbewerb können Sie viele neue Ideen einbringen, verschiedene Ansätze testen - nur um die Besten zu sein.  Und seien wir ehrlich, wann sonst können Sie Skelette fahren, Spiele spielen und das alles mit einem ernsthaften Blick und der Wissenschaft zuliebe? </li></ul><br><div class="spoiler">  <b class="spoiler_title">Coolstory: Visum und RL</b> <div class="spoiler_text"><p>  Ich empfehle dringend, dem Amerikaner nicht zu erklären, dass Sie zur Konferenz gehen, während Sie virtuelle Skelette für die Ausführung in Simulationen trainieren.  Gehen Sie einfach mit einem Vortrag zur Konferenz. </p></div></div><br><h1 id="itogi">  Zusammenfassung </h1><br><p>  Die Teilnahme an NeurIPS ist eine Erfahrung, die schwer zu überschätzen ist.  Haben Sie keine Angst vor hochkarätigen Schlagzeilen - Sie müssen sich nur zusammenreißen und sich entscheiden. </p><br><p>  Und gehen Sie zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Catalyst.RL</a> , was dann? </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de430712/">https://habr.com/ru/post/de430712/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de430702/index.html">Sehr seltsames Training</a></li>
<li><a href="../de430704/index.html">Wie Technologien für künstliche Intelligenz das Wachstum von Aviasales fördern: Sieben Beispiele</a></li>
<li><a href="../de430706/index.html">Neue Evolutionstheorie</a></li>
<li><a href="../de430708/index.html">Tic Tac Toe "Ohne Grenzen"</a></li>
<li><a href="../de430710/index.html">Was tun, wenn der Schwarze Freitag morgen ist und Ihre Server nicht bereit sind?</a></li>
<li><a href="../de430714/index.html">VMware kauft Heptio - was bedeutet das für Kubernetes?</a></li>
<li><a href="../de430718/index.html">Für welche Objekte lohnt sich die Cloud-Videoüberwachung?</a></li>
<li><a href="../de430720/index.html">Intel RealSense D435i: kleines Update und kurzer historischer Exkurs</a></li>
<li><a href="../de430722/index.html">PHP-Leistung: Planen, Profilieren, Optimieren</a></li>
<li><a href="../de430724/index.html">DEFCON 21. DNS-Konferenzen können gesundheitsschädlich sein. Teil 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>