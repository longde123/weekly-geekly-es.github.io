<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üï∞Ô∏è üßóüèΩ üè∏ NeurIPS: Wie man die beste ML-Konferenz erobert ü§º üßëüèª üë®‚Äçüë¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="NeurIPS - eine Konferenz, die derzeit als die Top-Veranstaltung in der Welt des maschinellen Lernens gilt. Heute erz√§hle ich Ihnen von meinen Erfahrun...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>NeurIPS: Wie man die beste ML-Konferenz erobert</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/430712/"><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NeurIPS</a> - eine Konferenz, die derzeit als die Top-Veranstaltung in der Welt des maschinellen Lernens gilt.  Heute erz√§hle ich Ihnen von meinen Erfahrungen bei der Teilnahme an NeurIPS-Wettbewerben: Wie kann man sich mit den besten Akademikern der Welt messen, einen Preis gewinnen und einen Artikel ver√∂ffentlichen? </p><br><img src="https://habrastorage.org/webt/hb/kq/-v/hbkq-vnd_xgxhvcixlo-u8b_pmk.jpeg"><a name="habracut"></a><br><hr><br><h1 id="v-chem-sut-konferencii">  Was ist das Wesentliche der Konferenz? </h1><br><p>  NeurIPS unterst√ºtzt die Einf√ºhrung von Methoden des maschinellen Lernens in verschiedenen wissenschaftlichen Disziplinen.  J√§hrlich werden etwa 10 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Titel ver√∂ffentlicht</a> , um dr√§ngende Probleme der akademischen Welt zu l√∂sen.  Nach den Ergebnissen des Wettbewerbs sprechen die Gewinner auf der Konferenz mit Berichten, neuen Entwicklungen und Algorithmen.  Vor allem besch√§ftige ich mich leidenschaftlich mit verst√§rktem Lernen (Reinforcement Learning oder RL). Deshalb nehme ich bereits seit zwei Jahren an RL-Wettbewerben teil, die NeurIPS gewidmet sind. </p><br><h1 id="pochemu-neurips">  Warum NeurIPS? </h1><br><img src="https://habrastorage.org/webt/ei/c2/us/eic2usvfs-brxmsjczvkvygpfwq.png"><br><br>  NeurIPS konzentriert sich haupts√§chlich auf die Wissenschaft, nicht auf Geld.  Durch die Teilnahme an Wettbewerben tun Sie etwas wirklich Wichtiges und besch√§ftigen sich mit dringenden Problemen. <br><p>  Zweitens ist diese Konferenz eine globale Veranstaltung, bei der sich Wissenschaftler aus verschiedenen L√§ndern an einem Ort versammeln, mit denen Sie jeweils sprechen k√∂nnen. </p><br><p>  Dar√ºber hinaus ist die gesamte Konferenz mit den neuesten wissenschaftlichen Erkenntnissen und den neuesten Ergebnissen gef√ºllt. F√ºr Menschen aus dem Bereich der Datenwissenschaft ist es √§u√üerst wichtig, diese zu kennen und zu √ºberwachen. </p><br><h1 id="kak-nachat">  Wie fange ich an? </h1><br><p>  Die Teilnahme an solchen Wettbewerben ist ganz einfach.  Wenn Sie DL so gut verstehen, dass Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ResNet trainieren k√∂nnen</a> - das ist genug: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Melden</a> Sie sich an und gehen Sie.  Es gibt immer eine √∂ffentliche Rangliste, auf der Sie Ihr Niveau im Vergleich zu anderen Teilnehmern n√ºchtern beurteilen k√∂nnen.  Und wenn etwas nicht klar ist - es gibt immer Kan√§le in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Slack</a> / Discord / Gitter / etc, um alle aufkommenden Probleme zu diskutieren.  Wenn das Thema wirklich ‚ÄûIhnen‚Äú geh√∂rt, hindert Sie nichts daran, das gesch√§tzte Ergebnis zu erhalten - bei allen Wettbewerben, an denen ich teilgenommen habe, wurden alle Ans√§tze und L√∂sungen direkt im Verlauf des Wettbewerbs untersucht und umgesetzt. </p><br><h1 id="neurips-na-primere-konkretnogo-keysa-learning-to-run">  NeurIPS-Fallstudie: Laufen lernen </h1><br><img src="https://habrastorage.org/webt/hu/ws/d5/huwsd5weqocxiuqv3hugygmfqea.jpeg"><br><br><h3 id="problematika">  Problem </h3><br><p>  Der Gang einer Person ist das Ergebnis des Zusammenspiels von Muskeln, Knochen, Sehorganen und dem Innenohr.  Im Falle einer St√∂rung des Zentralnervensystems k√∂nnen bestimmte motorische St√∂rungen auftreten, einschlie√ülich Gangst√∂rungen - Abasie. <br>  Forscher des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stanford Laboratory of Neuromuscular Biomechanics</a> beschlossen, maschinelles Lernen mit dem Behandlungsproblem zu verbinden, um ihre Theorien an einem virtuellen Modell des Skeletts und nicht an lebenden Menschen experimentieren und testen zu k√∂nnen. </p><br><h3 id="postanovka-zadachi">  Erkl√§rung des Problems </h3><br><p>  Die Teilnehmer erhielten ein virtuelles menschliches Skelett (im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenSim-</a> Simulator), das anstelle eines Beines eine Prothese hatte.  Die Aufgabe bestand darin, dem Skelett beizubringen, sich mit einer bestimmten Geschwindigkeit in eine bestimmte Richtung zu bewegen.  W√§hrend der Simulation k√∂nnen sich Richtung und Geschwindigkeit √§ndern. </p><br><img src="https://habrastorage.org/webt/od/vj/np/odvjnpxb7xogj5h_5ll85iokhp0.jpeg"><br><br>  Um ein virtuelles Skelettkontrollmodell zu erhalten, wurde vorgeschlagen, das Verst√§rkungslernen zu verwenden.  Der Simulator gab uns einen Zustand des Skeletts S (ein Vektor von ~ 400 Zahlen).  Es musste vorhergesagt werden, welche Aktion A ausgef√ºhrt werden muss (die Aktivierungskr√§fte der Beinmuskulatur sind ein Vektor von 19 Zahlen).  Im Verlauf der Simulation erhielt das Skelett eine R-Auszeichnung - als eine Art Konstante abz√ºglich einer Strafe f√ºr die Abweichung von einer bestimmten Geschwindigkeit und Richtung. <br><div class="spoiler">  <b class="spoiler_title">√úber Verst√§rkungstraining</b> <div class="spoiler_text"><p>  Reinforcement Learning (RL) ist ein Bereich, der sich mit Entscheidungstheorie und der Suche nach optimalen Verhaltensrichtlinien befasst. </p><br><p>  Erinnern Sie sich, wie sie unterrichten <del>  Katze </del>  Hund neue Tricks.  Wiederholen Sie eine Aktion, geben Sie einen Leckerbissen f√ºr die Ausf√ºhrung eines Tricks und geben Sie nicht f√ºr die Nichterf√ºllung.  Der Hund sollte dies alles verstehen und eine Verhaltensstrategie finden (‚ÄûRichtlinie‚Äú oder ‚ÄûRichtlinie‚Äú in Bezug auf RL), die die Anzahl der erhaltenen S√º√üigkeiten maximiert. </p><br><p>  Formal haben wir einen Agenten (Hund), der in der Geschichte der Interaktionen mit der Umwelt (Person) geschult ist.  Gleichzeitig bietet ihm die Umgebung, die die Aktionen des Agenten bewertet, eine Belohnung (lecker) - je besser sich der Agent verh√§lt, desto h√∂her ist die Belohnung.  Dementsprechend besteht die Aufgabe des Agenten darin, eine Richtlinie zu finden, die die Belohnung f√ºr die gesamte Zeit der Interaktion mit der Umgebung gut maximiert. </p><br><p>  Weiterentwicklung dieses Themas, regelbasierte L√∂sungen - Software 1.0, wenn alle Regeln vom Entwickler festgelegt wurden, √ºberwachtes Lernen - genau Software 2.0, wenn das System anhand der verf√ºgbaren Beispiele selbst lernt und Datenabh√§ngigkeiten findet, ist das verst√§rkte Lernen ein Schritt weiter, wenn das System selbst lernt zu recherchieren, zu experimentieren und die erforderlichen Abh√§ngigkeiten in seinen Entscheidungen zu finden.  Je weiter wir gehen, desto besser versuchen wir zu wiederholen, wie eine Person lernt. </p></div></div><br><h3 id="osobennosti-zadachi">  Aufgabenfunktionen </h3><br><p>  Die Aufgabe sieht aus wie ein typischer Vertreter des verst√§rkten Lernens f√ºr Aufgaben mit kontinuierlichem Aktionsraum (RL f√ºr kontinuierlichen Aktionsraum).  Es unterscheidet sich von gew√∂hnlichem RL darin, dass anstelle einer bestimmten Aktion (Dr√ºcken der Joystick-Taste) diese Aktion zur genauen Vorhersage erforderlich ist (und es gibt unendlich viele M√∂glichkeiten). </p><br><p>  Der grundlegende L√∂sungsansatz ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Deterministic Policy Gradient</a> ) wurde bereits 2015 erfunden. Nach den Standards von DL entwickelt sich die Region seit langem aktiv in Anwendungen f√ºr Robotik und reale RL-Anwendungen.  Es gibt etwas zu verbessern: robuste Ans√§tze (um einen echten Roboter nicht zu besch√§digen), Probeneffizienz (um monatelang keine Daten von echten Robotern zu sammeln) und andere RL-Probleme (Kompromiss zwischen Exploration und Ausbeutung usw.).  In diesem Wettbewerb gaben sie uns keinen echten Roboter - nur eine Simulation, aber der Simulator selbst war 2000-mal langsamer als die Open Source-Gegenst√ºcke (bei denen jeder seine RL-Algorithmen √ºberpr√ºft) und brachte daher das Problem der Probeneffizienz auf ein neues Niveau. </p><br><h3 id="etapy-sorevnovaniya">  Wettbewerbsphasen </h3><br><p>  Der Wettbewerb selbst fand in drei Phasen statt, in denen sich Aufgabe und Bedingungen etwas √§nderten. </p><br><ul><li>  Stufe 1: Das Skelett lernte, mit einer Geschwindigkeit von 3 Metern pro Sekunde geradeaus zu laufen.  Die Aufgabe wurde als erledigt angesehen, wenn der Agent 300 Schritte durchlaufen hatte. </li><li>  Stufe 2: Geschwindigkeit und Richtung werden regelm√§√üig ge√§ndert.  Die L√§nge der Entfernung wurde auf 1000 Schritte erh√∂ht. </li><li>  Stufe 3: Die endg√ºltige L√∂sung musste in ein Docker-Image verpackt und zur √úberpr√ºfung gesendet werden.  Insgesamt konnten 10 Pakete hergestellt werden. </li></ul><br><p>  Die Hauptqualit√§tsmetrik wurde als Gesamtbelohnung f√ºr die Simulation angesehen, die zeigte, wie gut das Skelett √ºber die gesamte Distanz an einer bestimmten Richtung und Geschwindigkeit festhielt. </p><br><p>  W√§hrend der 1. und 2. Phase wurde der Fortschritt jedes Teilnehmers in der Rangliste angezeigt.  Die endg√ºltige L√∂sung musste als Docker-Image gesendet werden.  Es sah Beschr√§nkungen der Arbeitszeiten und Ressourcen vor. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: √∂ffentliche Rangliste und RL</b> <div class="spoiler_text"><p>  Aufgrund der Verf√ºgbarkeit der Rangliste zeigt niemand sein bestes Modell, um in der Endrunde ‚Äûetwas mehr als sonst‚Äú zu geben und Rivalen zu √ºberraschen. </p></div></div><br><h6 id="pochemu-tak-vazhny-docker-obrazy">  Warum Docker-Bilder so wichtig sind </h6><br><p>  Letztes Jahr ereignete sich ein kleiner Vorfall bei der Bewertung von Entscheidungen in der ersten Runde.  Zu diesem Zeitpunkt durchlief die √úberpr√ºfung eine http-Interaktion mit der Plattform, und es wurde ein Gesicht der Testbedingungen gefunden.  Man konnte herausfinden, in welchen besonderen Situationen der Agent bewertet wurde, und ihn nur unter diesen Bedingungen neu trainieren.  Was nat√ºrlich das eigentliche Problem nicht l√∂ste.  Aus diesem Grund haben sie beschlossen, das System der Einreichungen auf Docker-Images zu √ºbertragen und auf den Remote-Servern der Organisatoren zu starten.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dbrain</a> verwendet dasselbe System, um das Ergebnis von Wettbewerben aus denselben Gr√ºnden genau zu berechnen. </p><br><h1 id="klyuchevye-momenty">  Wichtige Punkte </h1><br><h3 id="komanda">  Das Team </h3><br><img src="https://habrastorage.org/webt/ty/ur/gp/tyurgpqbzb2zl2wimtzri0mnpwk.jpeg"><br><br>  Das erste, was f√ºr den Erfolg des gesamten Unternehmens wichtig ist, ist das Team.  Egal wie gut Sie sind (und wie stark Ihre Pfoten sind) - die Teilnahme am Team erh√∂ht die Erfolgschancen erheblich.  Der Grund ist einfach: eine Vielzahl von Meinungen und Ans√§tzen, die √úberpr√ºfung von Hypothesen, die F√§higkeit, die Arbeit zu parallelisieren und mehr Experimente durchzuf√ºhren.  All dies ist √§u√üerst wichtig, wenn Sie neue Probleme l√∂sen, mit denen Sie konfrontiert sind. <br><p>  Idealerweise sollten Ihre Kenntnisse und F√§higkeiten auf dem gleichen Niveau sein und sich gegenseitig erg√§nzen.  So habe ich beispielsweise dieses Jahr unser Team auf PyTorch eingestellt und erste Ideen zur Implementierung eines verteilten Agententrainingsystems erhalten. </p><br><p>  Wie finde ich ein Team?  Erstens k√∂nnen Sie sich den Reihen der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ods anschlie√üen</a> und dort nach Gleichgesinnten suchen.  Zweitens gibt es f√ºr RL-Stipendiaten einen separaten Chatraum in einem Telegramm - den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RL-Club</a> .  Drittens k√∂nnen Sie einen wunderbaren Kurs von ShAD - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Practical RL</a> belegen, nach dem Sie sicherlich ein paar Bekanntschaften machen werden. </p><br><p>  Es lohnt sich jedoch, sich an die Politik der "Unterwerfung - oder war es nicht" zu erinnern.  Wenn Sie sich vereinen m√∂chten, treffen Sie zuerst Ihre Entscheidung, senden Sie sie ab, erscheinen Sie in der Rangliste und zeigen Sie Ihr Level.  Wie die Praxis zeigt, sind solche Teams viel ausgeglichener. </p><br><h3 id="motivaciya">  Motivation </h3><br><p>  Wie ich bereits schrieb, wird Sie nichts aufhalten, wenn das Thema ‚ÄûIhr‚Äú ist.  Das bedeutet, dass die Region Sie nicht nur mag, sondern inspiriert - Sie verbrennen es, Sie wollen der Beste darin werden. <br>  Ich habe RL vor 4 Jahren kennengelernt - w√§hrend der Passage des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Berkeley 188x - Einf√ºhrung in die KI</a> - und kann immer noch nicht aufh√∂ren, mich √ºber die Fortschritte in diesem Bereich zu wundern. </p><br><h3 id="sistematichnost">  Systematisch </h3><br><p>  Drittens, aber genauso wichtig - Sie m√ºssen in der Lage sein, das zu tun, was Sie versprochen haben, jeden Tag in den Wettbewerb zu investieren und ihn einfach zu l√∂sen.  T√§glich.  Kein angeborenes Talent kann mit der F√§higkeit verglichen werden, etwas zu tun, auch nicht ein bisschen, aber jeden Tag.  Daf√ºr ist Motivation gefragt.  Um erfolgreich zu sein, empfehle ich, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepWork</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AMA ternaus zu lesen</a> . </p><br><h3 id="time-management">  Zeitmanagement </h3><br><p>  Eine weitere √§u√üerst wichtige F√§higkeit ist die F√§higkeit, die eigene Kraft zu verteilen und die Freizeit richtig zu nutzen.  Die Kombination von Vollzeitarbeit und Teilnahme an Wettbewerben ist keine triviale Aufgabe.  Das Wichtigste unter diesen Bedingungen ist, nicht auszubrennen und der gesamten Last standzuhalten.  Dazu m√ºssen Sie Ihre Zeit richtig verwalten, Ihre Kraft n√ºchtern einsch√§tzen und nicht vergessen, sich rechtzeitig zu entspannen. </p><br><h3 id="overwork">  √úberarbeitung </h3><br><p>  In der letzten Phase des Wettbewerbs tritt normalerweise eine Situation auf, in der Sie buchst√§blich in einer Woche nicht nur viel, sondern SEHR viel tun m√ºssen.  Um das beste Ergebnis zu erzielen, m√ºssen Sie sich zwingen k√∂nnen, sich zu setzen und den letzten Schub zum begehrten Preis zu machen. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: Frist f√ºr Frist</b> <div class="spoiler_text"><p>  Aus welchen Gr√ºnden kann es im Allgemeinen erforderlich sein, zum Nutzen des Wettbewerbs zu recyceln?  Die Antwort ist ganz einfach - Frist√ºbertragung.  Bei solchen Wettbewerben k√∂nnen die Organisatoren oft nicht alles vorhersagen, weshalb es am einfachsten ist, den Teilnehmern mehr Zeit zu geben.  In diesem Jahr wurde der Wettbewerb dreimal verl√§ngert: zuerst um einen Monat, dann um eine Woche und im allerletzten Moment (24 Stunden vor Ablauf der Frist) - um weitere zwei Tage.  Und wenn Sie w√§hrend der ersten beiden Transfers nur die Verl√§ngerung richtig organisieren mussten, mussten Sie in den letzten zwei Tagen nur pfl√ºgen. </p></div></div><br><h3 id="theory">  Theorie </h3><br><img src="https://habrastorage.org/webt/gf/rg/9q/gfrg9ql1ukvjmlbglwcizlfcpto.png"><br><p>  Vergessen Sie unter anderem nicht die Theorie - sich bewusst zu sein, was auf dem Gebiet passiert, und die relevanten zu notieren.  Um beispielsweise im letzten Jahr eine L√∂sung zu finden, hat sich unser Team von den folgenden Artikeln verabschiedet: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kontinuierliche Kontrolle mit tiefem Verst√§rkungslernen</a> ist ein grundlegender Artikel √ºber tiefes Verst√§rkungslernen f√ºr Aufgaben mit kontinuierlichem Aktionsraum. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Parameter Space Noise for Exploration</a> - eine Studie zum Hinzuf√ºgen von Rauschen zu Agentengewichten f√ºr eine bessere Untersuchung der Umgebung.  Aus Erfahrung - eine der besten Erkundungstechniken in RL. </li></ul><br><p>  In diesem Jahr wurden einige weitere hinzugef√ºgt: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine Verteilungsperspektive zum Reinforcement Learning</a> - Ein neuer Blick auf die Vorhersagen einer m√∂glichen Belohnung.  Anstatt nur den Durchschnitt vorherzusagen, wird die Verteilung zuk√ºnftiger Belohnungen berechnet. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Das Lernen zur Verst√§rkung der Verteilung mit quantitativer Regression</a> ist eine Fortsetzung der vorherigen Arbeit, jedoch mit der ‚ÄûQuantisierung‚Äú der Verteilung. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Distributed Prioritized Experience Replay</a> - Arbeiten Sie aus der Richtung des vertieften Lernens in gro√üem Ma√üstab.  Informationen zur ordnungsgem√§√üen Organisation der Experimentarchitektur, um die Nutzung der verf√ºgbaren Ressourcen zu maximieren und die Geschwindigkeit der Schulungsagenten zu erh√∂hen. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verteilte verteilungsdeterministische Richtlinienverl√§ufe</a> - eine Kombination der drei vorherigen Artikel f√ºr Aufgaben mit einem kontinuierlichen Aktionsraum. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adressierungsfunktionsn√§herungsfehler bei akteurkritischen Methoden</a> - hervorragende Arbeit zur Erh√∂hung der Robustheit von RL-Agenten.  Ich empfehle es zu lesen. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dateneffizientes hierarchisches Verst√§rkungslernen</a> ist eine Weiterentwicklung eines fr√ºheren Artikels im Bereich des hierarchischen Verst√§rkungslernens (HRL). </li></ul><br><div class="spoiler">  <b class="spoiler_title">Zus√§tzliche Lekt√ºre</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Soft Actor-Critic: Off-Policy-Lernen mit maximaler Entropie und tiefem Verst√§rkungslernen mit einem stochastischen Akteur</a> - Die Autoren schlugen eine Methode zum Trainieren stochastischer Richtlinien mit nicht-politischem Verst√§rkungslernen vor.  Dank dieses Artikels wurde es m√∂glich, nicht deterministische Politiker auch bei Aufgaben mit kontinuierlichem Handlungsspielraum auszubilden. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Latent Space Policies f√ºr hierarchisches Reinforcement Learning</a> ist eine Fortsetzung eines fr√ºheren HRL-Artikels mit mehrstufigen stochastischen Richtlinien. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vielfalt ist alles, was Sie brauchen: Lernf√§higkeiten ohne Belohnungsfunktion</a> - Dieser Artikel enth√§lt einen Ansatz zum Erlernen vieler zuf√§lliger stochastischer Richtlinien auf niedriger Ebene ohne Belohnung durch die Umwelt.  Anschlie√üend, wenn wir die Belohnungsfunktion festgelegt haben, kann die am meisten mit der Auszeichnung korrelierte verwendet werden, um hochrangige Politik dar√ºber zu unterrichten. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Reinforcement Learning and Control als probabilistische Folgerung: Tutorial und Review</a> - Ein √úberblick √ºber alle Arten von Lernmethoden zur Verst√§rkung der maximalen Entropie von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sergey Levine</a> . </li></ul><br><p>  Ich empfehle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI auch eine Auswahl von Artikeln zum</a> Thema Verst√§rkungslernen und dessen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Version f√ºr Mendeley</a> .  Und wenn Sie sich f√ºr das Thema Verst√§rkungstraining interessieren, treten Sie dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RL-Club</a> und den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RL-Papieren bei</a> . </p></div></div><br><h3 id="practice">  √úbe </h3><br><img src="https://habrastorage.org/webt/xp/g7/it/xpg7itebqdpi3cwex33uzrzkidg.jpeg"><br><br>  Die Theorie allein zu kennen, reicht nicht aus - es ist wichtig, all diese Ans√§tze in die Praxis umsetzen und das richtige Validierungssystem f√ºr die Bewertung von Entscheidungen etablieren zu k√∂nnen.  Zum Beispiel haben wir in diesem Jahr erfahren, dass unser Agent nur zwei Tage vor dem Ende des Wettbewerbs mit einigen regionalen F√§llen schlecht zurechtkommt.  Aus diesem Grund hatten wir keine Zeit, unser Modell vollst√§ndig zu reparieren, und konnten nicht buchst√§blich ein paar Punkte auf den begehrten zweiten Platz bringen.  Wenn wir dies sogar in einer Woche finden w√ºrden, k√∂nnte das Ergebnis besser sein. <br><div class="spoiler">  <b class="spoiler_title">Coolstory: Folge III</b> <div class="spoiler_text"><p>  Die durchschnittliche Auszeichnung f√ºr 10 Testepisoden diente als endg√ºltige Bewertung der L√∂sung. </p><br><img src="https://habrastorage.org/webt/jq/bj/yc/jqbjyctkjettuu2bqd19xcahssk.png"><br><p>  Die Grafik zeigt die Ergebnisse des Testens unseres Agenten: 9 von 10 Episoden, unser Skelett lief gut (Durchschnitt - 9955,66), aber eine Episode ... Episode 3 wurde ihm nicht gegeben (Belohnung 9870).  Es war dieser Fehler, der zum Abfall der Endgeschwindigkeit auf 9947 (-8 Punkte) f√ºhrte. </p></div></div><br><h3 id="udacha">  Viel Gl√ºck </h3><br><p>  Und zum Schluss - vergessen Sie nicht das banale Gl√ºck.  Denken Sie nicht, dass dies ein kontroverser Punkt ist.  Im Gegenteil, ein wenig Gl√ºck tr√§gt wesentlich zur st√§ndigen Arbeit an sich selbst bei: Selbst wenn die Wahrscheinlichkeit des Gl√ºcks nur 10% betr√§gt, wird eine Person, die 100 Mal versucht hat, am Wettbewerb teilzunehmen, viel mehr Erfolg haben als jemand, der nur 1 Mal versucht hat und die Idee aufgegeben hat. </p><br><h1 id="tuda-i-obratno-reshenie-proshlogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2017-learning-to-runwinners">  Rundreise: letztj√§hrige Entscheidung - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dritter Platz</a> </h1><br><img src="https://habrastorage.org/webt/mq/lx/i_/mqlxi_alc8pt0acnzwoi8twb8oo.jpeg"><br><br>  Letztes Jahr hat unser Team - Mikhail Pavlov und ich - zum ersten Mal an den NeurIPS-Wettbewerben teilgenommen. Die Hauptmotivation bestand darin, einfach am ersten NeurIPS-Wettbewerb zum Thema ‚ÄûLernen zur St√§rkung‚Äú teilzunehmen.  Dann habe ich gerade den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">praktischen RL-</a> Kurs im SHAD beendet und wollte die erworbenen F√§higkeiten testen.  Infolgedessen haben wir einen ehrenvollen dritten Platz belegt und nur gegen Nnaisene (Schmidhuber) und das Universit√§ts-Team aus China verloren.  Zu dieser Zeit war unsere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">L√∂sung</a> ‚Äûziemlich einfach‚Äú und basierte auf verteiltem DDPG mit Parameterrauschen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ver√∂ffentlichung</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pr√§sentation auf ml</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Trainings</a> ). <br><h1 id="reshenie-etogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2018-ai-for-prosthetics-challengeleaderboards">  Die diesj√§hrige Entscheidung ist der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dritte Platz</a> </h1><br><img src="https://habrastorage.org/webt/gf/qq/to/gfqqtoneh51dn47m3f7oicyqixk.jpeg"><br><p>  In diesem Jahr gab es einige √Ñnderungen.  Erstens gab es keinen Wunsch, nur an diesem Wettbewerb teilzunehmen, ich wollte ihn gewinnen.  Zweitens hat sich auch die Zusammensetzung des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teams</a> ge√§ndert: Alexey Grinchuk, Anton Pechenko und ich.  Nehmen und gewinnen - hat nicht funktioniert, aber wir haben wieder den 3. Platz belegt. <br>  Unsere L√∂sung wird offiziell auf der NeurIPS vorgestellt, und jetzt beschr√§nken wir uns auf eine kleine Anzahl von Details.  Basierend auf der Entscheidung des letzten Jahres und dem Erfolg des Off-Policy-Enforcement-Lernens dieses Jahres (Artikel oben) haben wir eine Reihe unserer eigenen Entwicklungen hinzugef√ºgt, √ºber die wir bei NeurIPS sprechen werden, und den Distributed Quantile Ensemble Critic erhalten, mit dem wir den dritten Platz belegten. </p><br><p>  Alle unsere Best Practices - ein verteiltes Lernsystem, Algorithmen usw. - werden nach NeurIPS in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Catalyst.RL</a> ver√∂ffentlicht und verf√ºgbar sein. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: gro√üe Jungs - gro√üe Waffen</b> <div class="spoiler_text"><p>  Unser Team belegte w√§hrend des gesamten Wettbewerbs zuversichtlich den 1. Platz.  Die gro√üen Jungs hatten jedoch andere Pl√§ne - 2 gro√üe Spieler nahmen 2 Wochen vor Ende des Wettbewerbs am Wettbewerb teil: FireWork (Baidu) und nnaisense (Schmidhuber).  Und wenn mit dem chinesischen Google nichts getan werden konnte, dann konnten wir mit dem Schmidhuber-Team eine ganze Weile ehrlich um den zweiten Platz k√§mpfen und nur mit einem minimalen Vorsprung verlieren.  Es scheint mir ziemlich gut f√ºr Liebhaber. </p></div></div><br><h1 id="zachem-eto-vse">  Warum ist das alles? </h1><br><ul><li>  Kommunikation.  Zu der Konferenz kommen Spitzenforscher, mit denen Sie live chatten k√∂nnen und die keine E-Mail-Korrespondenz f√ºhren. </li><li>  Ver√∂ffentlichung  Wenn die L√∂sung den Preis erh√§lt, wird das Team zur Konferenz (oder m√∂glicherweise zu mehreren) eingeladen, um seine Entscheidung zu pr√§sentieren und den Artikel zu ver√∂ffentlichen. </li><li>  Stellenangebot und Promotion.  Die Ver√∂ffentlichung und ein Preis in einer solchen Konferenz erh√∂hen Ihre Chancen auf eine Position in f√ºhrenden Unternehmen wie OpenAI, DeepMind, Google, Facebook und Microsoft erheblich. </li><li>  Wert der realen Welt.  NeurIPS wird durchgef√ºhrt, um dr√§ngende Probleme der akademischen und realen Welt zu l√∂sen.  Sie k√∂nnen sicher sein, dass die Ergebnisse nicht auf den Tisch kommen, sondern wirklich gefragt sind und zur Verbesserung der Welt beitragen. </li><li>  Fahren  Solche Wettbewerbe zu l√∂sen ... einfach interessant.  In einem Wettbewerb k√∂nnen Sie viele neue Ideen einbringen, verschiedene Ans√§tze testen - nur um die Besten zu sein.  Und seien wir ehrlich, wann sonst k√∂nnen Sie Skelette fahren, Spiele spielen und das alles mit einem ernsthaften Blick und der Wissenschaft zuliebe? </li></ul><br><div class="spoiler">  <b class="spoiler_title">Coolstory: Visum und RL</b> <div class="spoiler_text"><p>  Ich empfehle dringend, dem Amerikaner nicht zu erkl√§ren, dass Sie zur Konferenz gehen, w√§hrend Sie virtuelle Skelette f√ºr die Ausf√ºhrung in Simulationen trainieren.  Gehen Sie einfach mit einem Vortrag zur Konferenz. </p></div></div><br><h1 id="itogi">  Zusammenfassung </h1><br><p>  Die Teilnahme an NeurIPS ist eine Erfahrung, die schwer zu √ºbersch√§tzen ist.  Haben Sie keine Angst vor hochkar√§tigen Schlagzeilen - Sie m√ºssen sich nur zusammenrei√üen und sich entscheiden. </p><br><p>  Und gehen Sie zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Catalyst.RL</a> , was dann? </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de430712/">https://habr.com/ru/post/de430712/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de430702/index.html">Sehr seltsames Training</a></li>
<li><a href="../de430704/index.html">Wie Technologien f√ºr k√ºnstliche Intelligenz das Wachstum von Aviasales f√∂rdern: Sieben Beispiele</a></li>
<li><a href="../de430706/index.html">Neue Evolutionstheorie</a></li>
<li><a href="../de430708/index.html">Tic Tac Toe "Ohne Grenzen"</a></li>
<li><a href="../de430710/index.html">Was tun, wenn der Schwarze Freitag morgen ist und Ihre Server nicht bereit sind?</a></li>
<li><a href="../de430714/index.html">VMware kauft Heptio - was bedeutet das f√ºr Kubernetes?</a></li>
<li><a href="../de430718/index.html">F√ºr welche Objekte lohnt sich die Cloud-Video√ºberwachung?</a></li>
<li><a href="../de430720/index.html">Intel RealSense D435i: kleines Update und kurzer historischer Exkurs</a></li>
<li><a href="../de430722/index.html">PHP-Leistung: Planen, Profilieren, Optimieren</a></li>
<li><a href="../de430724/index.html">DEFCON 21. DNS-Konferenzen k√∂nnen gesundheitssch√§dlich sein. Teil 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>