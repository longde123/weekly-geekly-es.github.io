<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍶 🍑 🕛 Mobil sudah di depan orang dalam tes membaca; tetapi apakah mereka mengerti apa yang mereka baca? 🐍 🔫 🏇</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Alat yang disebut BERT mampu menyalip orang dalam membaca dan memahami tes. Namun, ini juga menunjukkan ke arah mana AI masih perlu berjalan. 


 Pada...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mobil sudah di depan orang dalam tes membaca; tetapi apakah mereka mengerti apa yang mereka baca?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/479446/"><h3>  Alat yang disebut BERT mampu menyalip orang dalam membaca dan memahami tes.  Namun, ini juga menunjukkan ke arah mana AI masih perlu berjalan. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/1e6/359/58b/1e635958bf5af44bcc9b6256e1a0101f.jpg"><br><br>  Pada musim gugur 2017, <a href="http://www.nyu.edu/projects/bowman/">Sam Bowman</a> , ahli bahasa komputasi dari New York University, memutuskan bahwa komputer masih tidak memahami teks dengan sangat baik.  Tentu saja, mereka belajar cukup baik untuk mensimulasikan pemahaman ini di bidang sempit tertentu, seperti terjemahan otomatis atau analisis perasaan (misalnya, untuk menentukan apakah kalimat itu “kasar atau manis,” seperti katanya).  Namun, Bowman menginginkan kesaksian yang terukur: pemahaman yang benar tentang apa yang ditulis, ditulis dalam bahasa manusia.  Dan dia datang dengan ujian. <br><a name="habracut"></a><br>  Dalam <a href="https://arxiv.org/abs/1804.07461">makalah</a> April 2018 yang ditulis bekerja sama dengan rekan-rekan di Washington University dan DeepMind, sebuah perusahaan milik Google yang bergerak dalam kecerdasan buatan, Bowman mempresentasikan satu set sembilan tugas pemahaman bacaan untuk komputer dengan nama umum GLUE (General Language Understanding Evaluation) [pemahaman penilaian bahasa umum].  Tes ini dirancang sebagai "contoh yang cukup indikatif dari apa yang dianggap komunitas penelitian sebagai tugas yang menarik," kata Bowman, tetapi dengan cara yang "mudah bagi orang-orang."  Misalnya, dalam satu tugas, pertanyaan diajukan tentang kebenaran kalimat, yang harus diperkirakan berdasarkan informasi dari kalimat sebelumnya.  Jika Anda dapat mengatakan bahwa pesan "Presiden Trump telah mendarat di Irak, setelah memulai kunjungan tujuh harinya" menyiratkan bahwa "Presiden Trump sedang berkunjung ke luar negeri," Anda lulus ujian. <br><br>  Mobil membuatnya gagal.  Bahkan jaringan syaraf canggih mencetak tidak lebih dari 69 dari 100 poin total untuk semua tes - tiga besar dengan minus.  Bowman dan rekannya tidak terkejut.  Jaringan saraf - konstruksi berlapis-lapis dengan koneksi komputasi yang kira-kira menyerupai kerja neuron di otak mamalia - menunjukkan hasil yang baik di bidang "Pemrosesan Bahasa Alami", tetapi para peneliti tidak yakin bahwa sistem ini diajarkan sesuatu yang serius tentang bahasa  Dan LEM membuktikannya.  "Hasil awal menunjukkan bahwa lulus uji LEM melampaui kemampuan model dan metode yang ada," Bowman et al. <br><br>  Namun penilaian mereka tidak berlangsung lama.  Pada Oktober 2018, Google memperkenalkan metode baru, BERT (Bidirectional Encoder Representations from Transformers) [presentasi dua arah encoder bidirectional untuk transformer].  Ia menerima skor 80,5 di GLUE.  Hanya dalam enam bulan, mobil melonjak dari tiga dengan minus ke empat dengan minus dalam tes baru ini, yang mengukur pemahaman nyata bahasa alami oleh mesin. <br><br>  “Itu seperti 'sial,'” kenang Bowman, menggunakan kata yang lebih berwarna.  - Pesan ini diterima dengan rasa tidak percaya oleh komunitas.  BERT menerima dalam banyak nilai tes mendekati apa yang kami anggap semaksimal mungkin. ”  Memang, sebelum munculnya BERT dalam tes LEM, tidak ada penilaian prestasi manusia untuk dibandingkan.  Ketika Bowman dan salah satu mahasiswa pascasarjana menambahkan mereka ke GLUE pada Februari 2019, mereka hanya bertahan beberapa bulan, dan kemudian model berbasis Microsoft BERT juga <a href="https://blogs.msdn.microsoft.com/stevengu/2019/06/20/microsoft-achieves-human-performance-estimate-on-glue-benchmark/">mengalahkan mereka</a> . <br><br>  Pada saat penulisan ini, hampir semua <a href="https://gluebenchmark.com/leaderboard/">tempat pertama</a> dalam tes LEM ditempati oleh sistem yang mencakup, memperluas atau mengoptimalkan model BERT.  Lima di antaranya unggul dalam kemampuan manusia. <br><br>  Tetapi apakah ini berarti bahwa AI mulai memahami bahasa kita, atau hanya belajar untuk mengalahkan sistem kita?  Setelah jaringan saraf berbasis BERT mengambil tes tipe LUE oleh badai, metode evaluasi baru muncul yang menganggap sistem NLP ini sebagai versi komputer " <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D0%25BC%25D0%25BD%25D1%258B%25D0%25B9_%25D0%2593%25D0%25B0%25D0%25BD%25D1%2581">pintar Hans,</a> " seekor kuda yang hidup di awal abad ke-20 dan dianggap cukup pintar untuk untuk membuat perhitungan aritmatika dalam pikiran, tetapi sebenarnya membaca tanda-tanda tidak sadar yang diberikan kepadanya oleh pemiliknya. <br><br>  "Kami tahu bahwa kami berada di suatu tempat di zona abu-abu antara memahami bahasa dalam arti yang sangat membosankan dan sempit, dan menciptakan AI," kata Bowman.  - Secara umum, reaksi para spesialis dapat digambarkan sebagai berikut: Bagaimana ini terjadi?  Apa artinya ini?  Apa yang akan kita lakukan sekarang? " <br><br><h2>  Menulis Aturan Anda Sendiri </h2><br>  Dalam eksperimen pemikiran " <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B8%25D1%2582%25D0%25B0%25D0%25B9%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D0%25BA%25D0%25BE%25D0%25BC%25D0%25BD%25D0%25B0%25D1%2582%25D0%25B0">Kamar Cina</a> " yang terkenal, seseorang yang tidak tahu bahasa Cina duduk di ruangan yang dipenuhi banyak buku dengan peraturan.  Di buku-buku Anda dapat menemukan instruksi yang tepat tentang cara menerima urutan karakter Cina memasuki ruangan dan memberikan jawaban yang sesuai.  Seseorang di luar telapak tangan pertanyaan yang ditulis dalam bahasa Cina di bawah pintu kamar.  Orang di dalam membuka buku-buku dengan aturan, dan merumuskan jawaban yang masuk akal dalam bahasa Mandarin. <br><br>  Eksperimen ini digunakan untuk membuktikan bahwa terlepas dari kesan luar, orang tidak dapat mengatakan bahwa orang di ruangan itu memiliki pemahaman bahasa Cina.  Namun, bahkan simulasi pemahaman adalah tujuan yang dapat diterima dari NLP. <br><br>  Satu-satunya masalah adalah kurangnya buku yang sempurna dengan aturan, karena bahasa alami terlalu kompleks dan tidak sistematis untuk direduksi menjadi seperangkat spesifikasi yang solid.  Ambil, misalnya, sintaksis: aturan (termasuk empiris) yang menentukan pengelompokan kata menjadi kalimat yang bermakna.  Kalimat " <a href="https://books.google.com/books%3Fid%3D55YaAAAAIAAJ%26dq%3Dcolorless%2Bgreen%2Bideas%2Bsleep%2Bfuriously">ide-ide hijau tanpa tidur yang tidur nyenyak</a> " memiliki sintaksis, tetapi siapa pun yang tahu bahasa tersebut memahami artinya.  Apa buku peraturan yang dirancang khusus dapat mencakup fakta tidak tertulis ini yang berkaitan dengan bahasa alami - belum termasuk fakta lainnya yang tak terhitung jumlahnya? <br><br>  Peneliti NLP mencoba menemukan <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B2%25D0%25B0%25D0%25B4%25D1%2580%25D0%25B0%25D1%2582%25D1%2583%25D1%2580%25D0%25B0_%25D0%25BA%25D1%2580%25D1%2583%25D0%25B3%25D0%25B0">quadrature of the circle ini</a> , memaksa jaringan saraf untuk menulis buku aturan artisanal mereka sendiri dalam proses yang disebut  "Pra-pelatihan" atau pretraining. <br><br>  Hingga 2018, salah satu alat pelatihan utama adalah sesuatu seperti kamus.  Kamus ini menggunakan <a href="https://ru.wikipedia.org/wiki/%25D0%2592%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D1%2581%25D1%2582%25D0%25B0%25D0%25B2%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581%25D0%25BB%25D0%25BE%25D0%25B2">representasi vektor dari kata</a> [embedding kata], menggambarkan hubungan antara kata-kata dalam bentuk angka sehingga jaringan saraf dapat melihat informasi ini sebagai input - sesuatu seperti glosarium kasar untuk seseorang di ruang Cina.  Namun, pra-terlatih pada jaringan saraf kamus vektor masih tetap buta terhadap makna kata-kata di tingkat kalimat.  "Dari sudut pandangnya, kalimat 'pria menggigit anjing' dan 'anjing menggigit pria' adalah identik," kata <a href="http://tallinzen.net/">Tel Linsen</a> , ahli bahasa komputasi di Universitas Johns Hopkins. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/55a/245/f48/55a245f4866cf5ffcf2689d2161f8536.jpg" width="50%"><br>  <i>Tel Linsen, Ahli Bahasa Komputer di Universitas Johns Hopkins.</i> <br><br>  Metode yang ditingkatkan menggunakan pra-pelatihan untuk menyediakan jaringan saraf dengan buku aturan yang lebih kaya - tidak hanya kamus, tetapi juga sintaksis dengan konteks - sebelum mengajarkannya untuk melakukan tugas NLP tertentu.  Pada awal 2018, para peneliti dari OpenAI, Universitas San Francisco, Institut Allen untuk Kecerdasan Buatan, dan Universitas Washington pada saat yang sama datang dengan cara yang rumit untuk lebih dekat dengan ini.  Alih-alih melatih hanya satu, lapisan pertama jaringan menggunakan representasi vektor kata-kata, para peneliti mulai melatih seluruh jaringan untuk tugas yang lebih umum yang disebut pemodelan bahasa. <br><br>  "Cara paling sederhana untuk memodelkan bahasa adalah sebagai berikut: Saya akan membaca banyak kata dan mencoba memprediksi yang berikut," jelas <a href="https://research.fb.com/people/ott-myle/">Mile Ott</a> , seorang peneliti Facebook.  "Jika saya mengatakan, 'George W. Bush dilahirkan,' maka model perlu memprediksi kata berikutnya dalam kalimat ini." <br><br>  Model bahasa seperti itu dengan pelatihan mendalam dapat dibuat dengan cukup efisien.  Para peneliti hanya memberi makan sejumlah besar teks tertulis dari sumber daya gratis seperti Wikipedia ke jaringan saraf mereka - miliaran kata yang disusun dalam kalimat yang benar secara tata bahasa - dan memungkinkan jaringan untuk memprediksi kata berikutnya sendiri.  Bahkan, ini setara dengan fakta bahwa kami akan mengundang seseorang di ruang Cina untuk membuat seperangkat aturan mereka sendiri, menggunakan pesan Cina yang masuk untuk referensi. <br><br>  "Keindahan dari pendekatan ini adalah bahwa model mendapatkan satu ton pengetahuan sintaksis," kata Ott. <br><br>  Selain itu, jaringan saraf pra-terlatih seperti itu dapat menerapkan representasi bahasa mereka untuk mengajarkan tugas yang lebih sempit, tidak terkait dengan prediksi kata, dengan proses fine-tuning. <br><br>  "Anda dapat mengambil model dari fase pra-pelatihan dan menyesuaikannya dengan tugas nyata yang Anda butuhkan," jelas Ott.  "Dan setelah itu, kamu mendapatkan hasil yang jauh lebih baik daripada jika kamu mencoba menyelesaikan masalahmu langsung dari awal." <br><br>  Pada Juni 2018, ketika OpenAI memperkenalkan <a href="https://openai.com/blog/language-unsupervised/">jaringan saraf GPT</a> -nya, dengan model bahasa yang disertakan di dalamnya, yang menghabiskan satu bulan pelatihan untuk satu miliar kata (diambil dari 11.038 buku digital), hasilnya dalam tes GLUE, 72,8 poin, segera menjadi yang paling yang terbaik.  Namun demikian, Sam Bowman menyarankan bahwa area ini akan berkembang untuk waktu yang sangat lama sebelum sistem apa pun setidaknya bisa mendekati level manusia. <br><br>  Dan kemudian BERT muncul. <br><br><h2>  Resep yang menjanjikan </h2><br>  Jadi apa itu BERT? <br><br>  Pertama, itu bukan jaringan saraf yang terlatih penuh, yang mampu segera memberikan hasil pada tingkat manusia.  Bowman mengatakan ini adalah "resep yang sangat akurat untuk melatih jaringan saraf."  Seperti yang bisa dilakukan seorang tukang roti, mengikuti resepnya, menjamin untuk memberikan kue kue yang lezat - yang kemudian dapat digunakan untuk kue yang berbeda, dari blueberry hingga quiche bayam - dan peneliti Google telah menciptakan resep BERT yang dapat berfungsi sebagai fondasi ideal untuk "memanggang" jaringan saraf (yaitu (fine tuning mereka), sehingga mereka dapat mengatasi berbagai tugas dengan baik dalam memproses bahasa alami.  Google membuat kode BERT terbuka, yang berarti bahwa peneliti lain tidak perlu lagi mengulangi resep ini dari awal - mereka hanya dapat mengunduhnya;  itu seperti membeli kue yang sudah dipanggang untuk kue di toko. <br><br>  Jika BERT adalah resep, lalu apa daftar bahannya?  "Ini adalah hasil dari tiga hal berbeda yang terhubung bersama sehingga sistem mulai bekerja," kata <a href="https://levyomer.wordpress.com/">Omer Levy</a> , seorang peneliti Facebook yang <a href="https://arxiv.org/abs/1906.04341">menganalisis</a> perangkat BERT. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1ee/f23/473/1eef234733ede2389825e047170009e0.jpg" width="50%"><br>  <i>Omer Levy, Peneliti Facebook</i> <br><br>  Yang pertama adalah model bahasa pra-dilatih, yaitu, direktori yang sama dari ruang Cina.  Yang kedua adalah kesempatan untuk memutuskan fitur mana dari proposal yang paling penting. <br><br>  Pada tahun 2017, <a href="http://jakob.uszkoreit.net/">Jacob Uzkoreit</a> , seorang insinyur di Google Brain, mengerjakan berbagai cara untuk mempercepat upaya perusahaan untuk memahami bahasa tersebut.  Dia mencatat bahwa semua jaringan saraf maju menderita dari keterbatasan yang melekat: mereka mempelajari kalimat dengan kata-kata.  "Urutan" seperti itu tampaknya bertepatan dengan gagasan tentang bagaimana orang membaca teks.  Namun, Uzkoreit menjadi tertarik, "mungkinkah memahami bahasa secara linear, mode sekuensial bukanlah yang paling optimal." <br><br>  Tingkat sempit dengan kolega mengembangkan arsitektur baru jaringan saraf, dengan fokus pada "perhatian", sebuah mekanisme yang memungkinkan setiap lapisan jaringan saraf untuk menetapkan bobot besar pada fitur-fitur tertentu dari data input dibandingkan dengan yang lain.  Arsitektur baru ini dengan perhatian, sebuah transformator, dapat mengambil kalimat seperti "seekor anjing menggigit manusia" sebagai input dan menyandikan setiap kata secara paralel dengan cara yang berbeda.  Misalnya, transformator dapat mengikat "gigitan" dan "orang" sebagai kata kerja dan objek-objek, mengabaikan artikel "a";  pada saat yang sama, ia dapat mengaitkan "bite" dan "dog" sebagai kata kerja dan subjek-subjek, mengabaikan artikel "the". <br><br>  Sifat transformator yang tidak konsisten menghadirkan kalimat yang lebih ekspresif, atau, seperti kata Uzkoreit, seperti pohon.  Setiap lapisan jaringan saraf membangun banyak koneksi paralel antara kata-kata tertentu, mengabaikan sisanya - kira-kira bagaimana seorang siswa di sekolah dasar membongkar sebuah kalimat menjadi beberapa bagian.  Koneksi ini sering dibuat antara kata-kata yang mungkin tidak ada di dekatnya.  "Struktur seperti itu terlihat seperti hamparan beberapa pohon," Uzkoreit menjelaskan. <br><br>  Representasi kalimat seperti pohon memberi transformer kesempatan untuk memodelkan makna kontekstual, serta secara efektif mempelajari hubungan antara kata-kata yang jauh berbeda dalam kalimat kompleks.  "Ini agak berlawanan dengan intuisi," kata Uzkoreit, "tetapi berasal dari linguistik, yang telah lama terlibat dalam model bahasa seperti pohon." <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/0a1/fe2/21e0a1fe2331c110f5cb2a966b2b173d.jpg" width="50%"><br>  <i>Jacob Uzkoreit, kepala tim Berlin Google AI Brain</i> <br><br>  Akhirnya, bahan ketiga dalam resep BERT semakin memperluas bacaan non-linear. <br><br>  Tidak seperti model bahasa pra-terlatih lainnya yang dibuat dengan memproses terabyte teks dari kiri ke kanan oleh jaringan saraf, model BERT membaca dari kanan ke kiri dan secara simultan dari kiri ke kanan, dan belajar memprediksi kata mana yang secara acak dikeluarkan dari kalimat.  Misalnya, BERT dapat menerima kalimat dalam bentuk "George W. Bush [...] di Connecticut pada tahun 1946" dan memperkirakan kata mana yang disembunyikan di tengah kalimat (dalam hal ini, "lahir"), setelah memproses teks dalam dua arah.  "Bi-directionality ini memaksa jaringan saraf untuk mengekstrak informasi sebanyak mungkin dari setiap subset kata," kata Uzkoreit. <br><br>  Berpura-pura berbasis BERT digunakan seperti permainan kata - pemodelan bahasa dengan masking - bukan hal yang baru.  Ini telah digunakan selama beberapa dekade untuk mengukur pemahaman orang tentang bahasa.  Untuk Google, ia menyediakan cara praktis untuk menggunakan bi-directionality di jaringan saraf daripada metode pra-pelatihan satu arah yang telah mendominasi bidang ini sebelumnya.  "Sebelum BERT, pemodelan bahasa satu arah adalah standar, meskipun ini adalah batasan opsional," kata <a href="https://kentonl.com/">Kenton Lee</a> , seorang peneliti Google. <br><br>  Masing-masing dari ketiga bahan - model bahasa yang mendalam dengan pra-pelatihan, perhatian dan dua arah - ada sebelum BERT secara terpisah.  Tetapi sampai Google merilis resep mereka pada akhir tahun 2018, tidak ada yang menggabungkan mereka dengan cara yang begitu sukses. <br><br><h2>  Resep Pemurnian </h2><br>  Seperti resep bagus lainnya, BRET segera diadaptasi oleh berbagai koki sesuai selera mereka.  Pada musim semi 2019, ada periode "ketika Microsoft dan Alibaba saling bertukar posisi, mengubah tempat dalam peringkat mingguan, menyesuaikan model mereka," kenang Bowman.  Ketika versi perbaikan dari BERT pertama kali dirilis pada bulan Agustus dengan nama RoBERTa, peneliti <a href="http://ruder.io/">Sebastian Ruder</a> dari DeepMind dengan kering berkomentar di <a href="http://newsletter.ruder.io/issues/nlp-in-industry-leaderboard-madness-fast-ai-nlp-transfer-learning-tools-186245">buletin NLP yang</a> populer: "Bulan baru, dan model bahasa maju baru dengan pra-pelatihan." <br><br>  Seperti halnya kue, BERT memiliki beberapa keputusan desain yang memengaruhi kualitas pekerjaannya.  Ini termasuk ukuran jaringan saraf yang dipanggang, jumlah data yang digunakan untuk pra-pelatihan, metode menutupi kata-kata, dan berapa lama jaringan saraf telah bekerja dengan data ini.  Dan dalam resep selanjutnya, seperti RoBERTa, para peneliti mengubah keputusan ini - seperti koki yang menentukan resep. <br><br>  Dalam kasus RoBERTa, peneliti dari Facebook dan Washington University meningkatkan jumlah beberapa bahan (data pra-pelatihan, panjang urutan masuk, waktu pelatihan), satu bahan telah dihapus (tugas "memprediksi kalimat berikutnya", yang awalnya di BERT dan berdampak negatif pada hasil ), dan yang lainnya diubah (rumit tugas menutupi kata-kata individual).  Alhasil, mereka sempat menempati posisi pertama di peringkat GLUE.  Enam minggu kemudian, para peneliti dari Microsoft dan University of Maryland <a href="https://arxiv.org/abs/1909.11764">menambahkan</a> penyempurnaan mereka pada RoBERTa, dan menarik kemenangan berikutnya.  Saat ini, model lain mengambil tempat pertama di GLUE, ALBERT (singkatan untuk "lite BERT", yaitu, "lite BERT"), yang sedikit mengubah struktur dasar BERT. <br><br>  "Kami masih memilah resep mana yang bekerja, mana yang tidak," kata Ott dari Facebook, yang bekerja di RoBERTa. <br><br>  Tapi, karena peningkatan teknik kue pra-memanggang tidak mengajarkan Anda dasar-dasar kimia, peningkatan BERT secara bertahap tidak akan memberi Anda banyak pengetahuan teoritis tentang pengembangan NLP.  "Saya akan sangat jujur ​​dengan Anda - saya tidak mengikuti karya-karya ini, karena bagi saya, mereka sangat membosankan," kata Linsen, ahli bahasa komputasi di Johns Hopkins University.  "Ada misteri ilmiah tertentu," akunya, tetapi tidak bagaimana membuat BERT dan semua keturunannya lebih pintar, dan bahkan tidak mengetahui mengapa mereka begitu pintar.  Alih-alih, "kami mencoba memahami seberapa besar model-model ini benar-benar memahami bahasa," katanya, "daripada mempelajari trik-trik aneh yang entah bagaimana bekerja pada dataset yang biasanya kami evaluasi model-model ini." <br><br>  Dengan kata lain, BERT melakukan sesuatu yang benar.  Tetapi bagaimana jika dia melakukannya karena alasan yang salah? <br><br><h2>  Tricky tapi tidak pintar </h2><br>  Pada Juli 2019, dua peneliti dari Universitas Negeri Taiwan, Cheng Kun, menggunakan BERT dengan hasil yang mengesankan pada tes kinerja yang relatif sedikit dikenal yang disebut "tugas pemahaman argumen".  Untuk menyelesaikan tugas, perlu untuk memilih kondisi awal implisit ("yayasan") yang mendukung argumen yang mendukung pernyataan apa pun.  Misalnya, untuk membuktikan bahwa "merokok menyebabkan kanker" (pernyataan) karena "studi ilmiah telah menunjukkan hubungan antara merokok dan kanker" (argumentasi), Anda perlu memilih argumen "penelitian ilmiah dapat dipercaya" ("yayasan"), dan bukan pilihan lain: “Penelitian ilmiah itu mahal” (meskipun demikian, ini tidak relevan dalam konteks ini).  Apakah semuanya jelas? <br><br>  Jika tidak semua, jangan khawatir.  Bahkan orang tidak pandai mengerjakan tugas ini tanpa latihan.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Garis dasar rata-rata untuk orang yang tidak berolahraga adalah 80 dari 100. BERT mencapai 77 - yang menurut penulis adalah "tidak terduga."</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tetapi alih-alih memutuskan bahwa BERT mampu memberikan jaringan saraf kemampuan untuk bernalar tidak lebih buruk daripada Aristoteles, mereka menduga bahwa semuanya sebenarnya lebih sederhana: BERT menemukan pola-pola yang dangkal dalam perumusan alasan. Memang, setelah menganalisis data pelatihan mereka, penulis menemukan banyak bukti yang disebut "Petunjuk palsu." Misalnya, jika Anda hanya memilih semua pangkalan yang mengandung partikel "tidak", Anda dapat menjawab pertanyaan dengan benar di 61% kasus. Setelah menghapus semua keteraturan dari data, para ilmuwan menemukan bahwa hasil BERT turun dari 77 menjadi 53 - yang hampir setara dengan pilihan acak. Sebuah artikel di majalah pembelajaran mesin The Gradient dari Stanford Artificial Intelligence Lab </font></font><a href="https://thegradient.pub/nlps-clever-hans-moment-has-arrived/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">membandingkan</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERT dengan Smart Hans, seekor kuda yang konon kuat dalam aritmatika. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dalam artikel lain, " </font></font><a href="https://www.aclweb.org/anthology/P19-1334"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hak untuk</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Alasan yang Salah," Linsen et al menerbitkan bukti bahwa hasil tinggi BERT dalam tes GLUE tertentu juga dapat dikaitkan dengan adanya petunjuk palsu dalam data pelatihan. Seperangkat data alternatif dikembangkan yang dirancang untuk menghilangkan BERT dari kemampuan untuk bekerja dengan cara ini. Dataset itu disebut Hans (Analisis Heuristik untuk Sistem Inferensi Bahasa-Alam, HANS) [analisis heuristik sistem yang menarik kesimpulan berdasarkan bahasa alami].</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jadi, apakah BERT dan semua kerabatnya menyerbu meja skor tinggi hanya tipuan? Bowman setuju dengan Lensen bahwa beberapa data LEM adalah ceroboh. Mereka dipenuhi dengan distorsi kognitif yang melekat pada orang-orang yang menciptakannya, dan ini berpotensi dieksploitasi oleh jaringan berbasis BERT yang kuat. "Tidak ada trik universal yang akan menyelesaikan semua masalah dalam LEM, tetapi ada banyak kemungkinan untuk" mengambil jalan pintas "yang membantu dalam hal ini," kata Bowman, "dan model dapat menemukannya." Tetapi dia tidak berpikir bahwa BERT didasarkan pada sesuatu yang bernilai. "Kami tampaknya memiliki model yang telah mempelajari sesuatu yang sangat menarik tentang bahasa tersebut," katanya. "Namun, dia tentu saja tidak mengerti bahasa manusia secara umum."</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Menurut Yojin Choi, seorang ilmuwan komputer di University of Washington dan Allen Institute, salah satu cara untuk merangsang kemajuan menuju pemahaman bersama bahasa adalah untuk berkonsentrasi tidak hanya pada peningkatan versi BERT, tetapi juga pada pengembangan tes kualitas yang lebih baik dan data pelatihan yang mengurangi kemungkinan terjadinya teknologi palsu dalam gaya "Hans pintar." Karyanya mengeksplorasi pendekatan penyaringan permusuhan yang menggunakan algoritma untuk memvalidasi data pelatihan untuk NLP dan menghapus contoh yang terlalu berulang atau meninggalkan petunjuk implisit ke jaringan saraf. Setelah pemfilteran kompetitif seperti itu, "efektivitas BERT dapat turun secara signifikan," katanya, dan "efektivitas manusia tidak banyak turun."</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Namun demikian, beberapa peneliti NLP percaya bahwa bahkan dengan peningkatan prosedur pengajaran untuk model bahasa, masih akan ada hambatan nyata untuk pemahaman bahasa yang sebenarnya. Bahkan dengan pelatihan yang kuat, BERT tidak dapat memodelkan bahasa dengan sempurna dalam kasus umum. Setelah tweak, ia memodelkan "tugas NLP tertentu, atau bahkan set data spesifik untuk tugas itu," kata </font></font><a href="https://www.cs.uml.edu/~arogers/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anna Rogers</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ahli bahasa komputasi di Laboratorium Teks Mesin di Universitas Massachusetts. Sangat mungkin bahwa tidak ada set data pelatihan, betapapun dipersiapkan dengan hati-hati atau difilter dengan hati-hati, akan dapat mencakup semua kasus ekstrem dan data input yang tidak dapat diprediksi yang dapat dengan mudah ditangani oleh orang yang menggunakan bahasa alami.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bowman menunjukkan bahwa sulit bahkan untuk memahami apa yang dapat meyakinkan kita bahwa jaringan saraf telah mencapai pemahaman bahasa yang sebenarnya. Tes standar harus mengungkapkan sesuatu yang disosialisasikan mengenai pengetahuan yang diuji. Namun, setiap siswa tahu bahwa tes mudah untuk dibodohi. “Sangat sulit bagi kami untuk membuat tes yang cukup berat dan cukup terlindungi dari penipuan sehingga solusi mereka meyakinkan kami bahwa kami benar-benar menyelesaikan masalah dalam beberapa aspek teknologi bahasa AI,” katanya. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bowman dan rekannya baru-baru ini mempresentasikan tes yang disebut </font></font><a href="https://super.gluebenchmark.com/leaderboard"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SuperGLUE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dirancang khusus untuk menjadi kompleks untuk sistem berbasis BERT. Sejauh ini, tidak ada jaringan yang dapat menyalip seseorang di dalamnya. Tetapi bahkan jika (atau ketika) ini terjadi, apakah ini berarti bahwa mesin dapat belajar untuk memahami bahasa dengan lebih baik dari sebelumnya? Atau apakah hanya ilmu yang akan mengajarkan mobil lebih baik cara lulus tes ini?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Analogi yang bagus," kata Bowman. </font><font style="vertical-align: inherit;">“Kami menemukan cara untuk lulus tes LSAT dan MCAT, tetapi kami mungkin tidak memiliki kualifikasi untuk menjadi dokter atau pengacara.” </font><font style="vertical-align: inherit;">Namun, dilihat dari semuanya, ini adalah persis bagaimana penelitian di bidang AI bergerak. </font><font style="vertical-align: inherit;">"Catur tampak seperti ujian kecerdasan yang serius sampai kami menemukan cara menulis program untuk permainan ini," katanya. </font><font style="vertical-align: inherit;">"Kami benar-benar memasuki era ketika tujuannya adalah untuk menciptakan tugas yang semakin kompleks yang mewakili pemahaman bahasa, dan menemukan cara untuk menyelesaikannya."</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id479446/">https://habr.com/ru/post/id479446/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id479428/index.html">Acara digital di Moskow dari 9 hingga 15 Desember</a></li>
<li><a href="../id479430/index.html">Acara digital di St. Petersburg dari 9 hingga 15 Desember</a></li>
<li><a href="../id479432/index.html">Yandex.Maps: Saya pergi ke pengontrol kartu - Saya langsung mendapatkan posisi pengguna (oke, sekarang serius)</a></li>
<li><a href="../id479438/index.html">Postgres-Tuesday # 5: “PostgreSQL dan Kubernetes. CI / CD. Otomasi Uji »</a></li>
<li><a href="../id479442/index.html">Alexey Savvateev: Game-theoretic model of social schism (+ nginx survey)</a></li>
<li><a href="../id479450/index.html">AppCode 2019.3: bekerja lebih cepat, memahami Swift lebih baik, tahu tentang Mac Catalyst, dengan mudah menampilkan pesan perakitan</a></li>
<li><a href="../id479452/index.html">Bagaimana Sistem Nama Domain Dikembangkan: Era ARPANET</a></li>
<li><a href="../id479458/index.html">Keindahan atau kepraktisan di ruang server</a></li>
<li><a href="../id479460/index.html">Panduan untuk Mobil Terbang</a></li>
<li><a href="../id479462/index.html">Serialisasi dalam C ++</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>