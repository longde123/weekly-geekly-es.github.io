<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏼‍🚀 🐿️ 🕵🏼 Bagaimana kami menciptakan teknologi pengenalan teks optik. OCR di Yandex 🍌 🖱️ 🍩</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hai Hari ini saya akan memberi tahu pembaca Habr tentang bagaimana kami menciptakan teknologi pengenalan teks yang bekerja dalam 45 bahasa dan dapat d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bagaimana kami menciptakan teknologi pengenalan teks optik. OCR di Yandex</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/475956/">  Hai  Hari ini saya akan memberi tahu pembaca Habr tentang bagaimana kami menciptakan teknologi pengenalan teks yang bekerja dalam 45 bahasa dan dapat diakses oleh pengguna Yandex.Cloud, tugas apa yang kami tetapkan, dan bagaimana kami menyelesaikannya.  Ini akan berguna jika Anda mengerjakan proyek serupa atau ingin mengetahui bagaimana hal itu terjadi sehingga hari ini Anda hanya perlu memotret tanda toko Turki sehingga Alice menerjemahkannya ke dalam bahasa Rusia. <br><br><img src="https://habrastorage.org/webt/fm/c4/rx/fmc4rxrj9iczvwvfjku7cm-nwim.png"><br><a name="habracut"></a><br>  Teknologi optical character recognition (OCR) telah berkembang di dunia selama beberapa dekade.  Kami di Yandex mulai mengembangkan teknologi OCR kami sendiri untuk meningkatkan layanan kami dan memberi pengguna lebih banyak opsi.  Gambar adalah bagian besar dari Internet, dan tanpa kemampuan untuk memahaminya, pencarian di internet tidak akan lengkap. <br><br>  Solusi analisis gambar menjadi semakin populer.  Hal ini disebabkan oleh proliferasi jaringan saraf tiruan dan perangkat dengan sensor berkualitas tinggi.  Jelas bahwa pertama-tama kita berbicara tentang smartphone, tetapi tidak hanya tentang mereka. <br><br>  Kompleksitas tugas di bidang pengenalan teks terus berkembang - semuanya dimulai dengan pengakuan dokumen yang dipindai.  Kemudian <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pengenalan</a> Born-Digital-gambar dengan teks dari Internet ditambahkan.  Kemudian, dengan semakin populernya kamera ponsel, pengakuan akan hasil jepretan kamera yang bagus ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">teks adegan terfokus</a> ).  Dan semakin jauh, semakin rumit parameternya: teks bisa kabur ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Incidental scene text</a> ), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ditulis</a> dengan sembarang tikungan atau spiral, dari berbagai kategori - dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">foto</a> kwitansi hingga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">rak</a> toko dan papan nama. <br><br><h3>  Kemana kita pergi </h3><br>  Pengenalan teks adalah kelas yang terpisah dari tugas visi komputer.  Seperti banyak algoritma visi komputer, sebelum popularitas jaringan saraf, sebagian besar didasarkan pada fitur manual dan heuristik.  Namun, baru-baru ini, dengan transisi ke pendekatan jaringan saraf, kualitas teknologi telah tumbuh secara signifikan.  Lihatlah contoh di foto.  Bagaimana ini terjadi, saya akan ceritakan lebih lanjut. <br><br>  Bandingkan hasil pengakuan hari ini dengan hasil di awal 2018: <br><div class="scrollable-table"><table><tbody><tr><td><img src="https://habrastorage.org/webt/uf/ux/-g/ufux-gtblu3a_fc96rdowuul7co.png"></td><td><img src="https://habrastorage.org/webt/so/pa/yg/sopaygej-n6c9gurt75zfwa-f3y.png"></td></tr><tr><td> <b><i>2018</i></b> <b><i><br></i></b>  <b><i>Pelembab</i></b> <b><i><br></i></b>  <b><i>n HO - micellar</i></b> <b><i><br></i></b>  <b><i>air kehalusan mewah.</i></b> <b><i><br></i></b>  <b><i>lembut multifungsi</i></b> <b><i><br></i></b>  <b><i>formulanya</i></b> <b><i><br></i></b>  <b><i>gunakan sebagai sarana</i></b> <b><i><br></i></b>  <b><i>Sl UNTUK</i></b> <b><i><br></i></b>  <b><i>bukan lotion pembersih atau</i></b> <b><i><br></i></b>  <b><i>tonik</i></b> <b><i><br></i></b>  <b><i>Tidak ada alkohol, pewarna, paraben</i></b> <b><i><br></i></b>  <b><i>...</i></b> </td><td>  <b><i>2019</i></b> <b><i><br></i></b>  <b><i>MOISTURISING</i></b> <b><i><br></i></b>  <b><i>AIR TERMAL-MICELLAR</i></b> <b><i><br></i></b>  <b><i>HALUS MEWAH</i></b> <b><i><br></i></b>  <b><i>AUBY Lembut dan lembut</i></b> <b><i><br></i></b>  <b><i>formula multifungsi</i></b> <b><i><br></i></b>  <b><i>untuk penggunaan sehari - hari di</i></b> <b><i><br></i></b>  <b><i>sebagai sarana untuk</i></b> <b><i><br></i></b>  <b><i>make-up remover bukan cleansing</i></b> <b><i><br></i></b>  <b><i>lotion atau tonik.</i></b> <b><i><br></i></b>  <b><i>Tidak ada alkohol, pewarna, paraben</i></b> <b><i><br></i></b>  <b><i>...</i></b> </td></tr></tbody></table></div><h3>  Kesulitan apa yang kita hadapi pada awalnya? </h3><br>  Pada awal perjalanan kami, kami membuat teknologi pengenalan untuk Rusia dan Inggris, dan kasing utama adalah halaman teks dan gambar yang difoto dari Internet.  Tetapi dalam perjalanan, kami menyadari bahwa ini tidak cukup: teks pada gambar ditemukan dalam bahasa apa pun, pada permukaan apa pun, dan gambar-gambar itu kadang-kadang ternyata memiliki kualitas yang sangat berbeda.  Ini berarti bahwa pengenalan harus bekerja dalam situasi apa pun dan pada semua jenis data yang masuk. <br><br>  Dan di sini kita dihadapkan pada sejumlah kesulitan.  Berikut ini beberapa di antaranya: <br><br><ul><li>  <b>Detail</b>  Untuk orang yang terbiasa mendapatkan informasi dari teks, teks dalam gambar adalah paragraf, garis, kata-kata dan huruf, tetapi untuk jaringan saraf semuanya terlihat berbeda.  Karena sifat teks yang kompleks, jaringan dipaksa untuk melihat gambar secara keseluruhan (misalnya, jika orang bergandengan tangan dan membuat prasasti), dan detail terkecil (dalam bahasa Vietnam, simbol serupa  dan ừ mengubah arti kata).  Tantangan terpisah adalah mengenali teks sewenang-wenang dan font yang tidak standar. </li><li>  <b>Multilingualisme</b> .  Semakin banyak bahasa yang kami tambahkan, semakin banyak kami dihadapkan dengan kekhususannya: dalam bahasa Cyrillic dan Latin terdiri dari huruf-huruf yang terpisah, dalam bahasa Arab keduanya ditulis bersama, dalam bahasa Jepang tidak ada kata yang berbeda yang dibedakan.  Beberapa bahasa menggunakan ejaan dari kiri ke kanan, beberapa dari kanan ke kiri.  Beberapa kata ditulis secara horizontal, beberapa secara vertikal.  Alat universal harus memperhitungkan semua fitur ini. </li><li>  <b>Struktur teks</b> .  Untuk mengenali gambar tertentu, seperti cek atau dokumen yang kompleks, struktur yang memperhitungkan tata letak paragraf, tabel, dan elemen lainnya sangat penting. </li><li>  <b>Performa</b> .  Teknologi ini digunakan pada berbagai perangkat, termasuk offline, jadi kami harus memperhitungkan persyaratan kinerja yang ketat. </li></ul><br><h3>  Pemilihan Model Deteksi </h3><br>  Langkah pertama untuk mengenali teks adalah menentukan posisinya (deteksi). <br>  Deteksi teks dapat dianggap sebagai tugas pengenalan objek, di mana <b>karakter</b> individu, <b>kata</b> atau <b>garis</b> dapat bertindak sebagai objek. <br><br>  Penting bagi kami bahwa model selanjutnya diskalakan ke bahasa lain (sekarang kami mendukung 45 bahasa). <br><br>  Banyak artikel penelitian tentang deteksi teks menggunakan model yang memprediksi posisi <b>kata</b> - <b>kata</b> individual.  Tetapi dalam kasus <b>model universal,</b> pendekatan ini memiliki beberapa keterbatasan - misalnya, konsep kata untuk bahasa Cina pada dasarnya berbeda dari konsep kata, misalnya, dalam bahasa Inggris.  Kata-kata individual dalam bahasa Cina tidak dipisahkan oleh spasi.  Di Thailand, hanya satu kalimat yang dibuang dengan spasi. <br><br>  Berikut adalah contoh teks yang sama dalam bahasa Rusia, Cina, dan Thailand: <br><br> <code>  .    . <br>今天天气很好 这是一个美丽的一天散步。 <br> สภาพอากาศสมบูรณ์แบบในวันนี้ มันเป็นวันที่สวยงามสำหรับเดินเล่นกันหน่อยแล้ว</code> <br> <br>  <b>Baris</b> , pada gilirannya, sangat bervariasi dalam hal rasio aspek.  Karena itu, kemungkinan model pendeteksian umum seperti itu (misalnya, berbasis SSD atau RCNN) untuk prediksi saluran terbatas, karena model ini didasarkan pada daerah kandidat / kotak jangkar dengan banyak rasio aspek yang telah ditentukan.  Selain itu, garis dapat memiliki bentuk sewenang-wenang, misalnya, melengkung, oleh karena itu untuk deskripsi kualitatif garis itu tidak cukup eksklusif untuk menggambarkan segi empat, bahkan dengan sudut rotasi. <br><br>  Terlepas dari kenyataan bahwa posisi masing-masing <b>karakter bersifat</b> lokal dan dideskripsikan, kelemahan mereka adalah bahwa diperlukan langkah pasca-pemrosesan yang terpisah - Anda perlu memilih heuristik untuk menempelkan karakter ke dalam kata dan garis. <br><br>  Oleh karena itu, kami menggunakan <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">model SegLink</a></b> sebagai dasar untuk deteksi, ide utamanya adalah untuk menguraikan baris / kata menjadi dua entitas lokal: segmen dan hubungan di antara mereka. <br><br><h3>  Arsitektur detektor </h3><br>  Arsitektur model didasarkan pada SSD, yang memprediksi posisi objek pada beberapa skala fitur.  Hanya selain memprediksi koordinat "segmen" individu juga diprediksi "koneksi" antara segmen yang berdekatan, yaitu, apakah dua segmen milik garis yang sama.  "Koneksi" diprediksi baik untuk segmen tetangga pada skala tanda yang sama, dan untuk segmen yang terletak di daerah yang berdekatan pada skala tetangga (segmen dari skala tanda yang berbeda mungkin sedikit berbeda dalam ukuran dan termasuk dalam garis yang sama). <br><br>  Untuk setiap skala, setiap sel fitur dikaitkan dengan "segmen" yang sesuai.  Untuk setiap segmen s <sup>(x, y, l)</sup> pada titik (x, y) pada skala l, berikut ini dilatih: <br>  - p <sub>s</sub> apakah segmen yang diberikan adalah teks; <br>  - x <sub>s</sub> , y <sub>s</sub> , w <sub>s</sub> , h <sub>s</sub> , θ <sub>s</sub> - offset koordinat basis dan sudut kemiringan segmen; <br>  - 8 skor untuk kehadiran "koneksi" dengan segmen yang berdekatan dengan skala ke-l ( <sup>Ls</sup> <sub>, s '</sub> , s' dari {s <sup>(x ', y', l)</sup> } / s <sup>(x, y, l)</sup> , di mana x –1 ≤ x '≤ x + 1, y - 1 ≤ y' ≤ y + 1); <br>  - 4 skor untuk kehadiran "koneksi" dengan segmen yang berdekatan dengan skala l-1 (L <sup>c</sup> <sub>s, s '</sub> , s' dari {s <sup>(x ', y', l-1)</sup> }, di mana 2x ≤ x '≤ 2x + 1 , 2y ≤ y '≤ 2y + 1) (yang benar karena fakta bahwa dimensi fitur pada skala tetangga berbeda persis 2 kali). <br><br><img src="https://habrastorage.org/webt/fn/ox/en/fnoxen3f1izpei9ecdbdl_eq2xk.png"><br><h5>  <sup><sub>Ilustrasi Operasional SegLink Detector dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Mendeteksi Teks Berorientasi pada Gambar Alami dengan Menghubungkan Segmen</a></sub></sup> </h5><br>  Menurut prediksi seperti itu, jika kita mengambil sebagai simpul semua segmen yang probabilitas bahwa mereka adalah teks lebih besar dari ambang α, dan sebagai ujung semua ikatan yang probabilitasnya lebih besar dari ambang β, maka segmen membentuk komponen yang terhubung, masing-masing menggambarkan garis teks . <br><br>  Model yang dihasilkan memiliki <b>kemampuan generalisasi yang tinggi</b> : bahkan dilatih dalam pendekatan pertama pada data Rusia dan Inggris, secara kualitatif ditemukan teks Cina dan Arab. <br><br><h3>  Sepuluh skrip </h3><br>  Jika untuk pendeteksian kami dapat membuat model yang bekerja langsung untuk semua bahasa, maka untuk pengenalan garis-garis yang ditemukan model seperti itu jauh lebih sulit diperoleh.  Karena itu, kami memutuskan untuk menggunakan <b>model terpisah untuk setiap skrip</b> (Sirilik, Latin, Arab, Ibrani, Yunani, Armenia, Georgia, Korea, Thailand).  Model umum yang terpisah digunakan untuk Cina dan Jepang karena persimpangan besar dalam hieroglif. <br><br>  Model yang umum untuk seluruh skrip berbeda dari model terpisah untuk setiap bahasa kurang dari 1 p.p.  kualitas.  Pada saat yang sama, pembuatan dan implementasi satu model lebih sederhana daripada, misalnya, 25 model (jumlah bahasa Latin yang didukung oleh model kami).  Tetapi karena seringnya bahasa Inggris di semua bahasa, semua model kami dapat memprediksi, selain skrip utama, karakter Latin. <br><br>  Untuk memahami model mana yang harus digunakan untuk pengakuan, pertama-tama kita menentukan apakah baris yang diterima milik salah satu dari 10 skrip yang tersedia untuk pengakuan. <br><br>  Perlu dicatat secara terpisah bahwa tidak selalu mungkin untuk secara unik menentukan skripnya.  Misalnya, angka atau karakter Latin tunggal terkandung dalam banyak skrip, sehingga salah satu kelas output dari model adalah skrip "tidak terdefinisi". <br><br><h3>  Definisi skrip </h3><br>  Untuk mendefinisikan skrip, kami membuat classifier terpisah.  Tugas mendefinisikan naskah jauh lebih sederhana daripada tugas pengakuan, dan jaringan saraf mudah dilatih ulang pada data sintetik.  Oleh karena itu, dalam percobaan kami, peningkatan yang signifikan dalam kualitas model diberikan oleh <b>pra-pelatihan tentang masalah pengenalan string</b> .  Untuk melakukan ini, kami pertama-tama melatih jaringan untuk masalah pengenalan untuk semua bahasa yang tersedia.  Setelah itu, tulang punggung yang dihasilkan digunakan untuk menginisialisasi model ke tugas klasifikasi skrip. <br><br>  Sementara sebuah skrip pada baris individual sering kali sangat berisik, gambar secara keseluruhan paling sering berisi teks dalam satu bahasa, baik di samping diselingi utama dengan bahasa Inggris (atau dalam kasus pengguna Rusia kami).  Oleh karena itu, untuk <b>meningkatkan</b> stabilitas, kami mengumpulkan prediksi garis dari gambar untuk mendapatkan prediksi skrip gambar yang lebih stabil.  Baris dengan kelas prediksi "tidak terbatas" tidak diperhitungkan dalam agregasi. <br><br><h3>  Pengenalan garis </h3><br>  Langkah selanjutnya, ketika kita telah menentukan posisi setiap baris dan skripnya, kita perlu <b>mengenali urutan karakter dari skrip</b> yang <b>diberikan</b> yang ditampilkan di atasnya, yaitu dari urutan piksel untuk memprediksi urutan karakter.  Setelah banyak percobaan, kami sampai pada model berdasarkan perhatian Sequence2 berikut: <br><br><img src="https://habrastorage.org/webt/_0/6k/sf/_06ksfdetbjobwudopmi4xq0j4c.png"><br><br>  Menggunakan CNN + BiLSTM dalam encoder memungkinkan Anda untuk mendapatkan tanda yang menangkap konteks lokal dan global.  Untuk teks, ini penting - seringkali ditulis dalam satu font (membedakan huruf serupa dengan informasi font jauh lebih mudah).  Dan untuk membedakan dua huruf yang ditulis dengan spasi dari yang berurutan, statistik global juga diperlukan untuk saluran tersebut. <br><br>  <b>Pengamatan yang menarik</b> : dalam model yang dihasilkan, output topeng perhatian untuk simbol tertentu dapat digunakan untuk memprediksi posisinya dalam gambar. <br><br>  Ini mengilhami kami untuk mencoba <b>"memfokuskan" perhatian model dengan jelas</b> .  Gagasan semacam itu juga ditemukan dalam artikel - misalnya, dalam artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Perhatian Fokus: Menuju Pengakuan Teks yang Akurat dalam Gambar Alam</a> . <br><br>  Karena mekanisme perhatian memberikan distribusi probabilitas di atas ruang fitur, jika kita mengambil sebagai tambahan kerugian jumlah output perhatian di dalam topeng yang sesuai dengan huruf yang diprediksi pada langkah ini, kita mendapatkan bagian dari "perhatian" yang berfokus langsung padanya. <br><br>  Dengan memperkenalkan loss -log (∑ <sub>i, j∈M <sub>t</sub></sub> α <sub>i, j</sub> ), di mana <sub>Mt</sub> adalah topeng dari huruf tth, α adalah output dari perhatian, kami akan mendorong "perhatian" untuk fokus pada simbol yang diberikan dan dengan demikian membantu jaringan saraf belajar lebih baik. <br><br>  Untuk contoh pelatihan yang lokasi karakter individu tidak diketahui atau tidak akurat (tidak semua data pelatihan memiliki tanda di tingkat karakter individu, bukan kata-kata), istilah ini tidak diperhitungkan dalam kerugian akhir. <br><br>  Fitur bagus lainnya: arsitektur ini memungkinkan Anda untuk memprediksi <b>pengenalan</b> garis <b>kanan-ke-kiri</b> tanpa perubahan tambahan (yang penting, misalnya, untuk bahasa seperti Arab, Ibrani).  Model itu sendiri mulai mengeluarkan pengakuan dari kanan ke kiri. <br><br><h3>  Model cepat dan lambat </h3><br>  Dalam prosesnya, kami mengalami masalah: <b>untuk font "tinggi"</b> , yaitu font yang memanjang secara vertikal, modelnya bekerja dengan buruk.  Ini disebabkan oleh fakta bahwa dimensi tanda pada level perhatian 8 kali lebih kecil dari dimensi gambar asli karena langkah dan tarikan arsitektur bagian konvolusional jaringan.  Dan lokasi beberapa karakter tetangga di gambar sumber dapat sesuai dengan lokasi vektor fitur yang sama, yang dapat menyebabkan kesalahan dalam contoh-contoh tersebut.  Penggunaan arsitektur dengan penyempitan dimensi fitur yang lebih kecil menyebabkan peningkatan kualitas, tetapi juga peningkatan waktu pemrosesan. <br><br>  Untuk mengatasi masalah ini dan <b>menghindari bertambahnya waktu pemrosesan</b> , kami melakukan penyempurnaan berikut pada model: <br><br><img src="https://habrastorage.org/webt/po/pp/ob/poppobis-rbsdqtyrbgyaq8jzik.png"><br><br>  Kami melatih kedua model cepat dengan banyak langkah dan lambat dengan kurang.  Pada layer di mana parameter model mulai berbeda, kami menambahkan output jaringan terpisah yang memperkirakan model mana yang akan memiliki kesalahan pengenalan yang lebih sedikit.  Kehilangan total model terdiri dari L <sub>kecil</sub> + L <sub>besar</sub> + <sub>kualitas</sub> L.  Jadi, pada lapisan menengah, model belajar untuk menentukan "kompleksitas" dari contoh ini.  Selanjutnya, pada tahap aplikasi, bagian umum dan prediksi "kompleksitas" dari contoh dipertimbangkan untuk semua lini, dan tergantung pada outputnya, baik model cepat atau lambat digunakan di masa depan sesuai dengan nilai ambang batas.  Ini memungkinkan kami untuk mendapatkan kualitas yang hampir tidak berbeda dari kualitas model lama, sementara kecepatannya hanya meningkat 5% persen daripada perkiraan 30%. <br><br><h3>  Data Pelatihan </h3><br>  Tahap penting dalam menciptakan model berkualitas tinggi adalah persiapan sampel pelatihan yang besar dan beragam.  Sifat "sintetik" dari teks memungkinkan untuk menghasilkan sejumlah besar contoh dan mendapatkan hasil yang layak pada data nyata. <br><br>  Setelah pendekatan pertama untuk menghasilkan data sintetis, kami dengan hati-hati melihat hasil model yang diperoleh dan menemukan bahwa model tersebut tidak mengenali huruf tunggal 'I' dengan baik karena bias dalam teks yang digunakan untuk membuat set pelatihan.  Oleh karena itu, kami jelas menghasilkan <b>serangkaian contoh "bermasalah"</b> , dan ketika kami menambahkannya ke data awal model, kualitasnya meningkat secara signifikan.  Kami mengulangi proses ini berkali-kali, menambahkan irisan yang semakin kompleks, di mana kami ingin meningkatkan kualitas pengakuan. <br><br>  Poin penting adalah bahwa <b>data yang</b> dihasilkan <b>harus beragam dan mirip dengan yang asli</b> .  Dan jika Anda ingin model bekerja pada foto teks pada lembaran kertas, dan seluruh dataset sintetis berisi teks yang ditulis di atas lanskap, maka ini mungkin tidak berfungsi. <br><br>  Langkah penting lainnya adalah menggunakan untuk melatih contoh-contoh di mana pengakuan saat ini salah.  Jika ada sejumlah besar gambar yang tidak ada markup, Anda dapat mengambil output dari sistem pengenalan saat ini di mana dia tidak yakin, dan hanya menandai mereka, sehingga mengurangi biaya markup. <br><br>  Untuk contoh kompleks, kami meminta pengguna layanan Yandex.Tolok untuk memotret dan mengirimi kami <b>gambar grup "kompleks" tertentu</b> - misalnya, foto paket barang: <br><br><img src="https://habrastorage.org/webt/tm/zx/0k/tmzx0kmyswtdxz6u_ri_yfxdrzy.png" width="50%"><img src="https://habrastorage.org/webt/n9/pb/ru/n9pbrufm0gcwp8lggc9bk9kxaxe.png" width="50%"><br><br><h3>  Kualitas pekerjaan pada data "kompleks" </h3><br>  Kami ingin memberi pengguna kami kesempatan untuk bekerja dengan foto-foto dengan kompleksitas apa pun, karena mungkin perlu mengenali atau menerjemahkan teks tidak hanya pada halaman buku atau dokumen yang dipindai, tetapi juga pada tanda jalan, pengumuman atau kemasan produk.  Oleh karena itu, sambil mempertahankan kualitas kerja yang tinggi pada aliran buku dan dokumen (kami akan mencurahkan cerita terpisah untuk topik ini), kami memberikan perhatian khusus pada "set gambar yang kompleks". <br><br>  Dengan cara yang dijelaskan di atas, kami telah menyusun serangkaian gambar yang berisi teks di alam liar, yang mungkin berguna bagi pengguna kami: foto-foto papan nama, pengumuman, piring, sampul buku, teks pada peralatan rumah tangga, pakaian dan benda.  Pada kumpulan data ini (tautannya di bawah), kami mengevaluasi kualitas algoritma kami. <br><br>  Sebagai metrik untuk perbandingan, kami menggunakan metrik standar akurasi dan kelengkapan pengenalan kata dalam dataset, serta ukuran-F.  Kata yang dikenali dianggap benar ditemukan jika koordinatnya sesuai dengan koordinat kata yang ditandai (IoU&gt; 0,3) dan pengakuannya bertepatan dengan tanda yang ditandai persis dengan case.  Angka pada dataset yang dihasilkan: <br><div class="scrollable-table"><table><tbody><tr><td>  Sistem pengakuan </td><td>  Kelengkapan </td><td>  Akurasi </td><td>  F-ukur </td></tr><tr><td>  Visi Yandex </td><td>  73,99 </td><td>  86.57 </td><td>  79,79 </td></tr></tbody></table></div><br>  Kumpulan data, metrik, dan skrip untuk mereproduksi hasil tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  Pembaruan.  Teman, membandingkan teknologi kami dengan solusi serupa dari Abbyy menyebabkan banyak kontroversi.  Kami menghormati pendapat komunitas dan rekan-rekan industri.  Tetapi pada saat yang sama kami yakin dengan hasil kami, jadi kami memutuskan dengan cara ini: kami akan menghapus hasil produk lain dari perbandingan, mendiskusikan metodologi pengujian dengan mereka lagi dan kembali ke hasil di mana kami mencapai kesepakatan umum. <br><br><h3>  Langkah selanjutnya </h3><br>  Di persimpangan langkah-langkah individual, seperti deteksi dan pengakuan, masalah selalu muncul: perubahan sekecil apa pun dalam model deteksi memerlukan kebutuhan untuk mengubah model pengakuan, jadi kami secara aktif bereksperimen dengan menciptakan solusi ujung ke ujung. <br><br>  Selain cara yang telah dijelaskan untuk meningkatkan teknologi, kami akan mengembangkan arah untuk menganalisis struktur dokumen, yang secara fundamental penting ketika mengekstraksi informasi dan diminati oleh pengguna. <br><br><h3>  Kesimpulan </h3><br>  Pengguna sudah terbiasa dengan teknologi yang nyaman dan tanpa ragu menyalakan kamera, arahkan ke tanda toko, menu di restoran atau halaman dalam buku dalam bahasa asing dan cepat menerima terjemahan.  Kami mengenali teks dalam 45 bahasa dengan akurasi yang telah terbukti, dan peluang hanya akan berkembang.  Seperangkat alat di dalam Yandex.Cloud memungkinkan siapa saja yang ingin menggunakan praktik terbaik yang telah dilakukan Yandex sendiri sejak lama. <br><br>  Hari ini Anda bisa mengambil teknologi yang sudah jadi, mengintegrasikannya ke dalam aplikasi Anda sendiri dan menggunakannya untuk menciptakan produk baru dan mengotomatiskan proses Anda sendiri.  Dokumentasi untuk OCR kami tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  Apa yang harus dibaca: <br><br><ol><li><a name="karatzas1"></a>  D. Karatzas, SR Mestre, J. Mas, F. Nourbakhsh, dan PP Roy, “ICDAR 2011 kompetisi membaca yang kuat-tantangan 1: membaca teks dalam gambar digital-lahir (web dan email),” dalam Analisis dan Pengakuan Dokumen (ICDAR ), Konferensi Internasional 2011 tentang.  IEEE, 2011, hlm.  1485-1490. </li><li><a name="karatzas2"></a>  Karatzas D. et al.  Kompetisi ICDAR 2015 tentang Bacaan Kuat // 2015 Konferensi Internasional ke-13 tentang Analisis dan Pengakuan Dokumen (ICDAR).  - IEEE, 2015 .-- S. 1156-1160. </li><li><a name="chng"></a>  Chee-Kheng Chng et.  al.  ICDAR2019 Tantangan Membaca Kuat pada Teks Berbentuk Sewenang-wenang (RRC-ArT) [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">arxiv: 1909.07145v1</a> ] </li><li><a name="icdar2019"></a>  ICDAR 2019 Tantangan Membaca Kuat pada Tanda Terima yang Dipindai OCR dan Ekstraksi Informasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">rrc.cvc.uab.es/?ch=13</a> </li><li><a name="shopsign"></a>  ShopSign: Dataset Teks Adegan Beragam dari Tanda Toko Cina di Street Views [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">arxiv: 1903.10412</a> ] </li><li><a name="seglink"></a>  Baoguang Shi, Xiang Bai, Serge Belongie Mendeteksi Teks Berorientasi dalam Gambar Alam dengan Menautkan Segmen [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">arxiv: 1703.06520</a> ]. </li><li><a name="focusing"></a>  Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Pu Shiliang, Shuigeng Zhou Memfokuskan Perhatian: Menuju Pengakuan Teks Akurat dalam Gambar Alam [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">arxiv: 1709.02054</a> ]. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id475956/">https://habr.com/ru/post/id475956/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id475940/index.html">Vue Storefront: Pendekatan Shell Kedua</a></li>
<li><a href="../id475942/index.html">Panduan Illustrated OAuth dan OpenID Connect</a></li>
<li><a href="../id475944/index.html">Berlari adalah olahraga yang ideal untuk pekerja jarak jauh. Bagian 2: fisika dan material</a></li>
<li><a href="../id475948/index.html">JH Rainwater "Cara merumput kucing" (bagian dua): semua yang tersisa untuk menguasai teknis</a></li>
<li><a href="../id475950/index.html">Mengapa robot membatasi dirinya untuk mengumpulkan bola golf? Ada juga tenis</a></li>
<li><a href="../id475958/index.html">Kisah tentang bagaimana gadis itu berkumpul di IT</a></li>
<li><a href="../id475960/index.html">AHURATUS Smart Home Voice Assistant</a></li>
<li><a href="../id475968/index.html">Berita menarik Vue 3</a></li>
<li><a href="../id475974/index.html">Bagaimana kami membuat hackathon di kereta dan apa yang terjadi</a></li>
<li><a href="../id475978/index.html">Untuk apa markas kereta itu?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>