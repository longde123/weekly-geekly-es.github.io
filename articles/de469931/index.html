<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📟 🆗 📿 Was ist ein Lerngeschwindigkeitsfaktor und wie verbessert er die Eigenschaften des tiefen Lernens? 💕 👩🏽‍🚀 🚴</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dieser Artikel ist mein Versuch, meine Meinung zu folgenden Aspekten zu äußern: 



1. Was ist ein Lerngeschwindigkeitsfaktor und welchen Wert hat er?...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Was ist ein Lerngeschwindigkeitsfaktor und wie verbessert er die Eigenschaften des tiefen Lernens?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469931/">  Dieser Artikel ist mein Versuch, meine Meinung zu folgenden Aspekten zu äußern: <br><br><ol><li>  Was ist ein Lerngeschwindigkeitsfaktor und welchen Wert hat er? </li><li>  Wie wählt man diesen Koeffizienten beim Training von Modellen? </li><li>  Warum ist es notwendig, den Koeffizienten der Lerngeschwindigkeit während des Trainings von Modellen zu ändern? </li><li>  Was tun mit einem Lerngeschwindigkeitsfaktor bei Verwendung eines vorab trainierten Modells? </li></ol><br>  Der größte Teil dieses Beitrags basiert auf Materialien, die von <i>fast.ai</i> erstellt wurden: [1], [2], [5] und [3] - eine prägnante Version ihrer Arbeit, die zum schnellsten Verständnis des Wesens des Problems bestimmt ist.  Um sich mit den Details vertraut zu machen, wird empfohlen, auf die unten angegebenen Links zu klicken. <br><a name="habracut"></a><br><h3>  <b>Was ist ein Lerngeschwindigkeitsfaktor?</b> </h3><br>  Der Lerngeschwindigkeitskoeffizient ist ein Hyperparameter, der die Reihenfolge bestimmt, in der wir unsere Skalen unter Berücksichtigung der Verlustfunktion beim Gradientenabstieg anpassen.  Je niedriger der Wert, desto langsamer bewegen wir uns entlang der Neigung.  Wenn wir einen niedrigen Lerngeschwindigkeitskoeffizienten verwenden, können wir zwar einen positiven Effekt erzielen, indem wir kein einziges lokales Minimum verpassen. Dies kann jedoch auch bedeuten, dass wir viel Zeit für die Konvergenz aufwenden müssen, insbesondere wenn wir uns in der Hochebene befinden. <br><br>  Die Beziehung wird durch die folgende Formel veranschaulicht <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo>&amp;#x2022;</mo><mi>n</mi><mi>e</mi><msub><mi>w</mi><mi>w</mi></msub><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>e</mi><msub><mi>s</mi><mi>g</mi></msub><mi>e</mi><mi>w</mi><mi>i</mi><mi>c</mi><mi>h</mi><mi>t</mi><mo>&amp;#x2212;</mo><mi>L</mi><mi>e</mi><mi>r</mi><mi>n</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>&amp;#x2217;</mo><mi>G</mi><mi>r</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.486ex" height="2.66ex" viewBox="0 -780.1 26042.4 1145.2" role="img" focusable="false" style="vertical-align: -0.848ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMAIN-2219" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-6E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="1101" y="0"></use><g transform="translate(1567,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-77" x="1013" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="2890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="3357" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-67" x="3702" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-68" x="4183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="4759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMAIN-3D" x="5398" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="6455" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-78" x="6921" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="7494" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-73" x="7839" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="8309" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="8670" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="9016" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-72" x="9482" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="9934" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-6E" x="10400" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-64" x="11001" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="11524" y="0"></use><g transform="translate(11991,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-73" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-67" x="663" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="12900" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-77" x="13366" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="14083" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-63" x="14428" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-68" x="14862" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="15438" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMAIN-2212" x="16022" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-4C" x="17023" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="17704" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-72" x="18171" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-6E" x="18622" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-72" x="19223" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-61" x="19674" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="20204" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="20565" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMAIN-2217" x="21254" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-47" x="21977" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-72" x="22763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-61" x="23215" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-64" x="23744" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="24268" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="24613" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-6E" x="25080" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="25680" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo>•</mo><mi>n</mi><mi>e</mi><msub><mi>w</mi><mi>w</mi></msub><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>e</mi><msub><mi>s</mi><mi>g</mi></msub><mi>e</mi><mi>w</mi><mi>i</mi><mi>c</mi><mi>h</mi><mi>t</mi><mo>−</mo><mi>L</mi><mi>e</mi><mi>r</mi><mi>n</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>∗</mo><mi>G</mi><mi>r</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> • new_weight = existierendes_gewicht - Lernrate * Gradient </script></p><br><img src="https://habrastorage.org/webt/dn/j1/nj/dnj1njm2womrahwlbv_dzs25xqs.jpeg"><br>  <b>Gradientenabstieg mit kleinen (oben) und großen (unten) Lerngeschwindigkeitsfaktoren.</b>  <b>Quelle: Andrew Ngs Kurs über maschinelles Lernen auf Coursera</b> <b><br></b> <br>  Meistens wird der Lerngeschwindigkeitsfaktor vom Benutzer willkürlich eingestellt.  Im besten Fall kann er sich für ein intuitives Verständnis dessen, welcher Wert für die Bestimmung des Lerngeschwindigkeitskoeffizienten am besten geeignet ist, auf frühere Experimente (oder eine andere Art von Trainingsmaterial) verlassen. <br><br>  Im Grunde ist es schwierig genug, den richtigen Wert zu wählen.  Das folgende Diagramm zeigt verschiedene Szenarien, die auftreten können, wenn der Benutzer die Lerngeschwindigkeit unabhängig anpasst. <br><br><img src="https://habrastorage.org/webt/qm/uh/qd/qmuhqdtbcagnnzltgnfy53wuxxi.jpeg"><br>  <b>Der Einfluss verschiedener Lernratenfaktoren auf die Konvergenz.</b>  <b>(Bildnachweis: cs231n)</b> <b><br></b> <br>  Darüber hinaus beeinflusst der Lerngeschwindigkeitsfaktor, wie schnell unser Modell ein lokales Minimum erreicht (auch bekannt als die beste Genauigkeit).  Die richtige Wahl von Anfang an garantiert somit weniger Zeitverschwendung für das Training des Modells.  Je weniger Schulungszeit, desto weniger Geld wird für die GPU-Rechenleistung in der Cloud ausgegeben. <br><br><h4>  Gibt es eine bequemere Möglichkeit, den Lernkoeffizienten zu bestimmen? <br></h4><br>  In Absatz 3.3.  " <i>Zyklische Lernratenkoeffizienten für neuronale Netze</i> " Leslie Smith verteidigte den folgenden Punkt: Die Effektivität der Lerngeschwindigkeit kann geschätzt werden, indem das Modell mit einer anfänglich festgelegten niedrigen Lerngeschwindigkeit trainiert wird, die dann in jeder Iteration (linear oder exponentiell) zunimmt. <br><br><img src="https://habrastorage.org/webt/j9/zu/yi/j9zuyi5do_thph6ylxtaxetvm7q.jpeg"><br>  <b>Der Lerngeschwindigkeitsfaktor erhöht sich nach jedem Minipaket.</b> <b><br></b> <br>  Wenn wir die Werte der Indikatoren bei jeder Iteration festlegen, werden wir sehen, dass mit zunehmender Lerngeschwindigkeit ein Punkt erreicht wird (an dem), an dem die Werte der Verlustfunktion nicht mehr abnehmen und zunehmen.  In der Praxis sollte unsere Lerngeschwindigkeit idealerweise irgendwo links vom unteren Punkt des Diagramms liegen (wie im folgenden Diagramm gezeigt).  In diesem Fall (der Wert wird sein) von 0,001 bis 0,01. <br><br><img src="https://habrastorage.org/webt/tq/sw/m7/tqswm7bda8qr9zed3h9s3fafbj4.jpeg"><br><br><h4>  Das obige sieht nützlich aus.  Wie fange ich an, es zu benutzen? </h4><br>  Momentan <i>enthält</i> das von Jeremy Howard entwickelte <i>fast.ia-</i> Paket eine vorgefertigte Funktion. Dies ist eine Art Abstraktion / Add-On über der Pytorch-Bibliothek (ähnlich wie bei Keras und Tensorflow). <br><br>  Es ist nur erforderlich, den folgenden Befehl einzugeben, um mit der Suche nach dem optimalen Lerngeschwindigkeitskoeffizienten zu beginnen, bevor das neuronale Netzwerk (gestartet) wird. <br><br><pre><code class="python hljs">learn.lr_find() learn.sched.plot_lr()</code> </pre> <br><br><h3>  <b>Das Modell verbessern</b> </h3><br>  Wir haben also darüber gesprochen, wie hoch der Lerngeschwindigkeitskoeffizient ist, welchen Wert er hat und wie er seinen optimalen Wert erreicht, bevor er mit dem Training des Modells selbst beginnt. <br>  Jetzt konzentrieren wir uns darauf, wie der Lerngeschwindigkeitsfaktor zum Optimieren von Modellen verwendet werden kann. <br><br><h4>  Konventionelle Weisheit </h4><br>  Wenn der Benutzer seinen Lerngeschwindigkeitskoeffizienten festlegt und mit dem Training des Modells beginnt, muss er normalerweise warten, bis der Lerngeschwindigkeitskoeffizient zu fallen beginnt und das Modell den optimalen Wert erreicht. <br><br>  Ab dem Moment, in dem der Gradient ein Plateau erreicht, wird es jedoch schwieriger, die Werte der Verlustfunktion beim Training des Modells zu verbessern.  In [3] weist Dauphin darauf hin, dass die Schwierigkeit bei der Minimierung der Verlustfunktion vom Sattelpunkt und nicht vom lokalen Minimum herrührt. <br><br><img src="https://habrastorage.org/webt/-t/jm/uw/-tjmuwg7a8etbhsc36cw97flhh8.png"><br>  <b>Ein Sattelpunkt auf der Oberfläche von Fehlern.</b>  <b>Ein Sattelpunkt ist ein Punkt aus dem Definitionsbereich einer Funktion, der für eine bestimmte Funktion stationär ist, aber nicht ihr lokales Extremum ist</b> .  (ImgCredit: safaribooksonline) <br><br><h4>  Wie kann dies vermieden werden? </h4><br>  Ich schlage vor, mehrere Optionen in Betracht zu ziehen.  Einer von ihnen, allgemein, unter Verwendung des Zitats aus [1], <br><blockquote>  ... anstatt einen festen Wert für den Lerngeschwindigkeitskoeffizienten zu verwenden und ihn im Laufe der Zeit zu verringern, werden wir den Lerngeschwindigkeitskoeffizienten in jeder Iteration gemäß einer zyklischen Funktion f ändern, wenn das Training unseren Verlust nicht mehr glättet.  Jede Schleife hat - gemessen an der Anzahl der Iterationen - eine feste Länge.  Diese Methode ermöglicht es, den Lerngeschwindigkeitskoeffizienten zwischen vernünftigen Grenzwerten zu variieren.  Dies hilft wirklich, denn wenn wir in den Sattelpunkten stecken bleiben, erhalten wir durch Erhöhen des Lerngeschwindigkeitskoeffizienten einen schnelleren Schnittpunkt des Plateaus der Sattelpunkte </blockquote><br>  In [2] schlägt Leslie die „Dreiecksmethode“ vor, bei der der Lerngeschwindigkeitskoeffizient nach jeder von mehreren Iterationen überarbeitet wird. <br><br><img src="https://habrastorage.org/webt/j4/_w/ga/j4_wga1vdfg3qmiovtzdbtrmb4e.jpeg"><br><br><img src="https://habrastorage.org/webt/pn/tu/f5/pntuf5w2svpsbk9nsiyme8iqf98.jpeg"><br>  <b>"Die Methode der Dreiecke" und "Die Methode der Dreiecke-2" sind Methoden zum zyklischen Testen von Lernratenkoeffizienten, die von Leslie N. Smith vorgeschlagen wurden.</b>  <b>In der oberen Grafik werden das minimale und maximale Ir gleich gehalten.</b> <br><br>  Eine andere Methode, die nicht weniger beliebt ist und als „stochastischer Gradientenabstieg mit warmem Reset“ bezeichnet wird, wurde von Lonchilov &amp; Hutter [6] vorgeschlagen.  Diese Methode, die auf der Verwendung der Kosinusfunktion als zyklische Funktion basiert, startet den Koeffizienten der Lerngeschwindigkeit am Maximalpunkt in jedem Zyklus neu.  Das Auftreten des "Hot" -Bits beruht auf der Tatsache, dass der Lernratenkoeffizient beim Neustart nicht von der Nullstufe ausgeht, sondern von den Parametern, zu denen das Modell den vorherigen Schritt erreicht hat. <br><br>  Da diese Methode Variationen aufweist, zeigt die folgende Grafik eine der Methoden ihrer Anwendung, bei der jeder Zyklus an dasselbe Zeitintervall gebunden ist. <br><br><img src="https://habrastorage.org/webt/kb/8o/pe/kb8opexd3ppx8ynj2bj7dtaphzg.jpeg"><br>  <b>SGDR - Grafik, Lernratenkoeffizient vs.</b>  <b>Iterationen</b> <b><br></b> <br>  Auf diese Weise können wir die Dauer des Trainings verkürzen, indem wir von Zeit zu Zeit einfach über die „Spitzen“ springen (wie unten gezeigt). <br><br><img src="https://habrastorage.org/webt/85/cn/-b/85cn-blk82myspio7d5pxfwfeqk.png"><br>  <b>Vergleich von festen und zyklischen Lernratenkoeffizienten</b> (img credit: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv.org/abs/1704.00109</a> <br>  Diese Methode spart nicht nur Zeit, sondern verbessert laut Studien auch die Klassifizierungsgenauigkeit ohne Optimierung und für weniger Iterationen. <br><br><h3>  Transfer-Lernverhältnis in Transfer-Lernen </h3><br>  Im Verlauf von <i>fast.ai liegt der</i> Schwerpunkt auf der Verwaltung eines vorab trainierten Modells zur Lösung von Problemen der künstlichen Intelligenz.  Bei der Lösung von Bildklassifizierungsproblemen werden die Schüler beispielsweise darin geschult, vorab trainierte Modelle wie VGG und Resnet50 zu verwenden und diese mit der Stichprobe der Bilddaten zu verknüpfen, die vorhergesagt werden müssen. <br>  Um zusammenzufassen, wie das Modell im Programm <i>fast.ai</i> erstellt wird (nicht zu verwechseln mit dem Paket <i>fast. Ai</i> - das Paket aus dem Programm), werden im Folgenden die Schritte aufgeführt, die wir in einer normalen Situation ausführen werden: <br><br><ol><li>  Datenerweiterung aktivieren und Vorberechnung = True </li><li>  Verwenden Sie Ir_find (), um den höchsten Lernratenkoeffizienten zu ermitteln, bei dem sich der Verlust noch deutlich verbessert. </li><li>  Trainiere die letzte Ebene vorberechneter Aktivierungen für die 1-2-Ära. </li><li>  Trainieren Sie die letzte Schicht mit Datenverstärkung (d. H. Berechnen = falsch) für 1-2 Epochen mit dem Zyklus _len 1. </li><li>  Alle Schichten auftauen. </li><li>  Platzieren Sie frühere Ebenen mit einem Lerngeschwindigkeitsfaktor, der 3x-10x unter der nächsthöheren Ebene liegt </li><li>  Ir_find () wiederverwenden </li><li>  Trainiere ein komplettes Netzwerk mit dem Zyklus _mult = 2 = 2, bis es mit der Umschulung beginnt. </li></ol><br>  Möglicherweise stellen Sie fest, dass die Schritte zwei, fünf und sieben (oben) mit der Lernrate zusammenhängen.  In einem früheren Teil unseres Beitrags haben wir den Punkt der zweitgenannten Schritte hervorgehoben - wo wir angesprochen haben, wie Sie den besten Lerngeschwindigkeitskoeffizienten erhalten, bevor Sie mit dem Training des Modells beginnen. <br><br>  Im nächsten Abschnitt haben wir darüber gesprochen, wie Sie die Trainingszeit mithilfe von SGDR verkürzen und den Lerngeschwindigkeitsfaktor regelmäßig neu starten können, um die Genauigkeit zu verbessern und Bereiche zu vermeiden, in denen der Gradient nahe Null liegt. <br>  Im letzten Abschnitt werden wir das Konzept des differenzierten Lernens ansprechen und erklären, wie es verwendet wird, um den Lerngeschwindigkeitskoeffizienten zu bestimmen, wenn ein trainiertes Modell mit einem vorab trainierten ... <br><br><h3>  Was ist differenzielles Lernen? </h3><br>  Dies ist eine Methode, bei der während des Trainings verschiedene Trainingsgeschwindigkeitsfaktoren im Netzwerk eingestellt werden.  Es bietet eine Alternative zu der Art und Weise, wie Benutzer normalerweise Lerngeschwindigkeitsfaktoren anpassen - nämlich die Verwendung des gleichen Lerngeschwindigkeitsfaktors über das Netzwerk während des Trainings. <br><br><img src="https://habrastorage.org/webt/xb/aw/-e/xbaw-e9-pehhvaeylpidgeykwwo.png"><br>  <b>Der Grund, warum ich Twitter liebe, ist eine direkte Antwort von der Person selbst.</b> <b><br></b>  (Während er diesen Beitrag schrieb, veröffentlichte Jeremy einen Artikel mit Sebastian Ruder, der sich noch tiefer mit diesem Thema befasste. Ich glaube, der differenzielle Koeffizient der Lerngeschwindigkeit hat jetzt einen anderen Namen - diskriminierende exakte Abstimmung :) <br><br>  Um das Konzept klarer zu demonstrieren, können wir uns auf das folgende Diagramm beziehen, in dem das zuvor trainierte Modell in drei Gruppen „aufgeteilt“ ist, wobei jede mit zunehmendem Wert des Lerngeschwindigkeitskoeffizienten angepasst wird. <br><br><img src="https://habrastorage.org/webt/cv/3l/ax/cv3laxkfy-60oz9ftqnhotviqss.jpeg"><br>  <b>CNN-Beispiel mit differenziertem Lernratenkoeffizienten</b> .  Bildnachweis von [3] <br><br>  Diese Konfigurationsmethode basiert auf dem folgenden Verständnis: Die ersten Ebenen enthalten normalerweise sehr kleine Details der Daten, wie z. B. Linien und Winkel. Wir werden nicht versuchen, viel zu ändern und die darin enthaltenen Informationen zu speichern.  Im Allgemeinen besteht keine ernsthafte Notwendigkeit, ihr Gewicht auf eine große Anzahl zu ändern. <br><br>  Im Gegenteil, für nachfolgende Ebenen - wie die auf dem Bild grün gestrichenen, in denen wir detaillierte Anzeichen der Daten erhalten, wie z. B. Weiß der Augen, des Mundes oder der Nase - verschwindet die Notwendigkeit, sie zu speichern. <br><br><h4>  Wie ist dies im Vergleich zu anderen Feinabstimmungsmethoden? </h4><br>  In [9] wurde bewiesen, dass die Feinabstimmung des gesamten Modells zu kostspielig wäre, da Benutzer über 100 Schichten erhalten können.  In den meisten Fällen wird das Modell schichtweise optimiert. <br><br>  Dies ist jedoch der Grund für eine Reihe von Anforderungen, die sogenannten  Beeinträchtigung der Parallelität und erfordert mehrere Eingaben über einen Datensatz, was zu einem Übertraining kleiner Sätze führt. <br><br>  Wir haben auch gezeigt, dass die in [9] vorgestellten Methoden sowohl die Genauigkeit verbessern als auch die Anzahl der Fehler bei verschiedenen Aufgaben im Zusammenhang mit der NRL-Klassifizierung verringern können. <br><br><img src="https://habrastorage.org/webt/by/no/yr/bynoyrrrl8edulvd8udbo8hv-uk.png"><br>  <b>Ergebnisse aus der Quelle [9]</b> <br><br>  Referenzen: <br>  [1] Verbesserung der Art und Weise, wie wir mit der Lernrate arbeiten. <br>  [2] Die zyklische Lernratentechnik. <br>  [3] Transferlernen mit unterschiedlichen Lernraten. <br>  [4] Leslie N. Smith.  Zyklische Lernraten für das Training neuronaler Netze. <br>  [5] Schätzung einer optimalen Lernrate für ein tiefes neuronales Netzwerk <br>  [6] Stochastischer Gradientenabstieg mit Warmstart <br>  [7] Optimierung für Deep Learning-Highlights im Jahr 2017 <br>  [8] Lektion 1 Notizbuch, fast.ai Teil 1 V2 <br>  [9] Fein abgestimmte Sprachmodelle für die Textklassifizierung </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de469931/">https://habr.com/ru/post/de469931/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de469919/index.html">Überprüfung des Telegramm-Open-Network-Codes durch den PVS-Studio-Analysator</a></li>
<li><a href="../de469921/index.html">[Fall] Überwachung der Luftqualität in einem Bauerndorf</a></li>
<li><a href="../de469923/index.html">Unerwartete Sicherheitslücke in Apple-Produkten. Völlig unerwartet</a></li>
<li><a href="../de469925/index.html">"F # ist nicht schwerer zu meistern als Entity Framework oder WPF": Interview mit Scott Vlashin</a></li>
<li><a href="../de469927/index.html">10 Gebote des Entwicklers</a></li>
<li><a href="../de469933/index.html">Wie man die Effektivität misst und die Probleme von Entwicklern löst, wenn man hundert hat</a></li>
<li><a href="../de469935/index.html">Kurs "Grundlagen effektiver Arbeit mit Wolfram Technologies": mehr als 13 Stunden Videovorträge, Theorie und Probleme</a></li>
<li><a href="../de469941/index.html">Extreme Nematoden vom Mono Lake: Schwimmen Sie in Arsen und überleben Sie</a></li>
<li><a href="../de469945/index.html">Ist es wichtig, dass Computer und Menschen die Welt anders sehen?</a></li>
<li><a href="../de469947/index.html">Kleine Docker-Bilder, die an sich glaubten *</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>