<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìü üÜó üìø Was ist ein Lerngeschwindigkeitsfaktor und wie verbessert er die Eigenschaften des tiefen Lernens? üíï üë©üèΩ‚ÄçüöÄ üö¥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dieser Artikel ist mein Versuch, meine Meinung zu folgenden Aspekten zu √§u√üern: 



1. Was ist ein Lerngeschwindigkeitsfaktor und welchen Wert hat er?...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Was ist ein Lerngeschwindigkeitsfaktor und wie verbessert er die Eigenschaften des tiefen Lernens?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469931/">  Dieser Artikel ist mein Versuch, meine Meinung zu folgenden Aspekten zu √§u√üern: <br><br><ol><li>  Was ist ein Lerngeschwindigkeitsfaktor und welchen Wert hat er? </li><li>  Wie w√§hlt man diesen Koeffizienten beim Training von Modellen? </li><li>  Warum ist es notwendig, den Koeffizienten der Lerngeschwindigkeit w√§hrend des Trainings von Modellen zu √§ndern? </li><li>  Was tun mit einem Lerngeschwindigkeitsfaktor bei Verwendung eines vorab trainierten Modells? </li></ol><br>  Der gr√∂√üte Teil dieses Beitrags basiert auf Materialien, die von <i>fast.ai</i> erstellt wurden: [1], [2], [5] und [3] - eine pr√§gnante Version ihrer Arbeit, die zum schnellsten Verst√§ndnis des Wesens des Problems bestimmt ist.  Um sich mit den Details vertraut zu machen, wird empfohlen, auf die unten angegebenen Links zu klicken. <br><a name="habracut"></a><br><h3>  <b>Was ist ein Lerngeschwindigkeitsfaktor?</b> </h3><br>  Der Lerngeschwindigkeitskoeffizient ist ein Hyperparameter, der die Reihenfolge bestimmt, in der wir unsere Skalen unter Ber√ºcksichtigung der Verlustfunktion beim Gradientenabstieg anpassen.  Je niedriger der Wert, desto langsamer bewegen wir uns entlang der Neigung.  Wenn wir einen niedrigen Lerngeschwindigkeitskoeffizienten verwenden, k√∂nnen wir zwar einen positiven Effekt erzielen, indem wir kein einziges lokales Minimum verpassen. Dies kann jedoch auch bedeuten, dass wir viel Zeit f√ºr die Konvergenz aufwenden m√ºssen, insbesondere wenn wir uns in der Hochebene befinden. <br><br>  Die Beziehung wird durch die folgende Formel veranschaulicht <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo>&amp;#x2022;</mo><mi>n</mi><mi>e</mi><msub><mi>w</mi><mi>w</mi></msub><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>e</mi><msub><mi>s</mi><mi>g</mi></msub><mi>e</mi><mi>w</mi><mi>i</mi><mi>c</mi><mi>h</mi><mi>t</mi><mo>&amp;#x2212;</mo><mi>L</mi><mi>e</mi><mi>r</mi><mi>n</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>&amp;#x2217;</mo><mi>G</mi><mi>r</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.486ex" height="2.66ex" viewBox="0 -780.1 26042.4 1145.2" role="img" focusable="false" style="vertical-align: -0.848ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMAIN-2219" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-6E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="1101" y="0"></use><g transform="translate(1567,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-77" x="1013" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="2890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="3357" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-67" x="3702" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-68" x="4183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="4759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMAIN-3D" x="5398" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="6455" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-78" x="6921" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="7494" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-73" x="7839" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="8309" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="8670" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="9016" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-72" x="9482" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="9934" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-6E" x="10400" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-64" x="11001" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="11524" y="0"></use><g transform="translate(11991,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-73" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-67" x="663" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="12900" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-77" x="13366" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="14083" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-63" x="14428" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-68" x="14862" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="15438" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMAIN-2212" x="16022" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-4C" x="17023" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="17704" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-72" x="18171" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-6E" x="18622" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-72" x="19223" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-61" x="19674" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="20204" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="20565" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMAIN-2217" x="21254" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-47" x="21977" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-72" x="22763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-61" x="23215" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-64" x="23744" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-69" x="24268" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-65" x="24613" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-6E" x="25080" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhimpclZzZJ2RSuRcKhUGhzCfvlt_Q#MJMATHI-74" x="25680" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo>‚Ä¢</mo><mi>n</mi><mi>e</mi><msub><mi>w</mi><mi>w</mi></msub><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>e</mi><msub><mi>s</mi><mi>g</mi></msub><mi>e</mi><mi>w</mi><mi>i</mi><mi>c</mi><mi>h</mi><mi>t</mi><mo>‚àí</mo><mi>L</mi><mi>e</mi><mi>r</mi><mi>n</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>‚àó</mo><mi>G</mi><mi>r</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> ‚Ä¢ new_weight = existierendes_gewicht - Lernrate * Gradient </script></p><br><img src="https://habrastorage.org/webt/dn/j1/nj/dnj1njm2womrahwlbv_dzs25xqs.jpeg"><br>  <b>Gradientenabstieg mit kleinen (oben) und gro√üen (unten) Lerngeschwindigkeitsfaktoren.</b>  <b>Quelle: Andrew Ngs Kurs √ºber maschinelles Lernen auf Coursera</b> <b><br></b> <br>  Meistens wird der Lerngeschwindigkeitsfaktor vom Benutzer willk√ºrlich eingestellt.  Im besten Fall kann er sich f√ºr ein intuitives Verst√§ndnis dessen, welcher Wert f√ºr die Bestimmung des Lerngeschwindigkeitskoeffizienten am besten geeignet ist, auf fr√ºhere Experimente (oder eine andere Art von Trainingsmaterial) verlassen. <br><br>  Im Grunde ist es schwierig genug, den richtigen Wert zu w√§hlen.  Das folgende Diagramm zeigt verschiedene Szenarien, die auftreten k√∂nnen, wenn der Benutzer die Lerngeschwindigkeit unabh√§ngig anpasst. <br><br><img src="https://habrastorage.org/webt/qm/uh/qd/qmuhqdtbcagnnzltgnfy53wuxxi.jpeg"><br>  <b>Der Einfluss verschiedener Lernratenfaktoren auf die Konvergenz.</b>  <b>(Bildnachweis: cs231n)</b> <b><br></b> <br>  Dar√ºber hinaus beeinflusst der Lerngeschwindigkeitsfaktor, wie schnell unser Modell ein lokales Minimum erreicht (auch bekannt als die beste Genauigkeit).  Die richtige Wahl von Anfang an garantiert somit weniger Zeitverschwendung f√ºr das Training des Modells.  Je weniger Schulungszeit, desto weniger Geld wird f√ºr die GPU-Rechenleistung in der Cloud ausgegeben. <br><br><h4>  Gibt es eine bequemere M√∂glichkeit, den Lernkoeffizienten zu bestimmen? <br></h4><br>  In Absatz 3.3.  " <i>Zyklische Lernratenkoeffizienten f√ºr neuronale Netze</i> " Leslie Smith verteidigte den folgenden Punkt: Die Effektivit√§t der Lerngeschwindigkeit kann gesch√§tzt werden, indem das Modell mit einer anf√§nglich festgelegten niedrigen Lerngeschwindigkeit trainiert wird, die dann in jeder Iteration (linear oder exponentiell) zunimmt. <br><br><img src="https://habrastorage.org/webt/j9/zu/yi/j9zuyi5do_thph6ylxtaxetvm7q.jpeg"><br>  <b>Der Lerngeschwindigkeitsfaktor erh√∂ht sich nach jedem Minipaket.</b> <b><br></b> <br>  Wenn wir die Werte der Indikatoren bei jeder Iteration festlegen, werden wir sehen, dass mit zunehmender Lerngeschwindigkeit ein Punkt erreicht wird (an dem), an dem die Werte der Verlustfunktion nicht mehr abnehmen und zunehmen.  In der Praxis sollte unsere Lerngeschwindigkeit idealerweise irgendwo links vom unteren Punkt des Diagramms liegen (wie im folgenden Diagramm gezeigt).  In diesem Fall (der Wert wird sein) von 0,001 bis 0,01. <br><br><img src="https://habrastorage.org/webt/tq/sw/m7/tqswm7bda8qr9zed3h9s3fafbj4.jpeg"><br><br><h4>  Das obige sieht n√ºtzlich aus.  Wie fange ich an, es zu benutzen? </h4><br>  Momentan <i>enth√§lt</i> das von Jeremy Howard entwickelte <i>fast.ia-</i> Paket eine vorgefertigte Funktion. Dies ist eine Art Abstraktion / Add-On √ºber der Pytorch-Bibliothek (√§hnlich wie bei Keras und Tensorflow). <br><br>  Es ist nur erforderlich, den folgenden Befehl einzugeben, um mit der Suche nach dem optimalen Lerngeschwindigkeitskoeffizienten zu beginnen, bevor das neuronale Netzwerk (gestartet) wird. <br><br><pre><code class="python hljs">learn.lr_find() learn.sched.plot_lr()</code> </pre> <br><br><h3>  <b>Das Modell verbessern</b> </h3><br>  Wir haben also dar√ºber gesprochen, wie hoch der Lerngeschwindigkeitskoeffizient ist, welchen Wert er hat und wie er seinen optimalen Wert erreicht, bevor er mit dem Training des Modells selbst beginnt. <br>  Jetzt konzentrieren wir uns darauf, wie der Lerngeschwindigkeitsfaktor zum Optimieren von Modellen verwendet werden kann. <br><br><h4>  Konventionelle Weisheit </h4><br>  Wenn der Benutzer seinen Lerngeschwindigkeitskoeffizienten festlegt und mit dem Training des Modells beginnt, muss er normalerweise warten, bis der Lerngeschwindigkeitskoeffizient zu fallen beginnt und das Modell den optimalen Wert erreicht. <br><br>  Ab dem Moment, in dem der Gradient ein Plateau erreicht, wird es jedoch schwieriger, die Werte der Verlustfunktion beim Training des Modells zu verbessern.  In [3] weist Dauphin darauf hin, dass die Schwierigkeit bei der Minimierung der Verlustfunktion vom Sattelpunkt und nicht vom lokalen Minimum herr√ºhrt. <br><br><img src="https://habrastorage.org/webt/-t/jm/uw/-tjmuwg7a8etbhsc36cw97flhh8.png"><br>  <b>Ein Sattelpunkt auf der Oberfl√§che von Fehlern.</b>  <b>Ein Sattelpunkt ist ein Punkt aus dem Definitionsbereich einer Funktion, der f√ºr eine bestimmte Funktion station√§r ist, aber nicht ihr lokales Extremum ist</b> .  (ImgCredit: safaribooksonline) <br><br><h4>  Wie kann dies vermieden werden? </h4><br>  Ich schlage vor, mehrere Optionen in Betracht zu ziehen.  Einer von ihnen, allgemein, unter Verwendung des Zitats aus [1], <br><blockquote>  ... anstatt einen festen Wert f√ºr den Lerngeschwindigkeitskoeffizienten zu verwenden und ihn im Laufe der Zeit zu verringern, werden wir den Lerngeschwindigkeitskoeffizienten in jeder Iteration gem√§√ü einer zyklischen Funktion f √§ndern, wenn das Training unseren Verlust nicht mehr gl√§ttet.  Jede Schleife hat - gemessen an der Anzahl der Iterationen - eine feste L√§nge.  Diese Methode erm√∂glicht es, den Lerngeschwindigkeitskoeffizienten zwischen vern√ºnftigen Grenzwerten zu variieren.  Dies hilft wirklich, denn wenn wir in den Sattelpunkten stecken bleiben, erhalten wir durch Erh√∂hen des Lerngeschwindigkeitskoeffizienten einen schnelleren Schnittpunkt des Plateaus der Sattelpunkte </blockquote><br>  In [2] schl√§gt Leslie die ‚ÄûDreiecksmethode‚Äú vor, bei der der Lerngeschwindigkeitskoeffizient nach jeder von mehreren Iterationen √ºberarbeitet wird. <br><br><img src="https://habrastorage.org/webt/j4/_w/ga/j4_wga1vdfg3qmiovtzdbtrmb4e.jpeg"><br><br><img src="https://habrastorage.org/webt/pn/tu/f5/pntuf5w2svpsbk9nsiyme8iqf98.jpeg"><br>  <b>"Die Methode der Dreiecke" und "Die Methode der Dreiecke-2" sind Methoden zum zyklischen Testen von Lernratenkoeffizienten, die von Leslie N. Smith vorgeschlagen wurden.</b>  <b>In der oberen Grafik werden das minimale und maximale Ir gleich gehalten.</b> <br><br>  Eine andere Methode, die nicht weniger beliebt ist und als ‚Äûstochastischer Gradientenabstieg mit warmem Reset‚Äú bezeichnet wird, wurde von Lonchilov &amp; Hutter [6] vorgeschlagen.  Diese Methode, die auf der Verwendung der Kosinusfunktion als zyklische Funktion basiert, startet den Koeffizienten der Lerngeschwindigkeit am Maximalpunkt in jedem Zyklus neu.  Das Auftreten des "Hot" -Bits beruht auf der Tatsache, dass der Lernratenkoeffizient beim Neustart nicht von der Nullstufe ausgeht, sondern von den Parametern, zu denen das Modell den vorherigen Schritt erreicht hat. <br><br>  Da diese Methode Variationen aufweist, zeigt die folgende Grafik eine der Methoden ihrer Anwendung, bei der jeder Zyklus an dasselbe Zeitintervall gebunden ist. <br><br><img src="https://habrastorage.org/webt/kb/8o/pe/kb8opexd3ppx8ynj2bj7dtaphzg.jpeg"><br>  <b>SGDR - Grafik, Lernratenkoeffizient vs.</b>  <b>Iterationen</b> <b><br></b> <br>  Auf diese Weise k√∂nnen wir die Dauer des Trainings verk√ºrzen, indem wir von Zeit zu Zeit einfach √ºber die ‚ÄûSpitzen‚Äú springen (wie unten gezeigt). <br><br><img src="https://habrastorage.org/webt/85/cn/-b/85cn-blk82myspio7d5pxfwfeqk.png"><br>  <b>Vergleich von festen und zyklischen Lernratenkoeffizienten</b> (img credit: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv.org/abs/1704.00109</a> <br>  Diese Methode spart nicht nur Zeit, sondern verbessert laut Studien auch die Klassifizierungsgenauigkeit ohne Optimierung und f√ºr weniger Iterationen. <br><br><h3>  Transfer-Lernverh√§ltnis in Transfer-Lernen </h3><br>  Im Verlauf von <i>fast.ai liegt der</i> Schwerpunkt auf der Verwaltung eines vorab trainierten Modells zur L√∂sung von Problemen der k√ºnstlichen Intelligenz.  Bei der L√∂sung von Bildklassifizierungsproblemen werden die Sch√ºler beispielsweise darin geschult, vorab trainierte Modelle wie VGG und Resnet50 zu verwenden und diese mit der Stichprobe der Bilddaten zu verkn√ºpfen, die vorhergesagt werden m√ºssen. <br>  Um zusammenzufassen, wie das Modell im Programm <i>fast.ai</i> erstellt wird (nicht zu verwechseln mit dem Paket <i>fast. Ai</i> - das Paket aus dem Programm), werden im Folgenden die Schritte aufgef√ºhrt, die wir in einer normalen Situation ausf√ºhren werden: <br><br><ol><li>  Datenerweiterung aktivieren und Vorberechnung = True </li><li>  Verwenden Sie Ir_find (), um den h√∂chsten Lernratenkoeffizienten zu ermitteln, bei dem sich der Verlust noch deutlich verbessert. </li><li>  Trainiere die letzte Ebene vorberechneter Aktivierungen f√ºr die 1-2-√Ñra. </li><li>  Trainieren Sie die letzte Schicht mit Datenverst√§rkung (d. H. Berechnen = falsch) f√ºr 1-2 Epochen mit dem Zyklus _len 1. </li><li>  Alle Schichten auftauen. </li><li>  Platzieren Sie fr√ºhere Ebenen mit einem Lerngeschwindigkeitsfaktor, der 3x-10x unter der n√§chsth√∂heren Ebene liegt </li><li>  Ir_find () wiederverwenden </li><li>  Trainiere ein komplettes Netzwerk mit dem Zyklus _mult = 2 = 2, bis es mit der Umschulung beginnt. </li></ol><br>  M√∂glicherweise stellen Sie fest, dass die Schritte zwei, f√ºnf und sieben (oben) mit der Lernrate zusammenh√§ngen.  In einem fr√ºheren Teil unseres Beitrags haben wir den Punkt der zweitgenannten Schritte hervorgehoben - wo wir angesprochen haben, wie Sie den besten Lerngeschwindigkeitskoeffizienten erhalten, bevor Sie mit dem Training des Modells beginnen. <br><br>  Im n√§chsten Abschnitt haben wir dar√ºber gesprochen, wie Sie die Trainingszeit mithilfe von SGDR verk√ºrzen und den Lerngeschwindigkeitsfaktor regelm√§√üig neu starten k√∂nnen, um die Genauigkeit zu verbessern und Bereiche zu vermeiden, in denen der Gradient nahe Null liegt. <br>  Im letzten Abschnitt werden wir das Konzept des differenzierten Lernens ansprechen und erkl√§ren, wie es verwendet wird, um den Lerngeschwindigkeitskoeffizienten zu bestimmen, wenn ein trainiertes Modell mit einem vorab trainierten ... <br><br><h3>  Was ist differenzielles Lernen? </h3><br>  Dies ist eine Methode, bei der w√§hrend des Trainings verschiedene Trainingsgeschwindigkeitsfaktoren im Netzwerk eingestellt werden.  Es bietet eine Alternative zu der Art und Weise, wie Benutzer normalerweise Lerngeschwindigkeitsfaktoren anpassen - n√§mlich die Verwendung des gleichen Lerngeschwindigkeitsfaktors √ºber das Netzwerk w√§hrend des Trainings. <br><br><img src="https://habrastorage.org/webt/xb/aw/-e/xbaw-e9-pehhvaeylpidgeykwwo.png"><br>  <b>Der Grund, warum ich Twitter liebe, ist eine direkte Antwort von der Person selbst.</b> <b><br></b>  (W√§hrend er diesen Beitrag schrieb, ver√∂ffentlichte Jeremy einen Artikel mit Sebastian Ruder, der sich noch tiefer mit diesem Thema befasste. Ich glaube, der differenzielle Koeffizient der Lerngeschwindigkeit hat jetzt einen anderen Namen - diskriminierende exakte Abstimmung :) <br><br>  Um das Konzept klarer zu demonstrieren, k√∂nnen wir uns auf das folgende Diagramm beziehen, in dem das zuvor trainierte Modell in drei Gruppen ‚Äûaufgeteilt‚Äú ist, wobei jede mit zunehmendem Wert des Lerngeschwindigkeitskoeffizienten angepasst wird. <br><br><img src="https://habrastorage.org/webt/cv/3l/ax/cv3laxkfy-60oz9ftqnhotviqss.jpeg"><br>  <b>CNN-Beispiel mit differenziertem Lernratenkoeffizienten</b> .  Bildnachweis von [3] <br><br>  Diese Konfigurationsmethode basiert auf dem folgenden Verst√§ndnis: Die ersten Ebenen enthalten normalerweise sehr kleine Details der Daten, wie z. B. Linien und Winkel. Wir werden nicht versuchen, viel zu √§ndern und die darin enthaltenen Informationen zu speichern.  Im Allgemeinen besteht keine ernsthafte Notwendigkeit, ihr Gewicht auf eine gro√üe Anzahl zu √§ndern. <br><br>  Im Gegenteil, f√ºr nachfolgende Ebenen - wie die auf dem Bild gr√ºn gestrichenen, in denen wir detaillierte Anzeichen der Daten erhalten, wie z. B. Wei√ü der Augen, des Mundes oder der Nase - verschwindet die Notwendigkeit, sie zu speichern. <br><br><h4>  Wie ist dies im Vergleich zu anderen Feinabstimmungsmethoden? </h4><br>  In [9] wurde bewiesen, dass die Feinabstimmung des gesamten Modells zu kostspielig w√§re, da Benutzer √ºber 100 Schichten erhalten k√∂nnen.  In den meisten F√§llen wird das Modell schichtweise optimiert. <br><br>  Dies ist jedoch der Grund f√ºr eine Reihe von Anforderungen, die sogenannten  Beeintr√§chtigung der Parallelit√§t und erfordert mehrere Eingaben √ºber einen Datensatz, was zu einem √úbertraining kleiner S√§tze f√ºhrt. <br><br>  Wir haben auch gezeigt, dass die in [9] vorgestellten Methoden sowohl die Genauigkeit verbessern als auch die Anzahl der Fehler bei verschiedenen Aufgaben im Zusammenhang mit der NRL-Klassifizierung verringern k√∂nnen. <br><br><img src="https://habrastorage.org/webt/by/no/yr/bynoyrrrl8edulvd8udbo8hv-uk.png"><br>  <b>Ergebnisse aus der Quelle [9]</b> <br><br>  Referenzen: <br>  [1] Verbesserung der Art und Weise, wie wir mit der Lernrate arbeiten. <br>  [2] Die zyklische Lernratentechnik. <br>  [3] Transferlernen mit unterschiedlichen Lernraten. <br>  [4] Leslie N. Smith.  Zyklische Lernraten f√ºr das Training neuronaler Netze. <br>  [5] Sch√§tzung einer optimalen Lernrate f√ºr ein tiefes neuronales Netzwerk <br>  [6] Stochastischer Gradientenabstieg mit Warmstart <br>  [7] Optimierung f√ºr Deep Learning-Highlights im Jahr 2017 <br>  [8] Lektion 1 Notizbuch, fast.ai Teil 1 V2 <br>  [9] Fein abgestimmte Sprachmodelle f√ºr die Textklassifizierung </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de469931/">https://habr.com/ru/post/de469931/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de469919/index.html">√úberpr√ºfung des Telegramm-Open-Network-Codes durch den PVS-Studio-Analysator</a></li>
<li><a href="../de469921/index.html">[Fall] √úberwachung der Luftqualit√§t in einem Bauerndorf</a></li>
<li><a href="../de469923/index.html">Unerwartete Sicherheitsl√ºcke in Apple-Produkten. V√∂llig unerwartet</a></li>
<li><a href="../de469925/index.html">"F # ist nicht schwerer zu meistern als Entity Framework oder WPF": Interview mit Scott Vlashin</a></li>
<li><a href="../de469927/index.html">10 Gebote des Entwicklers</a></li>
<li><a href="../de469933/index.html">Wie man die Effektivit√§t misst und die Probleme von Entwicklern l√∂st, wenn man hundert hat</a></li>
<li><a href="../de469935/index.html">Kurs "Grundlagen effektiver Arbeit mit Wolfram Technologies": mehr als 13 Stunden Videovortr√§ge, Theorie und Probleme</a></li>
<li><a href="../de469941/index.html">Extreme Nematoden vom Mono Lake: Schwimmen Sie in Arsen und √ºberleben Sie</a></li>
<li><a href="../de469945/index.html">Ist es wichtig, dass Computer und Menschen die Welt anders sehen?</a></li>
<li><a href="../de469947/index.html">Kleine Docker-Bilder, die an sich glaubten *</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>