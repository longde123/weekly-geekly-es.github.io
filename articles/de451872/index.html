<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🚒 👨🏼‍🎓 📖 Python ist ein Assistent bei der Suche nach günstigen Flügen für diejenigen, die gerne reisen 👩🏽‍💼 🤾🏼 ♋️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Autorin des Artikels, dessen Übersetzung wir heute veröffentlichen, sagt, dass ihr Ziel darin besteht, über die Entwicklung eines Web-Scraper in P...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python ist ein Assistent bei der Suche nach günstigen Flügen für diejenigen, die gerne reisen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/451872/">  Die Autorin des Artikels, dessen Übersetzung wir heute veröffentlichen, sagt, dass ihr Ziel darin besteht, über die Entwicklung eines Web-Scraper in Python mit Selenium zu sprechen, der nach Flugpreisen sucht.  Bei der Suche nach Tickets werden flexible Daten verwendet (+ - 3 Tage relativ zu den angegebenen Daten).  Scraper speichert die Suchergebnisse in einer Excel-Datei und sendet an die Person, die sie gestartet hat, eine E-Mail mit allgemeinen Informationen darüber, was er gefunden hat.  Das Ziel dieses Projekts ist es, Reisenden zu helfen, die besten Angebote zu finden. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/xl/jo/rr/xljorr2xue-q63wegfrfyt5uxu4.jpeg"></a> <br><br>  Wenn Sie beim Umgang mit dem Material das Gefühl haben, verloren zu sein, lesen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen</a> Artikel. <br><a name="habracut"></a><br><h2>  <font color="#3AC1EF">Was suchen wir?</font> </h2><br>  Sie können das hier beschriebene System nach Ihren Wünschen verwenden.  Zum Beispiel habe ich damit nach Wochenendtouren und Tickets für meine Heimatstadt gesucht.  Wenn Sie es ernst meinen, profitable Tickets zu finden, können Sie das Skript auf dem Server ausführen (ein einfacher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Server</a> für 130 Rubel pro Monat ist dafür gut geeignet) und es ein- oder zweimal am Tag ausführen lassen.  Die Suchergebnisse werden per E-Mail an Sie gesendet.  Außerdem empfehle ich, dass Sie alles so konfigurieren, dass das Skript die Excel-Datei mit den Suchergebnissen im Dropbox-Ordner speichert, sodass Sie solche Dateien von überall und jederzeit anzeigen können. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d39/b7b/f3b/d39b7bf3b4f8c11fa9617ae308f01247.png"></div><br>  <i><font color="#999999">Ich habe noch keine fehlerhaften Tarife gefunden, aber ich glaube, dass dies möglich ist</font></i> <br><br>  Bei der Suche wird, wie bereits gesagt, ein „flexibles Datum“ verwendet, das Skript findet Angebote, die innerhalb von drei Tagen ab dem angegebenen Datum liegen.  Obwohl beim Starten des Skripts nur in einer Richtung nach Angeboten gesucht wird, kann es leicht verfeinert werden, sodass Daten in mehreren Flugrichtungen erfasst werden können.  Mit seiner Hilfe können Sie sogar nach fehlerhaften Tarifen suchen, solche Funde können sehr interessant sein. <br><br><h2>  <font color="#3AC1EF">Warum brauche ich einen anderen Web Scraper?</font> </h2><br>  Als ich anfing, Web Scraping zu machen, war es ehrlich gesagt nicht besonders interessant.  Ich wollte mehr Projekte im Bereich der prädiktiven Modellierung, der Finanzanalyse und möglicherweise im Bereich der Analyse der emotionalen Färbung von Texten durchführen.  Es stellte sich jedoch heraus, dass es sehr interessant war, herauszufinden, wie ein Programm erstellt werden kann, das Daten von Websites sammelt.  Als ich mich mit diesem Thema befasste, wurde mir klar, dass Web Scraping die „Engine“ des Internets ist. <br><br>  Sie können entscheiden, dass dies eine zu kühne Aussage ist.  Aber denken Sie daran, wie Google mit einem Web-Scraper begann, den Larry Page mit Java und Python erstellt hat.  Googlebots haben das Internet recherchiert und erkundet, um ihren Nutzern die bestmöglichen Antworten auf ihre Fragen zu bieten.  Web Scraping hat unendlich viele Anwendungen, und selbst wenn Sie im Bereich Data Science an etwas anderem interessiert sind, benötigen Sie einige Scraping-Kenntnisse, um Daten für die Analyse zu erhalten. <br><br>  Einige der hier verwendeten Tricks habe ich in einem wunderbaren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Buch</a> über Web Scraping gefunden, das ich kürzlich erworben habe.  Darin finden Sie viele einfache Beispiele und Ideen zur praktischen Anwendung des Studierten.  Darüber hinaus gibt es ein sehr interessantes Kapitel zum reCaptcha-Test-Bypass.  Für mich war dies eine Neuigkeit, da ich nicht wusste, dass es spezielle Tools und sogar ganze Services zur Lösung solcher Probleme gibt. <br><br><h2>  <font color="#3AC1EF">Reisen Sie gerne ?!</font> </h2><br>  Auf die einfache und ziemlich harmlose Frage in der Überschrift dieses Abschnitts kann man oft eine positive Antwort hören, die ein paar Reisegeschichten der Person enthält, zu der er gefragt wurde.  Die meisten von uns werden zustimmen, dass Reisen eine großartige Möglichkeit ist, in neue kulturelle Umgebungen einzutauchen und unseren Horizont zu erweitern.  Wenn Sie jedoch jemandem eine Frage stellen, ob er gerne nach Flugtickets sucht, bin ich sicher, dass die Antwort darauf alles andere als positiv sein wird.  Tatsächlich kommt hier Python zur Rettung. <br><br>  Die erste Aufgabe, die wir auf dem Weg zur Schaffung eines Systems zur Suche nach Informationen über Flugtickets lösen müssen, ist die Auswahl einer geeigneten Plattform, mit der wir Informationen aufnehmen.  Die Lösung für dieses Problem war für mich nicht einfach, aber am Ende entschied ich mich für den Kajak-Service.  Ich habe die Dienste von Momondo, Skyscanner, Expedia und anderen ausprobiert, aber die Schutzmechanismen gegen Roboter auf diesen Ressourcen waren undurchdringlich.  Nach mehreren Versuchen, bei denen ich mich mit Ampeln, Fußgängerüberwegen und Fahrrädern auseinandersetzen musste, um die Systeme davon zu überzeugen, dass ich ein Mensch bin, entschied ich, dass Kajak am besten zu mir passt, auch wenn dies hier der Fall ist Laden Sie in kurzer Zeit zu viele Seiten, und die Überprüfungen beginnen ebenfalls.  Ich habe es geschafft, dass der Bot in Abständen von 4 bis 6 Stunden Anfragen an die Site sendet, und alles hat gut funktioniert.  Bei der Arbeit mit Kayak treten auch regelmäßig Schwierigkeiten auf. Wenn Sie jedoch anfangen, sich mit Schecks zu beschäftigen, müssen Sie entweder manuell damit umgehen, dann den Bot starten oder einige Stunden warten, und die Schecks sollten aufhören.  Bei Bedarf können Sie den Code gut für eine andere Plattform anpassen. Wenn Sie dies tun, können Sie ihn in den Kommentaren melden. <br><br>  Wenn Sie gerade erst mit dem Web-Scraping beginnen und nicht wissen, warum einige Websites damit zu kämpfen haben, tun Sie sich selbst einen Gefallen und suchen Sie bei Google nach Wörtern, bevor Sie Ihr erstes Projekt in diesem Bereich starten "Web Scraping Etikette".  Ihre Experimente können früher enden als Sie denken, wenn Sie sich unangemessen mit Web Scraping beschäftigen. <br><br><h2>  <font color="#3AC1EF">Erste Schritte</font> </h2><br>  Hier ist eine allgemeine Übersicht darüber, was im Code unseres Web Scraper passieren wird: <br><br><ul><li>  Importieren Sie die erforderlichen Bibliotheken. </li><li>  Öffnen Sie die Registerkarte Google Chrome. </li><li>  Aufruf der Funktion, die den Bot startet, Übergabe der Stadt und des Datums, die bei der Suche nach Tickets verwendet werden. </li><li>  Diese Funktion empfängt die ersten Suchergebnisse, sortiert nach den Kriterien der attraktivsten (besten), und drückt die Taste, um zusätzliche Ergebnisse zu laden. </li><li>  Eine andere Funktion sammelt Daten von der gesamten Seite und gibt einen Datenrahmen zurück. </li><li>  Die beiden vorherigen Schritte werden unter Verwendung von Sortiertypen nach Ticketpreis (günstig) und Fluggeschwindigkeit (am schnellsten) ausgeführt. </li><li>  An den Skriptbenutzer wird eine E-Mail mit einer kurzen Zusammenfassung der Ticketpreise (günstigste Tickets und Durchschnittspreis) gesendet, und ein Datenrahmen mit Informationen, die nach den drei oben genannten Indikatoren sortiert sind, wird als Excel-Datei gespeichert. </li><li>  Alle oben genannten Aktionen werden in einem Zyklus nach einem bestimmten Zeitraum ausgeführt. </li></ul><br>  Es ist zu beachten, dass jedes Selenium-Projekt mit einem Webtreiber beginnt.  Ich benutze <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Chromedriver</a> , arbeite mit Google Chrome, aber es gibt andere Möglichkeiten.  Beliebt sind auch PhantomJS und Firefox.  Nachdem Sie den Treiber geladen haben, müssen Sie ihn in den entsprechenden Ordner legen. Damit ist die Vorbereitung für die Verwendung abgeschlossen.  In den ersten Zeilen unseres Skripts wird ein neuer Chrome-Tab geöffnet. <br><br>  Denken Sie daran, dass ich in meiner Geschichte nicht versuche, neue Horizonte zu eröffnen, um profitable Angebote für Flugtickets zu finden.  Es gibt viel fortgeschrittenere Techniken, um solche Angebote zu finden.  Ich möchte den Lesern dieses Materials nur eine einfache, aber praktische Möglichkeit bieten, dieses Problem zu lösen. <br><br>  Hier ist der Code, über den wir oben gesprochen haben. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep, strftime <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> randint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> selenium <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> webdriver <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> selenium.webdriver.common.keys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Keys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> smtplib <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> email.mime.multipart <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MIMEMultipart <span class="hljs-comment"><span class="hljs-comment">#      chromedriver! chromedriver_path = 'C:/{YOUR PATH HERE}/chromedriver_win32/chromedriver.exe' driver = webdriver.Chrome(executable_path=chromedriver_path) #     Chrome sleep(2)</span></span></code> </pre> <br>  Am Anfang des Codes sehen Sie die Paketimportbefehle, die in unserem Projekt verwendet werden.  <code>randint</code> wird also verwendet, damit der Bot für eine zufällige Anzahl von Sekunden "einschlafen" kann, bevor ein neuer <code>randint</code> wird.  Normalerweise kann kein einziger Bot darauf verzichten.  Wenn Sie den obigen Code ausführen, wird ein Chrome-Fenster geöffnet, in dem der Bot mit Websites arbeitet. <br><br>  Lassen Sie uns ein kleines Experiment durchführen und die Website kayak.com in einem separaten Fenster öffnen.  Wählen Sie die Stadt, von der aus wir fliegen, die Stadt, in die wir fliegen möchten, sowie die Flugdaten.  Bei der Auswahl der Daten überprüfen wir, ob der Bereich + -3 Tage beträgt.  Ich habe den Code unter Berücksichtigung dessen geschrieben, was die Site als Antwort auf solche Anfragen produziert.  Wenn Sie beispielsweise nur für bestimmte Daten nach Tickets suchen müssen, müssen Sie höchstwahrscheinlich den Bot-Code ändern.  Wenn ich über den Code spreche, gebe ich entsprechende Erklärungen ab, aber wenn Sie sich verwirrt fühlen, lassen Sie es mich wissen. <br><br>  Klicken Sie nun auf die Schaltfläche Start der Suche und sehen Sie sich den Link in der Adressleiste an.  Es sollte wie der Link aussehen, den ich im folgenden Beispiel verwende, in dem die <code>kayak</code> , in der die URL gespeichert ist, deklariert und die <code>get</code> Methode des Webtreibers verwendet wird.  Nach dem Klicken auf die Suchschaltfläche sollten die Ergebnisse auf der Seite angezeigt werden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae6/ac7/1f7/ae6ac71f71c92c6ff76d5dd6a8fcfa25.png"></div><br>  Als ich den Befehl <code>get</code> in wenigen Minuten mehr als zwei- bis dreimal verwendet habe, wurde ich gebeten, einen Test mit reCaptcha zu bestehen.  Sie können diese Prüfung manuell durchführen und die Experimente fortsetzen, bis das System beschließt, eine neue Prüfung durchzuführen.  Als ich das Skript getestet habe, hatte ich das Gefühl, dass die erste Suchsitzung immer ohne Probleme verläuft. Wenn Sie also mit dem Code experimentieren möchten, müssen Sie ihn nur regelmäßig manuell überprüfen und den Code in langen Intervallen zwischen den Suchsitzungen ausführen lassen.  Ja, und wenn Sie darüber nachdenken, ist es unwahrscheinlich, dass eine Person Informationen zu Ticketpreisen benötigt, die zwischen den Suchvorgängen in 10-Minuten-Intervallen eingehen. <br><br><h2>  <font color="#3AC1EF">Arbeiten mit einer Seite mit XPath</font> </h2><br>  Also haben wir das Fenster geöffnet und die Seite geladen.  Um Preise und andere Informationen zu erhalten, müssen wir die XPath-Technologie oder CSS-Selektoren verwenden.  Ich habe mich für XPath entschieden und hatte nicht das Bedürfnis, CSS-Selektoren zu verwenden, aber es ist durchaus möglich, so zu arbeiten.  Das Bewegen einer Seite mit XPath kann eine entmutigende Aufgabe sein, und selbst wenn Sie die in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem</a> Artikel beschriebenen Methoden verwenden, bei denen die entsprechenden Bezeichner aus dem Seitencode kopiert wurden, wurde mir klar, dass dies tatsächlich nicht der beste Weg ist, darauf zuzugreifen notwendige Elemente.  Übrigens finden Sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem</a> Buch eine hervorragende Beschreibung der Grundlagen der Arbeit mit Seiten mit XPath- und CSS-Selektoren.  So sieht die entsprechende Webtreibermethode aus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fa6/5c0/5c0/fa65c05c0385c658a4eee0c08a6274ff.png"></div><br>  Also arbeiten wir weiter am Bot.  Nutzen Sie das Programm, um die günstigsten Tickets auszuwählen.  In der folgenden Abbildung ist der XPath-Auswahlcode rot hervorgehoben.  Um den Code anzuzeigen, müssen Sie mit der rechten Maustaste auf das Element der Seite klicken, an der Sie interessiert sind, und im angezeigten Menü den Befehl Inspizieren auswählen.  Dieser Befehl kann für verschiedene Seitenelemente aufgerufen werden, deren Code im Code-Anzeigefenster angezeigt und hervorgehoben wird. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2c4/eee/9dc/2c4eee9dc28a8ef0ff540e1c01d3eda5.png"></div><br>  <i><font color="#999999">Seitencode anzeigen</font></i> <br><br>  Beachten Sie die folgenden Funktionen, um eine Bestätigung meiner Überlegungen zu den Nachteilen des Kopierens von Selektoren aus Code zu erhalten. <br><br>  Folgendes erhalten Sie beim Kopieren von Code: <br><br><pre> <code class="python hljs">//*[@id=<span class="hljs-string"><span class="hljs-string">"wtKI-price_aTab"</span></span>]/div[<span class="hljs-number"><span class="hljs-number">1</span></span>]/div/div/div[<span class="hljs-number"><span class="hljs-number">1</span></span>]/div/span/span</code> </pre> <br>  Um etwas Ähnliches zu kopieren, müssen Sie mit der rechten Maustaste auf den Teil des Codes klicken, der Sie interessiert, und im angezeigten Menü Kopieren&gt; XPath kopieren auswählen. <br><br>  Folgendes habe ich verwendet, um die Schaltfläche "Günstigstes" zu definieren: <br><br><pre> <code class="python hljs">cheap_results = <span class="hljs-string"><span class="hljs-string">'//a[@data-code = "price"]'</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9c9/4e3/8a4/9c94e38a436fa588ade8a2f92d97aa2d.png"></div><br>  <i><font color="#999999">Kopieren&gt; XPath-Befehl kopieren</font></i> <br><br>  Es ist ziemlich offensichtlich, dass die zweite Option viel einfacher aussieht.  Bei der Verwendung wird nach dem Element a gesucht, dessen <code>data-code</code> Attribut dem <code>price</code> .  Mit der ersten Option wird nach einem <code>id</code> Element gesucht, das <code>wtKI-price_aTab</code> , und der XPath-Pfad zum Element sieht wie folgt aus: <code>/div[1]/div/div/div[1]/div/span/span</code> .  Eine ähnliche XPath-Anforderung an eine Seite reicht aus, jedoch nur einmal.  Ich kann jetzt sagen, dass sich die <code>id</code> beim nächsten Laden der Seite ändert.  Die <code>wtKI</code> Zeichenfolge ändert sich bei jedem Laden der Seite dynamisch. Daher ist der Code, in dem sie verwendet wird, nach dem nächsten erneuten Laden der Seite unbrauchbar.  Nehmen Sie sich also etwas Zeit, um XPath herauszufinden.  Dieses Wissen wird Ihnen gut dienen. <br><br>  Es sollte jedoch beachtet werden, dass das Kopieren von XPath-Selektoren nützlich sein kann, wenn Sie mit relativ einfachen Websites arbeiten, und wenn dies zu Ihnen passt, ist daran nichts auszusetzen. <br><br>  Lassen Sie uns nun überlegen, was zu tun ist, wenn Sie alle Suchergebnisse in mehreren Zeilen innerhalb der Liste abrufen möchten.  Sehr einfach.  Jedes Ergebnis befindet sich in einem Objekt mit der <code>resultWrapper</code> Klasse.  Das Herunterladen aller Ergebnisse kann in einer Schleife erfolgen, die der unten gezeigten ähnelt. <br><br>  Es sollte beachtet werden, dass Sie, wenn Sie das oben Genannte verstehen, den größten Teil des Codes, den wir analysieren werden, leicht verstehen sollten.  Im Verlauf der Arbeit dieses Codes wenden wir uns dem zu, was wir brauchen (tatsächlich ist dies das Element, in das das Ergebnis eingeschlossen ist), indem wir einen Mechanismus verwenden, um den Pfad anzugeben (XPath).  Dies geschieht, um den Text des Elements <code>flight_containers</code> und in ein Objekt zu platzieren, aus dem Daten gelesen werden können (verwenden Sie zuerst <code>flight_containers</code> , dann <code>flights_list</code> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0b2/3fc/b7f/0b23fcb7f32738fd6012541c40f68b97.png"></div><br>  Die ersten drei Zeilen werden angezeigt und wir können alles klar sehen, was wir brauchen.  Wir haben jedoch interessantere Möglichkeiten, Informationen zu erhalten.  Wir müssen Daten von jedem Element separat nehmen. <br><br><h2>  <font color="#3AC1EF">Zu arbeiten!</font> </h2><br>  Es ist am einfachsten, eine Funktion zu schreiben, um zusätzliche Ergebnisse zu laden. Beginnen wir also damit.  Ich möchte die Anzahl der Flüge maximieren, über die das Programm Informationen erhält, und gleichzeitig keine Verdächtigungen im Dienst hervorrufen, die zur Überprüfung führen. Daher klicke ich jedes Mal, wenn die Seite angezeigt wird, auf die Schaltfläche Weitere Ergebnisse laden.  In diesem Code sollten Sie auf den <code>try</code> Block achten, den ich hinzugefügt habe, da die Schaltfläche manchmal nicht normal geladen wird.  Wenn Sie auch darauf stoßen, kommentieren Sie die Aufrufe dieser Funktion im Code der Funktion <code>start_kayak</code> aus, den wir weiter unten diskutieren werden. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      ,      def load_more():   try:       more_results = '//a[@class = "moreButton"]'       driver.find_element_by_xpath(more_results).click()       #            ,          print('sleeping.....')       sleep(randint(45,60))   except:       pass</span></span></code> </pre> <br>  Nach einer langen Analyse dieser Funktion (manchmal kann ich mich mitreißen lassen) sind wir nun bereit, eine Funktion zu deklarieren, die sich mit dem Scraping von Seiten befasst. <br><br>  Ich habe bereits das meiste gesammelt, was in der nächsten Funktion namens <code>page_scrape</code> .  Manchmal stellen sich die zurückgegebenen Daten über die Stufen des Pfades als kombiniert heraus, für ihre Trennung verwende ich eine einfache Methode.  Zum Beispiel verwende ich zum ersten Mal die Variablen <code>section_a_list</code> und <code>section_b_list</code> .  Unsere Funktion gibt den <code>flights_df</code> . Auf diese Weise können wir die mit verschiedenen <code>flights_df</code> erhaltenen Ergebnisse trennen und später kombinieren. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">page_scrape</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-string"><span class="hljs-string">"""This function takes care of the scraping part"""</span></span>     xp_sections = <span class="hljs-string"><span class="hljs-string">'//*[@class="section duration"]'</span></span>   sections = driver.find_elements_by_xpath(xp_sections)   sections_list = [value.text <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> value <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sections]   section_a_list = sections_list[::<span class="hljs-number"><span class="hljs-number">2</span></span>] <span class="hljs-comment"><span class="hljs-comment">#          section_b_list = sections_list[1::2]     #     reCaptcha,    - .   #  ,  -   ,     ,       #   if        -   #    ,           #    SystemExit           if section_a_list == []:       raise SystemExit     #     A     B     a_duration = []   a_section_names = []   for n in section_a_list:       #         a_section_names.append(''.join(n.split()[2:5]))       a_duration.append(''.join(n.split()[0:2]))   b_duration = []   b_section_names = []   for n in section_b_list:       #         b_section_names.append(''.join(n.split()[2:5]))       b_duration.append(''.join(n.split()[0:2]))   xp_dates = '//div[@class="section date"]'   dates = driver.find_elements_by_xpath(xp_dates)   dates_list = [value.text for value in dates]   a_date_list = dates_list[::2]   b_date_list = dates_list[1::2]   #      a_day = [value.split()[0] for value in a_date_list]   a_weekday = [value.split()[1] for value in a_date_list]   b_day = [value.split()[0] for value in b_date_list]   b_weekday = [value.split()[1] for value in b_date_list]     #     xp_prices = '//a[@class="booking-link"]/span[@class="price option-text"]'   prices = driver.find_elements_by_xpath(xp_prices)   prices_list = [price.text.replace('$','') for price in prices if price.text != '']   prices_list = list(map(int, prices_list))   # stops -   ,         ,   -     xp_stops = '//div[@class="section stops"]/div[1]'   stops = driver.find_elements_by_xpath(xp_stops)   stops_list = [stop.text[0].replace('n','0') for stop in stops]   a_stop_list = stops_list[::2]   b_stop_list = stops_list[1::2]   xp_stops_cities = '//div[@class="section stops"]/div[2]'   stops_cities = driver.find_elements_by_xpath(xp_stops_cities)   stops_cities_list = [stop.text for stop in stops_cities]   a_stop_name_list = stops_cities_list[::2]   b_stop_name_list = stops_cities_list[1::2]     #   -,          xp_schedule = '//div[@class="section times"]'   schedules = driver.find_elements_by_xpath(xp_schedule)   hours_list = []   carrier_list = []   for schedule in schedules:       hours_list.append(schedule.text.split('\n')[0])       carrier_list.append(schedule.text.split('\n')[1])   #          a  b   a_hours = hours_list[::2]   a_carrier = carrier_list[1::2]   b_hours = hours_list[::2]   b_carrier = carrier_list[1::2]     cols = (['Out Day', 'Out Time', 'Out Weekday', 'Out Airline', 'Out Cities', 'Out Duration', 'Out Stops', 'Out Stop Cities',           'Return Day', 'Return Time', 'Return Weekday', 'Return Airline', 'Return Cities', 'Return Duration', 'Return Stops', 'Return Stop Cities',           'Price'])   flights_df = pd.DataFrame({'Out Day': a_day,                              'Out Weekday': a_weekday,                              'Out Duration': a_duration,                              'Out Cities': a_section_names,                              'Return Day': b_day,                              'Return Weekday': b_weekday,                              'Return Duration': b_duration,                              'Return Cities': b_section_names,                              'Out Stops': a_stop_list,                              'Out Stop Cities': a_stop_name_list,                              'Return Stops': b_stop_list,                              'Return Stop Cities': b_stop_name_list,                              'Out Time': a_hours,                              'Out Airline': a_carrier,                              'Return Time': b_hours,                              'Return Airline': b_carrier,                                                     'Price': prices_list})[cols]     flights_df['timestamp'] = strftime("%Y%m%d-%H%M") #      return flights_df</span></span></code> </pre> <br>  Ich habe versucht, die Variablen so zu benennen, dass der Code klar ist.  Denken Sie daran, dass Variablen, die mit <code>a</code> sich auf den ersten Schritt des Pfads und <code>b</code> auf den zweiten beziehen.  Fahren Sie mit der nächsten Funktion fort. <br><br><h2>  <font color="#3AC1EF">Hilfsmechanismen</font> </h2><br>  Jetzt haben wir eine Funktion, mit der Sie zusätzliche Suchergebnisse laden können, und eine Funktion, um diese Ergebnisse zu verarbeiten.  Dieser Artikel könnte hierzu vervollständigt werden, da diese beiden Funktionen alles Notwendige zum Scraping von Seiten bieten, die unabhängig geöffnet werden können.  Einige der oben diskutierten Hilfsmechanismen haben wir jedoch noch nicht berücksichtigt.  Dies ist beispielsweise ein Code zum Senden von E-Mails und anderen Dingen.  All dies finden Sie in der Funktion <code>start_kayak</code> , die wir jetzt betrachten. <br><br>  Um diese Funktion nutzen zu können, benötigen Sie Informationen zu Städten und Daten.  Mithilfe dieser Informationen bildet sie einen Link in der <code>kayak</code> , über den zu der Seite gewechselt wird, auf der die Suchergebnisse nach ihrer besten Übereinstimmung mit der Abfrage sortiert werden.  Nach der ersten Scraping-Sitzung arbeiten wir mit den Preisen in der Tabelle oben auf der Seite.  Wir finden nämlich den minimalen Ticketpreis und den Durchschnittspreis.  All dies wird zusammen mit der von der Website ausgegebenen Vorhersage per E-Mail gesendet.  Auf der Seite sollte sich die entsprechende Tabelle in der oberen linken Ecke befinden.  Die Arbeit mit dieser Tabelle kann übrigens zu Fehlern bei der Suche nach genauen Daten führen, da in diesem Fall die Tabelle nicht auf der Seite angezeigt wird. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">start_kayak</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(city_from, city_to, date_start, date_end)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-string"><span class="hljs-string">"""City codes - it's the IATA codes!   Date format -  YYYY-MM-DD"""</span></span>     kayak = (<span class="hljs-string"><span class="hljs-string">'https://www.kayak.com/flights/'</span></span> + city_from + <span class="hljs-string"><span class="hljs-string">'-'</span></span> + city_to +            <span class="hljs-string"><span class="hljs-string">'/'</span></span> + date_start + <span class="hljs-string"><span class="hljs-string">'-flexible/'</span></span> + date_end + <span class="hljs-string"><span class="hljs-string">'-flexible?sort=bestflight_a'</span></span>)   driver.get(kayak)   sleep(randint(<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>))     <span class="hljs-comment"><span class="hljs-comment">#    ,           try   try:       xp_popup_close = '//button[contains(@id,"dialog-close") and contains(@class,"Button-No-Standard-Style close ")]'       driver.find_elements_by_xpath(xp_popup_close)[5].click()   except Exception as e:       pass   sleep(randint(60,95))   print('loading more.....')  #     load_more()     print('starting first scrape.....')   df_flights_best = page_scrape()   df_flights_best['sort'] = 'best'   sleep(randint(60,80))     #      ,        matrix = driver.find_elements_by_xpath('//*[contains(@id,"FlexMatrixCell")]')   matrix_prices = [price.text.replace('$','') for price in matrix]   matrix_prices = list(map(int, matrix_prices))   matrix_min = min(matrix_prices)   matrix_avg = sum(matrix_prices)/len(matrix_prices)     print('switching to cheapest results.....')   cheap_results = '//a[@data-code = "price"]'   driver.find_element_by_xpath(cheap_results).click()   sleep(randint(60,90))   print('loading more.....')  #     load_more()     print('starting second scrape.....')   df_flights_cheap = page_scrape()   df_flights_cheap['sort'] = 'cheap'   sleep(randint(60,80))     print('switching to quickest results.....')   quick_results = '//a[@data-code = "duration"]'   driver.find_element_by_xpath(quick_results).click()    sleep(randint(60,90))   print('loading more.....')  #     load_more()     print('starting third scrape.....')   df_flights_fast = page_scrape()   df_flights_fast['sort'] = 'fast'   sleep(randint(60,80))     #     Excel-,         final_df = df_flights_cheap.append(df_flights_best).append(df_flights_fast)   final_df.to_excel('search_backups//{}_flights_{}-{}_from_{}_to_{}.xlsx'.format(strftime("%Y%m%d-%H%M"),                                                                                  city_from, city_to,                                                                                  date_start, date_end), index=False)   print('saved df.....')     #    ,  ,  ,      xp_loading = '//div[contains(@id,"advice")]'   loading = driver.find_element_by_xpath(xp_loading).text   xp_prediction = '//span[@class="info-text"]'   prediction = driver.find_element_by_xpath(xp_prediction).text   print(loading+'\n'+prediction)     #    loading   , , ,        #    -    "Not Sure"   weird = '¯\\_(ツ)_/¯'   if loading == weird:       loading = 'Not sure'     username = 'YOUREMAIL@hotmail.com'   password = 'YOUR PASSWORD'   server = smtplib.SMTP('smtp.outlook.com', 587)   server.ehlo()   server.starttls()   server.login(username, password)   msg = ('Subject: Flight Scraper\n\n\ Cheapest Flight: {}\nAverage Price: {}\n\nRecommendation: {}\n\nEnd of message'.format(matrix_min, matrix_avg, (loading+'\n'+prediction)))   message = MIMEMultipart()   message['From'] = 'YOUREMAIL@hotmail.com'   message['to'] = 'YOUROTHEREMAIL@domain.com'   server.sendmail('YOUREMAIL@hotmail.com', 'YOUROTHEREMAIL@domain.com', msg)   print('sent email.....')</span></span></code> </pre> <br>  Ich habe dieses Skript mit einem Outlook-Konto (hotmail.com) getestet.  Ich habe nicht überprüft, ob das Google Mail-Konto ordnungsgemäß funktioniert. Dieses Mail-System ist sehr beliebt, aber es gibt viele mögliche Optionen.  Wenn Sie ein Hotmail-Konto verwenden, müssen Sie nur Ihre Daten in den Code eingeben, damit alles funktioniert. <br><br>  Wenn Sie verstehen möchten, was genau in separaten Abschnitten des Codes dieser Funktion ausgeführt wird, können Sie sie kopieren und mit ihnen experimentieren.  Codeexperimente sind der einzige Weg, dies zu verstehen. <br><br><h2>  <font color="#3AC1EF">Bereites System</font> </h2><br>  Nachdem alles, worüber wir gesprochen haben, erledigt ist, können wir eine einfache Schleife erstellen, in der unsere Funktionen aufgerufen werden.  Das Skript fragt den Benutzer nach Daten zu Städten und Daten.  Wenn Sie mit einem konstanten Neustart des Skripts testen, ist es unwahrscheinlich, dass Sie diese Daten jedes Mal manuell eingeben möchten. Daher können die entsprechenden Zeilen für den Testzeitpunkt auskommentiert werden, indem Sie diejenigen auskommentieren, die darunter liegen und in denen die für das Skript erforderlichen Daten fest festgelegt sind. <br><br><pre> <code class="python hljs">city_from = input(<span class="hljs-string"><span class="hljs-string">'From which city? '</span></span>) city_to = input(<span class="hljs-string"><span class="hljs-string">'Where to? '</span></span>) date_start = input(<span class="hljs-string"><span class="hljs-string">'Search around which departure date? Please use YYYY-MM-DD format only '</span></span>) date_end = input(<span class="hljs-string"><span class="hljs-string">'Return when? Please use YYYY-MM-DD format only '</span></span>) <span class="hljs-comment"><span class="hljs-comment"># city_from = 'LIS' # city_to = 'SIN' # date_start = '2019-08-21' # date_end = '2019-09-07' for n in range(0,5):   start_kayak(city_from, city_to, date_start, date_end)   print('iteration {} was complete @ {}'.format(n, strftime("%Y%m%d-%H%M")))     #  4    sleep(60*60*4)   print('sleep finished.....')</span></span></code> </pre> <br>  Hier ist der Testlauf des Skripts. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/44c/305/023/44c305023996a98a3dec745493dce7a7.png"></div><br>  <i><font color="#999999">Testlaufskript</font></i> <br><br><h2>  <font color="#3AC1EF">Zusammenfassung</font> </h2><br>  Wenn Sie an diesen Punkt kommen - herzlichen Glückwunsch!  Jetzt haben Sie einen funktionierenden Web-Scraper, obwohl ich bereits viele Möglichkeiten sehe, ihn zu verbessern.  Beispielsweise kann es in Twilio integriert werden, sodass anstelle von E-Mails Textnachrichten gesendet werden.  Sie können ein VPN oder etwas anderes verwenden, um gleichzeitig Ergebnisse von mehreren Servern zu erhalten.  Es gibt auch ein wiederkehrendes Problem beim Überprüfen des Site-Benutzers, ob er eine Person ist, aber dieses Problem kann auch gelöst werden.  In jedem Fall haben Sie jetzt eine Basis, die Sie erweitern können, wenn Sie möchten.  Stellen Sie beispielsweise sicher, dass die Excel-Datei als Anhang an eine E-Mail an den Benutzer gesendet wird. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de451872/">https://habr.com/ru/post/de451872/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de451860/index.html">Mathematiker haben den perfekten Weg gefunden, um Zahlen zu multiplizieren</a></li>
<li><a href="../de451862/index.html">Joe Diprims musikalischer Blitz: Ein Autodidakt stellt Tesla-Spulen zur Unterhaltung und zum Verdienen her</a></li>
<li><a href="../de451864/index.html">Kritische RCE-Sicherheitsanfälligkeit der EternalBlue-Ebene unter Windows erkannt</a></li>
<li><a href="../de451866/index.html">Wählen Sie die nächsten Knoten im Netzwerk</a></li>
<li><a href="../de451870/index.html">Moderne C ++ - Funktionen, die alle Programmierer kennen müssen</a></li>
<li><a href="../de451874/index.html">Top SEO Trends bei Google</a></li>
<li><a href="../de451876/index.html">Rechenzentrum Frankfurt: Rechenzentrum Telehouse</a></li>
<li><a href="../de451878/index.html">Live-Streaming von Stereovideos auf VR-Brillen (Oculus Go)</a></li>
<li><a href="../de451880/index.html">DevPRO'19: Blick vom Wrike-Stand</a></li>
<li><a href="../de451884/index.html">Sieben Jahre als Entwickler gearbeitet: Welche Lektionen habe ich gelernt?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>