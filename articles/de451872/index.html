<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äçüöí üë®üèº‚Äçüéì üìñ Python ist ein Assistent bei der Suche nach g√ºnstigen Fl√ºgen f√ºr diejenigen, die gerne reisen üë©üèΩ‚Äçüíº ü§æüèº ‚ôãÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Autorin des Artikels, dessen √úbersetzung wir heute ver√∂ffentlichen, sagt, dass ihr Ziel darin besteht, √ºber die Entwicklung eines Web-Scraper in P...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python ist ein Assistent bei der Suche nach g√ºnstigen Fl√ºgen f√ºr diejenigen, die gerne reisen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/451872/">  Die Autorin des Artikels, dessen √úbersetzung wir heute ver√∂ffentlichen, sagt, dass ihr Ziel darin besteht, √ºber die Entwicklung eines Web-Scraper in Python mit Selenium zu sprechen, der nach Flugpreisen sucht.  Bei der Suche nach Tickets werden flexible Daten verwendet (+ - 3 Tage relativ zu den angegebenen Daten).  Scraper speichert die Suchergebnisse in einer Excel-Datei und sendet an die Person, die sie gestartet hat, eine E-Mail mit allgemeinen Informationen dar√ºber, was er gefunden hat.  Das Ziel dieses Projekts ist es, Reisenden zu helfen, die besten Angebote zu finden. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/xl/jo/rr/xljorr2xue-q63wegfrfyt5uxu4.jpeg"></a> <br><br>  Wenn Sie beim Umgang mit dem Material das Gef√ºhl haben, verloren zu sein, lesen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen</a> Artikel. <br><a name="habracut"></a><br><h2>  <font color="#3AC1EF">Was suchen wir?</font> </h2><br>  Sie k√∂nnen das hier beschriebene System nach Ihren W√ºnschen verwenden.  Zum Beispiel habe ich damit nach Wochenendtouren und Tickets f√ºr meine Heimatstadt gesucht.  Wenn Sie es ernst meinen, profitable Tickets zu finden, k√∂nnen Sie das Skript auf dem Server ausf√ºhren (ein einfacher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Server</a> f√ºr 130 Rubel pro Monat ist daf√ºr gut geeignet) und es ein- oder zweimal am Tag ausf√ºhren lassen.  Die Suchergebnisse werden per E-Mail an Sie gesendet.  Au√üerdem empfehle ich, dass Sie alles so konfigurieren, dass das Skript die Excel-Datei mit den Suchergebnissen im Dropbox-Ordner speichert, sodass Sie solche Dateien von √ºberall und jederzeit anzeigen k√∂nnen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d39/b7b/f3b/d39b7bf3b4f8c11fa9617ae308f01247.png"></div><br>  <i><font color="#999999">Ich habe noch keine fehlerhaften Tarife gefunden, aber ich glaube, dass dies m√∂glich ist</font></i> <br><br>  Bei der Suche wird, wie bereits gesagt, ein ‚Äûflexibles Datum‚Äú verwendet, das Skript findet Angebote, die innerhalb von drei Tagen ab dem angegebenen Datum liegen.  Obwohl beim Starten des Skripts nur in einer Richtung nach Angeboten gesucht wird, kann es leicht verfeinert werden, sodass Daten in mehreren Flugrichtungen erfasst werden k√∂nnen.  Mit seiner Hilfe k√∂nnen Sie sogar nach fehlerhaften Tarifen suchen, solche Funde k√∂nnen sehr interessant sein. <br><br><h2>  <font color="#3AC1EF">Warum brauche ich einen anderen Web Scraper?</font> </h2><br>  Als ich anfing, Web Scraping zu machen, war es ehrlich gesagt nicht besonders interessant.  Ich wollte mehr Projekte im Bereich der pr√§diktiven Modellierung, der Finanzanalyse und m√∂glicherweise im Bereich der Analyse der emotionalen F√§rbung von Texten durchf√ºhren.  Es stellte sich jedoch heraus, dass es sehr interessant war, herauszufinden, wie ein Programm erstellt werden kann, das Daten von Websites sammelt.  Als ich mich mit diesem Thema befasste, wurde mir klar, dass Web Scraping die ‚ÄûEngine‚Äú des Internets ist. <br><br>  Sie k√∂nnen entscheiden, dass dies eine zu k√ºhne Aussage ist.  Aber denken Sie daran, wie Google mit einem Web-Scraper begann, den Larry Page mit Java und Python erstellt hat.  Googlebots haben das Internet recherchiert und erkundet, um ihren Nutzern die bestm√∂glichen Antworten auf ihre Fragen zu bieten.  Web Scraping hat unendlich viele Anwendungen, und selbst wenn Sie im Bereich Data Science an etwas anderem interessiert sind, ben√∂tigen Sie einige Scraping-Kenntnisse, um Daten f√ºr die Analyse zu erhalten. <br><br>  Einige der hier verwendeten Tricks habe ich in einem wunderbaren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Buch</a> √ºber Web Scraping gefunden, das ich k√ºrzlich erworben habe.  Darin finden Sie viele einfache Beispiele und Ideen zur praktischen Anwendung des Studierten.  Dar√ºber hinaus gibt es ein sehr interessantes Kapitel zum reCaptcha-Test-Bypass.  F√ºr mich war dies eine Neuigkeit, da ich nicht wusste, dass es spezielle Tools und sogar ganze Services zur L√∂sung solcher Probleme gibt. <br><br><h2>  <font color="#3AC1EF">Reisen Sie gerne ?!</font> </h2><br>  Auf die einfache und ziemlich harmlose Frage in der √úberschrift dieses Abschnitts kann man oft eine positive Antwort h√∂ren, die ein paar Reisegeschichten der Person enth√§lt, zu der er gefragt wurde.  Die meisten von uns werden zustimmen, dass Reisen eine gro√üartige M√∂glichkeit ist, in neue kulturelle Umgebungen einzutauchen und unseren Horizont zu erweitern.  Wenn Sie jedoch jemandem eine Frage stellen, ob er gerne nach Flugtickets sucht, bin ich sicher, dass die Antwort darauf alles andere als positiv sein wird.  Tats√§chlich kommt hier Python zur Rettung. <br><br>  Die erste Aufgabe, die wir auf dem Weg zur Schaffung eines Systems zur Suche nach Informationen √ºber Flugtickets l√∂sen m√ºssen, ist die Auswahl einer geeigneten Plattform, mit der wir Informationen aufnehmen.  Die L√∂sung f√ºr dieses Problem war f√ºr mich nicht einfach, aber am Ende entschied ich mich f√ºr den Kajak-Service.  Ich habe die Dienste von Momondo, Skyscanner, Expedia und anderen ausprobiert, aber die Schutzmechanismen gegen Roboter auf diesen Ressourcen waren undurchdringlich.  Nach mehreren Versuchen, bei denen ich mich mit Ampeln, Fu√üg√§nger√ºberwegen und Fahrr√§dern auseinandersetzen musste, um die Systeme davon zu √ºberzeugen, dass ich ein Mensch bin, entschied ich, dass Kajak am besten zu mir passt, auch wenn dies hier der Fall ist Laden Sie in kurzer Zeit zu viele Seiten, und die √úberpr√ºfungen beginnen ebenfalls.  Ich habe es geschafft, dass der Bot in Abst√§nden von 4 bis 6 Stunden Anfragen an die Site sendet, und alles hat gut funktioniert.  Bei der Arbeit mit Kayak treten auch regelm√§√üig Schwierigkeiten auf. Wenn Sie jedoch anfangen, sich mit Schecks zu besch√§ftigen, m√ºssen Sie entweder manuell damit umgehen, dann den Bot starten oder einige Stunden warten, und die Schecks sollten aufh√∂ren.  Bei Bedarf k√∂nnen Sie den Code gut f√ºr eine andere Plattform anpassen. Wenn Sie dies tun, k√∂nnen Sie ihn in den Kommentaren melden. <br><br>  Wenn Sie gerade erst mit dem Web-Scraping beginnen und nicht wissen, warum einige Websites damit zu k√§mpfen haben, tun Sie sich selbst einen Gefallen und suchen Sie bei Google nach W√∂rtern, bevor Sie Ihr erstes Projekt in diesem Bereich starten "Web Scraping Etikette".  Ihre Experimente k√∂nnen fr√ºher enden als Sie denken, wenn Sie sich unangemessen mit Web Scraping besch√§ftigen. <br><br><h2>  <font color="#3AC1EF">Erste Schritte</font> </h2><br>  Hier ist eine allgemeine √úbersicht dar√ºber, was im Code unseres Web Scraper passieren wird: <br><br><ul><li>  Importieren Sie die erforderlichen Bibliotheken. </li><li>  √ñffnen Sie die Registerkarte Google Chrome. </li><li>  Aufruf der Funktion, die den Bot startet, √úbergabe der Stadt und des Datums, die bei der Suche nach Tickets verwendet werden. </li><li>  Diese Funktion empf√§ngt die ersten Suchergebnisse, sortiert nach den Kriterien der attraktivsten (besten), und dr√ºckt die Taste, um zus√§tzliche Ergebnisse zu laden. </li><li>  Eine andere Funktion sammelt Daten von der gesamten Seite und gibt einen Datenrahmen zur√ºck. </li><li>  Die beiden vorherigen Schritte werden unter Verwendung von Sortiertypen nach Ticketpreis (g√ºnstig) und Fluggeschwindigkeit (am schnellsten) ausgef√ºhrt. </li><li>  An den Skriptbenutzer wird eine E-Mail mit einer kurzen Zusammenfassung der Ticketpreise (g√ºnstigste Tickets und Durchschnittspreis) gesendet, und ein Datenrahmen mit Informationen, die nach den drei oben genannten Indikatoren sortiert sind, wird als Excel-Datei gespeichert. </li><li>  Alle oben genannten Aktionen werden in einem Zyklus nach einem bestimmten Zeitraum ausgef√ºhrt. </li></ul><br>  Es ist zu beachten, dass jedes Selenium-Projekt mit einem Webtreiber beginnt.  Ich benutze <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Chromedriver</a> , arbeite mit Google Chrome, aber es gibt andere M√∂glichkeiten.  Beliebt sind auch PhantomJS und Firefox.  Nachdem Sie den Treiber geladen haben, m√ºssen Sie ihn in den entsprechenden Ordner legen. Damit ist die Vorbereitung f√ºr die Verwendung abgeschlossen.  In den ersten Zeilen unseres Skripts wird ein neuer Chrome-Tab ge√∂ffnet. <br><br>  Denken Sie daran, dass ich in meiner Geschichte nicht versuche, neue Horizonte zu er√∂ffnen, um profitable Angebote f√ºr Flugtickets zu finden.  Es gibt viel fortgeschrittenere Techniken, um solche Angebote zu finden.  Ich m√∂chte den Lesern dieses Materials nur eine einfache, aber praktische M√∂glichkeit bieten, dieses Problem zu l√∂sen. <br><br>  Hier ist der Code, √ºber den wir oben gesprochen haben. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep, strftime <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> randint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> selenium <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> webdriver <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> selenium.webdriver.common.keys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Keys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> smtplib <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> email.mime.multipart <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MIMEMultipart <span class="hljs-comment"><span class="hljs-comment">#      chromedriver! chromedriver_path = 'C:/{YOUR PATH HERE}/chromedriver_win32/chromedriver.exe' driver = webdriver.Chrome(executable_path=chromedriver_path) #     Chrome sleep(2)</span></span></code> </pre> <br>  Am Anfang des Codes sehen Sie die Paketimportbefehle, die in unserem Projekt verwendet werden.  <code>randint</code> wird also verwendet, damit der Bot f√ºr eine zuf√§llige Anzahl von Sekunden "einschlafen" kann, bevor ein neuer <code>randint</code> wird.  Normalerweise kann kein einziger Bot darauf verzichten.  Wenn Sie den obigen Code ausf√ºhren, wird ein Chrome-Fenster ge√∂ffnet, in dem der Bot mit Websites arbeitet. <br><br>  Lassen Sie uns ein kleines Experiment durchf√ºhren und die Website kayak.com in einem separaten Fenster √∂ffnen.  W√§hlen Sie die Stadt, von der aus wir fliegen, die Stadt, in die wir fliegen m√∂chten, sowie die Flugdaten.  Bei der Auswahl der Daten √ºberpr√ºfen wir, ob der Bereich + -3 Tage betr√§gt.  Ich habe den Code unter Ber√ºcksichtigung dessen geschrieben, was die Site als Antwort auf solche Anfragen produziert.  Wenn Sie beispielsweise nur f√ºr bestimmte Daten nach Tickets suchen m√ºssen, m√ºssen Sie h√∂chstwahrscheinlich den Bot-Code √§ndern.  Wenn ich √ºber den Code spreche, gebe ich entsprechende Erkl√§rungen ab, aber wenn Sie sich verwirrt f√ºhlen, lassen Sie es mich wissen. <br><br>  Klicken Sie nun auf die Schaltfl√§che Start der Suche und sehen Sie sich den Link in der Adressleiste an.  Es sollte wie der Link aussehen, den ich im folgenden Beispiel verwende, in dem die <code>kayak</code> , in der die URL gespeichert ist, deklariert und die <code>get</code> Methode des Webtreibers verwendet wird.  Nach dem Klicken auf die Suchschaltfl√§che sollten die Ergebnisse auf der Seite angezeigt werden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae6/ac7/1f7/ae6ac71f71c92c6ff76d5dd6a8fcfa25.png"></div><br>  Als ich den Befehl <code>get</code> in wenigen Minuten mehr als zwei- bis dreimal verwendet habe, wurde ich gebeten, einen Test mit reCaptcha zu bestehen.  Sie k√∂nnen diese Pr√ºfung manuell durchf√ºhren und die Experimente fortsetzen, bis das System beschlie√üt, eine neue Pr√ºfung durchzuf√ºhren.  Als ich das Skript getestet habe, hatte ich das Gef√ºhl, dass die erste Suchsitzung immer ohne Probleme verl√§uft. Wenn Sie also mit dem Code experimentieren m√∂chten, m√ºssen Sie ihn nur regelm√§√üig manuell √ºberpr√ºfen und den Code in langen Intervallen zwischen den Suchsitzungen ausf√ºhren lassen.  Ja, und wenn Sie dar√ºber nachdenken, ist es unwahrscheinlich, dass eine Person Informationen zu Ticketpreisen ben√∂tigt, die zwischen den Suchvorg√§ngen in 10-Minuten-Intervallen eingehen. <br><br><h2>  <font color="#3AC1EF">Arbeiten mit einer Seite mit XPath</font> </h2><br>  Also haben wir das Fenster ge√∂ffnet und die Seite geladen.  Um Preise und andere Informationen zu erhalten, m√ºssen wir die XPath-Technologie oder CSS-Selektoren verwenden.  Ich habe mich f√ºr XPath entschieden und hatte nicht das Bed√ºrfnis, CSS-Selektoren zu verwenden, aber es ist durchaus m√∂glich, so zu arbeiten.  Das Bewegen einer Seite mit XPath kann eine entmutigende Aufgabe sein, und selbst wenn Sie die in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem</a> Artikel beschriebenen Methoden verwenden, bei denen die entsprechenden Bezeichner aus dem Seitencode kopiert wurden, wurde mir klar, dass dies tats√§chlich nicht der beste Weg ist, darauf zuzugreifen notwendige Elemente.  √úbrigens finden Sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem</a> Buch eine hervorragende Beschreibung der Grundlagen der Arbeit mit Seiten mit XPath- und CSS-Selektoren.  So sieht die entsprechende Webtreibermethode aus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fa6/5c0/5c0/fa65c05c0385c658a4eee0c08a6274ff.png"></div><br>  Also arbeiten wir weiter am Bot.  Nutzen Sie das Programm, um die g√ºnstigsten Tickets auszuw√§hlen.  In der folgenden Abbildung ist der XPath-Auswahlcode rot hervorgehoben.  Um den Code anzuzeigen, m√ºssen Sie mit der rechten Maustaste auf das Element der Seite klicken, an der Sie interessiert sind, und im angezeigten Men√º den Befehl Inspizieren ausw√§hlen.  Dieser Befehl kann f√ºr verschiedene Seitenelemente aufgerufen werden, deren Code im Code-Anzeigefenster angezeigt und hervorgehoben wird. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2c4/eee/9dc/2c4eee9dc28a8ef0ff540e1c01d3eda5.png"></div><br>  <i><font color="#999999">Seitencode anzeigen</font></i> <br><br>  Beachten Sie die folgenden Funktionen, um eine Best√§tigung meiner √úberlegungen zu den Nachteilen des Kopierens von Selektoren aus Code zu erhalten. <br><br>  Folgendes erhalten Sie beim Kopieren von Code: <br><br><pre> <code class="python hljs">//*[@id=<span class="hljs-string"><span class="hljs-string">"wtKI-price_aTab"</span></span>]/div[<span class="hljs-number"><span class="hljs-number">1</span></span>]/div/div/div[<span class="hljs-number"><span class="hljs-number">1</span></span>]/div/span/span</code> </pre> <br>  Um etwas √Ñhnliches zu kopieren, m√ºssen Sie mit der rechten Maustaste auf den Teil des Codes klicken, der Sie interessiert, und im angezeigten Men√º Kopieren&gt; XPath kopieren ausw√§hlen. <br><br>  Folgendes habe ich verwendet, um die Schaltfl√§che "G√ºnstigstes" zu definieren: <br><br><pre> <code class="python hljs">cheap_results = <span class="hljs-string"><span class="hljs-string">'//a[@data-code = "price"]'</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9c9/4e3/8a4/9c94e38a436fa588ade8a2f92d97aa2d.png"></div><br>  <i><font color="#999999">Kopieren&gt; XPath-Befehl kopieren</font></i> <br><br>  Es ist ziemlich offensichtlich, dass die zweite Option viel einfacher aussieht.  Bei der Verwendung wird nach dem Element a gesucht, dessen <code>data-code</code> Attribut dem <code>price</code> .  Mit der ersten Option wird nach einem <code>id</code> Element gesucht, das <code>wtKI-price_aTab</code> , und der XPath-Pfad zum Element sieht wie folgt aus: <code>/div[1]/div/div/div[1]/div/span/span</code> .  Eine √§hnliche XPath-Anforderung an eine Seite reicht aus, jedoch nur einmal.  Ich kann jetzt sagen, dass sich die <code>id</code> beim n√§chsten Laden der Seite √§ndert.  Die <code>wtKI</code> Zeichenfolge √§ndert sich bei jedem Laden der Seite dynamisch. Daher ist der Code, in dem sie verwendet wird, nach dem n√§chsten erneuten Laden der Seite unbrauchbar.  Nehmen Sie sich also etwas Zeit, um XPath herauszufinden.  Dieses Wissen wird Ihnen gut dienen. <br><br>  Es sollte jedoch beachtet werden, dass das Kopieren von XPath-Selektoren n√ºtzlich sein kann, wenn Sie mit relativ einfachen Websites arbeiten, und wenn dies zu Ihnen passt, ist daran nichts auszusetzen. <br><br>  Lassen Sie uns nun √ºberlegen, was zu tun ist, wenn Sie alle Suchergebnisse in mehreren Zeilen innerhalb der Liste abrufen m√∂chten.  Sehr einfach.  Jedes Ergebnis befindet sich in einem Objekt mit der <code>resultWrapper</code> Klasse.  Das Herunterladen aller Ergebnisse kann in einer Schleife erfolgen, die der unten gezeigten √§hnelt. <br><br>  Es sollte beachtet werden, dass Sie, wenn Sie das oben Genannte verstehen, den gr√∂√üten Teil des Codes, den wir analysieren werden, leicht verstehen sollten.  Im Verlauf der Arbeit dieses Codes wenden wir uns dem zu, was wir brauchen (tats√§chlich ist dies das Element, in das das Ergebnis eingeschlossen ist), indem wir einen Mechanismus verwenden, um den Pfad anzugeben (XPath).  Dies geschieht, um den Text des Elements <code>flight_containers</code> und in ein Objekt zu platzieren, aus dem Daten gelesen werden k√∂nnen (verwenden Sie zuerst <code>flight_containers</code> , dann <code>flights_list</code> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0b2/3fc/b7f/0b23fcb7f32738fd6012541c40f68b97.png"></div><br>  Die ersten drei Zeilen werden angezeigt und wir k√∂nnen alles klar sehen, was wir brauchen.  Wir haben jedoch interessantere M√∂glichkeiten, Informationen zu erhalten.  Wir m√ºssen Daten von jedem Element separat nehmen. <br><br><h2>  <font color="#3AC1EF">Zu arbeiten!</font> </h2><br>  Es ist am einfachsten, eine Funktion zu schreiben, um zus√§tzliche Ergebnisse zu laden. Beginnen wir also damit.  Ich m√∂chte die Anzahl der Fl√ºge maximieren, √ºber die das Programm Informationen erh√§lt, und gleichzeitig keine Verd√§chtigungen im Dienst hervorrufen, die zur √úberpr√ºfung f√ºhren. Daher klicke ich jedes Mal, wenn die Seite angezeigt wird, auf die Schaltfl√§che Weitere Ergebnisse laden.  In diesem Code sollten Sie auf den <code>try</code> Block achten, den ich hinzugef√ºgt habe, da die Schaltfl√§che manchmal nicht normal geladen wird.  Wenn Sie auch darauf sto√üen, kommentieren Sie die Aufrufe dieser Funktion im Code der Funktion <code>start_kayak</code> aus, den wir weiter unten diskutieren werden. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      ,      def load_more():   try:       more_results = '//a[@class = "moreButton"]'       driver.find_element_by_xpath(more_results).click()       #            ,          print('sleeping.....')       sleep(randint(45,60))   except:       pass</span></span></code> </pre> <br>  Nach einer langen Analyse dieser Funktion (manchmal kann ich mich mitrei√üen lassen) sind wir nun bereit, eine Funktion zu deklarieren, die sich mit dem Scraping von Seiten befasst. <br><br>  Ich habe bereits das meiste gesammelt, was in der n√§chsten Funktion namens <code>page_scrape</code> .  Manchmal stellen sich die zur√ºckgegebenen Daten √ºber die Stufen des Pfades als kombiniert heraus, f√ºr ihre Trennung verwende ich eine einfache Methode.  Zum Beispiel verwende ich zum ersten Mal die Variablen <code>section_a_list</code> und <code>section_b_list</code> .  Unsere Funktion gibt den <code>flights_df</code> . Auf diese Weise k√∂nnen wir die mit verschiedenen <code>flights_df</code> erhaltenen Ergebnisse trennen und sp√§ter kombinieren. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">page_scrape</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-string"><span class="hljs-string">"""This function takes care of the scraping part"""</span></span>     xp_sections = <span class="hljs-string"><span class="hljs-string">'//*[@class="section duration"]'</span></span>   sections = driver.find_elements_by_xpath(xp_sections)   sections_list = [value.text <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> value <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sections]   section_a_list = sections_list[::<span class="hljs-number"><span class="hljs-number">2</span></span>] <span class="hljs-comment"><span class="hljs-comment">#          section_b_list = sections_list[1::2]     #     reCaptcha,    - .   #  ,  -   ,     ,       #   if        -   #    ,           #    SystemExit           if section_a_list == []:       raise SystemExit     #     A     B     a_duration = []   a_section_names = []   for n in section_a_list:       #         a_section_names.append(''.join(n.split()[2:5]))       a_duration.append(''.join(n.split()[0:2]))   b_duration = []   b_section_names = []   for n in section_b_list:       #         b_section_names.append(''.join(n.split()[2:5]))       b_duration.append(''.join(n.split()[0:2]))   xp_dates = '//div[@class="section date"]'   dates = driver.find_elements_by_xpath(xp_dates)   dates_list = [value.text for value in dates]   a_date_list = dates_list[::2]   b_date_list = dates_list[1::2]   #      a_day = [value.split()[0] for value in a_date_list]   a_weekday = [value.split()[1] for value in a_date_list]   b_day = [value.split()[0] for value in b_date_list]   b_weekday = [value.split()[1] for value in b_date_list]     #     xp_prices = '//a[@class="booking-link"]/span[@class="price option-text"]'   prices = driver.find_elements_by_xpath(xp_prices)   prices_list = [price.text.replace('$','') for price in prices if price.text != '']   prices_list = list(map(int, prices_list))   # stops -   ,         ,   -     xp_stops = '//div[@class="section stops"]/div[1]'   stops = driver.find_elements_by_xpath(xp_stops)   stops_list = [stop.text[0].replace('n','0') for stop in stops]   a_stop_list = stops_list[::2]   b_stop_list = stops_list[1::2]   xp_stops_cities = '//div[@class="section stops"]/div[2]'   stops_cities = driver.find_elements_by_xpath(xp_stops_cities)   stops_cities_list = [stop.text for stop in stops_cities]   a_stop_name_list = stops_cities_list[::2]   b_stop_name_list = stops_cities_list[1::2]     #   -,          xp_schedule = '//div[@class="section times"]'   schedules = driver.find_elements_by_xpath(xp_schedule)   hours_list = []   carrier_list = []   for schedule in schedules:       hours_list.append(schedule.text.split('\n')[0])       carrier_list.append(schedule.text.split('\n')[1])   #          a  b   a_hours = hours_list[::2]   a_carrier = carrier_list[1::2]   b_hours = hours_list[::2]   b_carrier = carrier_list[1::2]     cols = (['Out Day', 'Out Time', 'Out Weekday', 'Out Airline', 'Out Cities', 'Out Duration', 'Out Stops', 'Out Stop Cities',           'Return Day', 'Return Time', 'Return Weekday', 'Return Airline', 'Return Cities', 'Return Duration', 'Return Stops', 'Return Stop Cities',           'Price'])   flights_df = pd.DataFrame({'Out Day': a_day,                              'Out Weekday': a_weekday,                              'Out Duration': a_duration,                              'Out Cities': a_section_names,                              'Return Day': b_day,                              'Return Weekday': b_weekday,                              'Return Duration': b_duration,                              'Return Cities': b_section_names,                              'Out Stops': a_stop_list,                              'Out Stop Cities': a_stop_name_list,                              'Return Stops': b_stop_list,                              'Return Stop Cities': b_stop_name_list,                              'Out Time': a_hours,                              'Out Airline': a_carrier,                              'Return Time': b_hours,                              'Return Airline': b_carrier,                                                     'Price': prices_list})[cols]     flights_df['timestamp'] = strftime("%Y%m%d-%H%M") #      return flights_df</span></span></code> </pre> <br>  Ich habe versucht, die Variablen so zu benennen, dass der Code klar ist.  Denken Sie daran, dass Variablen, die mit <code>a</code> sich auf den ersten Schritt des Pfads und <code>b</code> auf den zweiten beziehen.  Fahren Sie mit der n√§chsten Funktion fort. <br><br><h2>  <font color="#3AC1EF">Hilfsmechanismen</font> </h2><br>  Jetzt haben wir eine Funktion, mit der Sie zus√§tzliche Suchergebnisse laden k√∂nnen, und eine Funktion, um diese Ergebnisse zu verarbeiten.  Dieser Artikel k√∂nnte hierzu vervollst√§ndigt werden, da diese beiden Funktionen alles Notwendige zum Scraping von Seiten bieten, die unabh√§ngig ge√∂ffnet werden k√∂nnen.  Einige der oben diskutierten Hilfsmechanismen haben wir jedoch noch nicht ber√ºcksichtigt.  Dies ist beispielsweise ein Code zum Senden von E-Mails und anderen Dingen.  All dies finden Sie in der Funktion <code>start_kayak</code> , die wir jetzt betrachten. <br><br>  Um diese Funktion nutzen zu k√∂nnen, ben√∂tigen Sie Informationen zu St√§dten und Daten.  Mithilfe dieser Informationen bildet sie einen Link in der <code>kayak</code> , √ºber den zu der Seite gewechselt wird, auf der die Suchergebnisse nach ihrer besten √úbereinstimmung mit der Abfrage sortiert werden.  Nach der ersten Scraping-Sitzung arbeiten wir mit den Preisen in der Tabelle oben auf der Seite.  Wir finden n√§mlich den minimalen Ticketpreis und den Durchschnittspreis.  All dies wird zusammen mit der von der Website ausgegebenen Vorhersage per E-Mail gesendet.  Auf der Seite sollte sich die entsprechende Tabelle in der oberen linken Ecke befinden.  Die Arbeit mit dieser Tabelle kann √ºbrigens zu Fehlern bei der Suche nach genauen Daten f√ºhren, da in diesem Fall die Tabelle nicht auf der Seite angezeigt wird. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">start_kayak</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(city_from, city_to, date_start, date_end)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-string"><span class="hljs-string">"""City codes - it's the IATA codes!   Date format -  YYYY-MM-DD"""</span></span>     kayak = (<span class="hljs-string"><span class="hljs-string">'https://www.kayak.com/flights/'</span></span> + city_from + <span class="hljs-string"><span class="hljs-string">'-'</span></span> + city_to +            <span class="hljs-string"><span class="hljs-string">'/'</span></span> + date_start + <span class="hljs-string"><span class="hljs-string">'-flexible/'</span></span> + date_end + <span class="hljs-string"><span class="hljs-string">'-flexible?sort=bestflight_a'</span></span>)   driver.get(kayak)   sleep(randint(<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>))     <span class="hljs-comment"><span class="hljs-comment">#    ,           try   try:       xp_popup_close = '//button[contains(@id,"dialog-close") and contains(@class,"Button-No-Standard-Style close ")]'       driver.find_elements_by_xpath(xp_popup_close)[5].click()   except Exception as e:       pass   sleep(randint(60,95))   print('loading more.....')  #     load_more()     print('starting first scrape.....')   df_flights_best = page_scrape()   df_flights_best['sort'] = 'best'   sleep(randint(60,80))     #      ,        matrix = driver.find_elements_by_xpath('//*[contains(@id,"FlexMatrixCell")]')   matrix_prices = [price.text.replace('$','') for price in matrix]   matrix_prices = list(map(int, matrix_prices))   matrix_min = min(matrix_prices)   matrix_avg = sum(matrix_prices)/len(matrix_prices)     print('switching to cheapest results.....')   cheap_results = '//a[@data-code = "price"]'   driver.find_element_by_xpath(cheap_results).click()   sleep(randint(60,90))   print('loading more.....')  #     load_more()     print('starting second scrape.....')   df_flights_cheap = page_scrape()   df_flights_cheap['sort'] = 'cheap'   sleep(randint(60,80))     print('switching to quickest results.....')   quick_results = '//a[@data-code = "duration"]'   driver.find_element_by_xpath(quick_results).click()    sleep(randint(60,90))   print('loading more.....')  #     load_more()     print('starting third scrape.....')   df_flights_fast = page_scrape()   df_flights_fast['sort'] = 'fast'   sleep(randint(60,80))     #     Excel-,         final_df = df_flights_cheap.append(df_flights_best).append(df_flights_fast)   final_df.to_excel('search_backups//{}_flights_{}-{}_from_{}_to_{}.xlsx'.format(strftime("%Y%m%d-%H%M"),                                                                                  city_from, city_to,                                                                                  date_start, date_end), index=False)   print('saved df.....')     #    ,  ,  ,      xp_loading = '//div[contains(@id,"advice")]'   loading = driver.find_element_by_xpath(xp_loading).text   xp_prediction = '//span[@class="info-text"]'   prediction = driver.find_element_by_xpath(xp_prediction).text   print(loading+'\n'+prediction)     #    loading   , , ,        #    -    "Not Sure"   weird = '¬Ø\\_(„ÉÑ)_/¬Ø'   if loading == weird:       loading = 'Not sure'     username = 'YOUREMAIL@hotmail.com'   password = 'YOUR PASSWORD'   server = smtplib.SMTP('smtp.outlook.com', 587)   server.ehlo()   server.starttls()   server.login(username, password)   msg = ('Subject: Flight Scraper\n\n\ Cheapest Flight: {}\nAverage Price: {}\n\nRecommendation: {}\n\nEnd of message'.format(matrix_min, matrix_avg, (loading+'\n'+prediction)))   message = MIMEMultipart()   message['From'] = 'YOUREMAIL@hotmail.com'   message['to'] = 'YOUROTHEREMAIL@domain.com'   server.sendmail('YOUREMAIL@hotmail.com', 'YOUROTHEREMAIL@domain.com', msg)   print('sent email.....')</span></span></code> </pre> <br>  Ich habe dieses Skript mit einem Outlook-Konto (hotmail.com) getestet.  Ich habe nicht √ºberpr√ºft, ob das Google Mail-Konto ordnungsgem√§√ü funktioniert. Dieses Mail-System ist sehr beliebt, aber es gibt viele m√∂gliche Optionen.  Wenn Sie ein Hotmail-Konto verwenden, m√ºssen Sie nur Ihre Daten in den Code eingeben, damit alles funktioniert. <br><br>  Wenn Sie verstehen m√∂chten, was genau in separaten Abschnitten des Codes dieser Funktion ausgef√ºhrt wird, k√∂nnen Sie sie kopieren und mit ihnen experimentieren.  Codeexperimente sind der einzige Weg, dies zu verstehen. <br><br><h2>  <font color="#3AC1EF">Bereites System</font> </h2><br>  Nachdem alles, wor√ºber wir gesprochen haben, erledigt ist, k√∂nnen wir eine einfache Schleife erstellen, in der unsere Funktionen aufgerufen werden.  Das Skript fragt den Benutzer nach Daten zu St√§dten und Daten.  Wenn Sie mit einem konstanten Neustart des Skripts testen, ist es unwahrscheinlich, dass Sie diese Daten jedes Mal manuell eingeben m√∂chten. Daher k√∂nnen die entsprechenden Zeilen f√ºr den Testzeitpunkt auskommentiert werden, indem Sie diejenigen auskommentieren, die darunter liegen und in denen die f√ºr das Skript erforderlichen Daten fest festgelegt sind. <br><br><pre> <code class="python hljs">city_from = input(<span class="hljs-string"><span class="hljs-string">'From which city? '</span></span>) city_to = input(<span class="hljs-string"><span class="hljs-string">'Where to? '</span></span>) date_start = input(<span class="hljs-string"><span class="hljs-string">'Search around which departure date? Please use YYYY-MM-DD format only '</span></span>) date_end = input(<span class="hljs-string"><span class="hljs-string">'Return when? Please use YYYY-MM-DD format only '</span></span>) <span class="hljs-comment"><span class="hljs-comment"># city_from = 'LIS' # city_to = 'SIN' # date_start = '2019-08-21' # date_end = '2019-09-07' for n in range(0,5):   start_kayak(city_from, city_to, date_start, date_end)   print('iteration {} was complete @ {}'.format(n, strftime("%Y%m%d-%H%M")))     #  4    sleep(60*60*4)   print('sleep finished.....')</span></span></code> </pre> <br>  Hier ist der Testlauf des Skripts. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/44c/305/023/44c305023996a98a3dec745493dce7a7.png"></div><br>  <i><font color="#999999">Testlaufskript</font></i> <br><br><h2>  <font color="#3AC1EF">Zusammenfassung</font> </h2><br>  Wenn Sie an diesen Punkt kommen - herzlichen Gl√ºckwunsch!  Jetzt haben Sie einen funktionierenden Web-Scraper, obwohl ich bereits viele M√∂glichkeiten sehe, ihn zu verbessern.  Beispielsweise kann es in Twilio integriert werden, sodass anstelle von E-Mails Textnachrichten gesendet werden.  Sie k√∂nnen ein VPN oder etwas anderes verwenden, um gleichzeitig Ergebnisse von mehreren Servern zu erhalten.  Es gibt auch ein wiederkehrendes Problem beim √úberpr√ºfen des Site-Benutzers, ob er eine Person ist, aber dieses Problem kann auch gel√∂st werden.  In jedem Fall haben Sie jetzt eine Basis, die Sie erweitern k√∂nnen, wenn Sie m√∂chten.  Stellen Sie beispielsweise sicher, dass die Excel-Datei als Anhang an eine E-Mail an den Benutzer gesendet wird. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de451872/">https://habr.com/ru/post/de451872/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de451860/index.html">Mathematiker haben den perfekten Weg gefunden, um Zahlen zu multiplizieren</a></li>
<li><a href="../de451862/index.html">Joe Diprims musikalischer Blitz: Ein Autodidakt stellt Tesla-Spulen zur Unterhaltung und zum Verdienen her</a></li>
<li><a href="../de451864/index.html">Kritische RCE-Sicherheitsanf√§lligkeit der EternalBlue-Ebene unter Windows erkannt</a></li>
<li><a href="../de451866/index.html">W√§hlen Sie die n√§chsten Knoten im Netzwerk</a></li>
<li><a href="../de451870/index.html">Moderne C ++ - Funktionen, die alle Programmierer kennen m√ºssen</a></li>
<li><a href="../de451874/index.html">Top SEO Trends bei Google</a></li>
<li><a href="../de451876/index.html">Rechenzentrum Frankfurt: Rechenzentrum Telehouse</a></li>
<li><a href="../de451878/index.html">Live-Streaming von Stereovideos auf VR-Brillen (Oculus Go)</a></li>
<li><a href="../de451880/index.html">DevPRO'19: Blick vom Wrike-Stand</a></li>
<li><a href="../de451884/index.html">Sieben Jahre als Entwickler gearbeitet: Welche Lektionen habe ich gelernt?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>