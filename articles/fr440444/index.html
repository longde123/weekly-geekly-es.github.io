<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤴🏿 🥃 🏇🏻 Systèmes basés sur la file d'attente 🚇 🤖 🚶🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut, habrozhiteli! 

 Nous avons décidé de partager la traduction du chapitre «Systèmes basés sur des files d'attente de tâches» de la nouveauté à v...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Systèmes basés sur la file d'attente</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/440444/">  Salut, habrozhiteli! <br><br>  Nous avons décidé de partager la traduction du chapitre «Systèmes basés sur des files d'attente de tâches» de la nouveauté à venir «Systèmes distribués.  Design patterns »(déjà dans l'imprimerie). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/c0/c7/6k/c0c76kbloj9pjb2olgoma345bes.png" alt="image"></div><br>  La forme la plus simple de traitement par lots est la file d'attente des tâches.  Dans un système avec une file d'attente de tâches, il existe un ensemble de tâches qui doivent être terminées.  Chaque tâche est complètement indépendante des autres et peut être traitée sans aucune interaction avec elles.  Dans le cas général, l'objectif d'un système avec une file d'attente de tâches est de s'assurer que chaque étape du travail est terminée dans un laps de temps donné.  Le nombre de workflows augmente ou diminue en fonction de l'évolution de la charge.  Le schéma de la file d'attente de tâches généralisée est présenté à la Fig.  10.1. <br><a name="habracut"></a><br><h3>  Un système basé sur une file d'attente de tâches généralisée </h3><br>  La ligne de tâches est un exemple idéal qui montre toute la puissance des modèles de conception de systèmes distribués.  La plupart de la logique de la file d'attente des tâches ne dépend pas du type de travail effectué.  Dans de nombreux cas, il en va de même pour l'exécution des tâches elles-mêmes. <br><br>  Illustrons cette déclaration à l'aide de la file d'attente des tâches illustrée à la Fig.  10.1.  Après l'avoir réexaminé, déterminez quelles fonctions peuvent être fournies par un ensemble partagé de conteneurs.  Il devient évident que la plupart de l'implémentation d'une file d'attente de tâches conteneurisée peut être utilisée par un large éventail d'utilisateurs. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8h/b2/qf/8hb2qfylszx_qmk8dicocvnvxtg.png" alt="image"></div><br>  La mise en file d'attente des tâches basée sur des conteneurs nécessite des interfaces correspondantes entre les conteneurs de bibliothèque et les conteneurs avec une logique utilisateur.  Dans la file d'attente des tâches conteneurisées, deux interfaces sont distinguées: l'interface du conteneur source, qui fournit un flux de tâches nécessitant un traitement, et l'interface du conteneur d'exécution, qui sait comment les gérer. <br><br><h3>  Interface du conteneur source </h3><br>  Toute file d'attente de tâches fonctionne sur la base d'un ensemble de tâches qui nécessitent un traitement.  Selon l'application spécifique implémentée sur la base de la file d'attente des tâches, il existe de nombreuses sources de tâches qui y entrent.  Mais après avoir reçu un ensemble de tâches, le schéma d'opération de file d'attente est assez simple.  Par conséquent, nous pouvons séparer la logique spécifique à l'application de la source de tâche du schéma généralisé de traitement de la file d'attente de tâches.  En rappelant les modèles de groupes de conteneurs discutés précédemment, vous pouvez voir ici la mise en œuvre du modèle Ambassador.  Le conteneur de file d'attente de tâches généralisé est le conteneur d'application principal et le conteneur source spécifique à l'application est un ambassadeur diffusant les demandes du conteneur du gestionnaire de files d'attente aux exécuteurs de tâches spécifiques.  Ce groupe de conteneurs est illustré à la Fig.  10.2. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bx/7u/pc/bx7upcqfw7gxqus2zqrhbrhqpes.png" alt="image"></div><br>  Soit dit en passant, bien que l'ambassadeur de conteneur soit spécifique à l'application (ce qui est évident), il existe également un certain nombre d'implémentations généralisées de l'API de source de tâches.  Par exemple, la source peut être une liste de photos situées dans un stockage cloud, un ensemble de fichiers sur un lecteur réseau, ou même une file d'attente dans des systèmes fonctionnant sur le principe de "publication / abonnement", tels que Kafka ou Redis.  Bien que les utilisateurs puissent choisir les ambassadeurs de conteneurs les mieux adaptés à leur tâche, ils doivent utiliser une implémentation de «bibliothèque» généralisée du conteneur lui-même.  Cela minimisera la quantité de travail et maximisera la réutilisation du code. <br><br>  <b>API de file d'attente des tâches</b>  Étant donné le mécanisme d'interaction entre la file d'attente des tâches et le conteneur dépendant de l'application, nous devons formuler une définition formelle de l'interface entre les deux conteneurs.  Il existe de nombreux protocoles différents, mais les API HTTP RESTful sont faciles à implémenter et constituent la norme de facto pour de telles interfaces.  La file d'attente des tâches s'attend à ce que les URL suivantes soient implémentées dans le conteneur après: <br><br><ul><li>  GET <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">localhost / api / v1 / items;</a> </li><li>  GET <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">localhost / api / v1 / items</a> &lt;item-name&gt;. </li></ul><blockquote>  Pourquoi ajouter la v1 à votre définition d'API, demandez-vous?  Y aura-t-il jamais une deuxième version de l'interface?  Cela semble illogique, mais le coût de versioning de l'API lorsqu'il est initialement défini est minime.  La réalisation du refactoring approprié plus tard sera extrêmement coûteuse.  Faites-en une règle pour ajouter des versions à toutes les API, même si vous n'êtes pas sûr qu'elles changeront jamais.  Dieu sauve le coffre-fort. <br></blockquote>  URL / éléments / renvoie une liste de toutes les tâches: <br><br><pre><code class="plaintext hljs">{ kind: ItemList, apiVersion: v1, items: [ "item-1", "item-2", …. ] }</code> </pre> <br>  L'URL / items / &lt;item-name&gt; fournit des informations détaillées sur une tâche spécifique: <br><br><pre> <code class="plaintext hljs">{ kind: Item, apiVersion: v1, data: { "some": "json", "object": "here", } }</code> </pre> <br>  Veuillez noter que l'API ne fournit aucun mécanisme pour corriger le fait de la tâche.  On pourrait développer une API plus complexe et transférer la plupart de l'implémentation à un ambassadeur de conteneurs.  N'oubliez pas, cependant, que notre objectif est de concentrer le plus possible l'implémentation globale dans le gestionnaire de files d'attente de tâches.  À cet égard, le gestionnaire de files d'attente de tâches doit lui-même surveiller les tâches qui ont déjà été traitées et celles qui doivent encore l'être. <br><br>  De cette API, nous obtenons des informations sur une tâche spécifique, puis passons la valeur du champ item.data de l'interface conteneur de l'exécuteur. <br><br><h3>  Exécution de l'interface de conteneur </h3><br>  Dès que le gestionnaire de files d'attente a reçu la tâche suivante, il doit la confier à un exécuteur.  Il s'agit de la deuxième interface de la file d'attente de tâches généralisée.  Le conteneur lui-même et son interface sont légèrement différents de l'interface du conteneur source pour plusieurs raisons.  Tout d'abord, il s'agit d'une API unique.  Le travail de l'exécuteur testamentaire commence par un seul appel, et pendant le cycle de vie du conteneur, plus aucun appel n'est effectué.  Deuxièmement, le conteneur en cours d'exécution et le gestionnaire de files d'attente de tâches se trouvent dans des groupes de conteneurs différents.  L'exécuteur de conteneur est lancé via l'API d'orchestrateur de conteneur dans son propre groupe.  Cela signifie que le gestionnaire de files d'attente de tâches doit effectuer un appel distant pour lancer le conteneur d'exécution.  Cela signifie également que vous devez faire plus attention aux problèmes de sécurité, car un utilisateur malveillant du cluster peut le charger avec un travail inutile. <br><br>  Dans le conteneur source, nous avons utilisé un simple appel HTTP pour envoyer la liste des tâches au gestionnaire de tâches.  Cela a été fait en supposant que cet appel d'API devait être effectué plusieurs fois, et les problèmes de sécurité n'étaient pas pris en compte, car tout fonctionnait dans le cadre de l'hôte local.  L'API de conteneur ne doit être appelée qu'une seule fois et il est important de s'assurer que les autres utilisateurs du système ne peuvent pas ajouter de travail aux exécuteurs, même par accident ou par intention malveillante.  Par conséquent, pour le conteneur en cours d'exécution, nous utiliserons l'API de fichier.  Lors de la création, nous transmettrons au conteneur une variable d'environnement appelée WORK_ITEM_FILE, dont la valeur fait référence à un fichier dans le système de fichiers interne du conteneur.  Ce fichier contient des données sur la tâche à effectuer.  Ce type d'API, comme illustré ci-dessous, peut être implémenté par l'objet ConfigMap Kubernetes.  Il peut être monté dans un groupe de conteneurs sous forme de fichier (Fig. 10.3). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jn/p0/zb/jnp0zbiduvl1qsstlwa1mphwrxq.png" alt="image"></div><br>  Un tel mécanisme d'API de fichier est plus facile à implémenter à l'aide d'un conteneur.  Un exécuteur dans une file d'attente de tâches est souvent un simple script shell qui accède à plusieurs outils.  Il n'est pas pratique d'élever un serveur Web entier pour la gestion des tâches - cela entraîne une complication de l'architecture.  Comme dans le cas des sources de tâches, la plupart des exécuteurs de conteneurs seront des conteneurs spécialisés pour certaines tâches, mais il existe également des exécuteurs généralisés applicables pour résoudre plusieurs tâches différentes. <br><br>  Prenons l'exemple d'un conteneur en cours d'exécution qui télécharge un fichier à partir du stockage cloud, exécute un script shell dessus, puis copie le résultat vers le stockage cloud.  Un tel conteneur peut être pour la plupart général, mais un scénario spécifique peut lui être transmis en tant que paramètre.  Ainsi, la plupart du code de gestion de fichiers peut être réutilisé par de nombreux utilisateurs / files d'attente de tâches.  L'utilisateur final n'a qu'à fournir un script contenant les spécificités du traitement des fichiers. <br><br><h3>  Infrastructure de file d'attente de tâches commune </h3><br>  Que reste-t-il à implémenter dans une implémentation de file d'attente réutilisable si vous avez déjà des implémentations des deux interfaces de conteneur décrites précédemment?  L'algorithme de base de la file d'attente des tâches est assez simple. <br><br><ol><li>  Téléchargez les tâches actuellement disponibles à partir du conteneur source. </li><li>  Clarifiez le statut de la file d'attente des tâches pour quelles tâches ont déjà été terminées ou sont encore en cours d'exécution. </li><li>  Pour chacune des tâches non résolues, créez des conteneurs de conteneurs avec une interface appropriée. </li><li>  Une fois le conteneur exécuté avec succès, notez que la tâche est terminée. </li></ol><br>  Cet algorithme est simple en mots, mais en réalité il n'est pas si facile à mettre en œuvre.  Heureusement, l'orchestre Kubernetes possède plusieurs fonctionnalités qui simplifient considérablement sa mise en œuvre.  À savoir: Kubernetes possède un objet Job qui garantit un fonctionnement fiable de la file d'attente des tâches.  Vous pouvez configurer l'objet Job pour qu'il démarre le conteneur d'exécution correspondant soit une fois, soit jusqu'à ce que la tâche soit terminée avec succès.  Si vous configurez le conteneur en cours d'exécution de sorte qu'il s'exécute avant la fin de la tâche, même lorsque la machine du cluster échoue, la tâche sera finalement terminée avec succès. <br><br>  Ainsi, la mise en file d'attente des tâches est grandement simplifiée, car l'orchestre assume la responsabilité de l'exécution fiable des tâches. <br><br>  De plus, Kubernetes vous permet d'annoter des tâches, ce qui nous permet de marquer chaque objet de tâche avec le nom de l'élément de file d'attente de tâches traité.  Il devient de plus en plus facile de distinguer les tâches qui sont traitées et terminées avec succès et avec une erreur. <br><br>  Cela signifie que nous pouvons implémenter la file d'attente des tâches au-dessus de l'orchestrateur Kubernetes sans utiliser notre propre référentiel.  Tout cela simplifie considérablement la tâche de construction de l'infrastructure de la file d'attente des tâches. <br><br>  Par conséquent, un algorithme détaillé pour le fonctionnement du conteneur, le gestionnaire de files d'attente de tâches, est le suivant. <br><br>  Répétez sans cesse. <br><br><ol><li>  Obtenez la liste des tâches via l'interface du conteneur - la source des tâches. </li><li>  Obtenez une liste des tâches desservant cette file d'attente de tâches. </li><li>  Sur la base de ces listes, sélectionnez une liste de tâches non traitées. </li><li>  Pour chaque tâche non traitée, créez un objet Job qui génère le conteneur d'exécution correspondant. </li></ol><br>  Voici un script Python qui implémente cette file d'attente: <br><br><pre> <code class="plaintext hljs">import requests import json from kubernetes import client, config import time namespace = "default" def make_container(item, obj): container = client.V1Container() container.image = "my/worker-image" container.name = "worker" return container def make_job(item): response = requests.get("http://localhost:8000/items/{}".format(item)) obj = json.loads(response.text) job = client.V1Job() job.metadata = client.V1ObjectMeta() job.metadata.name = item job.spec = client.V1JobSpec() job.spec.template = client.V1PodTemplate() job.spec.template.spec = client.V1PodTemplateSpec() job.spec.template.spec.restart_policy = "Never" job.spec.template.spec.containers = [ make_container(item, obj) ] return job def update_queue(batch): response = requests.get("http://localhost:8000/items") obj = json.loads(response.text) items = obj['items'] ret = batch.list_namespaced_job(namespace, watch=False) for item in items: found = False for i in ret.items: if i.metadata.name == item: found = True if not found: #    Job,  #   job = make_job(item) batch.create_namespaced_job(namespace, job) config.load_kube_config() batch = client.BatchV1Api() while True: update_queue(batch) time.sleep(10)</code> </pre> <br><h3>  Atelier  Implémentation d'un générateur de vignettes pour les fichiers vidéo </h3><br>  Comme exemple d'utilisation de la file d'attente des tâches, considérez la tâche de génération de vignettes de fichiers vidéo.  Sur la base de ces miniatures, les utilisateurs décident quelles vidéos ils souhaitent regarder. <br><br>  Pour implémenter les vignettes, vous avez besoin de deux conteneurs.  Le premier concerne la source des tâches.  Il sera plus facile de placer des tâches sur un lecteur réseau partagé connecté, par exemple via NFS (Network File System, système de fichiers réseau).  La source de la tâche reçoit une liste de fichiers dans ce répertoire et les transmet à l'appelant. <br><br>  Je vais donner un programme simple sur NodeJS: <br><br><pre> <code class="plaintext hljs">const http = require('http'); const fs = require('fs'); const port = 8080; const path = process.env.MEDIA_PATH; const requestHandler = (request, response) =&gt; { console.log(request.url); fs.readdir(path + '/*.mp4', (err, items) =&gt; { var msg = { 'kind': 'ItemList', 'apiVersion': 'v1', 'items': [] }; if (!items) { return msg; } for (var i = 0; i &lt; items.length; i++) { msg.items.push(items[i]); } response.end(JSON.stringify(msg)); }); } const server = http.createServer(requestHandler); server.listen(port, (err) =&gt; { if (err) { return console.log('  ', err); } console.log(`    ${port}`) });</code> </pre> <br>  Cette source définit la liste des films à traiter.  L'utilitaire ffmpeg est utilisé pour extraire les vignettes. <br><br>  Vous pouvez créer un conteneur qui exécute la commande suivante: <br><br><pre> <code class="plaintext hljs">ffmpeg -i ${INPUT_FILE} -frames:v 100 thumb.png</code> </pre> <br>  La commande extrait une trame sur 100 (paramètre -frames: v 100) et l'enregistre au format PNG (par exemple, thumb1.png, thumb2.png, etc.). <br><br>  Ce type de traitement peut être implémenté sur la base de l'image Docker ffmpeg existante.  L' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">image de jrottenberg / ffmpeg</a> est populaire. <br><br>  En définissant un conteneur source simple et un conteneur d'exécution encore plus simple, il est facile de voir les avantages d'un système de gestion de file d'attente générique et orienté conteneur.  Il réduit considérablement le temps entre la conception et la mise en œuvre de la file d'attente des tâches. <br><br><h3>  Mise à l'échelle dynamique des artistes </h3><br>  La file d'attente des tâches considérée précédemment est bien adaptée au traitement des tâches au fur et à mesure qu'elles deviennent disponibles, mais peut entraîner une charge brutale sur les ressources de l'orchestrateur de cluster de conteneurs.  Cela est utile lorsque vous avez de nombreux types de tâches différents qui créent des pics de charge à des moments différents et répartissent ainsi la charge sur le cluster de manière uniforme dans le temps. <br><br>  Mais si vous n'avez pas assez de types de charge, l'approche «puis épaisse, puis vide» pour faire évoluer la file d'attente des tâches peut nécessiter la réservation de ressources supplémentaires pour prendre en charge les rafales de charge.  Le reste du temps, les ressources seront inactives, vidant inutilement votre portefeuille. <br><br>  Pour résoudre ce problème, vous pouvez limiter le nombre total d'objets Job générés par la file d'attente des tâches.  Cela limitera naturellement le nombre de travaux traités en parallèle et, par conséquent, réduira l'utilisation des ressources lors des pics de charge.  En revanche, la durée de chaque tâche individuelle augmentera avec une charge élevée sur le cluster. <br><br>  Si la charge est spasmodique, ce n'est pas effrayant, car les intervalles de temps d'arrêt peuvent être utilisés pour terminer les tâches accumulées.  Cependant, si la charge constante est trop élevée, la file d'attente des tâches n'aura pas le temps de traiter les tâches entrantes et de plus en plus de temps sera consacré à leur mise en œuvre. <br><br>  Dans une telle situation, vous devrez ajuster dynamiquement le nombre maximal de tâches parallèles et, par conséquent, les ressources informatiques disponibles pour maintenir le niveau de performances requis.  Heureusement, il existe des formules mathématiques qui vous permettent de déterminer quand il est nécessaire de mettre à l'échelle la file d'attente des tâches pour traiter plus de demandes. <br><br>  Imaginez une file d'attente de tâches dans laquelle une nouvelle tâche apparaît en moyenne une fois par minute et son achèvement prend en moyenne 30 secondes.  Une telle file d'attente est capable de faire face au flux de tâches qui y pénètrent.  Même si un grand ensemble de tâches arrive à la fois, créant un embouteillage, alors l'embouteillage sera éliminé au fil du temps, car la file d'attente parvient à traiter deux tâches en moyenne avant l'arrivée de la tâche suivante. <br><br>  Si une nouvelle tâche arrive toutes les minutes et qu'il faut en moyenne 1 minute pour traiter une tâche, alors un tel système est parfaitement équilibré, mais en même temps réagit mal aux changements de la charge.  Elle est capable de faire face à des éclats de charge, mais cela lui prendra beaucoup de temps.  Le système ne sera pas inactif, mais il n'y aura pas de réserve de temps informatique pour compenser l'augmentation à long terme de la vitesse de réception des nouvelles tâches.  Pour maintenir la stabilité du système, il est nécessaire d'avoir une réserve en cas de croissance de charge à long terme ou de retards imprévus dans les tâches de traitement. <br><br>  Enfin, considérons un système dans lequel une tâche par minute arrive et le traitement des tâches prend deux minutes.  Un tel système perdra constamment ses performances.  La longueur de la file d'attente des tâches augmentera avec le délai entre la réception et le traitement des tâches (et le degré de gêne des utilisateurs). <br><br>  Les valeurs de ces deux indicateurs doivent être surveillées en permanence.  En faisant la moyenne du temps entre la réception des tâches pendant une longue période de temps, par exemple, sur la base du nombre de tâches par jour, nous obtenons une estimation de l'intervalle entre les tâches.  Il est également nécessaire de surveiller le temps de traitement moyen de la tâche (hors temps passé dans la file d'attente).  Dans une file d'attente de tâches stable, le temps de traitement moyen des tâches doit être inférieur à l'intervalle entre les tâches.  Pour garantir que cette condition est remplie, il est nécessaire d'ajuster dynamiquement le nombre de files d'attente disponibles de ressources informatiques.  Si les travaux sont traités en parallèle, le temps de traitement doit être divisé par le nombre de travaux traités en parallèle.  Par exemple, si une tâche est traitée par minute, mais que quatre tâches sont traitées en parallèle, le temps de traitement effectif d'une tâche est de 15 secondes, ce qui signifie que l'intervalle entre les tâches doit être d'au moins 16 secondes. <br><br>  Cette approche vous permet de créer facilement un module pour faire évoluer la file d'attente des tâches vers le haut.  La réduction d'échelle est un peu plus problématique.  Néanmoins, il est possible d'utiliser les mêmes calculs que précédemment, en posant en plus la réserve de ressources informatiques déterminée par la voie heuristique.  Par exemple, vous pouvez réduire le nombre de tâches parallèles jusqu'à ce que le temps de traitement pour une tâche soit de 90% de l'intervalle entre les tâches. <br><br><h3>  Modèle multi-travailleurs </h3><br>  L'un des principaux sujets de ce livre est l'utilisation de conteneurs pour encapsuler et réutiliser le code.  Il est également pertinent pour les modèles de mise en file d'attente des tâches décrits dans ce chapitre.  En plus des conteneurs qui gèrent la file d'attente elle-même, vous pouvez réutiliser des groupes de conteneurs qui composent l'implémentation des acteurs.  Supposons que vous ayez besoin de traiter chaque tâche d'une file d'attente de trois manières différentes.  Par exemple, pour détecter des visages sur une photo, associez-les à des personnes spécifiques, puis brouillez les parties correspondantes de l'image.  Vous pouvez placer tous les traitements dans un seul conteneur d'exécution, mais il s'agit d'une solution unique qui ne peut pas être réutilisée.  Pour couvrir autre chose, comme des voitures, sur la photo, vous devrez créer un artiste conteneur à partir de zéro. <br><br>  La possibilité de ce type de réutilisation peut être obtenue en appliquant le modèle Multi-Worker, qui est en fait un cas spécial du modèle Adapter décrit au début du livre.  Le modèle Multi-Worker convertit un ensemble de conteneurs en un conteneur commun avec l'interface logicielle du conteneur en cours d'exécution.  Ce conteneur partagé délègue le traitement à plusieurs conteneurs réutilisables distincts.  Ce processus est schématisé sur la Fig.  10.4. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ei/wx/zv/eiwxzvdwvre9k_ftflo2xfz_bz4.png" alt="image"></div><br>  En réutilisant le code en combinant des conteneurs d'exécution, le travail des personnes qui conçoivent des systèmes de traitement par lots distribués est réduit. <br><br>  »Plus d'informations sur le livre sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site Web de l'éditeur</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Contenu</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Extrait</a> <br><br>  Pour habrozhitelami 20% de réduction sur le coupon - <b>Systèmes distribués</b> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr440444/">https://habr.com/ru/post/fr440444/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr440432/index.html">Vendredi SciFi sur les métiers du futur: «Real Girls»</a></li>
<li><a href="../fr440434/index.html">L'industrie automobile russe: la voie des technologies additives</a></li>
<li><a href="../fr440436/index.html">Tâches pratiques Java - pour les cours et autres activités</a></li>
<li><a href="../fr440438/index.html">Pré-roll MQTT / UDP: configuration à distance et signature numérique</a></li>
<li><a href="../fr440440/index.html">Comment j'ai obtenu un emploi avec un salaire de 300 000 $ dans la Silicon Valley</a></li>
<li><a href="../fr440446/index.html">Série de webinaires TDMS Fair Workflow</a></li>
<li><a href="../fr440448/index.html">Qu'est-ce qui a mis fin à l'histoire avec le programmeur sans-abri</a></li>
<li><a href="../fr440450/index.html">Il y a un rouble pour l'entrée, mais il n'y a pas d'issue: comment les fichiers tiers entrent dans la blockchain et que faire à ce sujet</a></li>
<li><a href="../fr440454/index.html">Paul Graham: ce que j'ai appris de Hacker News</a></li>
<li><a href="../fr440458/index.html">Prédiction de l'état du réseau VoIP sur la base des fichiers journaux de texte du serveur d'applications SIP</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>