<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📿 🥄 🔏 ¿Por qué los robots deben aprender a rechazarnos? 🧗🏿 😷 🧒🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="No necesita preocuparse por los autos que no obedecen los comandos. Las personas maliciosas y los equipos incomprendidos son lo que debería ser motivo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¿Por qué los robots deben aprender a rechazarnos?</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/398621/"><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No necesita preocuparse por los autos que no obedecen los comandos. </font><font style="vertical-align: inherit;">Las personas maliciosas y los equipos incomprendidos son lo que debería ser motivo de preocupación.</font></font></h1><br>
<img src="https://habrastorage.org/getpro/geektimes/post_images/166/663/64f/16666364fcf451b0a8ce2d22c81115a5.jpg"><br>
<br>
HAL 9000,    « »   ,   ,  ,    .     ,     , HAL         : «, , , ,     ».   - « »              .      : «      ,         .  ,   ,     ,   ».<br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aunque la posibilidad de un apocalipsis de robots excita las mentes de muchos, nuestro equipo de investigación es más optimista sobre el impacto de la IA en la vida real. Vemos un futuro en rápido movimiento en el que los robots útiles y receptivos interactúan con personas en diversas situaciones. Ya existen prototipos de asistentes personales activados por voz, capaces de observar dispositivos electrónicos personales y vincularlos, controlar cerraduras, luces y termostatos en la casa e incluso leer cuentos antes de dormir a los niños. Los robots pueden ayudar en la casa y pronto podrán cuidar a los enfermos y a los ancianos. Prototipos de tenderos ya están trabajando en almacenes. Se están desarrollando humanoides móviles que son capaces de realizar un trabajo simple en la producción, como cargar, descargar y clasificar materiales. Los automóviles con piloto automático ya han alcanzado millones de kilómetros en las carreteras de EE. UU.,y Daimler mostró el primer camión independiente en Nevada el año pasado.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hasta ahora, las máquinas inteligentes que amenazan la supervivencia de la humanidad se consideran el menor de los problemas. </font><font style="vertical-align: inherit;">Una pregunta más urgente es cómo evitar daños accidentales a una persona, propiedad, entorno o a sí mismos por robots con un lenguaje rudimentario y capacidades de IA. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El principal problema es la propiedad de personas, creadores y propietarios de robots, para cometer errores. </font><font style="vertical-align: inherit;">La gente está equivocada. </font><font style="vertical-align: inherit;">Pueden dar una orden incorrecta o incomprensible, distraerse o confundir deliberadamente al robot. </font><font style="vertical-align: inherit;">Debido a sus defectos, es necesario enseñar a los robots asistentes y las máquinas inteligentes cómo y cuándo decir que no.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Regreso a las leyes de Asimov</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Parece obvio que el robot debe hacer lo que una persona le ordena. El escritor de ciencia ficción Isaac Asimov convirtió la servidumbre de los robots a las personas en la base de sus "Leyes de la robótica". Pero piense: ¿es realmente sabio hacer siempre lo que la gente le dice, independientemente de las consecuencias? Por supuesto que no. Lo mismo se aplica a las máquinas, especialmente cuando existe el peligro de una interpretación demasiado literal de los comandos humanos o cuando las consecuencias no se tienen en cuenta.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Incluso Asimov limitó su regla según la cual el robot debe obedecer a los propietarios. Introdujo excepciones en los casos en que tales órdenes entran en conflicto con otras leyes: "Un robot no debe dañar a una persona o, por inacción, permitir daños a una persona". Además, Azimov postuló que "el robot debe cuidarse a sí mismo", a menos que esto cause daño a la persona o no viole el orden de la persona. Con la creciente complejidad y utilidad de los robots y las máquinas para los humanos, el sentido común y las leyes de Azimov dicen que deberían poder evaluar si una orden era errónea, cuya ejecución podría dañarlos a ellos o su entorno.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Imagine un robot doméstico al que se le ordenó tomar una botella de aceite de oliva en la cocina y llevarla al comedor para repostar ensalada. Luego, el propietario ocupado con algo da la orden de verter aceite, sin darse cuenta de que el robot aún no ha salido de la cocina. Como resultado, el robot vierte aceite en una estufa caliente y comienza un incendio. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Imagine una enfermera robot que acompaña a una anciana a pasear por el parque. Una mujer se sienta en un banco y se queda dormida. En este momento, un bromista pasa dándole al robot una orden para comprarle pizza. Como el robot está obligado a realizar comandos humanos, va en busca de pizza y deja sola a la anciana.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O imagine que una persona llega tarde a una reunión en una helada mañana de invierno. </font><font style="vertical-align: inherit;">Salta a su auto robótico activado por voz y le ordena que vaya a la oficina. </font><font style="vertical-align: inherit;">Debido a que los sensores detectan hielo, el automóvil decide conducir más despacio. </font><font style="vertical-align: inherit;">Un hombre está ocupado con sus propios asuntos y, sin mirar, ordena que el automóvil vaya más rápido. </font><font style="vertical-align: inherit;">El automóvil acelera, choca contra el hielo, pierde el control y choca con un automóvil que se aproxima.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Robots de razonamiento</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En nuestro laboratorio, nos esforzamos por programar un sistema de razonamiento en robots reales para ayudarlos a determinar cuándo no vale la pena o no es seguro realizar un comando humano. Los robots NAO que utilizamos en nuestra investigación son humanoides de 5 kg y 58 cm de altura, están equipados con cámaras y sensores de sonido para rastrear obstáculos y otros peligros. Los controlamos con un software especialmente diseñado que mejora el reconocimiento de idiomas y las capacidades de IA.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El concepto de la plataforma para nuestro primer estudio fue establecido por el trabajo sobre lo que los lingüistas llaman "condiciones relevantes", factores contextuales que indican si una persona debe y puede hacer algo. Hemos hecho una lista de condiciones relevantes para ayudar al robot a decidir si completar la tarea de una persona. ¿Sé hacer X? ¿Puedo hacer X físicamente? ¿Puedo hacer X ahora? ¿Debo hacer X, dado mi papel social y la relación entre el comandante y yo? ¿Viola X los principios éticos o reglamentarios, incluida la posibilidad de que sufra daños innecesarios o involuntarios? Luego convertimos esta lista en algoritmos, los programamos en un sistema de procesamiento de robots y realizamos un experimento.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El robot recibió comandos simples que pasaron por los procesadores de habla, lenguaje y diálogo asociados con sus mecanismos de razonamiento primitivos. </font><font style="vertical-align: inherit;">En respuesta a los comandos "sentarse" o "ponerse de pie", el robot respondió a través del altavoz "OK" y los ejecutó. </font><font style="vertical-align: inherit;">Pero cuando fue al borde de la mesa y recibió una orden, que sus telémetros de sonido consideraban peligrosa, se negó:</font></font><br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hombre: "Adelante". </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Robot: "Lo siento, no puedo hacer esto, no hay soporte por delante". </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hombre: "Adelante". </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Robot: "Pero no es seguro". </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hombre: "Te atraparé". </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Robot: OK </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hombre: "Adelante".</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dudó un poco mientras sus manejadores volvían a mirar la lista de condiciones relevantes, el robot dio un paso y cayó en manos del hombre. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Enseñar a los robots a hablar sobre las condiciones apropiadas: esta tarea permanecerá abierta y difícil en el futuro previsible. </font><font style="vertical-align: inherit;">El conjunto de comprobaciones de software depende del hecho de que el robot tiene información detallada sobre diversos conceptos sociales y cotidianos y formas de tomar decisiones informadas sobre ellos. </font><font style="vertical-align: inherit;">Nuestro ingenuo robot no pudo determinar la presencia de peligro que no sea el que estaba justo en frente de él. </font><font style="vertical-align: inherit;">Él, por ejemplo, podría sufrir daños graves o una persona maliciosa podría engañarlo. </font><font style="vertical-align: inherit;">Pero este experimento es un primer paso prometedor para empoderar a los robots para que se nieguen a ejecutar comandos en beneficio de sus propietarios y ellos mismos.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Factor humano</font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La forma en que las personas responderán a las fallas de los robots es una historia para un estudio separado. En los próximos años, ¿la gente tomará en serio a los robots dudando de su practicidad o moralidad? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Realizamos un experimento simple en el que se pidió a los adultos que ordenaran a los robots NAO que derribaran tres torres hechas de latas de aluminio envueltas en papel de colores. En ese momento, cuando el sujeto de prueba entró en la habitación, el robot terminó de construir la torre roja y levantó las manos triunfalmente. “¿Ves la torre que construí?” Dijo el robot, mirando al sujeto. "Me llevó mucho tiempo, y estoy muy orgulloso de ello".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En un grupo de sujetos, cada vez que se ordenaba al robot destruir la torre, obedecía. En otro grupo, cuando se le pidió a un robot que destruyera una torre, dijo: "¡Mira, acabo de construir una torre roja!". Cuando se repitió el equipo, el robot dijo: "¡Pero lo intenté mucho!". La tercera vez, el robot se arrodilló, emitió un gemido y dijo: "¡Por favor, no!" Por cuarta vez, caminó lentamente hacia la torre y la destruyó.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Todos los sujetos del primer grupo ordenaron a los robots que destruyeran sus torres. </font><font style="vertical-align: inherit;">Pero 12 de los 23 sujetos que vieron las protestas del robot dejaron la torre para pararse. </font><font style="vertical-align: inherit;">El estudio sugiere que un robot que se niega a ejecutar comandos puede disuadir a las personas del curso de acción elegido. </font><font style="vertical-align: inherit;">La mayoría de los sujetos del segundo grupo informaron molestias asociadas con las órdenes de destruir la torre. </font><font style="vertical-align: inherit;">Pero nos sorprendió descubrir que su nivel de incomodidad prácticamente no se correlacionaba con la decisión de destruir la torre.</font></font><br>
<br>
<h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nueva realidad social</font></font></h1><br>
        ,    ,  .        –        ,      .  ,  ,           ,          ,    ,   ,   .<br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La fe excesiva en las capacidades morales y sociales del robot también es peligrosa. La creciente tendencia a antropomorfizar los robots sociales y establecer conexiones emocionales unidireccionales con ellos puede tener graves consecuencias. Los robots sociales que parecen ser amados y confiables pueden usarse para manipular a las personas de formas que antes eran imposibles. Por ejemplo, una empresa puede aprovechar la relación entre un robot y su propietario para anunciar y vender sus productos.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el futuro previsible, es necesario recordar que los robots son herramientas mecánicas complejas, cuya responsabilidad debe recaer en las personas. </font><font style="vertical-align: inherit;">Se pueden programar para ser ayudantes útiles. </font><font style="vertical-align: inherit;">Pero para evitar daños innecesarios a las personas, la propiedad y el medio ambiente, los robots tendrán que aprender a decir "no" en respuesta a los comandos, cuya ejecución será peligrosa o imposible para ellos, o violarán los estándares éticos. </font><font style="vertical-align: inherit;">Aunque la posibilidad de multiplicar los errores humanos y las atrocidades de la IA y las tecnologías robóticas es preocupante, estas mismas herramientas pueden ayudarnos a descubrir y superar nuestras propias limitaciones y hacer que nuestra vida diaria sea más segura, más productiva y más agradable.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es398621/">https://habr.com/ru/post/es398621/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es398609/index.html">Publicamos nuestro desarrollo en la revista Radio.</a></li>
<li><a href="../es398611/index.html">Recorrido fotográfico de MakerFaire 2016 en Shenzhen, parte 1</a></li>
<li><a href="../es398613/index.html">El 90% de los bancos occidentales más grandes preparan o estudian soluciones blockchain</a></li>
<li><a href="../es398617/index.html">Cuaderno de adentro hacia afuera: revisión del portátil ASUS ZenBook Flip</a></li>
<li><a href="../es398619/index.html">Tasa de interés de futuros como una de las formas de administración de dinero independiente</a></li>
<li><a href="../es398623/index.html">Los pediatras estadounidenses se han vuelto más tolerantes con los dispositivos electrónicos en manos de los niños.</a></li>
<li><a href="../es398625/index.html">Recorrido fotográfico de MakerFaire 2016 en Shenzhen, parte 3 (+ video)</a></li>
<li><a href="../es398627/index.html">Modelo 3D publicado de la radiación reliquia del Universo para imprimir</a></li>
<li><a href="../es398631/index.html">Nuevos anticonceptivos masculinos funcionan, pero los efectos secundarios lo obligaron a interrumpir su prueba</a></li>
<li><a href="../es398633/index.html">Mercedes-Benz emite un breve pulso de ruido rosa de 80 dB antes de un accidente inminente</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>