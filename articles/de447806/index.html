<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö¥ ‚õÖÔ∏è üë©üèª‚Äçüî¨ Beschleunigung der Leistung neuronaler Netze durch Hashing üßòüèº üëµ ü§Ωüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Branche hat sich auf die Beschleunigung der Matrixmultiplikation konzentriert, aber die Verbesserung des Suchalgorithmus kann zu einer ernsthafter...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Beschleunigung der Leistung neuronaler Netze durch Hashing</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/447806/"><h3>  Die Branche hat sich auf die Beschleunigung der Matrixmultiplikation konzentriert, aber die Verbesserung des Suchalgorithmus kann zu einer ernsthafteren Leistungssteigerung f√ºhren </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/8e7/9a0/8e3/8e79a08e31fdeadd8258ab001cc23ec5.jpg"><br><br>  In den letzten Jahren war die Computerindustrie damit besch√§ftigt, die f√ºr k√ºnstliche neuronale Netze erforderlichen Berechnungen zu beschleunigen - sowohl f√ºr das Training als auch um Schlussfolgerungen aus ihrer Arbeit zu ziehen.  Insbesondere wurde viel Aufwand in die Entwicklung von Spezialeisen gesteckt, an dem diese Berechnungen durchgef√ºhrt werden k√∂nnen.  Google hat die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensor Processing Unit</a> (TPU) entwickelt, die erstmals 2016 der √ñffentlichkeit vorgestellt wurde.  Nvidia f√ºhrte sp√§ter die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">V100-</a> Grafikverarbeitungseinheit ein und beschrieb sie als einen Chip, der speziell f√ºr das Training und die Verwendung von KI sowie f√ºr andere Hochleistungsrechneranforderungen entwickelt wurde.  Voller anderer Startups, die sich auf andere Arten von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hardwarebeschleunigern konzentrieren</a> . <br><a name="habracut"></a><br>  Vielleicht machen sie alle einen gro√üen Fehler. <br><br>  Diese Idee wurde in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit</a> ge√§u√üert, die Mitte M√§rz auf der Website arXiv erschien.  Darin argumentieren die Autoren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beidi Chen</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tarun Medini</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anshumali Srivastava</a> von der Rice University, dass m√∂glicherweise die f√ºr den Betrieb neuronaler Netze entwickelte Spezialausr√ºstung f√ºr den falschen Algorithmus optimiert wird. <br><br>  Die Arbeit neuronaler Netze h√§ngt normalerweise davon ab, wie schnell das Ger√§t die Multiplikation der Matrizen durchf√ºhren kann, mit denen die Ausgangsparameter jedes k√ºnstlichen Neutrons - seine ‚ÄûAktivierung‚Äú - f√ºr einen bestimmten Satz von Eingabewerten bestimmt werden.  Matrizen werden verwendet, weil jeder Eingabewert f√ºr ein Neuron mit dem entsprechenden Gewichtungsparameter multipliziert wird und dann alle summiert werden - und diese Multiplikation mit Addition ist die Grundoperation der Matrixmultiplikation. <br><br>  Forscher der Rice University stellten wie einige andere Wissenschaftler fest, dass die Aktivierung vieler Neuronen in einer bestimmten Schicht des neuronalen Netzwerks zu gering ist und den von den nachfolgenden Schichten berechneten Ausgabewert nicht beeinflusst.  Wenn Sie also wissen, was diese Neuronen sind, k√∂nnen Sie sie einfach ignorieren. <br><br>  Es scheint, dass der einzige Weg, um herauszufinden, welche Neuronen in einer Schicht nicht aktiviert sind, darin besteht, zuerst alle Operationen der Matrixmultiplikation f√ºr diese Schicht durchzuf√ºhren.  Die Forscher erkannten jedoch, dass Sie sich tats√§chlich f√ºr diesen effizienteren Weg entscheiden k√∂nnen, wenn Sie das Problem aus einem anderen Blickwinkel betrachten.  "Wir betrachten dieses Problem als L√∂sung f√ºr das Suchproblem", sagt Srivastava. <br><br>  Das hei√üt, anstatt Matrixmultiplikationen zu berechnen und zu pr√ºfen, welche Neuronen f√ºr eine bestimmte Eingabe aktiviert wurden, k√∂nnen Sie nur sehen, welche Art von Neuronen sich in der Datenbank befinden.  Der Vorteil dieses Ansatzes bei dem Problem besteht darin, dass Sie eine allgemeine Strategie verwenden k√∂nnen, die von Informatikern seit langem verbessert wurde, um die Suche nach Daten in der Datenbank zu beschleunigen: Hashing. <br><br>  Mit Hashing k√∂nnen Sie schnell √ºberpr√ºfen, ob die Datenbanktabelle einen Wert enth√§lt, ohne jede Zeile in einer Zeile durchgehen zu m√ºssen.  Sie verwenden einen Hash, der einfach berechnet werden kann, indem Sie eine Hash-Funktion auf den gew√ºnschten Wert anwenden und angeben, wo dieser Wert in der Datenbank gespeichert werden soll.  Dann k√∂nnen Sie nur einen Ort √ºberpr√ºfen, um herauszufinden, ob dieser Wert dort gespeichert ist. <br><br>  √Ñhnliches haben die Forscher f√ºr Berechnungen im Zusammenhang mit neuronalen Netzen getan.  Das folgende Beispiel soll ihren Ansatz veranschaulichen: <br><br>  Angenommen, wir haben ein neuronales Netzwerk erstellt, das die handschriftliche Eingabe von Zahlen erkennt.  Angenommen, die Eingabe besteht aus grauen Pixeln in einem 16x16-Array, dh insgesamt 256 Zahlen.  Wir geben diese Daten an eine verborgene Schicht von 512 Neuronen weiter, deren Aktivierungsergebnisse von der Ausgangsschicht von 10 Neuronen gespeist werden, eine f√ºr jede der m√∂glichen Zahlen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e73/dec/941/e73dec9413ed37b2235bbd1ee8369ca6.jpg"><br><br>  <i>Tabellen aus Netzwerken: Bevor wir die Aktivierung von Neuronen in verborgenen Schichten berechnen, verwenden wir Hashes, um zu bestimmen, welche Neuronen aktiviert werden.</i>  <i>Hier wird der Hash der Eingabewerte H1 verwendet, um nach den entsprechenden Neuronen in der ersten verborgenen Schicht zu suchen - in diesem Fall sind es die Neuronen 2 und 4. Der zweite Hash H2 zeigt, welche Neuronen aus der zweiten verborgenen Schicht dazu beitragen.</i>  <i>Eine solche Strategie reduziert die Anzahl der Aktivierungen, die berechnet werden m√ºssen.</i> <br><br>  Es ist ziemlich schwierig, ein solches Netzwerk zu trainieren, aber lassen Sie uns diesen Moment aus und stellen Sie sich vor, dass wir bereits alle Gewichte jedes Neurons so angepasst haben, dass das neuronale Netzwerk handgeschriebene Zahlen perfekt erkennt.  Wenn eine leserlich geschriebene Zahl an ihrem Eingang ankommt, liegt die Aktivierung eines der Ausgangsneuronen (entsprechend dieser Zahl) nahe bei 1. Die Aktivierung der anderen neun liegt nahe bei 0. Klassischerweise erfordert der Betrieb eines solchen Netzwerks eine Matrixmultiplikation f√ºr jedes der 512 versteckten Neuronen. und noch eine f√ºr jedes Wochenende - was uns viele Multiplikationen gibt. <br><br>  Forscher verfolgen einen anderen Ansatz.  Der erste Schritt besteht darin, die Gewichte jedes der 512 Neuronen in der verborgenen Schicht unter Verwendung von "lokalit√§tsempfindlichem Hashing" zu hashen. Eine der Eigenschaften besteht darin, dass √§hnliche Eingabedaten √§hnliche Hashwerte ergeben.  Sie k√∂nnen dann Neuronen mit √§hnlichen Hashes gruppieren, was bedeutet, dass diese Neuronen √§hnliche Gewichtss√§tze haben.  Jede Gruppe kann in einer Datenbank gespeichert und durch den Hash der Eingabewerte bestimmt werden, die zur Aktivierung dieser Gruppe von Neuronen f√ºhren. <br><br>  Nach all dem Hashing stellt sich heraus, dass es einfach ist zu bestimmen, welches der versteckten Neuronen durch eine neue Eingabe aktiviert wird.  Sie m√ºssen 256 Eingabewerte √ºber einfach zu berechnende Hash-Funktionen ausf√ºhren und das Ergebnis verwenden, um die Datenbank nach den Neuronen zu durchsuchen, die aktiviert werden.  Auf diese Weise m√ºssen Sie die Aktivierungswerte f√ºr nur wenige wichtige Neuronen berechnen.  Es ist nicht notwendig, die Aktivierung aller anderen Neuronen in der Schicht zu berechnen, um herauszufinden, dass sie nicht zum Ergebnis beitragen. <br><br>  Die Eingabe eines solchen neuronalen Datennetzwerks kann als Ausf√ºhrung einer Suchabfrage in einer Datenbank dargestellt werden, die nach allen Neuronen fragt, die durch direktes Z√§hlen aktiviert w√ºrden.  Sie erhalten die Antwort schnell, da Sie zur Suche Hashes verwenden.  Und dann k√∂nnen Sie einfach die Aktivierung einer kleinen Anzahl von Neuronen berechnen, die wirklich wichtig sind. <br><br>  Forscher haben diese Technik, die sie SLIDE (Sub-LInear Deep Learning Engine) nannten, verwendet, um ein neuronales Netzwerk zu trainieren - f√ºr einen Prozess, der mehr Rechenanforderungen hat als f√ºr den beabsichtigten Zweck.  Anschlie√üend verglichen sie die Leistung des Lernalgorithmus mit einem traditionelleren Ansatz unter Verwendung einer leistungsstarken GPU - insbesondere der Nvidia V100-GPU.  Als Ergebnis erhielten sie etwas Erstaunliches: "Unsere Ergebnisse zeigen, dass die CPU SLIDE-Technologie im Durchschnitt um Gr√∂√üenordnungen schneller arbeiten kann als die bestm√∂gliche Alternative, die auf der besten Ausr√ºstung und mit jeder Genauigkeit implementiert ist." <br><br>  Es ist noch zu fr√ºh, um R√ºckschl√ºsse darauf zu ziehen, ob diese Ergebnisse (die von Experten noch nicht bewertet wurden) den Tests standhalten und ob sie die Chiphersteller dazu zwingen werden, die Entwicklung spezieller Ger√§te f√ºr Deep Learning anders zu betrachten.  Die Arbeit betont jedoch definitiv die Gefahr des Mitrei√üens einer bestimmten Eisensorte in F√§llen, in denen die M√∂glichkeit eines neuen und besseren Algorithmus f√ºr den Betrieb neuronaler Netze besteht. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de447806/">https://habr.com/ru/post/de447806/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de447792/index.html">Dunkle Muster und das Gesetz: Wie US-Regulierungsbeh√∂rden versuchen, die Produktmechanik zu kontrollieren und den Einfluss von Technologieunternehmen zu verringern</a></li>
<li><a href="../de447794/index.html">√úber einfache Dinge, kompliziert. Ein Brief eines Chemikers an einen 3D-Drucker. L√∂sungsmittel f√ºr Kunststoffe und Schutz gegen diese</a></li>
<li><a href="../de447798/index.html">Wie kann man ein riesiges Finanzdiagramm mit Geldw√§schemustern erstellen?</a></li>
<li><a href="../de447802/index.html">Isabella 2</a></li>
<li><a href="../de447804/index.html">Die Zwergenfestung verzichtet auf Textgrafiken, aber nicht auf deren Essenz</a></li>
<li><a href="../de447808/index.html">Lernen, Waves Smart-Vertr√§ge auf RIDE und RIDE4DAPPS zu schreiben. Teil 2 (DAO - Dezentrale Autonome Organisation)</a></li>
<li><a href="../de447810/index.html">Analytics f√ºr Azure DevOps Services ist jetzt √∂ffentlich verf√ºgbar</a></li>
<li><a href="../de447812/index.html">Wie wir die kontinuierliche Bereitstellung von Updates f√ºr die Kundenplattform implementiert haben</a></li>
<li><a href="../de447814/index.html">Wo und wie kann ein Entwicklungszentrum er√∂ffnet werden?</a></li>
<li><a href="../de447816/index.html">Ein bisschen C ++ - Vorlagenmagie und CRTP, um die Richtigkeit der Aktionen des Programmierers in der Kompilierungszeit zu kontrollieren</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>