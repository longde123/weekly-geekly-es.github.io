<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§πüèº üå´Ô∏è üë®üèø‚Äç‚öïÔ∏è Comparaci√≥n definitiva de plataformas integradas para IA üßùüèº üë®‚Äçüé§ ü§ûüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Las redes neuronales capturan el mundo. Cuentan visitantes, monitorean la calidad, mantienen estad√≠sticas y eval√∫an la seguridad. Un mont√≥n de startup...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comparaci√≥n definitiva de plataformas integradas para IA</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/recognitor/blog/468421/">  Las redes neuronales capturan el mundo.  Cuentan visitantes, monitorean la calidad, mantienen estad√≠sticas y eval√∫an la seguridad.  Un mont√≥n de startups, uso industrial. <br>  Grandes marcos.  Qu√© es PyTorch, cu√°l es el segundo TensorFlow.  Todo se est√° volviendo m√°s conveniente y conveniente, m√°s simple y m√°s simple ... <br>  Pero hay un lado oscuro.  Intentan guardar silencio sobre ella.  No hay nada alegre all√≠, solo oscuridad y desesperaci√≥n.  Cada vez que ves un art√≠culo positivo, suspiras tristemente, porque entiendes que solo una persona no entendi√≥ algo.  O lo escondi√≥. <br>  Hablemos de la producci√≥n en dispositivos integrados. <br><img src="https://habrastorage.org/webt/hb/pv/jm/hbpvjmosqm6fva4b2n6ytwqtqqq.jpeg"><br><a name="habracut"></a><br><br><h2>  Cual es el problema </h2><br>  Parecer√≠a.  Observe el rendimiento del dispositivo, aseg√∫rese de que sea suficiente, ejec√∫telo y obtenga ganancias. <br>  Pero, como siempre, hay un par de matices.  Pong√°moslos en los estantes: <br><ol><li>  Producci√≥n.  Si su dispositivo no est√° hecho en copias individuales, entonces debe asegurarse de que el sistema no se cuelgue, que los dispositivos no se sobrecalienten, que si hay una falla de energ√≠a, todo se inicie autom√°ticamente.  Y esto es en una gran fiesta.  Esto ofrece solo dos opciones: el dispositivo debe estar completamente dise√±ado teniendo en cuenta todos los posibles problemas.  O necesita superar los problemas del dispositivo fuente.  Bueno, por ejemplo, estos son ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> ).  Que, por supuesto, es esta√±o.  Para resolver los problemas del dispositivo de otra persona en grandes lotes, es necesario gastar una cantidad de energ√≠a poco realista. </li><li>  Puntos de referencia reales.  Muchas estafas y trucos.  NVIDIA en la mayor√≠a de los ejemplos sobreestima el rendimiento en un 30-40%.  Pero no solo ella se divierte.  A continuaci√≥n, doy muchos ejemplos cuando la productividad puede ser de 4 a 5 veces menos de lo que desea.  No se puede echar un polvo "todo funcion√≥ bien en la computadora, aqu√≠ ser√° proporcionalmente peor". <br></li><li>  Soporte muy limitado para la arquitectura de redes neuronales.  Hay muchas plataformas de hardware integradas que limitan en gran medida las redes que se pueden ejecutar en ellas (Coral, gyrfalcone, snapdragon).  Portar a tales plataformas ser√° doloroso. </li><li>  Apoyo.  Algo no funciona para usted, pero el problema est√° en el lado del dispositivo ... Este es el destino, no funcionar√°.  Solo para RPi, la comunidad cierra la mayor√≠a de los errores.  Y, en parte, por Jetson. </li><li>  Precio  A muchos les parece que incrustar es barato.  Pero, en realidad, con el crecimiento del rendimiento del dispositivo, el precio aumentar√° casi exponencialmente.  RPi-4 es 5 veces m√°s barato que Jetson Nano / Google Coral y 2-3 veces m√°s d√©bil.  Jetson Nano es 5 veces m√°s barato que Jetson TX2 / Intel NUC, y 2-3 veces m√°s d√©bil que ellos. <br></li><li>  Lorgus  ¬øRecuerdas este dise√±o de Zhelyazny?  <s>Parece que lo configur√© como la imagen del t√≠tulo ...</s> " <i>el Logrus es un laberinto tridimensional cambiante que representa las fuerzas del Caos en el multiverso</i> " <i>.</i>  Todo esto es una gran cantidad de errores y agujeros, todas estas diversas piezas de hierro, todos los marcos cambiantes ... Es normal cuando la imagen del mercado cambia completamente en 2-3 meses.  Durante este a√±o, ha cambiado 3-4 veces.  No puedes entrar al mismo r√≠o dos veces.  Entonces, todos los pensamientos actuales son ciertos para el verano de 2019. </li></ol><br><br><h2>  Que es </h2><br>  <s>Vamos a ponerlo en orden, no tiene un sabor dulce ...</s> ¬øQu√© hay ahora y es adecuado para las neuronas?  No hay tantas opciones, a pesar de su variabilidad.  Algunas palabras generales para limitar la b√∫squeda: <br><ol><li>  No analizar√© las neuronas / inferencias en los tel√©fonos.  Esto en s√≠ mismo es un gran tema.  Pero como los tel√©fonos son plataformas integradas con un ajuste de interferencia, no creo que sea malo. </li><li>  Tocar√© en Jetson TX1 | TX2.  En las condiciones actuales, estas no son las plataformas m√°s √≥ptimas por el precio, pero hay situaciones en las que a√∫n son convenientes de usar. </li><li>  No garantizo que la lista incluya todas las plataformas que existen hoy.  Tal vez olvid√© algo, tal vez no s√© sobre algo.  Si conoce m√°s plataformas interesantes, ¬°escriba! </li></ol><br><br>  Entonces  Las cosas principales que claramente est√°n incrustando.  En el art√≠culo los compararemos con precisi√≥n: <br><br><img width="800" src="https://habrastorage.org/webt/re/uw/k4/reuwk49r6lwkkhssus1lqwamlyq.jpeg"><br><br><ol><li>  Plataforma <b>Jetson</b> .  Hay varios dispositivos para ello: <br><ul><li>  <b>Jetson Nano</b> : un juguete barato y bastante moderno (primavera de 2019) </li><li>  <b>Jetson Tx1 | Tx2</b> : bastante costoso pero bueno en plataformas de rendimiento y versatilidad </li></ul><br></li><li>  <b>Raspberry Pi</b> .  En realidad, solo RPi4 tiene el rendimiento para redes neuronales.  Pero algunas tareas separadas se pueden hacer en la tercera generaci√≥n.  Incluso comenc√© cuadr√≠culas muy simples al principio. </li><li>  <b>Google Coral</b> Platform.  De hecho, para incrustar dispositivos solo hay un chip y dos dispositivos: placa de desarrollo y acelerador USB </li><li>  Plataforma <b>Intel Movidius</b> .  Si no es una gran empresa, solo estar√°n disponibles para usted los palos Movidius 1 | Movidius 2. </li><li>  Plataforma <b>Gyrfalcone</b> .  El milagro de la tecnolog√≠a china.  Ya hay dos generaciones: 2801, 2803 </li></ol><br><br>  Miscel√°neos  Hablaremos de ellos despu√©s de las principales comparaciones: <br><ol><li>  Procesadores Intel.  En primer lugar, los ensamblajes NUC. </li><li>  GPU Nvidia Mobile.  Las soluciones preparadas pueden considerarse no integradas.  Y si recolectas incrustaciones, entonces resultar√° decentemente en las finanzas. </li><li>  Tel√©fonos m√≥viles.  Android se caracteriza por el hecho de que para usar el m√°ximo rendimiento, es necesario usar exactamente el hardware que tiene un fabricante en particular.  O use algo universal, como la luz tensorflow.  Para Apple, lo mismo. </li><li>  Jetson AGX Xavier es una versi√≥n cara de Jetson con m√°s rendimiento. </li><li>  GAP8: procesadores de baja potencia para dispositivos s√∫per econ√≥micos. </li><li>  Mysterious Grove AI HAT </li></ol><br><br><h2>  Jetson </h2><br>  Hemos estado trabajando con Jetson durante mucho tiempo.  En 2014, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">Vasyutka</a> invent√≥ las matem√°ticas para el entonces <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Swift</a> precisamente en Jetson.  En 2015, en una reuni√≥n con Artec 3D, hablamos sobre qu√© plataforma es genial, despu√©s de lo cual sugirieron que construyamos un prototipo basado en √©l.  Despu√©s de un par de meses, el prototipo estaba listo.  Solo un par de a√±os de trabajo de toda la compa√±√≠a, un par de a√±os de maldiciones en la plataforma y en el cielo ... Y naci√≥ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Artec Leo</a> , el esc√°ner m√°s genial de su clase.  Incluso Nvidia en la presentaci√≥n de TX2 lo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mostr√≥</a> como uno de los proyectos m√°s interesantes creados en la plataforma. <br>  Desde entonces, TX1 / TX2 / Nano utilizamos en alg√∫n lugar en 5-6 proyectos. <br>  Y, probablemente, sabemos todos los problemas que estaban con la plataforma.  Vamos a tomarlo en orden. <br><br><h3>  Jetson tk1 </h3><br>  No voy a hablar especialmente de √©l.  La plataforma era muy eficiente en potencia inform√°tica en su d√≠a.  Pero ella no era una tienda de comestibles.  NVIDIA vendi√≥ los chips <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TegraTK1</a> que apuntalaron a Jetson.  Pero estos chips eran imposibles de usar para fabricantes peque√±os y medianos.  En realidad, solo Google / HTC / Xiaomi / Acer / Google podr√≠a hacer algo en ellos adem√°s de Nvidia.  Todos los dem√°s integrados en el producto ya sea depuran placas o saquearon otros dispositivos. <br><br><h3>  Jetson TX1 | TX2 </h3><br>  Nvidia lleg√≥ a las conclusiones correctas, y la pr√≥xima generaci√≥n se hizo incre√≠blemente.  TX1 | TX2, ya no son chips, sino un chip en el tablero. <br><img width="300" src="https://habrastorage.org/getpro/habr/post_images/c2c/6d4/dec/c2c6d4dec3a41c852c05d055b231bd06.jpg" alt="imagen"><img width="300" src="https://habrastorage.org/getpro/habr/post_images/411/694/e40/411694e401d5c4665e280ab0a6b8a4b6.png" alt="imagen"><br>  Son m√°s caros, pero tienen un nivel de comestibles completo.  Una peque√±a empresa puede integrarlos en su producto, este producto es predecible y estable.  Personalmente, vi c√≥mo se pusieron en producci√≥n 3-4 productos, y todo estuvo bien. <br>  Hablar√© sobre TX2, porque desde la l√≠nea actual es la placa principal. <br>  Pero, por supuesto, no todos agradecen a Dios.  Que pasa <br><ol><li>  Jetson TX2 es una plataforma costosa.  En la mayor√≠a de los productos, usar√° el m√≥dulo principal (seg√∫n tengo entendido, desde el tama√±o del lote, el precio estar√° en alg√∫n lugar entre 200-250 a 350-400 cu cada uno).  Necesita una tabla de transporte.  No conozco el mercado actual, pero antes era de unos 100-300 pies c√∫bicos  Dependiendo de la configuraci√≥n.  Bueno, y encima de tu kit de cuerpo. </li><li>  Jetson TX2 no es la plataforma m√°s r√°pida.  A continuaci√≥n discutiremos las velocidades comparativas, all√≠ mostrar√© por qu√© esta no es la mejor opci√≥n. </li><li>  Es necesario eliminar mucho calor.  Esto probablemente sea cierto para casi todas las plataformas de las que hablaremos.  La carcasa debe resolver el problema de la disipaci√≥n de calor.  Aficionados </li><li>  Esta es una mala plataforma para fiestas peque√±as.  Muchos cientos de dispositivos - aprox.  Ordenar placas base, desarrollar dise√±os y empaques es la norma.  ¬øMuchos miles de dispositivos?  Dise√±a tu placa base, y elegante.  Si necesitas 5-10 - mal.  Tendr√° que tomar DevBoard muy probablemente.  Son grandes, son un poco desagradables al destello.  Esta no es una plataforma lista para RPi. </li><li>  El pobre soporte t√©cnico de Nvidia.  Escuch√© muchas palabrotas de que las respuestas son respondidas, ya sea informaci√≥n secreta o respuestas mensuales. </li><li>  Pobre infraestructura en Rusia.  Es dif√≠cil de ordenar, lleva mucho tiempo.  Pero al mismo tiempo, los distribuidores funcionan bien.  Recientemente me encontr√© con un Jetson nano que se quem√≥ el d√≠a del lanzamiento, cambiado sin dudas.  Sam recogido por mensajer√≠a / trajo uno nuevo.  WAH!  Adem√°s, √©l mismo vio que la oficina de Mosc√∫ aconseja bien.  Pero tan pronto como su nivel de conocimiento no permita responder la pregunta y requiera una solicitud a la oficina internacional, tendr√°n que esperar las respuestas durante mucho tiempo. </li></ol><br><br>  Lo que es asombroso: <br><ol><li>  Mucha informaci√≥n, una comunidad muy grande. </li><li>  Alrededor de Nvidia hay muchas peque√±as empresas que producen accesorios.  Est√°n abiertos a negociaciones, puede ajustar su decisi√≥n.  Y CarierBoard, y firmware, y sistemas de enfriamiento. </li><li>  Soporte para todos los frameworks normales (TensorFlow | PyTorch) y soporte completo para todas las redes.  La √∫nica conversi√≥n que puede tener que hacer es transferir el c√≥digo a TensorRT.  Esto ahorrar√° memoria, posiblemente acelerar√°.  En comparaci√≥n con lo que habr√° en otras plataformas, esto es rid√≠culo. </li><li>  No s√© c√≥mo criar tablas.  Pero de aquellos que hicieron esto por Nvidia, escuch√© que TX2 es una buena opci√≥n.  Hay manuales que corresponden a la realidad. </li><li>  Buen consumo de energ√≠a.  Pero de todo eso exactamente "incrustado" estar√° con nosotros, lo peor :) </li><li>  Pinza en Rusia (explicado anteriormente por qu√©) </li><li>  A diferencia de movidius |  RPi |  Coral |  Gyrfalcon es una GPU real.  Puede conducir no solo cuadr√≠culas, sino tambi√©n algoritmos normales </li></ol><br><br>  Como resultado, esta es una buena plataforma para usted si tiene dispositivos de pieza, pero por alguna raz√≥n no puede entregar una computadora completa.  Algo masivo?  Biometr√≠a: muy probablemente no.  El reconocimiento de n√∫meros est√° en el borde, dependiendo del flujo.  Dispositivos port√°tiles con un precio de m√°s de 5k d√≥lares - posible.  Coches: no, es m√°s f√°cil poner una plataforma m√°s poderosa un poco m√°s cara. <br>  Me parece que con el lanzamiento de una nueva generaci√≥n de dispositivos baratos, TX2 morir√° con el tiempo. <br><br>  Las placas base para Jetson TX1 | TX2 | TX2i y otras se parecen a esto: <br><img src="https://habrastorage.org/getpro/habr/post_images/05c/ad5/2fc/05cad52fc3ba0b3468b7cea85a2f2552.png" alt="imagen"><br>  Y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠ hay</a> m√°s variaciones. <br><br><h3>  Jetson nano </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/9af/40b/1d8/9af40b1d8153cf212cbd2f76a533b143.png" alt="imagen"><br>  Jetson Nano es una cosa muy interesante.  Para Nvidia, este es un nuevo factor de forma que, en t√©rminos de revoluci√≥n, deber√≠a compararse con el TK1.  Pero los competidores ya se est√°n acabando.  Hay otros dispositivos de los que hablaremos.  Es 2 veces m√°s d√©bil que TX2, pero 4 veces m√°s barato.  M√°s precisamente ... las matem√°ticas son complicadas.  Jetson Nano en el tablero de demostraci√≥n cuesta 100 d√≥lares (en Europa).  Pero si compra solo un chip, ser√° m√°s caro.  Y tendr√°s que criarlo (todav√≠a no hay una placa base para √©l).  Y Dios no lo quiera, ser√° 2 veces m√°s barato en una gran fiesta que TX2. <br>  De hecho, Jetson Nano, en su placa base, es un producto publicitario para institutos / revendedores / aficionados, lo que deber√≠a estimular el inter√©s y la aplicaci√≥n comercial.  Por ventajas y desventajas (se cruza parcialmente con TX2): <br><ol><li>  El dise√±o es d√©bil y no se depura: <br><ul><li>  Se sobrecalienta, con una carga constante que cuelga / vuela peri√≥dicamente.  Una empresa familiar ha estado tratando de resolver todos los problemas durante 3 meses, no funciona. </li><li>  Tengo uno quemado cuando funciona con USB.  Escuch√© que un amigo ten√≠a una salida USB quemada, y el enchufe estaba funcionando.  Lo m√°s probable es que haya algunos problemas con la alimentaci√≥n USB. </li><li>  Si empaqueta la placa original, entonces no habr√° suficiente radiador de NVIDIA, por ejemplo, se sobrecalentar√°. </li></ul><br></li><li>  La velocidad de alguna manera no es suficiente.  Casi 2 veces menos que TX2 (en realidad, puede ser 1.5, pero depende de la tarea). </li><li>  Muchos dispositivos de 5-10 son generalmente muy buenos.  50-200: es dif√≠cil, tendr√° que compensar todos los errores del fabricante, colgarlo en sus perros, si necesita agregar algo como POE, doler√°.  Fiestas m√°s grandes.  Hoy no he o√≠do hablar de proyectos exitosos.  Pero me parece que pueden surgir dificultades como con TK1.  Para ser honesto, me gustar√≠a esperar que el pr√≥ximo a√±o se lance Jetson Nano 2, donde se corregir√°n estas enfermedades infantiles. </li><li>  El soporte es malo, igual que TX2 </li><li>  Infraestructura pobre </li></ol><br><br>  Bueno <br><ol><li>  Suficiente presupuesto en comparaci√≥n con los competidores.  Especialmente para fiestas peque√±as.  Precio / rendimiento favorable </li><li>  A diferencia de movidius |  RPi |  Coral |  Gyrfalcon es una GPU real.  Puede conducir no solo cuadr√≠culas, sino tambi√©n algoritmos normales </li><li>  Simplemente inicie cualquier red (igual que tx2) </li><li>  Consumo de energ√≠a (igual que tx2) </li><li>  Pinza en Rusia (igual que tx2) </li></ol><br><br>  Nano sali√≥ a principios de la primavera, en alg√∫n lugar de abril / mayo, lo mir√© activamente.  Ya hemos logrado hacer dos proyectos sobre ellos.  En general, los problemas identificados anteriormente.  Como producto de pasatiempo / producto para lotes peque√±os, muy bueno.  Pero a√∫n no est√° claro si es posible arrastrar la producci√≥n y c√≥mo hacerlo. <br><br><h3>  Habla sobre la velocidad de Jetson. </h3><br>  Vamos a comparar con otros dispositivos mucho m√°s tarde.  Mientras tanto, solo habla de Jetson y la velocidad.  Por qu√© nos est√° mintiendo Nvidia.  C√≥mo optimizar tus proyectos. <br>  Debajo de todo est√° escrito sobre TensorRT-5.1.  TensorRT-6.0.1 se lanz√≥ el 17 de septiembre de 2019, todas las declaraciones deben verificarse all√≠. <br>  Supongamos que creemos en Nvidia.  Abramos su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sitio web</a> y veamos el tiempo de inferencia de SSD-mobilenet-v2 a 300 * 300: <br><img width="800" src="https://habrastorage.org/getpro/habr/post_images/8bd/9aa/098/8bd9aa098ffe1e06c3b78d9f88f64dab.png" alt="imagen"><br>  Guau, 39 fps (25 ms).  S√≠, ¬°y se presenta el c√≥digo fuente! <br><br>  Hmm ... ¬øPero por qu√© est√° escrito <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> sobre 46ms? <br><br>  Espera ... Y aqu√≠ escriben que 309 ms es nativo, y 72ms est√° portado ... <br><br><img width="800" src="https://habrastorage.org/getpro/habr/post_images/32f/fd2/526/32ffd25260529f142b84a127c6d7a75c.png" alt="imagen"><br><br>  Donde esta la verdad <br>  La verdad es que todos piensan muy diferente: <br><ol><li>  SSD consta de dos partes.  Una parte es la neurona.  La segunda parte es el procesamiento posterior de lo que produce la neurona (supresi√≥n no m√°xima) + el procesamiento previo de lo que se carga en la entrada. </li><li>  Como dije antes, bajo Jetson todo debe convertirse a TensorRT.  Este es un marco tan nativo de NVIDIA.  Sin ella, todo ser√° malo.  Solo hay un problema.  No todo est√° portado all√≠, especialmente desde TensorFlow.  A nivel mundial, hay dos enfoques: <br><ul><li>  Google, al darse cuenta de que esto es un problema, lanz√≥ para TensorFlow una cosa llamada "tf-trt".  De hecho, este es un complemento en tf, que le permite convertir cualquier cuadr√≠cula a tensorrt.  Las partes que no son compatibles se infieren en la CPU, el resto en la GPU. </li><li>  Reescribe todas las capas / encuentra sus an√°logos </li></ul><br></li></ol><br>  En los ejemplos anteriores: <br><ul><li>  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este</a> enlace, 300 ms de tiempo es el flujo de tensor habitual sin optimizaci√≥n. </li><li>  All√≠, 72ms es la versi√≥n tf-trt.  All√≠, todos los nms se realizan esencialmente en el proceso. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Esta es una</a> versi√≥n para fan√°ticos, donde una persona transfiri√≥ todos los nms y lo escribi√≥ en gpu. </li><li>  Y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esto</a> ... Este NVIDIA decidi√≥ medir todo el rendimiento sin procesamiento posterior, sin mencionarlo expl√≠citamente en ninguna parte. </li></ul><br><br>  Debe comprender por s√≠ mismo que si fuera su neurona, que nadie habr√≠a convertido antes que usted, entonces, sin problemas, podr√≠a lanzarla a una velocidad de 72 ms.  Y a una velocidad de 46 ms, sentado sobre los manuales y sorsa d√≠a a semana. <br>  En comparaci√≥n con muchas otras opciones, esto es muy bueno.  Pero no olvide que, haga lo que haga, ¬°nunca crea los puntos de referencia de NVIDIA! <br><br><h2>  RaspberryPI 4 </h2><br>  ¬øProducci√≥n? ... Y escucho c√≥mo docenas de ingenieros comienzan a re√≠rse de la menci√≥n de las palabras "RPI" y "producci√≥n" cerca.  Pero, debo decir que RPI es a√∫n m√°s estable que Jetson Nano y Google Coral.  Pero, por supuesto, TX2 pierde y, aparentemente, la gerifalona. <br><img src="https://habrastorage.org/getpro/habr/post_images/667/aa5/dbc/667aa5dbc7be87aeda247492035b3c33.jpg" alt="imagen"><br>  (La imagen es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">de aqu√≠</a> . Me parece que unir a los fan√°ticos al RPi4 es una diversi√≥n popular por separado). <br>  De la lista completa, este es el √∫nico dispositivo que no ten√≠a en mis manos / no prob√©.  Pero √©l inici√≥ neuronas en Rpi, Rpi2, Rpi3 (por ejemplo, me lo dijo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> ).  En general, Rpi4, seg√∫n tengo entendido, solo difiere en rendimiento.  Me parece que los pros y los contras de RPi lo saben todo, pero a√∫n as√≠.  Contras: <br><br><ol><li>  Por mucho que no me gustar√≠a, esta no es una soluci√≥n de supermercado.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Sobrecalentamiento</a>  Congelaciones peri√≥dicas.  Pero debido a la gran comunidad, hay cientos de soluciones para cada problema.  Esto no hace que Rpi sea bueno para miles de tiradas de impresi√≥n.  Pero decenas / cientos - notas wai. </li><li>  Velocidad.  Este es el dispositivo m√°s lento de todos los principales de los que estamos hablando. </li><li>  Casi no hay soporte del fabricante.  Este producto est√° dirigido a entusiastas. </li></ol><br>  Pros: <br><br><ol><li>  Precio  No, por supuesto, si cr√≠as el tablero t√∫ mismo, entonces usando la gerifalona puedes hacer que sea m√°s barato en miles.  Pero lo m√°s probable es que esto no sea realista.  Cuando el rendimiento de RPi sea suficiente, ser√° la soluci√≥n m√°s barata. </li><li>  Popularidad.  Cuando sali√≥ Caffe2, hab√≠a una versi√≥n para Rpi en la versi√≥n base.  Tensorflow light?  Por supuesto que funciona.  I.T.D., I.T.P.  Lo que el fabricante no hace es transferir usuarios.  Corr√≠ con diferentes RPi y Caffe y Tensorflow y PyTorch, y un mont√≥n de cosas m√°s raras. </li><li>  Conveniencia para peque√±as fiestas / piezas.  Simplemente flashee la unidad flash y corra.  Hay WiFi a bordo, a diferencia de JetsonNano.  Simplemente puede alimentarlo a trav√©s de PoE (parece que necesita comprar un adaptador que se vende activamente). </li></ol><br><br>  Hablaremos sobre la velocidad de Rpi al final.  Dado que el fabricante no postula que su producto para las neuronas, hay pocos puntos de referencia.  Todos entienden que Rpi no es perfecto en velocidad.  Pero incluso √©l es adecuado para algunas tareas. <br>  Tuvimos un par de tareas de semiproductos que implementamos en Rpi.  La impresi√≥n fue agradable. <br><br><h2>  Movidio 2 </h2><br><img width="800" src="https://habrastorage.org/webt/ki/gq/l3/kigql3xgdsddqwgx-iwrjznthpc.jpeg"><br>  A partir de aqu√≠ y de abajo, no se utilizar√°n procesadores completos, sino procesadores dise√±ados espec√≠ficamente para redes neuronales.  Es como si sus fortalezas y debilidades al mismo tiempo. <br>  Entonces  Movidio  La compa√±√≠a fue comprada por Intel en 2016.  En el segmento que nos interesa, la compa√±√≠a lanz√≥ dos productos, Movidius y Movidius 2. El segundo es m√°s r√°pido, solo hablaremos del segundo. <br>  No, no asi.  La conversaci√≥n no deber√≠a comenzar con Movidius, sino con Intel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenVino</a> .  Yo dir√≠a que esto es ideolog√≠a.  M√°s espec√≠ficamente, el marco.  De hecho, este es un conjunto de neuronas preinformadas e inferencias a ellas, que est√°n optimizadas para productos Intel (procesadores, GPU, computadoras especiales).  Integrado con OpenCV, con la Raspberry Pi, con muchos otros silbatos y pedos. <br>  La ventaja de OpenVino es que tiene muchas neuronas.  En primer lugar, los detectores m√°s famosos.  Neuronas para el reconocimiento de personas, personas, n√∫meros, letras, poses, etc., etc.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> ).  Y est√°n entrenados.  No por conjuntos de datos abiertos, sino por conjuntos de datos compilados por el propio Intel.  Son mucho m√°s grandes / m√°s diversos y mejor abiertos.  Se pueden volver a entrenar de acuerdo con sus casos, luego funcionar√°n en general bien. <br>  ¬øEs posible hacerlo mejor?  Por supuesto que puedes.  Por ejemplo, el reconocimiento de n√∫meros que hicimos funcion√≥ significativamente mejor.  Pero pasamos muchos a√±os desarroll√°ndolo y entendiendo c√≥mo hacerlo perfecto.         ,      . <br>  OpenVino, ,   .      .  -   ‚Äî   .     .  GAN    .   . ,   ,     ,     -   ,    .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> ,    : <br><img src="https://habrastorage.org/getpro/habr/post_images/103/de3/8ad/103de38adbea7450a4f7888ef135303c.png" alt="imagen"><br>  ,  Intel   OpenVino    .     .  ,       .         ‚Äî      .    70%       OpenVino. <br>      Movidius    .         .       (    ,   ). <br>     . USB , ,   !!!       USB.  . Intel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> .  -        ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> ) <br>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>   .               -.     -         . <br>      ?..       :) <br>  ,   . OpenVino,   ,   ,    (    Computer Vision  ).      : <br><iframe width="560" height="315" src="https://www.youtube.com/embed/ogHrgixuFzg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br> (   AI 2.0,   OpenVino  ). <br><br> ,     .    Movidius 2. : <br><ol><li>    .  Rpi  Jetson Nano.            ‚Äî  .        .   Third Party ? </li><li>    .     .     . </li><li>    .    . </li><li>     .           USB 3.0 </li><li>    ,        .   -.     .  Movidius      .      . </li></ol><br>  Pros: <br><ol><li>    .        .    . </li><li>  ,   </li><li>  ,    </li></ol><br>         .          ‚Äî      . <br>      ,     ‚Äú   20-30   ,     ,  ‚Äù ‚Äî       Movidius. <br>  Intel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>  .      , . <br><img src="https://habrastorage.org/webt/d0/m-/ec/d0m-ecz9npk5f11jwlr0pzxruwi.jpeg" alt="imagen"><br>  <b>UPD</b> <br>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> .     .   embedded . PCI-e      .       .   ‚Äî        200 .. .         ‚Ä¶ <br><br><h2> Google Coral </h2><br>  Estoy decepcionado  No, no hay nada que no pueda predecir.  Pero estoy decepcionado de que Google haya decidido lanzar esto.  Las pruebas son un milagro al comienzo del verano.  Tal vez algo ha cambiado desde entonces, pero describir√© mi experiencia de esa √©poca. <br>  Configuraci√≥n ... Para flashear el Jetson Tk-Tx1-Tx2, ten√≠a que enchufarlo a la computadora host y a la fuente de alimentaci√≥n.  Y eso fue suficiente.  Para flashear Jetson Nano y RPi, solo necesita insertar la imagen en la unidad flash USB. <br>  Y para flashear Coral, debe pegar tres cables en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">orden correcto</a> : <br><img src="https://habrastorage.org/webt/de/b4/hs/deb4hsbxm_ra3ajx7_kuoljd0ls.jpeg"><br>  ¬°Y no intentes equivocarte!  Por cierto, hay errores / comportamiento indescriptible en la gu√≠a.  Probablemente no los describir√©, ya que desde el comienzo del verano podr√≠an haber arreglado algo.  Recuerdo que despu√©s de instalar Mendel se perdi√≥ cualquier acceso a trav√©s de ssh, incluido el descrito por ellos, tuve que editar manualmente algunas configuraciones de Linux. <br>  Me llev√≥ 2-3 horas completar este proceso. <br>  Ok  Lanzado  ¬øCrees que es f√°cil ejecutar tu grilla en √©l?  Casi nada :) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aqu√≠ hay una</a> lista de lo que puede dejar ir. <br>  Para ser honesto, no llegu√© a este punto r√°pidamente.  Pas√© medio d√≠a  No realmente  No puede descargar el modelo desde <a href="">el repositorio TF</a> y ejecutarlo en el dispositivo.  O all√≠ es necesario cortar todas las capas.  No encontr√© instrucciones. <br>  Entonces aqu√≠.  Es necesario tomar el modelo del repositorio desde arriba.  No hay muchos de ellos (se han agregado 3 modelos desde principios de verano).  ¬øY c√≥mo entrenarla?  Abrir en TensorFlow en una tuber√≠a est√°ndar?  HAHAHAHAHAHAHAHA.  ¬°Por supuesto que no! <br>  Tienes un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">contenedor Doker</a> especial, y el modelo solo entrenar√° en √©l.  (Probablemente, tambi√©n puedes burlarte de tu TF ... Pero hay instrucciones, instrucciones ... que no eran y no parecen ser). <br>  Descargar / Instalar / Lanzar.  ¬øQu√© es ... ¬øPor qu√© la GPU est√° en cero? ... PORQUE LA FORMACI√ìN ESTAR√Å EN LA CPU.  Docker es solo para √©l !!!  ¬øQuieres m√°s diversi√≥n?  El manual dice "basado en una CPU de 6 n√∫cleos con una estaci√≥n de trabajo de memoria 64G".  Parece que esto es solo un consejo?  Tal vez  Solo que ahora no ten√≠a suficientes de mis 8 conciertos en ese servidor donde entrenan la mayor√≠a de las modelos.  El entrenamiento a la 4¬™ hora los consumi√≥ a todos.  Una fuerte sensaci√≥n de que ten√≠an algo que flu√≠a.  Intent√© un par de d√≠as con diferentes par√°metros en diferentes m√°quinas, el efecto fue uno. <br>  No verifiqu√© esto dos veces antes de publicar el art√≠culo.  Para ser honesto, fue suficiente para m√≠ una vez. <br>  ¬øQu√© m√°s agregar?  ¬øQue este c√≥digo no genera un modelo?  Para generarlo debes: <br><br><ol><li>  Retraso </li><li>  Convi√©rtelo a tflite </li><li>  Compilar a Formal Edge TPU.  Gracias a Dios ahora esto se hace en una computadora.  En la primavera solo se pod√≠a hacer en l√≠nea.  Y all√≠ era necesario marcar "No lo usar√© para el mal / No violar√© ninguna ley con este modelo".  Ahora, gracias a Dios no hay nada de esto. </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/2ef/84d/b88/2ef84db88bcaba5d18a97c23bf5f2605.png" alt="imagen"><br>  Este es el mayor asco que he experimentado en relaci√≥n con un producto de TI en el √∫ltimo a√±o ... <br>  A nivel mundial, Coral deber√≠a tener la misma ideolog√≠a que OpenVino con Movidius.  Solo ahora Intel ha estado en este camino durante varios a√±os.  Con excelentes manuales, soporte y buenos productos ... Y Google.  Bueno, es solo Google ... <br>  Contras: <br><ol><li>  Este tablero no es un supermercado en el nivel AD.  No he o√≠do hablar de la venta de chips =&gt; la producci√≥n no es realista </li><li>  El nivel de desarrollo es lo m√°s terrible posible.  Todo bazhet.  La tuber√≠a de desarrollo no se ajusta a los esquemas tradicionales. </li><li>  El abanico.  En el "chip energ√©ticamente √≥ptimo" lo pusieron.  Bien, ya no hablar√© sobre producci√≥n. </li><li>  Costo  M√°s caro que TX2. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">No se pueden</a> guardar dos cuadr√≠culas en la memoria al mismo tiempo.  Es necesario realizar la carga y descarga.  Lo que ralentiza la inferencia de varias redes. </li></ol><br>  Pros: <br><ol><li>  De todo lo que hablamos, Coral es el m√°s r√°pido. </li><li>  Potencialmente, si se saca el chip, entonces es m√°s productivo que Movidius.  Y parece que su arquitectura est√° m√°s justificada para las neuronas. </li></ol><br><br><h2>  Gerifalte </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/640/86e/595/64086e5959ce67c2a21dc80c3106e49d.png" alt="imagen"><br>  El √∫ltimo a√±o y medio ha estado hablando de esta bestia china.  Incluso hace un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">a√±o, estaba</a> diciendo algo sobre √©l.  Pero hablar es una cosa, y dar informaci√≥n es otra.  Habl√© con 3-4 grandes empresas, donde los gerentes / directores de proyecto me dijeron lo genial que era este Girfalkon.  Pero no ten√≠an ninguna documentaci√≥n.  Y no lo vieron vivo.  El <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sitio</a> casi no tiene informaci√≥n.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Descargue</a> del sitio al menos algo que solo puedan asociarse (desarrolladores de hardware).  Adem√°s, la informaci√≥n en el sitio es muy contradictoria.  En un lugar escriben que solo admiten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VGG</a> , en otro que solo tienen sus propias neuronas basadas en GNet (que, seg√∫n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sus garant√≠as, son</a> muy peque√±as y sin p√©rdida de precisi√≥n).  En el tercero se escribe que todo se convierte con TF | Caffe | PyTorch, y en el cuarto se escribe sobre el tel√©fono m√≥vil y otros encantos. <br>  Comprender la verdad es casi imposible.  Una vez que estaba cavando y cavando algunos videos en los que al menos algunos n√∫meros se deslizan: <br><iframe width="560" height="315" src="https://www.youtube.com/embed/AoidCoMK8v0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><iframe width="560" height="315" src="https://www.youtube.com/embed/eS6eCAEL_1A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Si esto es cierto, significa SSD (¬øen dispositivos m√≥viles?). Bajo 224 * 224 en el chip GTI2801 tienen ~ 60ms, lo cual es bastante comparable con movidius. <br>  Parece que tienen un chip 2803 mucho m√°s r√°pido, pero la informaci√≥n es a√∫n menor: <br><iframe width="560" height="315" src="https://www.youtube.com/embed/yQvVqaVZUQ4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Este verano tenemos una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">placa</a> de firefly en nuestras manos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este</a> m√≥dulo est√° instalado all√≠ para realizar c√°lculos). <br><br>  Hab√≠a una esperanza de que finalmente lo ver√≠amos vivo.  Pero no funcion√≥.  El tablero era visible, pero no funcion√≥.  Al rastrear frases individuales en ingl√©s en la documentaci√≥n china, casi entendieron cu√°l era el problema (el sistema moleteado inicial no admit√≠a el m√≥dulo neural, era necesario reconstruir y volver a rodar todo por nosotros mismos).  Pero simplemente no funcion√≥, y ya hab√≠a sospechas de que la placa no encajar√≠a en nuestra tarea (2 GB de RAM es muy peque√±a para redes neuronales + sistemas. Adem√°s, no hab√≠a soporte para dos redes al mismo tiempo). <br>  Pero logr√© ver la documentaci√≥n original.  De ella, se entiende muy poco (chino).  Para bien, era necesario probar y mirar la fuente. <br>  El soporte t√©cnico de RockChip nos calific√≥ est√∫pidamente. <br>  A pesar de este horror, tengo claro que aqu√≠, de todos modos, las jambas de RockChip est√°n aqu√≠ en primer lugar.  Y tengo la esperanza de que, en una placa normal, Gyrfalcon pueda usarse bastante.  Pero debido a la falta de informaci√≥n, es dif√≠cil para m√≠ decirlo. <br><br>  Contras: <br><ol><li>  Sin ventas abiertas, solo interactuar con empresas </li><li>  Poca informaci√≥n, sin comunidad.  La informaci√≥n existente es a menudo en chino.  Las caracter√≠sticas de la plataforma no se pueden predecir de antemano </li><li>  Lo m√°s probable es que la inferencia no sea m√°s de una red a la vez. </li><li>  Solo los fabricantes de hierro pueden interactuar con el autogiro.  El resto necesita buscar algunos intermediarios / fabricantes de tableros. </li></ol><br>  Pros: <br><ol><li>  Seg√∫n tengo entendido, el precio de un chip girfcon es mucho m√°s barato que el resto.  Incluso en forma de unidades flash. </li><li>  Ya hay dispositivos de terceros con un chip integrado.  Por lo tanto, el desarrollo es algo m√°s f√°cil que movidius. </li><li>  Aseguran que hay muchas redes pre-entrenadas, la transferencia de redes es mucho m√°s simple que Movidius | Coral.  Pero no garantizar√≠a esto como la verdad.  No tuvimos √©xito. </li></ol><br>  En resumen, la conclusi√≥n es la siguiente: muy poca informaci√≥n.  No puedes solo ponerte en esta plataforma.  Y antes de hacer algo al respecto, debe hacer una gran revisi√≥n. <br><br><h2>  Velocidades </h2><br>  Realmente me gusta c√≥mo el 90% de las comparaciones de dispositivos integrados se reducen a la velocidad de las comparaciones.  Como has entendido anteriormente, esta caracter√≠stica es muy arbitraria.  Para Jetson Nano, puede ejecutar neuronas como flujo de tensor puro, puede usar tensorflow-tensorrt, o puede usar tensorrt puro.  Los dispositivos con arquitectura tensorial especial (movidius | coral | gyrfalcone) pueden ser r√°pidos, pero en primer lugar solo pueden funcionar con arquitecturas est√°ndar.  Incluso para Raspberry Pi, no todo es tan simple.  Las neuronas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">xnor.ai</a> dan una aceleraci√≥n una vez y media.  Pero no s√© cu√°n honestos son, y qu√© se gan√≥ al cambiar a int8 u otros chistes. <br>  Al mismo tiempo, otra cosa interesante es ese momento.  Cuanto m√°s compleja es la neurona, m√°s complejo es el dispositivo para la inferencia, m√°s impredecible es la aceleraci√≥n final que se puede extraer.  Toma un poco de OpenPose.  Hay una red no trivial, postprocesamiento complejo.  Tanto esto como aquello se pueden optimizar debido a: <br><br><ul><li>  Migraci√≥n de posprocesamiento de GPU </li><li>  Optimizar el postprocesamiento </li><li>  Optimizaci√≥n de la red neuronal para las caracter√≠sticas de la plataforma, por ejemplo: <br><ul><li>  Usar redes optimizadas para plataforma </li><li>  Usando m√≥dulos de red para la plataforma </li></ul><br></li><li>  Portar a int8 | int16 | binarizaci√≥n </li><li>  Usando calculadoras m√∫ltiples (GPU | CPU | etc.).  Recuerdo que en Jetson TX1 una vez aceleramos bien cuando transferimos toda la funcionalidad relacionada con la transmisi√≥n de video a los aceleradores incorporados para este prop√≥sito.  Trillado, pero la red se aceler√≥.  Al equilibrar, aparecen muchas combinaciones interesantes </li></ul><br>  A veces alguien intenta evaluar algo para todas las combinaciones posibles.  Pero realmente, como me parece, esto es in√∫til.  Primero debe decidir sobre la plataforma, y ‚Äã‚Äãsolo luego tratar de sacar por completo todo lo que sea posible. <br><br>  ¬øPor qu√© soy todo esto?  Adem√°s, la prueba de " <i>cu√°nto tiempo MobileNet</i> " es una prueba muy mala.  Puede decir que la plataforma X es √≥ptima.  Pero cuando intenta desplegar su neurona y postprocesar all√≠, puede sentirse muy decepcionado. <br>  Pero comparar mobilnet'ov a√∫n da informaci√≥n sobre la plataforma.  Para tareas simples.  Para situaciones en las que comprende que, de todos modos, la tarea es m√°s f√°cil de reducir a enfoques est√°ndar.  Cuando quieres evaluar la velocidad de la calculadora. <br>  La siguiente tabla est√° tomada de varios lugares: <br><ul><li>  Estos estudios son: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> </li><li>  Para SSD existe el par√°metro "n√∫mero de clases de salida".  Y a partir de este par√°metro, la tasa de inferencia puede variar mucho.  Trat√© de elegir estudios con el mismo n√∫mero de clases.  Pero este puede no ser el caso en todas partes. </li><li>  Nuestra experiencia con TensorRT.  Sab√≠a qu√© tipos funcionan y cu√°les no. </li><li>  Para el gerifalte, estos videos se basan en el hecho de que mobilnet v2 est√° all√≠ + una estimaci√≥n de cu√°nto cuesta el cambio de √°rea.  Este video dice que 2803 puede ser 3-4 veces m√°s r√°pido.  Pero para 2803 no hay clasificaciones SSD.  En general, dudo mucho de las velocidades en este punto. </li><li>  Trat√© de elegir el estudio que daba la velocidad m√°xima real (no tom√© la versi√≥n de Nvidia sin NMS, por ejemplo) </li><li>  Para Jetson TX2 utilic√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">estas</a> clasificaciones, pero hay 5 clases, en el mismo n√∫mero de clases que el resto ser√° m√°s lento.  De alguna manera descubr√≠ por la experiencia / comparaci√≥n con Nano en los n√∫cleos lo que deber√≠a estar all√≠ </li><li>  No tom√© en cuenta los chistes con tasa de bits.  No s√© en qu√© bitness trabajaron Movidius y Gyrfalcon. </li></ul><br>  Como resultado, tenemos: <br><img src="https://habrastorage.org/webt/it/eb/z7/itebz7bgvqmgcupodynat7rr0hy.png"><br><br><h2>  Comparaci√≥n de plataforma </h2><br>  Intentar√© llevar todo lo que dije arriba a una sola mesa.  Destaqu√© en amarillo aquellos lugares donde mi conocimiento no es suficiente para llegar a una conclusi√≥n inequ√≠voca.  Y, en realidad 1-6, esta es una evaluaci√≥n comparativa de las plataformas.  Cuanto m√°s cerca de 1, mejor. <br><img src="https://habrastorage.org/webt/6s/nm/xh/6snmxh9kx7xaufpayljpzvrfoaw.png"><br>  S√© que el consumo de energ√≠a es cr√≠tico para muchos.  Pero me parece que todo aqu√≠ es algo ambiguo, y lo entiendo muy mal, por lo que no entr√©.  Adem√°s, la ideolog√≠a misma parece ser la misma en todas partes. <br><br><h2>  Paso lateral </h2><br>  De lo que est√°bamos hablando es solo un peque√±o punto en el vasto espacio de variaciones de su sistema.  Probablemente las palabras comunes que pueden caracterizar esta √°rea: <br><ul><li>  Bajo consumo de energ√≠a </li><li>  Tama√±o peque√±o </li><li>  Alta potencia inform√°tica </li></ul><br>  Pero, globalmente, si reduce la importancia de uno de los criterios, puede agregar muchos otros dispositivos a la lista.  A continuaci√≥n, analizar√© todos los enfoques que he conocido. <br><br><h2>  Intel </h2><br>  Como dijimos cuando hablamos de Movidius, Intel tiene una plataforma OpenVino.  Permite un procesamiento muy eficiente de las neuronas en los procesadores Intel.  Adem√°s, la plataforma le permite admitir incluso todo tipo de Intel-GPU en un chip.  Ahora tengo miedo de decir exactamente qu√© tipo de rendimiento hay para qu√© tareas.  Pero, seg√∫n tengo entendido, una buena piedra con una GPU bastante integrada ‚Öì ofrece un rendimiento de 1080.  Para algunas tareas, incluso puede ser m√°s r√°pido. <br><img src="https://habrastorage.org/getpro/habr/post_images/3b3/16a/a09/3b316aa09dd4dcbe22f97e38474f300a.png" alt="imagen"><br>  En este caso, el factor de forma, por ejemplo Intel NUC, es bastante compacto.  Buen enfriamiento, embalaje, etc.  La velocidad ser√° m√°s r√°pida que la Jetson TX2.  Por disponibilidad / facilidad de compra: mucho m√°s f√°cil.  La estabilidad de la plataforma fuera de la caja es mayor. <br>  Dos contras: consumo de energ√≠a y precio.  El desarrollo es un poco m√°s complicado. <br><br><h2>  Jetson agx </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/0d7/eb9/4e0/0d7eb94e037d0311f8bf2176b3044329.jpg" alt="imagen"><br>  Este es otro jetson.  Esencialmente la versi√≥n m√°s antigua.  La velocidad es aproximadamente 2 veces m√°s r√°pida que Jetson TX2, adem√°s hay soporte para c√°lculos int8, lo que le permite overclockear otras 4 veces.  Por cierto, mira esta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">foto</a> de Nvidia: <br><img src="https://habrastorage.org/getpro/habr/post_images/800/c4a/bf1/800c4abf1ed98c8ec84815586c503491.png" alt="imagen"><br>  Comparan dos de sus propios Jetson.  Uno en int8, el segundo en int32.  Ni siquiera s√© qu√© palabras decir aqu√≠ ... En resumen: "NUNCA CREER GR√ÅFICOS NVIDIA". <br>  A pesar de que AGX es bueno, no alcanza las GPU normales de Nvidia en t√©rminos de potencia inform√°tica.  Sin embargo, en t√©rminos de eficiencia energ√©tica, son muy interesantes.  El principal menos el precio. <br>  Nosotros mismos no trabajamos con ellos, por lo que es dif√≠cil para m√≠ decir algo m√°s detallado, describir el rango de tareas donde son m√°s √≥ptimas. <br><br><h2>  Nvidia gpu |  versi√≥n port√°til </h2><br>  Si elimina la restricci√≥n estricta sobre el consumo de energ√≠a, entonces el Jetson TX2 no se ve √≥ptimo.  Como el AGX.  Por lo general, las personas tienen miedo de usar la GPU en la producci√≥n.  Pago por separado, todo eso. <br>  Pero hay millones de empresas que le ofrecen armar una soluci√≥n personalizada en una placa.  Por lo general, estas son placas para computadoras port√°tiles / minicomputadoras.  O, al final, as√≠: <br><img src="https://habrastorage.org/getpro/habr/post_images/ee9/37d/cdf/ee937dcdf590661d003a1b1201d3ce46.png" alt="imagen"><br>  Una de las startups en las que he estado trabajando durante los √∫ltimos 2.5 a√±os ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CherryHome</a> ) ha <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tomado</a> este mismo camino.  Y estamos muy satisfechos. <br>  Menos, como de costumbre, en el consumo de energ√≠a, que no fue cr√≠tico para nosotros.  Bueno, el precio muerde un poco. <br><br><h2>  Tel√©fonos m√≥viles </h2><br>  No quiero profundizar en este tema.  Para contar todo lo que hay en los tel√©fonos m√≥viles modernos para neuronas / qu√© marcos / qu√© hardware, etc., necesitar√° m√°s de un art√≠culo con este tama√±o.  Y teniendo en cuenta el hecho de que empujamos en esta direcci√≥n solo 2-3 veces, me considero incompetente para esto.  Tan solo un par de observaciones: <br><ol><li>  Hay muchos aceleradores de hardware en los que se pueden optimizar las neuronas. </li><li>  No hay una soluci√≥n general que funcione bien en todas partes.  Ahora hay alg√∫n intento de hacer que Tensorflow Lite sea una soluci√≥n.  Pero, seg√∫n tengo entendido, a√∫n no se ha convertido en uno. </li><li>  Algunos fabricantes tienen sus propios trabajos agr√≠colas especiales.  Ayudamos a optimizar el marco para Snapdragon hace un a√±o.  Y fue terrible.  La calidad de las neuronas all√≠ es mucho menor que en todo lo que habl√© hoy.  No hay soporte para el 90% de las capas, incluso las b√°sicas, como "adici√≥n". </li><li>  Como no hay python, la inferencia de redes es muy extra√±a, il√≥gica e inconveniente. </li><li>  En t√©rminos de rendimiento, sucede que todo est√° muy bien (por ejemplo, en algunos iphone). </li></ol><br>  Me parece que para los tel√©fonos m√≥viles integrados no es la mejor soluci√≥n (la excepci√≥n son algunos sistemas de reconocimiento facial de bajo presupuesto).  Pero vi un par de casos cuando se usaron como prototipos iniciales. <br><br><h2>  Gap8 </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Estuve</a> recientemente en una conferencia de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Usedata</a> .  Y all√≠ uno de los informes fue sobre la inferencia de neuronas en los porcentajes m√°s baratos (GAP8).  Y, como dicen, la necesidad de inventos es astuta.  En la historia, un ejemplo fue muy descabellado.  Pero el autor cont√≥ c√≥mo pudieron lograr la inferencia por la cara en aproximadamente un segundo.  En una cuadr√≠cula muy simple, esencialmente sin detector.  Por optimizaciones locas y largas y ahorros en partidos. <br>  Siempre no me gustan esas tareas.  Sin investigaci√≥n, solo sangre. <br>  Pero, vale la pena reconocer que puedo imaginar acertijos donde los porcentajes de bajo consumo dan un resultado genial.  Probablemente no para el reconocimiento facial.  Pero en alg√∫n lugar donde pueda reconocer la imagen de entrada en 5-10 segundos ... <br><br><h2>  Grove AI HAT </h2><br><img width="400" src="https://habrastorage.org/getpro/habr/post_images/3fa/b99/106/3fab99106dbf03025a9adbb98d7d77d5.jpg" alt="imagen"><br>  Mientras preparaba este art√≠culo, me encontr√© con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esta</a> plataforma integrada.  Hay muy poca informaci√≥n al respecto.  Seg√∫n tengo entendido, cero soporte.  La productividad tambi√©n est√° en cero ... Y ni una sola prueba de velocidad ... <br><br><h2>  Servidor / Reconocimiento remoto </h2><br>  Cada vez que nos solicitan asesoramiento sobre una plataforma integrada, quiero gritar "¬°corre, tontos!".  Es necesario evaluar cuidadosamente la necesidad de tal soluci√≥n.  Echa un vistazo a otras opciones.  Siempre aconsejo a todos que hagan un prototipo con la arquitectura del servidor.  Y durante su funcionamiento, depende de usted decidir si implementar√° un embebido real.  Despu√©s de todo, incrustado es: <br><ol><li>  Mayor tiempo de desarrollo, a menudo 2-3 veces. </li><li>  Soporte sofisticado y depuraci√≥n en producci√≥n.  Cualquier desarrollo con ML es una revisi√≥n constante, actualizaci√≥n de neuronas, actualizaciones del sistema.  Incrustado es a√∫n m√°s dif√≠cil.  ¬øC√≥mo recargar el firmware?  Y si ya tiene acceso a todas las unidades, ¬øpor qu√© calcularlas cuando puede calcular en un dispositivo? </li><li>  Complejidad del sistema / mayor riesgo.  M√°s puntos de fracaso.  Al mismo tiempo, aunque el sistema no funciona como un todo, uno no puede entender: ¬øes la plataforma adecuada para esta tarea? </li><li>  Aumento de precio.  Una cosa es poner una placa simple como nano pi.  Y el otro es comprar TX2. </li></ol><br>  S√≠, s√© que hay tareas en las que no se pueden tomar decisiones sobre el servidor.  Pero, curiosamente, son mucho m√°s peque√±os de lo que com√∫nmente se cree. <br><br><h2>  Conclusiones </h2><br>  En el art√≠culo, intent√© prescindir de conclusiones obvias.  Es m√°s bien una historia sobre lo que es ahora.  Para sacar conclusiones, es necesario investigar en cada caso.  Y no solo plataformas.  Pero la tarea en s√≠.  Cualquier tarea puede simplificarse / modificarse / afilarse ligeramente debajo del dispositivo. <br>  El problema con este tema es que el tema est√° cambiando.  Nuevos dispositivos / marcos / enfoques est√°n llegando.  Por ejemplo, si NVIDIA activa el soporte int8 para Jetson Nano ma√±ana, la situaci√≥n cambiar√° dram√°ticamente.  Cuando escribo este art√≠culo, no puedo estar seguro de que la informaci√≥n no haya cambiado hace dos d√≠as.  Pero espero que mi breve historia lo ayude a navegar mejor en su pr√≥ximo proyecto. <br>  Ser√≠a genial si tiene informaci√≥n adicional / Me perd√≠ algo / dije algo mal - escriba los detalles aqu√≠. <br><br>  ps <br>  Incluso cuando termin√© de escribir el art√≠culo casi, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">snakers4</a> dej√≥ caer una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicaci√≥n</a> reciente de su canal de telegramas Spark en m√≠, que tiene casi los mismos problemas con Jetson.  Pero, como escrib√≠ anteriormente, en las condiciones de cualquier consumo de energ√≠a, pondr√≠a algo como zotacs o IntelNUC.  Y como el jetson incrustado no es la peor plataforma. <br><br><img src="https://habrastorage.org/webt/k_/pg/ih/k_pgiheb6cx6kvxxebl4rc_jdei.jpeg"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/468421/">https://habr.com/ru/post/468421/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../468411/index.html">Una historia sobre c√≥mo resolver el problema de rendimiento de Moment.js</a></li>
<li><a href="../468413/index.html">Aceleraci√≥n instagram.com. Parte 2</a></li>
<li><a href="../468415/index.html">¬øPor qu√© no 1C?</a></li>
<li><a href="../468417/index.html">Lanzamiento de 3CX v16 Update 3 Beta: videollamadas en Android e iOS, conexi√≥n TLS de troncales SIP</a></li>
<li><a href="../468419/index.html">Google Analytics y GDPR: ¬øNecesito el consentimiento del usuario?</a></li>
<li><a href="../468423/index.html">¬øPor qu√© el est√°ndar USB tuvo que ser tan complicado?</a></li>
<li><a href="../468427/index.html">C√≥mo ser publicado en Google Play en 2019</a></li>
<li><a href="../468431/index.html">El resumen de materiales frescos del mundo del front-end para la √∫ltima semana No. 381 (del 16 al 22 de septiembre de 2019)</a></li>
<li><a href="../468437/index.html">Reconocer√© al dulce ... por la forma del canal auditivo. Una nueva forma de identificar usuarios</a></li>
<li><a href="../468439/index.html">Redes en los EE. UU. Para tontos. Experiencia personal</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>