<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìì üíó üëµüèª Visualisierung der neuronalen maschinellen √úbersetzung (seq2seq-Modelle mit Aufmerksamkeitsmechanismus) ‚ò£Ô∏è üõ¥ üöñ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habr Ich pr√§sentiere Ihnen die √úbersetzung des Artikels "Visualisierung eines neuronalen maschinellen √úbersetzungsmodells (Mechanik von Seq2seq-...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Visualisierung der neuronalen maschinellen √úbersetzung (seq2seq-Modelle mit Aufmerksamkeitsmechanismus)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/486158/"><p>  Hallo habr  Ich pr√§sentiere Ihnen die √úbersetzung des Artikels <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="nofollow">"Visualisierung eines neuronalen maschinellen √úbersetzungsmodells (Mechanik von Seq2seq-Modellen mit Aufmerksamkeit)"</a> von Jay Alammar. </p><br><p>  Sequence-to-Sequence-Modelle (seq2seq) sind Deep-Learning-Modelle, die bei Aufgaben wie maschineller √úbersetzung, Textzusammenfassung, Bildanmerkung usw. gro√üe Erfolge erzielt haben. Beispielsweise wurde Ende 2016 ein √§hnliches Modell in Google Translate integriert.  Die Grundlagen der seq2seq-Modelle wurden 2014 mit der Ver√∂ffentlichung von zwei Artikeln gelegt - <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow">Sutskever et al., 2014</a> , <a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf" rel="nofollow">Cho et al., 2014</a> . </p><br><p> Um diese Modelle ausreichend zu verstehen und anzuwenden, m√ºssen zun√§chst einige Konzepte gekl√§rt werden.  Die in diesem Artikel vorgeschlagenen Visualisierungen sind eine gute Erg√§nzung zu den oben genannten Artikeln. </p><br><p>  Sequenz-zu-Sequenz-Modell ist ein Modell, das eine Eingabesequenz von Elementen (W√∂rter, Buchstaben, Bildattribute usw.) akzeptiert und eine andere Sequenz von Elementen zur√ºckgibt.  Das trainierte Modell funktioniert wie folgt: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/seq2seq_1.mp4" type="video/mp4"></video></div></div></div><a name="habracut"></a><br><p>  Bei der neuronalen maschinellen √úbersetzung ist eine Folge von Elementen eine Sammlung von W√∂rtern, die nacheinander verarbeitet werden.  Die Schlussfolgerung ist auch eine Reihe von W√∂rtern: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/seq2seq_2.mp4" type="video/mp4"></video></div></div></div><br><h1 id="zaglyanem-pod-kapot">  Schau mal unter die Motorhaube </h1><br><p>  Unter der Haube hat das Modell einen Encoder und einen Decoder. </p><br><p>  Der Codierer verarbeitet jedes Element der Eingabesequenz und √ºbersetzt die empfangenen Informationen in einen als Kontext bezeichneten Vektor.  Nach der Verarbeitung der gesamten Eingabesequenz sendet der Encoder den Kontext an den Decoder, der dann Element f√ºr Element die Ausgabesequenz generiert. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/seq2seq_3.mp4" type="video/mp4"></video></div></div></div><br><p>  Dasselbe passiert mit maschineller √úbersetzung. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/seq2seq_4.mp4" type="video/mp4"></video></div></div></div><br><p>  Bei der maschinellen √úbersetzung ist der Kontext ein Vektor (ein Array von Zahlen), und der Codierer und der Decodierer sind wiederum am h√§ufigsten wiederkehrende neuronale Netze (siehe die Einf√ºhrung in RNN - <a href="https://www.youtube.com/watch%3Fv%3DUNmqTiOnRfg" rel="nofollow">Eine benutzerfreundliche Einf√ºhrung in wiederkehrende neuronale Netze</a> ). </p><br><p><img src="https://habrastorage.org/webt/yr/ir/92/yrir92d8paf_xdm29bovaw109gu.png" alt="Kontext"></p><br><p>  <em>Der Kontext ist ein Vektor von Gleitkommazahlen.</em>  <em>Weiter im Artikel werden Vektoren in Farbe dargestellt, so dass die hellere Farbe Zellen mit gro√üen Werten entspricht.</em> </p><br><p>  Beim Trainieren des Modells k√∂nnen Sie die Gr√∂√üe des Kontextvektors festlegen - die Anzahl der verborgenen Neuronen (verborgenen Einheiten) im RNN-Encoder.  Die Visualisierungsdaten zeigen einen 4-dimensionalen Vektor, aber in realen Anwendungen hat der Kontextvektor eine Dimension in der Gr√∂√üenordnung von 256, 512 oder 1024. </p><br><p>  Standardm√§√üig empf√§ngt RNN in jedem Zeitintervall zwei Elemente zur Eingabe: das Eingabeelement selbst (im Falle eines Encoders ein Wort aus dem urspr√ºnglichen Satz) und den verborgenen Zustand.  Das Wort muss jedoch durch einen Vektor dargestellt werden.  Um ein Wort in einen Vektor umzuwandeln, greifen sie auf eine Reihe von Algorithmen zur√ºck, die als Worteinbettungen bezeichnet werden.  Einbettungen √ºbersetzen W√∂rter in Vektorr√§ume, die semantische und semantische Informationen √ºber sie enthalten (z. B. <a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html" rel="nofollow">"K√∂nig" - "Mann" + "Frau" = "K√∂nigin"</a> ). </p><br><p><img src="https://habrastorage.org/webt/87/pp/s9/87pps99ndmgvp6gqxskkodf4zl4.png" alt="einbetten"></p><br><p>  <em>Bevor Sie W√∂rter verarbeiten, m√ºssen Sie sie in Vektoren konvertieren.</em>  <em>Diese Transformation wird unter Verwendung des Worteinbettungsalgorithmus ausgef√ºhrt.</em>  <em>Sie k√∂nnen sowohl vorab trainierte Einbettungen als auch Train-Einbettungen f√ºr Ihren Datensatz verwenden.</em>  <em>200-300 - typische Dimension des Einbettungsvektors;</em>  <em>In diesem Artikel wird der Einfachheit halber Dimension 4 verwendet.</em> </p><br><p>  Nachdem wir uns mit unseren Hauptvektoren / Tensoren vertraut gemacht haben, wollen wir uns an den Mechanismus von RNN erinnern und Visualisierungen erstellen, um ihn zu beschreiben: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/RNN_1.mp4" type="video/mp4"></video></div></div></div><br><p>  Im n√§chsten Schritt nimmt RNN den zweiten Eingangsvektor und den latenten Zustand # 1, um in diesem Zeitintervall den Ausgang zu bilden.  Sp√§ter in diesem Artikel wird eine √§hnliche Animation verwendet, um Vektoren in einem neuronalen maschinellen √úbersetzungsmodell zu beschreiben. </p><br><p>  In der folgenden Visualisierung beschreibt jeder Frame die Verarbeitung von Eingaben durch einen Codierer und die Erzeugung von Ausgaben durch einen Decodierer in einem Zeitintervall.  Da sowohl der Codierer als auch der Decodierer RNN sind, ist das neuronale Netzwerk in jedem Zeitintervall damit besch√§ftigt, seine verborgenen Zust√§nde basierend auf den aktuellen und allen vorherigen Eingaben zu verarbeiten und zu aktualisieren.  In diesem Fall ist der letzte der verborgenen Zust√§nde des Codierers der Kontext, der an den Decodierer √ºbertragen wird. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/seq2seq_5.mp4" type="video/mp4"></video></div></div></div><br><p>  Der Decoder enth√§lt auch versteckte Zust√§nde, die er von einem Zeitschlitz zu einem anderen √ºbertr√§gt.  (Dies ist nicht in der Visualisierung enthalten und zeigt nur die Hauptteile des Modells.) </p><br><p>  Wir wenden uns nun einer anderen Art der Visualisierung von Sequenz-zu-Sequenz-Modellen zu.  Diese Animation wird helfen, die statischen Grafiken zu verstehen, die diese Modelle beschreiben - die sogenannten  Eine nicht gerollte Ansicht, bei der anstelle eines Decoders f√ºr jedes Zeitintervall eine Kopie angezeigt wird.  So k√∂nnen wir uns die Eingabe- und Ausgabeelemente in jedem Zeitintervall ansehen. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/seq2seq_6.mp4" type="video/mp4"></video></div></div></div><br><h1 id="obratite-vnimanie">  Beachten Sie! </h1><br><p>  Der Kontextvektor ist ein Engpass f√ºr diese Art von Modell, der es ihnen erschwert, mit langen S√§tzen umzugehen.  Die L√∂sung wurde in Artikeln von <a href="" rel="nofollow">Bahdanau et al., 2014</a> und <a href="https://arxiv.org/abs/1508.04025" rel="nofollow">Luong et al., 2015 vorgeschlagen</a> , die eine Technik vorstellten, die als Aufmerksamkeitsmechanismus bezeichnet wird.  Dieser Mechanismus verbessert die Qualit√§t von maschinellen √úbersetzungssystemen erheblich und erm√∂glicht es den Modellen, sich auf die relevanten Teile der Eingabesequenzen zu konzentrieren. </p><br><p><img src="https://habrastorage.org/webt/b1/ru/kj/b1rukj6w-dgtyfncd3a62635zb0.png" alt="aufmerksamkeit"></p><br><p>  <em>In der 7. Zeitspanne erm√∂glicht der Aufmerksamkeitsmechanismus dem Decoder, sich auf das Wort √©tudiant (Student in Franz√∂sisch) zu konzentrieren, bevor eine √úbersetzung ins Englische erstellt wird.</em>  <em>Diese F√§higkeit, das Signal aus dem relevanten Teil der Eingabesequenz zu verst√§rken, erm√∂glicht es Modellen, die auf dem Aufmerksamkeitsmechanismus basieren, ein besseres Ergebnis im Vergleich zu anderen Modellen zu erzielen.</em> </p><br><p>  Bei der Betrachtung eines Modells mit einem Aufmerksamkeitsmechanismus auf einer hohen Abstraktionsebene k√∂nnen zwei Hauptunterschiede zum klassischen Sequenz-zu-Sequenz-Modell unterschieden werden. </p><br><p>  Erstens √ºbertr√§gt der Encoder deutlich mehr Daten an den Decoder: Anstatt nur den letzten versteckten Zustand nach der Codierungsstufe zu √ºbertragen, sendet der Encoder alle seine versteckten Zust√§nde an ihn: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/seq2seq_7.mp4" type="video/mp4"></video></div></div></div><br><p>  Zweitens durchl√§uft der Decoder einen zus√§tzlichen Schritt, bevor er die Ausgabe erzeugt.  Um sich auf die Teile der Eingabesequenz zu konzentrieren, die f√ºr die entsprechende Zeitspanne relevant sind, f√ºhrt der Decoder Folgendes aus: </p><br><ol><li>  Betrachtet eine Reihe latenter Zust√§nde, die von einem Codierer empfangen wurden. Jeder der latenten Zust√§nde korreliert am besten mit einem der W√∂rter in der Eingabesequenz. </li><li>  Weist jedem latenten Zustand eine bestimmte Bewertung zu (lassen Sie uns zun√§chst den Ablauf des Bewertungsverfahrens auslassen). </li><li>  Multipliziert jeden verborgenen Zustand mit einer softmax-konvertierten Bewertungsfunktion, um verborgene Zust√§nde mit einer hohen Bewertung hervorzuheben und verborgene Zust√§nde mit einem kleinen Zustand in den Hintergrund zu r√ºcken. </li></ol><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/attention_process.mp4" type="video/mp4"></video></div></div></div><br><p>  Diese ‚ÄûBewertungs√ºbung‚Äú wird in jedem Zeitintervall am Decoder durchgef√ºhrt. </p><br><p>  Zusammenfassend betrachten wir also den Prozess des Modells mit dem Aufmerksamkeitsmechanismus: </p><br><ol><li>  Beim Decodierer empf√§ngt der RNN die Einbettung &lt;ENDE&gt; des Tokens und des anf√§nglichen verborgenen Zustands. </li><li>  Die RNN verarbeitet das Eingabeelement, generiert die Ausgabe und einen neuen verborgenen Zustandsvektor (h4).  Die Ausgabe wird verworfen. </li><li>  Der Aufmerksamkeitsmechanismus verwendet die verborgenen Zust√§nde des Codierers und den Vektor h4, um den Kontextvektor (C4) in einem gegebenen Zeitintervall zu berechnen. </li><li>  Die Vektoren h4 und C4 sind zu einem einzigen Vektor verkettet. </li><li>  Dieser Vektor wird durch ein Feedforward-Neuronales Netzwerk (FFN) geleitet, das zusammen mit dem Modell trainiert wird. </li><li>  Der Ausgang des FFN-Netzwerks zeigt das Ausgangswort in einem bestimmten Zeitintervall an. </li><li>  Der Algorithmus wird f√ºr das n√§chste Zeitintervall wiederholt. </li></ol><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/attention_tensor_dance.mp4" type="video/mp4"></video></div></div></div><br><p>  Ein anderer Weg, um zu sehen, welchen Teil des urspr√ºnglichen Satzes das Modell auf jeder Stufe des Decoders fokussiert: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://jalammar.github.io/images/seq2seq_9.mp4" type="video/mp4"></video></div></div></div><br><p>  Beachten Sie, dass das Modell nicht nur das erste Wort in der Eingabe mit dem ersten Wort in der Ausgabe sinnlos verbindet.  W√§hrend des Trainings verstand sie tats√§chlich, wie man die W√∂rter in diesem betrachteten Sprachpaar (in unserem Fall Franz√∂sisch und Englisch) zusammenbringt.  Ein Beispiel daf√ºr, wie genau dieser Mechanismus funktionieren kann, finden Sie in den Artikeln √ºber den oben erw√§hnten Aufmerksamkeitsmechanismus. </p><br><p><img src="https://habrastorage.org/webt/cl/dv/9f/cldv9f7zdegsobuszn10hyy1nde.png" alt="Aufmerksamkeitssatz"></p><br><p>  Wenn Sie das Gef√ºhl haben, dass Sie bereit sind, die Anwendung dieses Modells zu erlernen, <a href="https://github.com/tensorflow/nmt" rel="nofollow">lesen Sie das</a> Handbuch f√ºr die <a href="https://github.com/tensorflow/nmt" rel="nofollow">neuronale maschinelle √úbersetzung (seq2seq)</a> in TensorFlow. </p><br><h1 id="avtory">  Die Autoren </h1><br><ul><li>  <strong>Original</strong> von <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="nofollow">Jay Alammar</a> </li><li>  <strong>√úbersetzung</strong> - <a href="https://habr.com/ru/users/smekur/">Ekaterina Smirnova</a> </li><li>  <strong>Bearbeitung und Layout</strong> - <a href="https://habr.com/ru/users/kouki_rus/">Shkarin Sergey</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de486158/">https://habr.com/ru/post/de486158/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de486124/index.html">Impala vs Hive vs Spark SQL: Auswahl der richtigen SQL-Engine f√ºr die ordnungsgem√§√üe Funktion im Cloudera Data Warehouse</a></li>
<li><a href="../de486128/index.html">Test Solution Architect: Wer ist es und wann wird es ben√∂tigt?</a></li>
<li><a href="../de486144/index.html">Warum sterben Altcoins und was kann in naher Zukunft mit Kryptow√§hrung passieren?</a></li>
<li><a href="../de486150/index.html">Entwicklung der IT-Sph√§re in der Slowakei. Arbeitsnutzen f√ºr junge Berufst√§tige</a></li>
<li><a href="../de486156/index.html">Da habe ich unterrichtet und dann ein Trainingshandbuch in Python geschrieben</a></li>
<li><a href="../de486164/index.html">Coronavirus 2019-nCoV. FAQ zu Atemschutz und Desinfektion</a></li>
<li><a href="../de486174/index.html">Ich habe keinen Umsatz</a></li>
<li><a href="../es432400/index.html">CS: GO ahora es gratis</a></li>
<li><a href="../es432402/index.html">Hola HR, ¬ød√≥nde est√° mi recuerdo?</a></li>
<li><a href="../es432404/index.html">Backup for Linux no escribe cartas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>