<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¤¹ ğŸ¥– ğŸŒ† Bagaimana layanan basis data dikelola diatur dalam Yandex. Cloud ğŸ” ğŸ‘ƒğŸ» ğŸ’“</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ketika Anda mempercayai seseorang hal paling berharga yang Anda miliki - data aplikasi atau layanan Anda - Anda ingin membayangkan bagaimana seseorang...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bagaimana layanan basis data dikelola diatur dalam Yandex. Cloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/477860/">  Ketika Anda mempercayai seseorang hal paling berharga yang Anda miliki - data aplikasi atau layanan Anda - Anda ingin membayangkan bagaimana seseorang ini akan menangani nilai terbesar Anda. <br><br>  Nama saya Vladimir Borodin, saya adalah kepala platform data Yandex.Cloud.  Hari ini saya ingin memberi tahu Anda bagaimana semuanya diatur dan bekerja di dalam layanan Database Dikelola Yandex, mengapa semuanya dilakukan begitu saja dan apa keuntungannya - dari sudut pandang pengguna - dari berbagai solusi kami.  Dan tentu saja, Anda pasti akan mengetahui apa yang kami rencanakan untuk diselesaikan dalam waktu dekat sehingga layanan menjadi lebih baik dan lebih nyaman bagi semua orang yang membutuhkannya. <br><br>  Ayo pergi! <br><br><img src="https://habrastorage.org/webt/xw/qq/id/xwqqid27hmguyyigukvb2z3o8da.png" alt="gambar"><br><a name="habracut"></a><br>  Managed Databases (Yandex Managed Databases) adalah salah satu layanan Yandex.Cloud paling populer.  Lebih tepatnya, ini adalah seluruh kelompok layanan, yang sekarang hanya kedua setelah mesin virtual Yandex Compute Cloud dalam popularitas. <br><br>  Database yang Dikelola Yandex memungkinkan untuk dengan cepat mendapatkan database yang berfungsi dan melakukan tugas-tugas seperti: <br><br><ul><li>  Penskalaan - dari kemampuan dasar untuk menambah sumber daya komputasi atau ruang disk hingga peningkatan jumlah replika dan pecahan. </li><li>  Instal pembaruan, minor dan utama. </li><li>  Cadangkan dan pulihkan. </li><li>  Memberikan toleransi kesalahan. </li><li>  Pemantauan </li><li>  Menyediakan alat konfigurasi dan manajemen yang praktis. </li></ul><br><h2>  Bagaimana layanan database dikelola diatur: tampilan atas </h2><br>  Layanan ini terdiri dari dua bagian utama: Control Plane dan Data Plane.  Control Plane adalah, sederhananya, API manajemen basis data yang memungkinkan Anda untuk membuat, memodifikasi, atau menghapus basis data.  Data Plane adalah tingkat penyimpanan data langsung. <br><br><img src="https://habrastorage.org/webt/vo/4c/o-/vo4co-q4zc3r6xwe0is2pl9hrri.png" alt="gambar"><br><br>  Faktanya, pengguna layanan memiliki dua titik masuk: <br><br><ul><li>  Di Control Plane.  Bahkan, ada banyak input - konsol Web, utilitas CLI, dan API gateway yang menyediakan API publik (gRPC dan REST).  Tetapi semuanya akhirnya menuju ke apa yang kami sebut API Internal, dan oleh karena itu kami akan mempertimbangkan titik masuk yang satu ini ke dalam Control Plane.  Bahkan, ini adalah titik dari mana area layanan Managed Databases (MDB) dimulai. </li><li> Di Bidang Data.  Ini adalah koneksi langsung ke database yang sedang berjalan melalui protokol akses ke DBMS.  Jika itu, misalnya, PostgreSQL, maka itu akan menjadi <a href="https://www.postgresql.org/docs/current/libpq.html">antarmuka libpq</a> . </li></ul><br><img src="https://habrastorage.org/webt/cr/l5/pq/crl5pqrbr2mqqlun6ivgwlvn0u8.png" alt="gambar"><br>  Di bawah ini kami akan menjelaskan secara lebih rinci segala sesuatu yang terjadi di Data Plane, dan kami akan menganalisis masing-masing komponen Control Plane. <br><br><h2>  Pesawat data </h2><br>  Sebelum melihat komponen-komponen dari Pesawat Kontrol, mari kita lihat apa yang terjadi di Pesawat Data. <br><br><h3>  Di dalam mesin virtual </h3><br>  MDB menjalankan basis data di mesin virtual yang sama yang disediakan di <a href="https://cloud.yandex.ru/services/compute">Yandex Compute Cloud</a> . <br><br><img src="https://habrastorage.org/webt/-w/gm/qj/-wgmqjlxi30m4jz7dgp4sct86cw.png" alt="gambar"><br><br>  Pertama-tama, mesin database, misalnya, PostgreSQL, ditempatkan di sana.  Secara paralel, berbagai program tambahan dapat diluncurkan.  Untuk PostgreSQL, ini akan menjadi <a href="https://github.com/yandex/odyssey">Odyssey</a> , penarik koneksi database. <br><br>  Juga di dalam mesin virtual, serangkaian layanan standar tertentu diluncurkan, masing-masing untuk setiap DBMS: <br><br><ul><li>  Layanan untuk membuat cadangan.  Untuk PostgreSQL, ini adalah alat <a href="https://github.com/wal-g/wal-g">WAL-G</a> open source <a href="https://github.com/wal-g/wal-g">.</a>  Ini membuat cadangan dan menyimpannya di <a href="https://cloud.yandex.ru/services/storage">Penyimpanan Objek Yandex</a> . </li><li>  Salt Minion adalah komponen dari sistem <a href="https://docs.saltstack.com/en/getstarted/">SaltStack</a> untuk operasi dan manajemen konfigurasi.  Informasi lebih lanjut tentang itu diberikan di bawah ini dalam deskripsi infrastruktur Deploy. </li><li>  Metrik MDB, yang bertanggung jawab untuk mentransmisikan metrik basis data ke <a href="https://cloud.yandex.ru/services/monitoring">Yandex Monitoring</a> dan ke layanan mikro kami untuk memantau status cluster dan host MDB Health. </li><li>  Push client, yang mengirimkan log DBMS dan log tagihan ke layanan Logbroker, adalah solusi khusus untuk mengumpulkan dan mengirimkan data. </li><li>  Cron MDB - sepeda kami, yang berbeda dari cron biasa dalam kemampuan untuk melakukan tugas-tugas berkala dengan akurasi satu detik. </li></ul><br><h3>  Topologi jaringan </h3><br><img src="https://habrastorage.org/webt/x8/_6/ge/x8_6ge0uluxqju22unryyrxxhxw.png" alt="gambar"><br><br>  Setiap host Data Plane memiliki dua antarmuka jaringan: <br><br><ul><li>  Salah satunya menempel ke jaringan pengguna.  Secara umum, diperlukan untuk memperbaiki muatan produk.  Melalui itu, replikasi mengejar. </li><li>  Yang kedua menempel ke salah satu jaringan terkelola kami yang digunakan host untuk mengakses Control Plane. </li></ul><br>  Ya, host dari klien yang berbeda terjebak dalam satu jaringan terkelola seperti itu, tetapi ini tidak menakutkan, karena pada antarmuka terkelola (hampir) tidak ada yang mendengarkan, koneksi jaringan keluar di Control Plane hanya dibuka darinya.  Hampir tidak ada, karena ada port terbuka (misalnya, SSH), tetapi ditutup oleh firewall lokal yang hanya memungkinkan koneksi dari host tertentu.  Dengan demikian, jika seorang penyerang mendapatkan akses ke mesin virtual dengan database, ia tidak dapat mencapai database orang lain. <br><br><h3>  Keamanan Pesawat Data </h3><br>  Karena kita berbicara tentang keamanan, harus dikatakan bahwa kita awalnya merancang layanan berdasarkan penyerang mendapatkan root pada mesin virtual cluster. <br><br>  Pada akhirnya, kami melakukan banyak upaya untuk melakukan hal berikut: <br><br><ul><li>  Firewall lokal dan besar; </li><li>  Enkripsi semua koneksi dan cadangan; </li><li>  Semua dengan otentikasi dan otorisasi; </li><li>  AppArmor </li><li>  IDS yang ditulis sendiri. </li></ul><br>  Sekarang perhatikan komponen Control Plane. <br><br><h2>  Kontrol pesawat </h2><br><h3>  API internal </h3><br>  API Internal adalah titik masuk pertama ke Control Plane.  Mari kita lihat bagaimana semuanya bekerja di sini. <br><br><img src="https://habrastorage.org/webt/vb/ru/j7/vbruj7qxxu2tmlaplki2dt_74cy.png" alt="gambar"><br><br>  Misalkan API Internal menerima permintaan untuk membuat cluster database. <br><br>  Pertama-tama, API Internal mengakses layanan Akses layanan cloud, yang bertanggung jawab untuk memeriksa otentikasi dan otorisasi pengguna.  Jika pengguna lolos verifikasi, API Internal akan memeriksa validitas permintaan itu sendiri.  Misalnya, permintaan untuk membuat sebuah cluster tanpa menyebutkan namanya atau dengan nama yang sudah diambil akan gagal dalam pengujian. <br><blockquote>  Dan API Internal dapat mengirim permintaan ke API layanan lain.  Jika Anda ingin membuat cluster di jaringan A tertentu, dan host tertentu di subnet B tertentu, API Internal harus memastikan bahwa Anda memiliki hak untuk jaringan A dan subnet B. yang ditentukan. Pada saat yang sama, ia akan memeriksa apakah subnet B milik jaringan A Ini membutuhkan akses ke API infrastruktur. </blockquote><br>  Jika permintaan itu valid, informasi tentang cluster yang dibuat akan disimpan dalam metabase.  Kami menyebutnya MetaDB, ini digunakan di PostgreSQL.  MetaDB memiliki tabel dengan antrian operasi.  API Internal menyimpan informasi tentang operasi dan menetapkan tugas secara transaksi.  Setelah itu, informasi tentang operasi dikembalikan kepada pengguna. <br><br>  Secara umum, untuk memproses sebagian besar permintaan API Internal, cukup menggunakan MetaDB dan API layanan terkait.  Tetapi ada dua komponen lagi yang digunakan oleh API Internal untuk menjawab beberapa pertanyaan - LogsDB, di mana log cluster pengguna berada, dan MDB Health.  Tentang masing-masing akan dijelaskan lebih detail di bawah ini. <br><br><h3>  Pekerja </h3><br>  Pekerja hanyalah seperangkat proses yang meminta antrian operasi di MetaDB, ambil dan jalankan. <br><br><img src="https://habrastorage.org/webt/ez/u7/q8/ezu7q86g82hjjt2nqrdssqhxpn0.png" alt="gambar"><br><br>  Apa sebenarnya yang dilakukan pekerja ketika sebuah cluster dibuat?  Pertama dia beralih ke API infrastruktur untuk membuat mesin virtual dari gambar kita (mereka sudah memiliki semua paket yang diperlukan diinstal dan sebagian besar hal dikonfigurasi, gambar diperbarui sekali sehari).  Ketika mesin virtual dibuat dan jaringan lepas landas di dalamnya, pekerja beralih ke infrastruktur Deploy (kami akan membicarakannya lebih lanjut nanti) untuk menyebarkan apa yang dibutuhkan pengguna ke mesin virtual. <br><br>  Selain itu, pekerja mengakses layanan Cloud lainnya.  Misalnya, untuk <a href="https://cloud.yandex.ru/services/storage">Penyimpanan Objek Yandex</a> untuk membuat ember di mana cadangan cluster akan disimpan.  Ke layanan <a href="https://cloud.yandex.ru/services/monitoring">Pemantauan Yandex</a> , yang akan mengumpulkan dan memvisualisasikan metrik basis data.  Pekerja harus membuat meta-informasi kluster di sana.  Untuk API DNS, jika pengguna ingin menetapkan alamat IP publik ke host cluster. <br><br>  Secara umum, pekerja bekerja dengan sangat sederhana.  Ia menerima tugas dari antrian metabase dan mengakses layanan yang diinginkan.  Setelah menyelesaikan setiap langkah, pekerja menyimpan informasi tentang kemajuan operasi di metabase.  Jika kegagalan terjadi, tugas hanya restart dan berjalan dari tempat ia tinggalkan.  Tetapi bahkan memulai kembali dari awal tidak menjadi masalah, karena hampir semua jenis tugas untuk pekerja ditulis dengan idempoten.  Ini karena pekerja dapat melakukan satu atau beberapa langkah operasi, tetapi tidak ada informasi tentang ini di MetaDB. <br><br><h3>  Menyebarkan Infrastruktur </h3><br>  Di bagian paling bawah adalah <a href="https://docs.saltstack.com/en/getstarted/">SaltStack</a> , sistem manajemen konfigurasi open source yang cukup umum ditulis dengan Python.  Sistem ini sangat <a href="https://docs.saltstack.com/en/latest/ref/index.html">dapat dikembangkan</a> , dan kami menyukainya. <br><br>  Komponen utama dari garam adalah master garam, yang menyimpan informasi tentang apa yang harus diterapkan dan di mana, dan garam minion, agen yang dipasang pada setiap host, berinteraksi dengan master dan dapat langsung mengaplikasikan garam dari master garam ke host.  Untuk keperluan artikel ini, kami memiliki cukup pengetahuan ini, dan Anda dapat membaca lebih banyak di <a href="https://docs.saltstack.com/en/getstarted/overview.html">dokumentasi SaltStack</a> . <br><br>  Satu master garam tidak toleran terhadap kesalahan dan tidak skala ke ribuan pelayan, beberapa master diperlukan.  Berinteraksi dengan ini langsung dari pekerja itu tidak nyaman, dan kami menulis ikatan kami pada Salt, yang kami sebut kerangka Deploy. <br><br><img src="https://habrastorage.org/webt/ir/x6/8o/irx68o0wo493tgj6o8xh0xj0x3o.png" alt="gambar"><br><br>  Untuk pekerja, satu-satunya titik masuk adalah Deploy API, yang mengimplementasikan metode seperti "Terapkan seluruh negara bagian atau masing-masing bagian ke antek tersebut" dan "Beri tahu status peluncuran ini dan itu".  Deploy API menyimpan informasi tentang semua peluncuran dan langkah spesifiknya di DeployDB, tempat kami juga menggunakan PostgreSQL.  Informasi tentang semua kaki tangan dan tuan dan tentang kepemilikan yang pertama hingga yang kedua juga disimpan di sana. <br><br>  Dua komponen tambahan dipasang pada pemilik garam: <br><br><ul><li>  <a href="https://docs.saltstack.com/en/develop/ref/netapi/all/salt.netapi.rest_cherrypy.html">Salt REST API</a> , yang digunakan Deploy API untuk meluncurkan peluncuran.  API REST pergi ke ahli garam lokal, dan dia sudah berkomunikasi dengan antek menggunakan ZeroMQ. </li><li>  Intinya adalah bahwa ia pergi ke Deploy API dan menerima kunci publik dari semua antek yang harus terhubung ke master garam ini.  Tanpa kunci publik pada master, antek tidak bisa terhubung ke master. </li></ul><br>  Selain antek garam, dua komponen juga dipasang di Data Plane: <br><br><ul><li>  <a href="https://docs.saltstack.com/en/latest/ref/returners/">Returner</a> - sebuah modul (salah satu bagian yang dapat dikembangkan dalam garam), yang membawa hasil peluncuran tidak hanya ke ahli garam, tetapi juga di Deploy API.  Deploy API memulai penyebaran dengan pergi ke REST API pada wizard, dan menerima hasilnya melalui kembali dari antek. </li><li>  Master pinger, yang secara berkala melakukan polling Deploy API yang mana master minions harus terhubung.  Jika Deploy API mengembalikan alamat penyihir baru (misalnya, karena yang lama mati atau kelebihan beban), pinger mengkonfigurasi ulang antek tersebut. </li></ul><br>  Tempat lain di mana kami menggunakan ekstensibilitas SaltStack adalah <a href="https://docs.saltstack.com/en/latest/ref/pillar/all/index.html">ext_pillar</a> - kemampuan untuk mendapatkan <a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html">pilar</a> dari suatu tempat di luar (beberapa informasi statis, misalnya, konfigurasi PostgreSQL, pengguna, basis data, ekstensi, dll.).  Kami pergi ke API Internal dari modul kami untuk mendapatkan pengaturan cluster-spesifik, karena mereka disimpan dalam MetaDB. <br><br>  Secara terpisah, perhatikan bahwa pilar juga mengandung informasi rahasia (kata sandi pengguna, sertifikat TLS, kunci GPG untuk mengenkripsi cadangan), dan oleh karena itu, pertama-tama, semua interaksi antara semua komponen dienkripsi (tidak ada dalam basis data kami mana pun) datang tanpa TLS, HTTPS di mana-mana, antek dan master juga mengenkripsi semua lalu lintas).  Dan kedua, semua rahasia ini dienkripsi dalam MetaDB, dan kami menggunakan pemisahan rahasia - pada mesin API Internal ada kunci publik yang mengenkripsi semua rahasia sebelum disimpan di MetaDB, dan bagian pribadi dari itu terletak pada pemilik garam dan hanya mereka yang bisa mendapatkan buka rahasia untuk mentransfer sebagai pilar ke antek (sekali lagi melalui saluran terenkripsi). <br><br><h3>  Kesehatan MDB </h3><br>  Saat bekerja dengan basis data, penting untuk mengetahui statusnya.  Untuk ini, kami memiliki layanan kesehatan MDB.  Ia menerima informasi status host dari komponen internal MDB mesin virtual MDB dan menyimpannya dalam database sendiri (dalam hal ini, Redis).  Dan ketika permintaan tentang status cluster tertentu tiba di API Internal, API Internal menggunakan data dari MetaDB dan MDB Health. <br><br><img width="500" src="https://habrastorage.org/webt/ee/su/qu/eesuquvflpn5k_frgbcqlakkfkc.png" alt="gambar"><br><br>  Informasi tentang semua host diproses dan disajikan dalam bentuk yang dapat dipahami dalam API.  Selain keadaan host dan cluster untuk beberapa DBMS, MDB Health juga mengembalikan apakah host tertentu adalah master atau replika. <br><br><h3>  DNS MDB </h3><br>  Layanan mikro DNS MDB diperlukan untuk mengelola data CNAME.  Jika pengandar untuk menyambungkan ke database tidak memungkinkan mentransfer beberapa host di string koneksi, Anda dapat menyambungkan ke <a href="https://cloud.yandex.ru/docs/managed-mysql/operations/connect">CNAME</a> khusus, yang selalu menunjukkan master saat ini di cluster.  Jika master beralih, CNAME berubah. <br><br><img width="500" src="https://habrastorage.org/webt/2y/g2/sd/2yg2sd8z2cgvuy9spzxo00w3eoy.png" alt="gambar"><br><br>  Bagaimana kabarnya?  Seperti yang kami katakan di atas, di dalam mesin virtual ada cron MDB, yang secara berkala mengirimkan detak jantung dari konten berikut ke DNS MDB: "Dalam klaster ini, catatan CNAME harus menunjuk ke saya."  MDB DNS menerima pesan semacam itu dari semua mesin virtual dan memutuskan apakah akan mengubah catatan CNAME.  Jika perlu, itu mengubah catatan melalui API DNS. <br><br>  Mengapa kami membuat layanan terpisah untuk ini?  Karena DNS API hanya memiliki kontrol akses di tingkat zona.  Seorang penyerang potensial, mendapatkan akses ke mesin virtual yang terpisah, dapat mengubah catatan CNAME dari pengguna lain.  DNS MDB mengecualikan skenario ini karena memeriksa otorisasi. <br><br><h3>  Pengiriman dan tampilan log basis data </h3><br>  Ketika database pada mesin virtual menulis ke log, komponen klien push khusus membaca catatan ini dan mengirimkan baris yang baru saja muncul ke Logbroker ( <a href="https://habr.com/ru/company/yandex/blog/239823/">mereka sudah menulis</a> tentang itu di HabrÃ©).  Interaksi klien push dengan LogBroker dibangun dengan semantik langsung: kami pasti akan mengirimkannya dan pastikan untuk sekali saja. <br><br>  Kumpulan mesin yang terpisah - LogConsumers - mengambil log dari antrian LogBroker dan menyimpannya dalam database LogsDB.  ClickHouse DBMS digunakan untuk database log. <br><br><img width="500" src="https://habrastorage.org/webt/w-/zc/mr/w-zcmrza7z9pg6gacszmpm0kojo.png" alt="gambar"><br><br>  Ketika permintaan dikirim ke API Internal untuk menampilkan log untuk interval waktu tertentu untuk kluster tertentu, API Internal memeriksa otorisasi dan mengirimkan permintaan ke LogsDB.  Dengan demikian, loop pengiriman log benar-benar independen dari loop tampilan log. <br><br><h3>  Tagihan </h3><br>  Skema penagihan dibuat dengan cara yang serupa.  Di dalam mesin virtual, ada komponen yang memeriksa dengan periodisitas tertentu bahwa semuanya sesuai dengan database.  Jika semuanya baik-baik saja, Anda dapat melakukan penagihan untuk interval waktu ini sejak saat peluncuran terakhir.  Dalam hal ini, catatan dibuat di log penagihan, dan kemudian klien push mengirim catatan ke LogBroker.  Data dari Logbroker ditransfer ke sistem penagihan dan perhitungan dilakukan di sana.  Ini adalah skema penagihan untuk menjalankan cluster. <br><br>  Jika gugus dimatikan, penggunaan sumber daya komputasi berhenti diisi, namun, ruang disk dibebankan.  Dalam hal ini, penagihan dari mesin virtual tidak dimungkinkan dan sirkuit kedua terlibat - sirkuit penagihan offline.  Ada kumpulan mesin yang terpisah yang menyapu daftar cluster shutdown dari MetaDB dan menulis log dalam format yang sama di Logbroker. <br><br>  Penagihan offline dapat digunakan untuk penagihan dan juga termasuk cluster, tetapi kami akan menagih host, meskipun mereka berjalan, tetapi mereka tidak berfungsi.  Misalnya, ketika Anda menambahkan host ke sebuah cluster, itu menyebarkan dari cadangan dan terjebak dengan replikasi.  Salah menagih pengguna untuk ini, karena tuan rumah tidak aktif selama periode waktu ini. <br><br><img width="500" src="https://habrastorage.org/webt/fa/gx/hw/fagxhwoaqe0fb5q60chfthefkv8.png" alt="gambar"><br><br><h3>  Cadangkan </h3><br>  Skema pencadangan mungkin sedikit berbeda untuk DBMS yang berbeda, tetapi prinsip umum selalu sama. <br><br>  Setiap mesin basis data menggunakan alat cadangannya sendiri.  Untuk PostgreSQL dan MySQL, ini adalah <a href="https://github.com/wal-g/wal-g">WAL-G</a> .  Itu membuat cadangan, kompres mereka, mengenkripsi mereka dan menempatkan mereka di <a href="https://cloud.yandex.ru/services/storage">Penyimpanan Objek Yandex</a> .  Pada saat yang sama, setiap cluster ditempatkan dalam ember yang terpisah (pertama, untuk isolasi, dan kedua, untuk membuatnya lebih mudah untuk menghemat ruang untuk cadangan) dan dienkripsi dengan kunci enkripsi sendiri. <br><br><img width="500" src="https://habrastorage.org/webt/mf/dh/w0/mfdhw0bgyo59pytnhaqkaqghbou.png" alt="gambar"><br><br>  Ini adalah bagaimana Control Plane dan Data Plane bekerja.  Dari semua ini, layanan basis data yang dikelola Yandex.Cloud terbentuk. <br><br><h2>  Mengapa semuanya diatur dengan cara ini </h2><br>  Tentu saja, di tingkat global, sesuatu dapat diimplementasikan sesuai dengan skema yang lebih sederhana.  Tapi kami punya alasan sendiri untuk tidak mengikuti jalan yang paling sedikit perlawanan. <br><br>  Pertama-tama, kami ingin memiliki Control Plane yang sama untuk semua jenis DBMS.  Tidak masalah yang mana yang Anda pilih, pada akhirnya permintaan Anda datang ke API Internal yang sama dan semua komponen di bawahnya juga umum untuk semua DBMS.  Ini membuat hidup kita sedikit lebih rumit dalam hal teknologi.  Di sisi lain, jauh lebih mudah untuk memperkenalkan fitur dan kemampuan baru yang memengaruhi semua DBMS.  Ini dilakukan sekali, bukan enam. <br><br>  Momen penting kedua bagi kami - kami ingin memastikan independensi Data Plane dari Control Plane sebanyak mungkin.  Dan hari ini, bahkan jika Control Plane benar-benar tidak tersedia, semua database akan terus berfungsi.  Layanan ini akan memastikan keandalan dan ketersediaannya. <br><br>  Ketiga, pengembangan hampir semua layanan selalu kompromi.  Dalam arti umum, secara kasar, di suatu tempat yang lebih penting adalah kecepatan rilis rilis, dan di suatu tempat keandalan tambahan.  Pada saat yang sama, sekarang tidak ada yang mampu melakukan satu atau dua rilis setahun, ini jelas.  Jika Anda melihat Control Plane, di sini kami fokus pada kecepatan pengembangan, pada pengenalan cepat fitur baru, meluncurkan pembaruan beberapa kali seminggu.  Dan Data Plane bertanggung jawab atas keamanan database Anda, untuk toleransi kesalahan, jadi di sini adalah siklus rilis yang sangat berbeda, diukur dalam beberapa minggu.  Dan fleksibilitas dalam hal pengembangan ini juga memberi kita kebebasan bersama mereka. <br><br>  Contoh lain: layanan basis data yang dikelola biasanya hanya menyediakan drive jaringan kepada pengguna.  Yandex.Cloud juga menawarkan drive lokal.  Alasannya sederhana: kecepatan mereka jauh lebih tinggi.  Dengan drive jaringan, misalnya, lebih mudah untuk menaikkan dan menurunkan mesin virtual.  Lebih mudah membuat cadangan dalam bentuk snapshot penyimpanan jaringan.  Tetapi banyak pengguna membutuhkan kecepatan tinggi, jadi kami membuat alat cadangan level yang lebih tinggi. <br><br><h2>  Rencana masa depan </h2><br>  Dan beberapa kata tentang rencana untuk meningkatkan layanan untuk jangka menengah.  Ini adalah paket yang memengaruhi seluruh Yandex Managed Database secara keseluruhan, bukan DBMS individual. <br><br>  Pertama-tama, kami ingin memberikan lebih banyak fleksibilitas dalam mengatur frekuensi pembuatan cadangan.  Ada beberapa skenario ketika perlu bahwa pada siang hari pencadangan dilakukan sekali setiap beberapa jam, selama seminggu - sekali sehari, selama sebulan - sekali seminggu, selama tahun - sebulan sekali.  Untuk melakukan ini, kami sedang mengembangkan komponen terpisah antara API Internal dan <a href="https://cloud.yandex.ru/services/storage">Penyimpanan Objek Yandex</a> . <br><br>  Poin penting lainnya, penting bagi pengguna dan kami, adalah kecepatan operasi.  Kami baru-baru ini membuat perubahan besar pada infrastruktur Penyebaran dan mengurangi waktu pelaksanaan hampir semua operasi menjadi beberapa detik.  Tidak tercakup hanya operasi membuat cluster dan menambahkan host ke cluster.  Waktu pelaksanaan operasi kedua tergantung pada jumlah data.  Tetapi yang pertama kami akan mempercepat dalam waktu dekat, karena pengguna sering ingin membuat dan menghapus cluster di pipa CI / CD mereka. <br><br>  Daftar kasus penting kami mencakup penambahan fungsi yang secara otomatis meningkatkan ukuran disk.  Sekarang ini dilakukan secara manual, yang sangat tidak nyaman dan tidak terlalu baik. <br><br>  Akhirnya, kami menawarkan kepada pengguna sejumlah besar grafik yang menunjukkan apa yang terjadi dengan basis data.  Kami memberikan akses ke log.  Pada saat yang sama, kami melihat bahwa data terkadang tidak mencukupi.  Perlu grafis lain, irisan lain.  Di sini kami juga merencanakan peningkatan. <br><br>  Kisah kami tentang layanan basis data yang dikelola ternyata lama dan mungkin cukup membosankan.  Lebih baik daripada kata-kata dan deskripsi, hanya latihan nyata.  Karena itu, jika Anda mau, Anda dapat mengevaluasi secara independen kemampuan layanan kami: <br><br><ul><li>  <a href="https://cloud.yandex.ru/services/managed-postgresql">Layanan Dikelola Yandex untuk PostgreSQL</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-mysql">Layanan Terkelola Yandex untuk MySQL</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-mongodb">Layanan Terkelola Yandex untuk MongoDB</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-clickhouse">Layanan Terkelola Yandex untuk ClickHouse</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-redis">Layanan Dikelola Yandex untuk Redis</a> </li><li>  <a href="https://cloud.yandex.ru/services/data-proc">Proc Data Yandex</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id477860/">https://habr.com/ru/post/id477860/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id477848/index.html">Rahasia kecil hati besar: kardiogram paus biru pertama dalam sejarah</a></li>
<li><a href="../id477850/index.html">Bereaksi Asli - peluru perak untuk semua masalah? Bagaimana kami memilih alat lintas platform untuk Profi.ru</a></li>
<li><a href="../id477852/index.html">Kemunafikan tidak beracun</a></li>
<li><a href="../id477856/index.html">Akselerator flash PCI-E dari 800GB hingga 6.4TB: dari awal hingga akhir di PC / server biasa</a></li>
<li><a href="../id477858/index.html">Pekerjaan di luar meja: proyek apa yang benar-benar terungkap setelah pra-akselerasi?</a></li>
<li><a href="../id477862/index.html">Jadi itu mungkin? Sains dan TI dalam satu konferensi</a></li>
<li><a href="../id477864/index.html">TabPy untuk bekerja dengan data di ClickHouse dari Tableau</a></li>
<li><a href="../id477866/index.html">Seminar: Solusi IT hibrid untuk bisnis. 5 Desember, St. Petersburg</a></li>
<li><a href="../id477870/index.html">Dasbor Grafana untuk sistem bir BeerTender</a></li>
<li><a href="../id477872/index.html">c.tech: Data Sense # 4 rilis Tahun Baru</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>