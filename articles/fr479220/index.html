<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêß üè¥ üêÄ Nano-neurone - 7 fonctions JavaScript simples montrant comment la machine peut "apprendre" ü§æ ü¶å üìâ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Un nano-neurone est une version simplifi√©e d'un neurone du concept d'un r√©seau neuronal. Le nano-neurone effectue la t√¢che la plus simple et est form√©...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nano-neurone - 7 fonctions JavaScript simples montrant comment la machine peut "apprendre"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/479220/"><p>  <a href="https://github.com/trekhleb/nano-neuron" rel="nofollow"><strong>Un nano-neurone</strong></a> est une version <em>simplifi√©e</em> d'un neurone du concept d'un r√©seau neuronal.  Le nano-neurone effectue la t√¢che la plus simple et est form√© pour convertir la temp√©rature de degr√©s Celsius en degr√©s Fahrenheit. </p><br><p>  Le code <a href="" rel="nofollow"><strong>NanoNeuron.js se</strong></a> compose de 7 fonctions JavaScript simples impliquant l'apprentissage, la formation, la pr√©vision et la propagation directe et en arri√®re du signal du mod√®le.  Le but de l'√©criture de ces fonctions √©tait de donner au lecteur une explication minimale (intuition) de la fa√ßon dont, apr√®s tout, une machine peut ¬´apprendre¬ª.  Le code n'utilise pas de biblioth√®ques tierces.  Comme le dit le proverbe, seules les fonctions JavaScript "vanille" simples. </p><br><p>  Ces fonctions ne sont en <strong>aucun cas</strong> un guide exhaustif de l'apprentissage automatique.  De nombreux concepts d'apprentissage automatique sont manquants ou simplifi√©s!  Cette simplification est autoris√©e dans le seul but - donner au lecteur la compr√©hension et l'intuition les plus <strong>√©l√©mentaires de</strong> la fa√ßon dont une machine peut, en principe, "apprendre", de sorte que, par cons√©quent, "MAGIE de l'apprentissage automatique" sonne de plus en plus pour le lecteur comme "MATH√âMATIQUES de l'apprentissage automatique". </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/98d/6c4/69e/98d6c469e1facbf97154fe29f698cd12.png" alt="Nanoneuron"></p><a name="habracut"></a><br><h2 id="chto-vyuchit-nash-nano-neyron">  Ce que notre nano-neurone ¬´apprendra¬ª </h2><br><p>  Vous avez peut-√™tre entendu parler des neurones dans le contexte des <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D1%2581%25D0%25BA%25D1%2583%25D1%2581%25D1%2581%25D1%2582%25D0%25B2%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">r√©seaux</a> de <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D1%2581%25D0%25BA%25D1%2583%25D1%2581%25D1%2581%25D1%2582%25D0%25B2%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">neurones</a> .  Un nano-neurone est une version simplifi√©e de ce m√™me neurone.  Dans cet exemple, nous allons √©crire son impl√©mentation √† partir de z√©ro.  Par souci de simplicit√©, nous ne construirons pas de r√©seau de nano-neurones.  Nous allons nous concentrer sur la cr√©ation d'un seul nano-neurone et essayer de lui apprendre √† convertir la temp√©rature de degr√©s Celsius en degr√©s Fahrenheit.  En d'autres termes, nous lui apprendrons √† <strong>pr√©dire la</strong> temp√©rature en degr√©s Fahrenheit en fonction de la temp√©rature en degr√©s Celsius. </p><br><p>  √Ä propos, la formule de conversion des degr√©s Celsius en degr√©s Fahrenheit est la suivante: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9fa/2e8/8b5/9fa2e88b5a7324c8b9fc359b274ba091.png" alt="Celsius √† fahrenheit"></p><br><p>  Mais pour le moment, notre nano-neurone ne sait rien de cette formule ... </p><br><h3 id="model-nano-neyrona">  Mod√®le de nano-neurone </h3><br><p> Commen√ßons par cr√©er une fonction qui d√©crit le mod√®le de notre nano-neurone.  Ce mod√®le est une simple relation lin√©aire entre <code>x</code> et <code>y</code> , qui ressemble √† ceci: <code>y = w * x + b</code> .  Autrement dit, notre nano-neurone est un enfant qui peut tracer une ligne droite dans le syst√®me de coordonn√©es <code>XY</code> . </p><br><p>  Les variables <code>w</code> et <code>b</code> sont des <strong>param√®tres de</strong> mod√®le.  Un nano-neurone ne conna√Æt que ces deux param√®tres d'une fonction lin√©aire.  Ces param√®tres sont pr√©cis√©ment ce que notre nano-neurone apprendra au cours du processus d'entra√Ænement. </p><br><p>  La seule chose qu'un nano-neurone peut faire √† ce stade est de simuler des relations lin√©aires.  Il le fait dans la m√©thode <code>predict()</code> , qui prend une variable <code>x</code> √† l'entr√©e et pr√©dit la variable <code>y</code> √† la sortie.  Pas de magie. </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">NanoNeuron</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">w, b</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.w = w; <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.b = b; <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.predict = <span class="hljs-function"><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">x</span></span></span><span class="hljs-function">) =&gt;</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x * <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.w + <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.b; } }</code> </pre> <br><p>  _ (... attendez ... <a href="https://en.wikipedia.org/wiki/Linear_regression" rel="nofollow">la r√©gression lin√©aire</a> c'est vous, ou quoi?) _ </p><br><h3 id="konvertaciya-gradusov-celsiya-v-gradusy-farengeyta">  Conversion degr√©s Celsius en degr√©s Fahrenheit </h3><br><p>  La temp√©rature en degr√©s Celsius peut √™tre convertie en degr√©s Fahrenheit selon la formule: <code>f = 1.8 * c + 32</code> , o√π <code>c</code> est la temp√©rature en degr√©s Celsius et <code>f</code> est la temp√©rature en degr√©s Fahrenheit. </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">celsiusToFahrenheit</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">c</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> w = <span class="hljs-number"><span class="hljs-number">1.8</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> b = <span class="hljs-number"><span class="hljs-number">32</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> f = c * w + b; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> f; };</code> </pre> <br><p>  En cons√©quence, nous voulons que notre nano-neurone puisse simuler cette fonction particuli√®re.  Il devra deviner (apprendre) que le param√®tre <code>w = 1.8</code> et <code>b = 32</code> sans le savoir √† l'avance. </p><br><p>  Voici √† quoi ressemble la fonction de conversion sur le graphique.  C'est ce que notre ¬´b√©b√©¬ª nano-neuronal doit apprendre √† ¬´dessiner¬ª: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/68b/0d6/8bc/68b0d68bcc7be00ec9526867b2fcecf3.png" alt="Conversion de Celsius en Fahrenheit"></p><br><h3 id="generirovanie-dannyh">  G√©n√©ration de donn√©es </h3><br><p>  En programmation classique, nous connaissons les donn√©es d'entr√©e ( <code>x</code> ) et l'algorithme de conversion de ces donn√©es (param√®tres <code>w</code> et <code>b</code> ), mais les donn√©es de sortie ( <code>y</code> ) sont inconnues.  La sortie est calcul√©e sur la base de l'entr√©e en utilisant un algorithme connu.  En apprentissage automatique, au contraire, seules les donn√©es d'entr√©e et de sortie ( <code>x</code> et <code>y</code> ) sont connues, mais l'algorithme de passage de <code>x</code> √† <code>y</code> inconnu (param√®tres <code>w</code> et <code>b</code> ). </p><br><p>  C'est la g√©n√©ration d'entr√©es et de sorties que nous allons maintenant faire.  Nous devons g√©n√©rer des donn√©es pour la <strong>formation de</strong> notre mod√®le et des donn√©es pour <strong>tester le</strong> mod√®le.  La fonction d'assistance <code>celsiusToFahrenheit()</code> nous y aidera.  Chacun des ensembles de donn√©es d'apprentissage et de test est un ensemble de paires <code>x</code> et <code>y</code> .  Par exemple, si <code>x = 2</code> , alors <code>y = 35,6</code> et ainsi de suite. </p><br><blockquote>  Dans le monde r√©el, la plupart des donn√©es sont susceptibles d'√™tre <em>collect√©es</em> et non <em>g√©n√©r√©es</em> .  Par exemple, ces donn√©es collect√©es peuvent √™tre un ensemble de paires de ¬´photos de visage¬ª -&gt; ¬´nom de la personne¬ª. </blockquote><p>  Nous utiliserons l'ensemble de donn√©es TRAINING pour entra√Æner notre nano-neurone.  Avant qu'il ne grandisse et soit capable de prendre des d√©cisions par lui-m√™me, nous devons lui apprendre ce qui est ¬´vrai¬ª et ce qui est ¬´faux¬ª en utilisant des donn√©es ¬´correctes¬ª d'un ensemble d'entra√Ænement. </p><br><blockquote>  Soit dit en passant, le principe de vie ¬´ordures √† l'entr√©e - ordures √† la sortie¬ª est clairement trac√©.  Si un nano-neurone jette un "mensonge" dans le kit de formation que 5 ¬∞ C est converti en 1000 ¬∞ F, puis apr√®s de nombreuses it√©rations de formation, il le croira et convertira correctement toutes les valeurs de temp√©rature <strong>sauf</strong> 5 ¬∞ C.  Nous devons √™tre tr√®s prudents avec les donn√©es d'entra√Ænement que nous chargeons quotidiennement dans notre r√©seau neuronal c√©r√©bral. </blockquote><p>  Distrait.  Continuons. </p><br><p>  Nous utiliserons l'ensemble de donn√©es TEST pour √©valuer dans quelle mesure notre nano-neurone s'est entra√Æn√© et peut faire des pr√©dictions correctes sur de nouvelles donn√©es qu'il n'a pas vues pendant sa formation. </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generateDataSets</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>) </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// xTrain -&gt; [0, 1, 2, ...], // yTrain -&gt; [32, 33.8, 35.6, ...] const xTrain = []; const yTrain = []; for (let x = 0; x &lt; 100; x += 1) { const y = celsiusToFahrenheit(x); xTrain.push(x); yTrain.push(y); } // xTest -&gt; [0.5, 1.5, 2.5, ...] // yTest -&gt; [32.9, 34.7, 36.5, ...] const xTest = []; const yTest = []; //   0.5    1,       //   ,       . for (let x = 0.5; x &lt; 100; x += 1) { const y = celsiusToFahrenheit(x); xTest.push(x); yTest.push(y); } return [xTrain, yTrain, xTest, yTest]; }</span></span></code> </pre> <br><h3 id="ocenka-pogreshnosti-predskazaniy">  Estimation d'erreur de pr√©diction </h3><br><p>  Nous avons besoin d'une certaine m√©trique (mesure, nombre, √©valuation) qui montrera √† quel point la pr√©diction d'un nano-neurone est vraie.  En d'autres termes, ce nombre / m√©trique / fonction devrait montrer √† quel point le nano neurone est correct ou non.  C'est comme √† l'√©cole, un √©l√®ve peut obtenir une note de <code>5</code> ou <code>2</code> pour son contr√¥le. </p><br><p>  Dans le cas d'un nano-neurone, son erreur (erreur) entre la vraie valeur de <code>y</code> et la valeur pr√©dite de <code>prediction</code> sera produite par la formule: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/8d8/e50/ac1/8d8e50ac12d03614e65975f7b5d36931.png" alt="Co√ªt de pr√©diction"></p><br><p>  Comme le montre la formule, nous consid√©rerons l'erreur comme une simple diff√©rence entre les deux valeurs.  Plus les valeurs sont proches les unes des autres, plus la diff√©rence est faible.  Nous utilisons ici la quadrature pour se d√©barrasser du signe, de sorte qu'√† la fin <code>(1 - 2) ^ 2</code> √©quivalent √† <code>(2 - 1) ^ 2</code> .  La division par <code>2</code> se produit uniquement dans le but de simplifier la signification de la d√©riv√©e de cette fonction dans la formule de r√©tro-propagation d'un signal (voir ci-dessous). </p><br><p>  La fonction d'erreur dans ce cas ressemblera √† ceci: </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predictionCost</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">y, prediction</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (y - prediction) ** <span class="hljs-number"><span class="hljs-number">2</span></span> / <span class="hljs-number"><span class="hljs-number">2</span></span>; <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 235.6 }</span></span></code> </pre> <br><h3 id="pryamoe-rasprostranenie-signala">  Propagation directe du signal </h3><br><p>  La propagation directe du signal √† travers notre mod√®le signifie faire des pr√©dictions pour toutes les paires √† partir du jeu de donn√©es d'apprentissage <code>xTrain</code> et <code>yTrain</code> et calculer l'erreur moyenne (erreur) de ces pr√©dictions. </p><br><p>  Nous laissons simplement notre nano-neurone ¬´s'exprimer¬ª, lui permettant de faire des pr√©dictions (convertir la temp√©rature).  En m√™me temps, un nano-neurone √† ce stade peut √™tre tr√®s mauvais.  La valeur moyenne de l'erreur de pr√©diction nous montrera √† quel point notre mod√®le est / est proche de la v√©rit√© en ce moment.  La valeur d'erreur est ici tr√®s importante, car en modifiant √† nouveau les param√®tres <code>w</code> et <code>b</code> et en propageant √† nouveau la propagation directe du signal, nous pouvons √©valuer si notre nano-neurone est devenu ¬´plus intelligent¬ª avec de nouveaux param√®tres ou non. </p><br><p>  L'erreur de pr√©diction moyenne d'un nano-neurone sera effectu√©e en utilisant la formule suivante: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/575/db3/e0a/575db3e0a0c872b29582147e41231344.png" alt="Co√ªt moyen"></p><br><p>  O√π <code>m</code> est le nombre de copies d'apprentissage (dans notre cas, nous avons <code>100</code> paires de donn√©es). </p><br><p>  Voici comment nous pouvons impl√©menter cela dans le code: </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forwardPropagation</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">model, xTrain, yTrain</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> m = xTrain.length; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> predictions = []; <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> cost = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">let</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; m; i += <span class="hljs-number"><span class="hljs-number">1</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> prediction = nanoNeuron.predict(xTrain[i]); cost += predictionCost(yTrain[i], prediction); predictions.push(prediction); } <span class="hljs-comment"><span class="hljs-comment">//     . cost /= m; return [predictions, cost]; }</span></span></code> </pre> <br><h3 id="obratnoe-rasprostranenie-signala">  Propagation inverse du signal </h3><br><p>  Maintenant que nous savons comment notre nano-neurone a raison ou tort dans ses pr√©dictions (bas√©es sur la valeur moyenne de l'erreur), comment pouvons-nous rendre les pr√©dictions plus pr√©cises? </p><br><p>  La propagation inverse du signal nous y aidera.  La propagation du signal de retour est le processus d'√©valuation de l'erreur d'un nano-neurone, puis d'ajustement de ses param√®tres <code>w</code> et <code>b</code> afin que les prochaines pr√©dictions du nano-neurone pour l'ensemble des donn√©es d'apprentissage deviennent un peu plus pr√©cises. </p><br><p>  C'est l√† que l'apprentissage automatique devient comme de la magie.  Le concept cl√© ici est un <strong>d√©riv√© de la fonction</strong> , qui montre quel pas de taille et quelle voie nous devons prendre afin d'approcher le minimum de la fonction (dans notre cas, le minimum de la fonction d'erreur). </p><br><p>  Le but ultime de la formation d'un nano-neurone est de trouver le minimum de la fonction d'erreur (voir fonction ci-dessus).  Si nous pouvons trouver de telles valeurs de <code>w</code> et <code>b</code> pour lesquelles la valeur moyenne de la fonction d'erreur est petite, cela signifiera que notre nano-neurone s'adapte bien aux pr√©visions de temp√©rature en degr√©s Fahrenheit. </p><br><p>  Les d√©riv√©s sont un sujet important et distinct que nous ne traiterons pas dans cet article.  <a href="https://www.mathsisfun.com/calculus/derivatives-introduction.html" rel="nofollow">MathIsFun</a> est une excellente ressource qui peut fournir une compr√©hension de base des d√©riv√©s. </p><br><p>  Une chose que nous devons apprendre de l'essence de la d√©riv√©e et qui nous aidera √† comprendre comment fonctionne la r√©tropropagation du signal est que la d√©riv√©e d'une fonction √† un point sp√©cifique <code>x</code> et <code>y</code> , par d√©finition, est une tangente √† la courbe de cette fonction √† <code>x</code> et <code>y</code> et <em>nous indique la direction au minimum de la fonction</em> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/66d/bfd/49a/66dbfd49aaf1ced48d7f6b5917fddb12.svg" alt="Pente d√©riv√©e"></p><br><p>  <em>Image prise √† partir de <a href="https://www.mathsisfun.com/calculus/derivatives-introduction.html" rel="nofollow">MathIsFun</a></em> </p><br><p>  Par exemple, dans le graphique ci-dessus, vous voyez qu'au point <code>(x=2, y=4)</code> pente de la tangente nous montre que nous devons nous d√©placer vers la <code></code> et <code></code> pour atteindre le minimum de la fonction.  Notez √©galement que plus la pente de la tangente est grande, plus nous devons nous d√©placer rapidement vers le point minimum. </p><br><p>  Les d√©riv√©es de notre fonction d'erreur moyenne <code>averageCost</code> par <code>averageCost</code> aux param√®tres <code>w</code> et <code>b</code> ressembleront √† ceci: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/4cc/bda/ba1/4ccbdaba120c399c1528e2bc38cf0efd.png" alt="dW"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e02/0cb/125/e020cb125449849009a9f565a32ff46f.png" alt="dB"></p><br><p>  O√π <code>m</code> est le nombre de copies d'apprentissage (dans notre cas, nous avons <code>100</code> paires de donn√©es). </p><br><p>  <em>Vous pouvez lire plus en d√©tail sur la fa√ßon de prendre la d√©riv√©e de fonctions complexes <a href="https://www.mathsisfun.com/calculus/derivatives-rules.html" rel="nofollow">ici</a> .</em> </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backwardPropagation</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">predictions, xTrain, yTrain</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> m = xTrain.length; <span class="hljs-comment"><span class="hljs-comment">//           'w'  'b'. //      0. let dW = 0; let dB = 0; for (let i = 0; i &lt; m; i += 1) { dW += (yTrain[i] - predictions[i]) * xTrain[i]; dB += yTrain[i] - predictions[i]; } //    . dW /= m; dB /= m; return [dW, dB]; }</span></span></code> </pre> <br><h3 id="trenirovka-modeli">  Formation mod√®le </h3><br><p>  Nous savons maintenant comment estimer l'erreur / erreur des pr√©dictions de notre mod√®le de nano-neurone pour toutes les donn√©es d'entra√Ænement (propagation directe du signal).  Nous savons √©galement ajuster les param√®tres <code>w</code> et <code>b</code> mod√®le des nano-neurones (r√©tropropagation du signal) afin d'am√©liorer la pr√©cision des pr√©dictions.  Le probl√®me est que si nous effectuons une propagation avant et arri√®re du signal une seule fois, cela ne suffira pas pour que notre mod√®le identifie et apprenne les d√©pendances et les lois dans les donn√©es d'apprentissage.  Vous pouvez comparer cela √† la visite d'une journ√©e d'un √©l√®ve √† l'√©cole.  Il / elle doit aller √† l'√©cole r√©guli√®rement, jour apr√®s jour, ann√©e apr√®s ann√©e, afin d'apprendre tout le mat√©riel. </p><br><p>  Il faut donc <em>r√©p√©ter</em> plusieurs fois la propagation avant et arri√®re du signal.  C'est <code>trainModel()</code> .  Elle est comme une ¬´enseignante¬ª pour le mod√®le de notre nano-neurone: </p><br><ul><li>  elle passera un certain temps ( <code>epochs</code> ) avec notre nano-neurone encore idiot, essayant de le former, </li><li>  elle utilisera des livres sp√©ciaux (jeux de donn√©es <code>xTrain</code> et <code>yTrain</code> ) pour la formation, </li><li>  il encourage notre ¬´√©tudiant¬ª √† √©tudier plus diligemment (plus rapidement) en utilisant le param√®tre <code>alpha</code> , qui contr√¥le essentiellement la vitesse d'apprentissage. </li></ul><br><p>  Quelques mots sur le param√®tre <code>alpha</code> .  Il s'agit simplement d'un coefficient (multiplicateur) pour les valeurs des variables <code>dW</code> et <code>dB</code> , que nous calculons lors de la <code>dW</code> du signal.  Ainsi, la d√©riv√©e nous a montr√© la direction vers le minimum de la fonction d'erreur (les signes des valeurs de <code>dW</code> et <code>dB</code> nous le disent).  La d√©riv√©e nous a √©galement montr√© √† quelle vitesse nous devons nous d√©placer vers le minimum de la fonction (les valeurs absolues de <code>dW</code> et <code>dB</code> nous le disent).  Maintenant, nous devons multiplier la taille du pas par <code>alpha</code> afin d'ajuster la vitesse de notre approche au minimum (la taille totale du pas).  Parfois, si nous utilisons de grandes valeurs pour <code>alpha</code> , nous pouvons aller dans des √©tapes si grandes que nous pouvons simplement <em>d√©passer le</em> minimum de la fonction, la sautant ainsi. </p><br><p>  Par analogie avec le ¬´professeur¬ª, plus elle forcerait notre ¬´nano-√©tudiant¬ª √† apprendre, plus vite il apprendrait, MAIS, si vous le forcez et exercez une pression tr√®s forte sur lui, alors notre ¬´nano-√©tudiant¬ª peut subir une d√©pression nerveuse et apathie compl√®te et il n‚Äôapprendra rien du tout. </p><br><p>  Nous mettrons √† jour les param√®tres de notre mod√®le <code>w</code> et <code>b</code> comme suit: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c7b/db8/84f/c7bdb884f2a940d62332246cdbcb44bc.png" alt="w"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b57/622/0ab/b576220ab6515d44255ef56699077bab.png" alt="b"></p><br><p>  Et voici √† quoi ressemble la formation elle-m√™me: </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">trainModel</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">{model, epochs, alpha, xTrain, yTrain}</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-comment"><span class="hljs-comment">//     -.  . const costHistory = []; //    ()  for (let epoch = 0; epoch &lt; epochs; epoch += 1) { //   . const [predictions, cost] = forwardPropagation(model, xTrain, yTrain); costHistory.push(cost); //   . const [dW, dB] = backwardPropagation(predictions, xTrain, yTrain); //    -,    . nanoNeuron.w += alpha * dW; nanoNeuron.b += alpha * dB; } return costHistory; }</span></span></code> </pre> <br><h3 id="soberem-vse-funkcii-vmeste">  Assembler toutes les fonctionnalit√©s </h3><br><p>  Il est temps d'utiliser ensemble toutes les fonctions pr√©c√©demment cr√©√©es. </p><br><p>  Cr√©ez une instance du mod√®le de nano-neurone.  Pour le moment, le nano-neurone ne sait rien des param√®tres <code>w</code> et <code>b</code> .  D√©finissons donc <code>w</code> et <code>b</code> au hasard. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> w = <span class="hljs-built_in"><span class="hljs-built_in">Math</span></span>.random(); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 0.9492 const b = Math.random(); // ie -&gt; 0.4570 const nanoNeuron = new NanoNeuron(w, b);</span></span></code> </pre> <br><p>  Nous g√©n√©rons des ensembles de donn√©es de formation et de test. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [xTrain, yTrain, xTest, yTest] = generateDataSets();</code> </pre> <br><p>  Essayons maintenant de former notre mod√®le en utilisant de petites √©tapes ( <code>0.0005</code> ) pour <code>70000</code> √©poques.  Vous pouvez exp√©rimenter ces param√®tres, ils sont d√©termin√©s empiriquement. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> epochs = <span class="hljs-number"><span class="hljs-number">70000</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> alpha = <span class="hljs-number"><span class="hljs-number">0.0005</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> trainingCostHistory = trainModel({<span class="hljs-attr"><span class="hljs-attr">model</span></span>: nanoNeuron, epochs, alpha, xTrain, yTrain});</code> </pre> <br><p>  V√©rifions comment la valeur d'erreur de notre mod√®le a chang√© pendant la formation.  Nous nous attendons √† ce que la valeur d'erreur apr√®s la formation soit nettement inf√©rieure √† celle avant la formation.  Cela signifierait que notre nano-neurone serait plus sage.  L'option inverse est √©galement possible, lorsque apr√®s l'entra√Ænement, l'erreur des pr√©dictions n'a fait qu'augmenter (par exemple, les grandes valeurs de l'√©tape d'apprentissage <code>alpha</code> ). </p><br><pre> <code class="javascript hljs"><span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">'  :'</span></span>, trainingCostHistory[<span class="hljs-number"><span class="hljs-number">0</span></span>]); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 4694.3335043 console.log('  :', trainingCostHistory[epochs - 1]); // ie -&gt; 0.0000024</span></span></code> </pre> <br><p>  Et voici comment la valeur de l'erreur de mod√®le a chang√© pendant la formation.  Sur l'axe des <code>x</code> trouvent les √©poques (en milliers).  Nous pr√©voyons que le graphique diminuera. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/191/860/d6f/191860d6f0cd8cf7d24127f04f779462.png" alt="Processus de formation"></p><br><p>  Voyons quels param√®tres notre nano-neurone a ¬´appris¬ª.  Nous nous attendons √† ce que les param√®tres <code>w</code> et <code>b</code> soient similaires aux param√®tres du m√™me nom de la fonction <code>celsiusToFahrenheit()</code> ( <code>w = 1.8</code> et <code>b = 32</code> ), car c'est son nano-neurone que j'ai essay√© de simuler. </p><br><pre> <code class="javascript hljs"><span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">' -:'</span></span>, {<span class="hljs-attr"><span class="hljs-attr">w</span></span>: nanoNeuron.w, <span class="hljs-attr"><span class="hljs-attr">b</span></span>: nanoNeuron.b}); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; {w: 1.8, b: 31.99}</span></span></code> </pre> <br><p>  Comme vous pouvez le voir, le nano-neurone est tr√®s proche de la fonction <code>celsiusToFahrenheit()</code> . </p><br><p>  Voyons maintenant √† quel point les pr√©dictions de notre nano-neurone sont pr√©cises pour les donn√©es de test qu'il n'a pas vues pendant l'entra√Ænement.  L'erreur de pr√©diction pour les donn√©es de test doit √™tre proche de l'erreur de pr√©diction pour les donn√©es d'apprentissage.  Cela signifiera que le nano-neurone a appris les d√©pendances correctes et peut correctement extraire son exp√©rience de donn√©es inconnues auparavant (c'est toute la valeur du mod√®le). </p><br><pre> <code class="javascript hljs">[testPredictions, testCost] = forwardPropagation(nanoNeuron, xTest, yTest); <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">'   :'</span></span>, testCost); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 0.0000023</span></span></code> </pre> <br><p>  Maintenant, puisque notre ¬´nano-b√©b√©¬ª √©tait bien form√© √† ¬´l'√©cole¬ª et sait maintenant convertir avec pr√©cision des degr√©s Celsius en degr√©s Fahrenheit m√™me pour des donn√©es qu'il n'a pas vues, nous pouvons l'appeler assez ¬´intelligent¬ª.  Maintenant, nous pouvons m√™me lui demander des conseils sur la conversion de temp√©rature, et c'√©tait le but de toute la formation. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> tempInCelsius = <span class="hljs-number"><span class="hljs-number">70</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> customPrediction = nanoNeuron.predict(tempInCelsius); <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">`- "",  </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${tempInCelsius}</span></span></span><span class="hljs-string">¬∞C   :`</span></span>, customPrediction); <span class="hljs-comment"><span class="hljs-comment">// -&gt; 158.0002 console.log('  :', celsiusToFahrenheit(tempInCelsius)); // -&gt; 158</span></span></code> </pre> <br><p>  Tr√®s proche!  Comme les gens, notre nano-neurone est bon, mais pas parfait :) </p><br><p>  Codage r√©ussi! </p><br><h2 id="kak-zapustit-i-protestirovat-nano-neyron">  Comment ex√©cuter et tester un nano-neurone </h2><br><p>  Vous pouvez cloner le r√©f√©rentiel et ex√©cuter le nano neurone localement: </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/trekhleb/nano-neuron.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> nano-neuron</code> </pre> <br><pre> <code class="bash hljs">node ./NanoNeuron.js</code> </pre> <br><h2 id="upuschennye-koncepcii">  Concepts manqu√©s </h2><br><p>  Les concepts d'apprentissage automatique suivants ont √©t√© omis ou simplifi√©s pour faciliter l'explication. </p><br><p>  <strong>S√©paration des ensembles de donn√©es de formation et de test</strong> </p><br><p>  Habituellement, vous avez un ensemble de donn√©es volumineuses.  Selon le nombre d'exemplaires de cet ensemble, sa division en ensembles d'apprentissage et de test peut √™tre effectu√©e dans la proportion de 70/30.  Les donn√©es de l'ensemble doivent √™tre m√©lang√©es au hasard avant d'√™tre divis√©es.  Si la quantit√© de donn√©es est importante (par exemple, des millions), la division en ensembles de test et de formation peut √™tre effectu√©e dans des proportions proches de 90/10 ou 95/5. </p><br><p>  <strong>Puissance en ligne</strong> </p><br><p>  Habituellement, vous ne trouverez pas de cas o√π un seul neurone est utilis√©.  La force r√©side dans le <a href="https://en.wikipedia.org/wiki/Neural_network" rel="nofollow">r√©seau de</a> ces neurones.  Un r√©seau de neurones peut apprendre des d√©pendances beaucoup plus complexes. </p><br><p>  Toujours dans l'exemple ci-dessus, notre nano-neurone peut ressembler davantage √† une simple <a href="https://en.wikipedia.org/wiki/Linear_regression" rel="nofollow">r√©gression lin√©aire</a> qu'√† un r√©seau neuronal. </p><br><p>  <strong>Normalisation d'entr√©e</strong> </p><br><p>  Avant l'entra√Ænement, il est d'usage de <a href="https://www.jeremyjordan.me/batch-normalization/" rel="nofollow">normaliser les donn√©es d'entr√©e</a> . </p><br><p>  <strong>Impl√©mentation vectorielle</strong> </p><br><p>  Pour les r√©seaux de neurones, les calculs vectoriels (matriciels) sont beaucoup plus rapides que les calculs <code>for</code> boucles.  Habituellement, la propagation directe et inverse du signal est effectu√©e √† l'aide d'op√©rations matricielles utilisant, par exemple, la <a href="https://numpy.org/" rel="nofollow">biblioth√®que</a> Python <a href="https://numpy.org/" rel="nofollow">Numpy</a> . </p><br><p>  <strong>Fonction d'erreur minimale</strong> </p><br><p>  La fonction d'erreur que nous avons utilis√©e pour le nano neurone est tr√®s simplifi√©e.  Il doit contenir des <a href="https://stackoverflow.com/questions/32986123/why-the-cost-function-of-logistic-regression-has-a-logarithmic-expression/32998675" rel="nofollow">composants logarithmiques</a> .  Un changement dans la formule de la fonction d'erreur entra√Ænera √©galement un changement dans les formules de propagation avant et arri√®re du signal. </p><br><p>  <strong>Fonction d'activation</strong> </p><br><p>  Habituellement, la valeur de sortie du neurone passe par la fonction d'activation.  Pour l'activation, des fonctions telles que <a href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="nofollow">Sigmoid</a> , <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="nofollow">ReLU</a> et autres peuvent √™tre utilis√©es. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr479220/">https://habr.com/ru/post/fr479220/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr479202/index.html">C ++ et m√©thodes num√©riques: int√©gration approximative de Newton-Cotes</a></li>
<li><a href="../fr479210/index.html">Qu'adviendra-t-il des achats dans les magasins en ligne √©trangers √† partir du 1er janvier 2020</a></li>
<li><a href="../fr479214/index.html">Une s√©lection d'√©v√©nements gratuits √† venir pour les d√©veloppeurs √† Moscou # 2</a></li>
<li><a href="../fr479216/index.html">Pandora DXL 3000 Second Wind ou comment j'ai viss√© ma propre t√©l√©m√©trie</a></li>
<li><a href="../fr479218/index.html">Comment faire un bot qui transforme une photo en bande dessin√©e: instructions √©tape par √©tape pour les nuls</a></li>
<li><a href="../fr479222/index.html">Le condens√© de mat√©riaux int√©ressants pour le d√©veloppeur mobile # 325 (du 2 au 8 d√©cembre)</a></li>
<li><a href="../fr479226/index.html">Habr-analysis: ce que les utilisateurs commandent en cadeau √† Habr</a></li>
<li><a href="../fr479230/index.html">Documentez votre API express avec des annotations fanfaronnades</a></li>
<li><a href="../fr479232/index.html">D√©veloppement d'applications MQ JMS sur Spring Boot</a></li>
<li><a href="../fr479234/index.html">Nouvelles du monde d'OpenStreetMap n ¬∞ 488 (19/11/2019 - 25/11/2019)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>