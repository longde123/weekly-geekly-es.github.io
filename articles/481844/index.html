<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äç‚öïÔ∏è ‚ÜïÔ∏è üìà 7 a√±os de exageraci√≥n de redes neuronales en gr√°ficos y perspectivas inspiradoras de Deep Learning 2020 ü§òüèº üôãüèº üòè</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Se acerca el A√±o Nuevo, los a√±os 2010 pronto terminar√°n, dando al mundo el renacimiento sensacional de las redes neuronales. Me preocupaba y me privab...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>7 a√±os de exageraci√≥n de redes neuronales en gr√°ficos y perspectivas inspiradoras de Deep Learning 2020</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/481844/"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/0a2/971/f4c0a297160b156ef22379a9555bd5fd.png"><br><br>  Se acerca el A√±o Nuevo, los a√±os 2010 pronto terminar√°n, dando al mundo el renacimiento sensacional de las redes neuronales.  Me preocupaba <s>y me privaba del sue√±o un</s> simple pensamiento: "¬øC√≥mo se puede estimar retrospectivamente la velocidad de desarrollo de las redes neuronales?" Para "El que conoce el pasado conoce el futuro".  ¬øQu√© tan r√°pido despegaron los diferentes algoritmos?  ¬øC√≥mo se puede evaluar la velocidad del progreso en esta √°rea y estimar la velocidad del progreso en la pr√≥xima d√©cada? <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/cad/e4a/11b/cade4a11b5be7a57182eddbf3765ba4c.png"><br><br>  Est√° claro que puede calcular aproximadamente el n√∫mero de art√≠culos en diferentes √°reas.  El m√©todo no es ideal, debe tener en cuenta los subdominios, pero en general puede intentarlo.  Les doy una idea, en <a href="https://scholar.google.com/scholar%3Fhl%3Den%26as_sdt%3D0%252C5%26q%3Dbatch%2Bnormalization" rel="nofollow">Google Scholar (BatchNorm)</a> es bastante real.  Puede considerar nuevos conjuntos de datos, puede nuevos cursos.  Su humilde servidor, despu√©s de haber seleccionado varias opciones, se decidi√≥ por <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">Google Trends (BatchNorm)</a> . <br><br>  Mis colegas y yo recibimos solicitudes de las principales tecnolog√≠as de ML / DL, por ejemplo, <a href="https://en.wikipedia.org/wiki/Batch_normalization" rel="nofollow">Normalizaci√≥n de lotes</a> , como en la imagen de arriba, agregamos la fecha de publicaci√≥n del art√≠culo con un punto y obtuvimos una l√≠nea de tiempo para despegar la popularidad del tema.  Pero no para todos, el <s>camino est√° lleno de rosas, el</s> despegue es tan obvio y hermoso, como el batnorm.  Algunos t√©rminos, como la regularizaci√≥n o la omisi√≥n de conexiones, no se pudieron crear debido al ruido de los datos.  Pero en general, logramos recolectar tendencias. <br><br>  ¬øA qui√©n le importa lo que sucedi√≥? ¬°Bienvenido al corte! <br><a name="habracut"></a><br><h1>  En lugar de presentar o sobre el reconocimiento de im√°genes </h1><br>  Entonces!  Los datos iniciales eran bastante ruidosos, a veces hab√≠a picos agudos. <img src="https://habrastorage.org/webt/wn/ej/-p/wnej-pvivjixvw84l9sibvriuno.png"><br>  <i>Fuente: <a href="https://twitter.com/karpathy/status/849338608297406465" rel="nofollow">Andrei Karpaty twitter: los estudiantes se paran en los pasillos de una gran audiencia para escuchar una conferencia sobre redes neuronales convolucionales</a></i> <br><br>  Convencionalmente, fue suficiente para <a href="https://en.wikipedia.org/wiki/Andrej_Karpathy" rel="nofollow">Andrey Karpaty</a> dar una conferencia sobre el legendario <a href="http://cs231n.stanford.edu/" rel="nofollow">CS231n: Redes neuronales convolucionales para el reconocimiento visual</a> para 750 personas con la popularizaci√≥n del concepto de c√≥mo va un pico agudo.  Por lo tanto, los datos se suavizaron con un <a href="http://nghiaho.com/%3Fp%3D1159" rel="nofollow">filtro de caja</a> simple (todos los suavizados se marcan como suavizados en el eje).  Como est√°bamos interesados ‚Äã‚Äãen comparar la tasa de crecimiento de la popularidad, despu√©s de suavizar, todos los datos se normalizaron.  Result√≥ bastante gracioso.  Aqu√≠ hay un gr√°fico de las principales arquitecturas que compiten en ImageNet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0fa/4d6/f7b/0fa4d6f7bad7b232ec50e2f0bde6559b.png"><br>  <i>Fuente: en adelante, los c√°lculos del autor seg√∫n Google Trends</i> <br><br>  El gr√°fico muestra muy claramente que despu√©s de la sensacional publicaci√≥n de <a href="https://en.wikipedia.org/wiki/AlexNet" rel="nofollow">AlexNet</a> , que <a href="https://en.wikipedia.org/wiki/AlexNet" rel="nofollow">gener√≥</a> la papilla de la actual exageraci√≥n de las redes neuronales a fines de 2012, durante casi dos a√±os fue hirviente, <s>contrariamente a las afirmaciones del mont√≥n,</s> solo se <s>uni√≥</s> un c√≠rculo relativamente estrecho de especialistas.  El tema fue al p√∫blico en general solo en el invierno de 2014-2015.  Preste atenci√≥n a la periodicidad del calendario a partir de 2017: m√°s picos cada primavera.  <s>En psiquiatr√≠a, esto se llama exacerbaci√≥n de primavera ...</s> Esta es una se√±al segura de que ahora el t√©rmino es utilizado principalmente por los estudiantes y, en promedio, el inter√©s en AlexNet disminuye en comparaci√≥n con el pico de popularidad. <br><br>  Adem√°s, en la segunda mitad de 2014, apareci√≥ <a href="https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c" rel="nofollow">VGG</a> .  Por cierto, <a href="" rel="nofollow">VGG</a> fue coautor con la supervisora ‚Äã‚Äãde <a href="" rel="nofollow">estudios de</a> mi ex alumna <a href="https://scholar.google.com/citations%3Fuser%3DL7lMQkQAAAAJ%26hl%3Den" rel="nofollow">Karen Simonyan</a> , que ahora trabaja en Google DeepMind ( <a href="https://en.wikipedia.org/wiki/AlphaGo" rel="nofollow">AlphaGo</a> , <a href="https://en.wikipedia.org/wiki/AlphaZero" rel="nofollow">AlphaZero</a> , etc.).  Mientras estudiaba en la Universidad Estatal de Mosc√∫ en el tercer a√±o, Karen implement√≥ un buen <a href="https://www.compression.ru/video/motion_estimation/index_en.html" rel="nofollow">algoritmo de estimaci√≥n de movimiento</a> , que ha servido como referencia para estudiantes de 2 a√±os durante 12 a√±os.  Adem√°s, las tareas all√≠ son algo esquivamente similares.  Compara: <br><br><img width="50%" src="https://habrastorage.org/webt/mo/w0/9w/mow09w8lyvaxyw3b1ztxm2wxtxe.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/01a/c45/c94/01ac45c944275e9045557c9d453ff938.png"><br>  <i>Fuente: Funci√≥n de p√©rdida para tareas de Estimaci√≥n de movimiento (materiales de autor) y <a href="https://arxiv.org/abs/1712.09913" rel="nofollow">VGG-56</a></i> <br><br>  A la izquierda necesita encontrar el punto m√°s profundo en una superficie no trivial dependiendo de los datos de entrada para el n√∫mero m√≠nimo de mediciones (son posibles muchos m√≠nimos locales), y a la derecha necesita encontrar un punto m√°s bajo con c√°lculos m√≠nimos (y tambi√©n un mont√≥n de m√≠nimos locales, y la superficie tambi√©n depende de los datos) .  A la izquierda, obtenemos el vector de movimiento predicho, y a la derecha, la red entrenada.  Y la diferencia es que a la izquierda solo hay una medida impl√≠cita del espacio de color, y a la derecha hay un par de medidas de cientos de millones.  Bueno, la complejidad computacional de la derecha es de aproximadamente 12 √≥rdenes de magnitud (!) Mayor.  Un poco as√≠ ... Pero el segundo a√±o, incluso con una tarea simple, se balancea como ... [cortado por censura].  Y el nivel de programaci√≥n de los escolares de ayer por razones desconocidas en los √∫ltimos 15 a√±os ha disminuido notablemente.  Tienen que decir: "¬°Lo har√°s bien, te llevar√°n a DeepMind!"  Se podr√≠a decir "inventar VGG", pero "llevar√°n a DeepMind" por alguna raz√≥n motiva mejor.  Esto, obviamente, es un an√°logo avanzado moderno del cl√°sico "¬°Comer√°s s√©mola, te convertir√°s en astronauta!".  Sin embargo, en nuestro caso, si contamos el n√∫mero de ni√±os en el pa√≠s y el tama√±o del cuerpo de cosmonautas, las posibilidades son millones de veces mayores, porque dos de nosotros ya trabajamos en DeepMind desde nuestro laboratorio. <br><br>  El siguiente fue <a href="https://en.wikipedia.org/wiki/Residual_neural_network" rel="nofollow">ResNet</a> , rompiendo el list√≥n para el n√∫mero de capas y comenzando a despegar despu√©s de seis meses.  Y finalmente, DenseNet, que lleg√≥ al comienzo de la exageraci√≥n <a href="https://towardsdatascience.com/densenet-2810936aeebb" rel="nofollow">,</a> despeg√≥ casi de inmediato, incluso m√°s fresco que ResNet. <br><br>  Si hablamos de popularidad, me gustar√≠a agregar algunas palabras sobre las caracter√≠sticas de la red y el rendimiento, de las cuales tambi√©n depende la popularidad.  Si observa c√≥mo se <a href="https://en.wikipedia.org/wiki/ImageNet" rel="nofollow">predice la</a> clase <a href="https://en.wikipedia.org/wiki/ImageNet" rel="nofollow">ImageNet</a> seg√∫n el n√∫mero de operaciones en la red, el dise√±o ser√° as√≠ (m√°s alto y a la izquierda, mejor): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c08/f56/f11/c08f56f11908ffb9f78a0d5d66a71342.png"><br>  <i>Fuente: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">An√°lisis comparativo de arquitecturas representativas de redes neuronales profundas</a></i> <br><br>  Escriba AlexNet ya no es un pastel, y gobiernan las redes basadas en ResNet.  Sin embargo, si observa la evaluaci√≥n pr√°ctica de <abbr title="cantidad de cuadros procesados ‚Äã‚Äãpor segundo">FPS</abbr> m√°s cerca de mi coraz√≥n, puede ver claramente que VGG est√° m√°s cerca del √≥ptimo aqu√≠ y, en general, la alineaci√≥n cambia notablemente.  Incluyendo AlexNet inesperadamente en el sobre √≥ptimo de Pareto (la escala horizontal es logar√≠tmica, mejor arriba y a la derecha): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/3b9/412/6423b941235280be5ec1182b91cf6d6e.png"><br>  <i>Fuente: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">An√°lisis comparativo de arquitecturas representativas de redes neuronales profundas</a></i> <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  En los pr√≥ximos a√±os, la alineaci√≥n de las arquitecturas con una alta probabilidad cambiar√° significativamente debido al <a href="https://habr.com/post/455353/">progreso de los aceleradores de redes neuronales</a> , cuando algunas arquitecturas van a las cestas y otras despegan repentinamente, simplemente porque es mejor instalar un nuevo hardware.  Por ejemplo, <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">en el art√≠culo mencionado</a> , se realiza una comparaci√≥n en el NVIDIA Titan X Pascal y el tablero NVIDIA Jetson TX1, y el dise√±o cambia notablemente.  Al mismo tiempo, el progreso de TPU, NPU y otros acaba de comenzar. <br></li><li>  Como profesional, no puedo evitar notar que la comparaci√≥n en ImageNet se realiza por defecto en ImageNet-1k, y no en ImageNet-22k, simplemente porque la mayor√≠a entrena sus redes en ImageNet-1k, donde hay 22 veces menos clases (esto tanto m√°s f√°cil como m√°s r√°pido).  Cambiar a ImageNet-22k, que es m√°s relevante para muchas aplicaciones pr√°cticas, tambi√©n cambiar√° la alineaci√≥n (para aquellos que est√°n afilados por 1k - mucho). <br></li></ul><br><h1>  M√°s profundo en tecnolog√≠a y arquitectura </h1><br>  Sin embargo, volvamos a la tecnolog√≠a.  El t√©rmino <a href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)" rel="nofollow">Abandono</a> como palabra de b√∫squeda es bastante ruidoso, pero un crecimiento de 5 veces est√° claramente asociado con las redes neuronales.  Y la disminuci√≥n en el inter√©s en √©l es m√°s probable con una <a href="https://patents.google.com/patent/US9406017B2/en" rel="nofollow">patente de Google</a> y el advenimiento de nuevos m√©todos.  Tenga en cuenta que ha pasado aproximadamente un a√±o y medio desde la publicaci√≥n del <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow">art√≠culo original</a> hasta el aumento de inter√©s en el m√©todo: <br><img src="https://habrastorage.org/getpro/habr/post_images/162/fca/4e4/162fca4e4b15574f66f3df40f4230090.png"><br><br>  Sin embargo, si hablamos sobre el per√≠odo anterior al aumento de la popularidad, entonces en DL uno de los primeros lugares est√° claramente ocupado por <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25BA%25D1%2583%25D1%2580%25D1%2580%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">las redes recurrentes</a> y <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25BE%25D0%25BB%25D0%25B3%25D0%25B0%25D1%258F_%25D0%25BA%25D1%2580%25D0%25B0%25D1%2582%25D0%25BA%25D0%25BE%25D1%2581%25D1%2580%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D1%258C" rel="nofollow">LSTM</a> : <br><img src="https://habrastorage.org/getpro/habr/post_images/9bf/423/789/9bf423789d111f7b95352dd9a7b062c1.png"><br><br>  Mucho 20 a√±os antes del pico de popularidad actual, y ahora, con su uso, la traducci√≥n autom√°tica y el an√°lisis del genoma se han mejorado dr√°sticamente, y en el futuro cercano (si tomas de mi √°rea), YouTube, el tr√°fico de Netflix con la misma calidad visual caer√° dos veces.  Si aprende correctamente las lecciones de la historia, es obvio que parte de las ideas del eje actual de los art√≠culos "despegar√°n" solo despu√©s de 20 a√±os.  ¬°Lleva un estilo de vida saludable, cu√≠date y lo ver√°s personalmente! <br><br>  Ahora m√°s cerca del bombo prometido.  <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="nofollow">Las GAN</a> despegaron as√≠: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/80c/272/c0d/80c272c0d6207b79b2736559480aea3d.png"></a> <br><br>  Se puede ver claramente que durante casi un a√±o hubo silencio por completo y solo en 2016, despu√©s de 2 a√±os, comenz√≥ un fuerte aumento (los resultados mejoraron notablemente).  Este despegue un a√±o despu√©s dio el sensacional DeepFake, que, sin embargo, tambi√©n despeg√≥ 1.5 a√±os.  Es decir, incluso las tecnolog√≠as muy prometedoras requieren una cantidad considerable de tiempo para pasar de una idea a aplicaciones que todos pueden usar. <br><br>  Si observa qu√© im√°genes gener√≥ la GAN en el <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow">art√≠culo original</a> y qu√© se puede construir con <a href="https://en.wikipedia.org/wiki/StyleGAN" rel="nofollow">StyleGAN</a> , resulta bastante obvio por qu√© hubo tal silencio.  En 2014, solo los especialistas pod√≠an evaluar lo genial que era: crear, en esencia, otra red como funci√≥n de p√©rdida y capacitarlos juntos.  Y en 2019, todos los escolares podr√≠an apreciar lo genial que es esto (sin comprender completamente c√≥mo se hace): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ee/6ca/51a/5ee6ca51ac69315327c2dc31f8f80c15.png"><br><br>  Hoy en d√≠a, las redes neuronales resuelven con √©xito <a href="https://www.eff.org/ai/metrics" rel="nofollow">muchos</a> problemas diferentes, puede tomar las mejores redes y crear gr√°ficos de popularidad para cada direcci√≥n, lidiar con el ruido y los picos de las consultas de b√∫squeda, etc.  Para no difundir mis pensamientos sobre el √°rbol, finalizaremos esta selecci√≥n con el tema de los algoritmos de segmentaci√≥n, donde las ideas de <a href="https://medium.com/%40sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90" rel="nofollow">convoluci√≥n atroz / dilatada</a> y <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" rel="nofollow">ASPP</a> en el √∫ltimo a√±o y medio se han disparado por completo <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php%3Fchallengeid%3D11%26compid%3D6" rel="nofollow">en el punto de referencia del algoritmo</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f52/6c8/6d5/f526c86d5b81454b2a62f77193be0335.png"><br>  Tambi√©n debe tenerse en cuenta que si <a href="https://arxiv.org/pdf/1412.7062.pdf" rel="nofollow">DeepLabv1</a> m√°s de un a√±o "esper√≥" el aumento de la popularidad, <a href="https://arxiv.org/pdf/1606.00915.pdf" rel="nofollow">DeepLabv2</a> despeg√≥ en un a√±o y <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">DeepLabv3</a> casi de inmediato.  Es decir  en general, podemos hablar sobre acelerar el crecimiento del inter√©s con el tiempo (bueno, o acelerar el crecimiento del inter√©s en tecnolog√≠as de autores de renombre). <br><br>  Todo esto en conjunto condujo a la creaci√≥n del siguiente problema global: un aumento explosivo en el n√∫mero de publicaciones sobre el tema: <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/99b/bd7/66e/99bbd766ebd801adb403efa6ee5efb4e.png"><br>  <i>Fuente: ¬ø <a href="http://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/" rel="nofollow">Demasiados documentos de aprendizaje autom√°tico?</a></i> <br><br>  Este a√±o recibimos entre 150 y 200 art√≠culos por d√≠a, dado que no todos se publican en arXiv-e.  Leer art√≠culos incluso en su propia sub√°rea hoy es completamente imposible.  Como resultado, muchas ideas interesantes ser√°n enterradas bajo los escombros de nuevas publicaciones, lo que afectar√° el momento de su "despegue".  Sin embargo, tambi√©n el aumento <i>explosivo</i> en el n√∫mero de especialistas competentes empleados en la regi√≥n da <s>pocas</s> esperanzas de hacer frente al problema. <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  Adem√°s de ImageNet y la historia detr√°s de escena de los √©xitos de juego de DeepMind, las GAN han dado lugar a una nueva ola de popularizaci√≥n de las redes neuronales.  Con ellos, era realmente posible <a href="https://www.youtube.com/watch%3Fv%3D5rPKeUXjEvE" rel="nofollow">"disparar" actores</a> sin <a href="https://www.youtube.com/watch%3Fv%3DWm3squcz7Aw" rel="nofollow">usar una c√°mara</a> .  ¬°Y si habr√° m√°s!  Bajo este ruido informativo, se financiar√°n tecnolog√≠as de procesamiento y reconocimiento menos sonoras, pero bastante funcionales. <br></li><li>  Como hay demasiadas publicaciones, esperamos la aparici√≥n de nuevos m√©todos de redes neuronales para el an√°lisis r√°pido de los art√≠culos, porque solo ellos nos salvar√°n (¬°una broma con una fracci√≥n de broma!). <br></li></ul><br><h1>  Robots de trabajo, hombre feliz </h1><br>  Durante 2 a√±os, AutoML ha ganado popularidad <s>en las p√°ginas de los peri√≥dicos</s> .  Todo comenz√≥ tradicionalmente con ImageNet, en el que, en Top-1 Accuracy, comenz√≥ a tomar firmemente los primeros lugares: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/9a1/498/e6b9a14988b3d708bdbc73aebbf567d2.png"><br>  La esencia de AutoML es muy simple, un sue√±o centenario de cient√≠ficos de datos se ha hecho realidad: una red neuronal para seleccionar hiperpar√°metros.  La idea fue recibida con una explosi√≥n: <br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4f8/c12/56a/4f8c1256ab40bc119fa0c971a8f8bbc1.png"></div><br>  A continuaci√≥n, en el gr√°fico, vemos una situaci√≥n bastante rara cuando, despu√©s de la publicaci√≥n de los art√≠culos iniciales sobre <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">NASNet</a> y <a href="https://arxiv.org/pdf/1802.01548.pdf" rel="nofollow">AmoebaNet</a> , comienzan a ganar popularidad seg√∫n los est√°ndares de las ideas anteriores casi instant√°neamente (se ve afectado un gran inter√©s en el tema): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0aa/81b/662/0aa81b6628967bc6e77468365cce8554.png"><br>  La imagen id√≠lica est√° algo estropeada por dos puntos.  En primer lugar, cualquier conversaci√≥n sobre AutoML comienza con la frase: "Si tiene una GPU dofigalion ...".  Y ese es el problema.  Google, por supuesto, afirma que con su <a href="https://cloud.google.com/automl/" rel="nofollow">Cloud AutoML</a> esto se resuelve f√°cilmente, lo <s>principal es que tiene suficiente dinero</s> , pero no todos est√°n de acuerdo con este enfoque.  En segundo lugar, funciona hasta ahora <a href="https://towardsdatascience.com/automl-is-overhyped-1b5511ded65f" rel="nofollow">imperfectamente</a> .  Por otro lado, recordando las GAN, cinco a√±os a√∫n no han pasado, y la idea en s√≠ misma parece muy prometedora. <br><br>  En cualquier caso, el despegue principal de AutoML comenzar√° con la pr√≥xima generaci√≥n de aceleradores de hardware para redes neuronales y, de hecho, con algoritmos mejorados. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ec/e67/3c5/7ece673c519b57348f556aa7f7b9dfa6.png"><br>  <i>Fuente: Imagen de Dmitry Konovalchuk, materiales del autor.</i> <br><br>  <b>Total: De hecho, los cient√≠ficos de datos no tendr√°n unas vacaciones eternas, por supuesto, porque durante mucho tiempo seguir√° habiendo un gran dolor de cabeza con los datos.</b>  <b>Pero antes del A√±o Nuevo y el comienzo de la d√©cada de 2020, ¬øpor qu√© no so√±ar?</b> <br><br><h1>  Algunas palabras sobre herramientas </h1><br>  La efectividad de la investigaci√≥n depende mucho de las herramientas.  Si para programar AlexNet, necesitaba una programaci√≥n no trivial, hoy dicha red se puede recopilar en varias l√≠neas en marcos nuevos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7c/002/004/b7c0020046f4f5a5b021c6e4a943c9a5.png"><br>  Se ve claramente c√≥mo la popularidad est√° cambiando en oleadas.  Hoy, el m√°s popular (incluso <a href="https://paperswithcode.com/trends" rel="nofollow">seg√∫n PapersWithCode</a> ) es <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> .  Y una vez que el popular <a href="http://caffe.berkeleyvision.org/" rel="nofollow">Caffe</a> bellamente se va muy suavemente.  (Nota: tema y software significa que se utiliz√≥ el filtrado de temas de Google al trazar). <br><br>  Bueno, dado que tocamos las herramientas de desarrollo, vale la pena mencionar las bibliotecas para acelerar la ejecuci√≥n de la red: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/de3/fb6/8af/de3fb68af88d9afabe23929e1b060309.png"></a> <br><br>  El m√°s antiguo en el tema es (respete NVIDIA) <a href="https://developer.nvidia.com/cudnn" rel="nofollow">cuDNN</a> , y, afortunadamente para los desarrolladores, en los √∫ltimos a√±os el n√∫mero de bibliotecas ha aumentado varias veces, y el comienzo de su popularidad se ha vuelto notablemente m√°s pronunciado.  Y parece que todo esto es solo el comienzo. <br><br>  <b>Total: incluso en los √∫ltimos 3 a√±os, las herramientas han cambiado seriamente para mejor.</b>  <b>Y hace 3 a√±os, seg√∫n los est√°ndares actuales, no lo eran en absoluto.</b>  <b>¬°El progreso es muy bueno!</b> <br><br><h1>  Perspectivas de la red neuronal prometidas </h1><br>  Pero la diversi√≥n comienza m√°s tarde.  Este verano, en un <a href="https://habr.com/post/455353/">art√≠culo extenso,</a> describ√≠ en detalle por qu√© la CPU e incluso la GPU no son lo suficientemente eficientes como para trabajar con redes neuronales, por qu√© miles de millones de d√≥lares est√°n fluyendo hacia el desarrollo de nuevos chips y cu√°les son las perspectivas.  No me repetir√©  A continuaci√≥n se muestra una generalizaci√≥n y adici√≥n del texto anterior. <br><br>  Para comenzar, debe comprender las diferencias entre los c√°lculos de la red neuronal y los c√°lculos en la arquitectura familiar de von Neumann (en la que, por supuesto, pueden calcularse, pero de manera menos eficiente): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a1c/b6a/2f7/a1cb6a2f730e5d5f66a0e29b3fa7d1ac.png"><br>  <i>Fuente: Imagen de Dmitry Konovalchuk, materiales del autor.</i> <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Arquitectura von Neumann</b> <br></td><td>  <b>Redes neuronales</b> <br></td></tr><tr><td>  La mayor√≠a de los c√°lculos son operaciones secuenciales. <br></td><td>  Computaci√≥n paralela masiva (necesita una arquitectura con una gran cantidad de m√≥dulos inform√°ticos y aceleraci√≥n de la inform√°tica tensorial) <br></td></tr><tr><td>  El curso de los c√°lculos est√° cambiando. <br>  dependiendo de las condiciones (se necesita <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2583%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C" rel="nofollow">superescalaridad</a> ) <br></td><td>  La estructura computacional es casi siempre fija y conocida de antemano (la superescalaridad es ineficiente) <br></td></tr><tr><td>  Hay localidad seg√∫n los datos (el cach√© funciona bien) <br></td><td>  Sin localidad de datos (el cach√© calienta el aire) <br></td></tr><tr><td>  C√°lculos precisos <br></td><td>  Los c√°lculos pueden no ser precisos. <br></td></tr><tr><td>  Los datos cambian de manera diferente para diferentes algoritmos <br></td><td>  Docenas de megabytes de coeficientes de red no cambian cuando los datos se ejecutan repetidamente a trav√©s de una red neuronal <br></td></tr></tbody></table></div><br>  La vez anterior, la discusi√≥n principal fue sobre FPGA / ASIC, y los c√°lculos inexactos pasaron casi desapercibidos, as√≠ que hablemos de ellos con m√°s detalle.  Las grandes posibilidades de reducir los chips de las pr√≥ximas generaciones radican precisamente en la capacidad de leer de manera incorrecta (y almacenar datos de coeficientes localmente).  El engrosamiento, de hecho, tambi√©n se usa en aritm√©tica exacta, cuando los pesos de la red se convierten a enteros y se cuantifican, pero a un nuevo nivel.  Como ejemplo, considere un sumador de un solo bit (el ejemplo es bastante abstracto): <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/959/bbf/f16/959bbff160d7f0b33e4d46ce7a5be453.png"><br>  <i>Fuente: <a href="https://www.researchgate.net/publication/270898651_A_High_Speed_and_Low_Power_8_Bit_x_8_Bit_Multiplier_Design_using_Novel_Two_Transistor_2T_XOR_Gates" rel="nofollow">Un dise√±o multiplicador de 8 bits x 8 bits de alta velocidad y baja potencia con puertas XOR de dos transistores de novela dos (2T)</a></i> <br><br>  Necesita 6 transistores (existen diferentes enfoques, la cantidad de transistores necesarios puede ser mayor o menor, pero en general, algo as√≠).  Para 8 bits, <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">se</a> requieren aproximadamente <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">48 transistores</a> .  En este caso, el sumador anal√≥gico requiere solo 2 (¬°dos!) Transistores, es decir  24 veces menos: <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f34/85f/2a5/f3485f2a5ac6fa241d2e59cd80ed1064.png"><br>  <i>Fuente: <a href="http://www.iitk.ac.in/eclub/ee381/AnalogMultipliers.pdf" rel="nofollow">Multiplicadores anal√≥gicos (an√°lisis y dise√±o de circuitos integrados anal√≥gicos)</a></i> <br><br>  Si la precisi√≥n es mayor (por ejemplo, equivalente a 10 o 16 bits digitales), la diferencia ser√° a√∫n mayor.  ¬°A√∫n m√°s interesante es la situaci√≥n con la multiplicaci√≥n!  Si un multiplexor digital de 8 bits requiere aproximadamente <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">400 transistores</a> , entonces un an√°logo 6, es decir  67 veces (!) Menos.  Por supuesto, los transistores "anal√≥gicos" y "digitales" son significativamente diferentes desde el punto de vista de los circuitos, pero la idea es clara: si logramos aumentar la precisi√≥n de los c√°lculos anal√≥gicos, entonces llegamos f√°cilmente a la situaci√≥n cuando necesitamos dos √≥rdenes de magnitud menos transistores.  Y el objetivo no es tanto reducir el tama√±o (que es importante en relaci√≥n con la "desaceleraci√≥n de la ley de Moore"), sino reducir el consumo de electricidad, que es fundamental para las plataformas m√≥viles.  Y para los centros de datos no ser√° superfluo. <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/077/0b2/42b/0770b242bb2693acbe79a2165a4e8c68.png"><br>  <i>Fuente: <a href="https://blocksandfiles.com/2019/02/11/ibms-ai-chips-change-phase/" rel="nofollow">IBM piensa que los chips anal√≥gicos aceleran el aprendizaje autom√°tico</a></i> <br><br>  La clave del √©xito aqu√≠ ser√° una reducci√≥n en la precisi√≥n, y nuevamente aqu√≠ IBM est√° a la vanguardia: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a87/570/9f1/a875709f1c76de4e228a567f293a6618.png"><br>  <i>Fuente: <a href="https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/" rel="nofollow">Blog de investigaci√≥n de IBM: Precisi√≥n de 8 bits para la formaci√≥n de sistemas de aprendizaje profundo</a></i> <br><br>  Ya est√°n involucrados en ASIC especializados para redes neuronales, que muestran una superioridad de m√°s de 10 veces sobre la GPU, y planean lograr una superioridad de 100 veces en los pr√≥ximos a√±os.  Parece extremadamente alentador, realmente estamos deseando que llegue, porque, repito, este ser√° un gran avance para los dispositivos m√≥viles. <br><br>  Mientras tanto, la situaci√≥n no es tan m√°gica, aunque hay serios √©xitos.  Aqu√≠ hay una prueba interesante de los actuales aceleradores de hardware m√≥vil de las redes neuronales (se puede hacer clic en la imagen, y eso nuevamente calienta el alma del autor, tambi√©n en im√°genes por segundo): <br><br> <a href="" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/175/fe0/10d/175fe010d6d9742ddabe6d20d50edb67.png"></a> <br>  <i>Fuente: <a href="https://www.groundai.com/project/ai-benchmark-all-about-deep-learning-on-smartphones-in-2019/1" rel="nofollow">Evoluci√≥n del rendimiento de los aceleradores de IA m√≥viles: rendimiento de imagen para el modelo flotante Inception-V3 (modelo FP16 con TensorFlow Lite y NNAPI)</a></i> <br><br>  Verde indica chips m√≥viles, azul indica CPU, naranja indica GPU.  Se ve claramente que los chips m√≥viles actuales y, en primer lugar, el chip de gama alta de Huawei, ya est√°n superando a las CPU decenas de veces m√°s grandes en tama√±o (y consumo de energ√≠a).  ¬°Y es fuerte!  Con la GPU, hasta ahora no todo es tan m√°gico, pero habr√° algo m√°s.  Puede ver los resultados con m√°s detalle en un sitio web separado <a href="http://ai-benchmark.com/" rel="nofollow">http://ai-benchmark.com/</a> , preste atenci√≥n a la secci√≥n de pruebas all√≠, eligieron un buen conjunto de algoritmos para comparar. <br><br>  <b>Total: El progreso de los aceleradores anal√≥gicos hoy es bastante dif√≠cil de evaluar.</b>  <b>Hay una carrera</b>  <b>Pero los productos a√∫n no han salido, por lo que hay <a href="https://www.google.com/search%3Fq%3Danalog%2Bdnn%2Baccelerator%2Bfiletype%253Apdf" rel="nofollow">relativamente pocas</a> publicaciones.</b>  <b>Puede controlar las patentes que aparecen con un retraso (por ejemplo, flujo denso <a href="https://patents.google.com/%3Fq%3D%2522resistive%2Bprocessing%2Bunit%2522%26q%3DRPU%26oq%3D%2522resistive%2Bprocessing%2Bunit%2522%2BRPU" rel="nofollow">de IBM</a> ) o <a href="https://patents.google.com/%3Fq%3Danalog%26q%3Ddnn%26q%3Daccelerator%26oq%3Danalog%2Bdnn%2Baccelerator" rel="nofollow">buscar patentes raras de</a> otros fabricantes.</b>  <b>Parece que esta ser√° una revoluci√≥n muy seria, principalmente en tel√©fonos inteligentes y TPU de servidor.</b> <br><br><h1>  En lugar de una conclusi√≥n </h1><br>  ML / DL hoy se llama una nueva tecnolog√≠a de programaci√≥n, cuando no escribimos un programa, pero insertamos un bloque y lo entrenamos.  Es decir  Como al principio hab√≠a un ensamblador, luego C, luego C ++, y ahora, despu√©s de 30 a√±os de espera, el siguiente paso es ML / DL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfd/8ae/5f7/cfd8ae5f7236a0c9781199044966d2c5.png"></div><br>  Eso tiene sentido.  Recientemente, en empresas avanzadas, los lugares de toma de decisiones en los programas son reemplazados por redes neuronales.  Es decir<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">si ayer hubo decisiones "sobre IFs" o sobre heur√≠sticas que fueron amables con el coraz√≥n del programador o incluso con las ecuaciones de Lagrange (¬°guau!) y otros logros m√°s complejos de d√©cadas de desarrollo de la teor√≠a de control, hoy utilizaron una red neuronal simple con 3-5 capas con varias entradas y docenas de probabilidades. Aprende al instante, trabaja de manera significativamente m√°s eficiente y el desarrollo de c√≥digo se vuelve m√°s r√°pido. Si antes era necesario sentarse, cham√°n </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, encender el cerebro</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ahora lo pegu√©, alimente datos y funcion√≥, y usted est√° ocupado con cosas de alto nivel. ¬°Solo un tipo de vacaciones! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naturalmente, la depuraci√≥n ahora es diferente. Si antes, cuando algo no funcionaba, hab√≠a una solicitud: "¬°Env√≠a un ejemplo en el que no funciona!" Y luego un </font><s><font style="vertical-align: inherit;">barbudo</font></s><font style="vertical-align: inherit;"> serio y experimentado</font></font><s><font style="vertical-align: inherit;"></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">el programador conoc√≠a el c√≥digo y la heur√≠stica, dictamin√≥ un par de coeficientes, y si adivin√≥ con una generalizaci√≥n del ejemplo para todos estos casos y lo corrigi√≥ en el lugar correcto, entonces otros ejemplos similares comenzaron a funcionar (¬°oh, felicidad!). Con un bloqueo de red neuronal, ese enfoque no funcionar√° y la solicitud ser√°: "¬°Da un ejemplo y datos marcados, por favor!" Y luego habr√° una capacitaci√≥n adicional con el control de obtener un n√∫mero suficiente de ejemplos en todos los nodos que est√°n potencialmente involucrados en la decisi√≥n incorrecta. Y a√∫n m√°s en la producci√≥n, un gran bot√≥n rojo "Retransformar" simplemente aparecer√° con la misma inscripci√≥n grande y roja debajo de "¬°Presione no m√°s de una vez al mes!" (Para limitar el ajuste del archivo). Y la econom√≠a global se volver√° a√∫n m√°s eficiente. ¬°Aleluya!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, como herramienta matem√°tica, ML / DL en general, y redes neuronales en particular, es claramente algo m√°s que la pr√≥xima tecnolog√≠a de programaci√≥n. </font><font style="vertical-align: inherit;">Las mismas redes neuronales ahora se encuentran simplemente en cada paso:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> El tel√©fono inteligente toma fotos del texto y lo reconoce: estas son redes neuronales, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Un tel√©fono inteligente se traduce bien sobre la marcha de un idioma a otro y habla una traducci√≥n: redes neuronales y una vez m√°s redes neuronales, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> El navegador y el altavoz inteligente reconocen bastante bien el habla, nuevamente redes neuronales, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> El televisor muestra una imagen de contraste brillante de 8K del video de entrada 2K, tambi√©n una red neuronal, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Los robots en producci√≥n se volvieron m√°s precisos, comenzaron a ver y reconocer mejor las situaciones anormales, nuevamente redes neuronales, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un escolar en el grado 10 descarga escaneos de ensayos escolares de Internet, los marca en letras y escribe </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sobre sus rodillas</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> un programa de reconocimiento de escritura sorprendentemente bueno, una tarea imposible para los megaprofesionales de los 90, y aqu√≠ hay una red neuronal,</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> En un servicio de autom√≥viles, registran el sonido del motor e inmediatamente obtienen una lista de posibles problemas, y estas son redes neuronales, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Y aun as√≠, en lugar de una soluci√≥n dif√≠cil y a√∫n lenta de las ecuaciones diferenciales de cinem√°tica inversa, el equipo ense√±a algo m√°gico y </font></font><a href="https://www.cgevent.ru/archives/28741" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">obtiene soluciones r√°pidas y bastante precisas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , por supuesto, es una red neuronal,</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> En general, ¬°las redes neuronales ahora est√°n absolutamente en todas partes! </font></font> ) <br></li></ul><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f28/b47/999/f28b47999ec31c004bc0b917fed05633.png"><br><br>  Solo han pasado 4 a√±os desde que las personas aprendieron a entrenar redes neuronales realmente profundas en muchos aspectos gracias a BatchNorm (2015) y omitir conexiones (2015), y han pasado 3 a√±os desde que "despegaron", y realmente estamos leyendo los resultados de su trabajo No lo vi.  Y ahora llegar√°n a los productos.  Algo nos dice que en los pr√≥ximos a√±os nos esperan muchas cosas interesantes.  Especialmente cuando los aceleradores "despegan" ... <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/a88/6b9/3dc/a886b93dc708002de50fac65c7d84a7a.png"><br><br>  √ârase una vez, si alguien lo recuerda, Prometeo rob√≥ el fuego del Olimpo y se lo entreg√≥ a la gente.  Zeus enojado con otros dioses cre√≥ la primera belleza de una mujer llamada Pandora, que estaba dotada de muchas cualidades femeninas maravillosas <s>(de repente me di cuenta de que la narraci√≥n pol√≠ticamente correcta de algunos de los mitos de la antigua Grecia es extremadamente dif√≠cil)</s> .  Pandora fue enviada a la gente, pero Prometeo, que sospechaba que algo andaba mal, se resisti√≥ a su hechizo, y su hermano Epimeteo no.  Como regalo para la boda, Zeus envi√≥ un hermoso cofre con Mercurio y Mercurio, un alma amable, cumpli√≥ la orden: le dio el cofre a Epimeteo, pero le advirti√≥ que no lo abriera en ning√∫n caso.  La curiosa Pandora le rob√≥ el ata√∫d a su esposo, lo abri√≥, pero solo hab√≠a pecados, enfermedades, guerras y otros problemas de la humanidad.  Ella trat√≥ de cerrar el ata√∫d, pero ya era demasiado tarde: <br><br><img src="https://habrastorage.org/webt/fi/hk/wh/fihkwhcd3uam5uyejc6thklshcm.png"><br>  <i>Fuente: <a href="https://regnum.ru/pictures/2414568/6.html" rel="nofollow">Artista Frederick Stuart Church, Caja abierta de Pandora</a></i> <br><br>  Desde entonces, la frase "abre la caja de Pandora" se ha ido, es decir, para realizar <s>por curiosidad una</s> acci√≥n irreversible, cuyas consecuencias pueden no ser tan hermosas como las decoraciones del ata√∫d en el exterior. <br><br>  Sabes, cuanto m√°s me sumerjo en las redes neuronales, m√°s clara es la sensaci√≥n de que esta es otra caja de Pandora.  Sin embargo, ¬°la humanidad tiene la experiencia m√°s rica en abrir tales cajas!  De lo reciente reciente: esto es energ√≠a nuclear e Internet.  Entonces, creo que podemos hacer frente juntos.  No es de extra√±ar que un grupo de hombres con <s>barba</s> dura entre los primeros.  Bueno, un ata√∫d es hermoso, ¬°de acuerdo!  Y no es cierto que solo haya problemas, ya se han conseguido un mont√≥n de cosas buenas.  Por lo tanto, se unieron y ... ¬°abrimos m√°s! <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  <b>El art√≠culo no incluy√≥ muchos temas interesantes, por ejemplo, algoritmos cl√°sicos de ML, aprendizaje de transferencia, aprendizaje de refuerzo, la popularidad de los conjuntos de datos, etc.</b>  <b>(Caballeros, ¬°pueden continuar con el tema!)</b> <b><br></b> </li><li>  <b>A la pregunta sobre el ata√∫d: personalmente creo que <a href="https://habr.com/ru/post/411323/">los programadores de Google</a> que hicieron posible que Google <a href="https://tproger.ru/news/google-drops-pentagon/" rel="nofollow">abandonara el contrato de $ 10 mil millones con el Pent√°gono</a> son geniales y atractivos.</b>  <b>Respetan y respetan.</b>  <b>Sin embargo, tenga en cuenta que alguien gan√≥ esta importante licitaci√≥n.</b> <b><br></b> </li></ul><br>  Lee tambi√©n: <br><br><ul><li>  <a href="https://habr.com/post/455353/">Aceleraci√≥n de hardware de redes neuronales profundas: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP y otras cartas</a> : el texto del autor sobre el estado actual y las perspectivas de aceleraci√≥n de hardware de redes neuronales en comparaci√≥n con los enfoques actuales. <br></li><li>  <a href="https://habr.com/post/480348/">Deep Fake Science, la crisis de reproducibilidad y de d√≥nde provienen los repositorios vac√≠os</a> , sobre los problemas en la ciencia generados por ML / DL. <br></li><li>  <a href="https://habr.com/post/451664/">Comparaci√≥n de c√≥dec m√°gico callejero.</a>  <a href="https://habr.com/post/451664/">Revelamos secretos</a> , un ejemplo de una falsificaci√≥n basada en redes neuronales. <br></li></ul><br><h3>  ¬°Toda una gran cantidad de <i>nuevos descubrimientos interesantes</i> en la d√©cada de 2020 en general y en el A√±o Nuevo en particular! </h3><br><h2>  Agradecimientos </h2><br>  Me gustar√≠a agradecerle cordialmente: <br><br><ul><li>  Laboratorio de Computaci√≥n Gr√°fica y Multimedia VMK Universidad Estatal de Mosc√∫  M.V.  Lomonosov por su contribuci√≥n al desarrollo del aprendizaje profundo en Rusia y no solo <br></li><li>  personalmente Konstantin Kozhemyakov y Dmitry Konovalchuk, quienes hicieron mucho para hacer este art√≠culo mejor y m√°s visual, <br></li><li>  y finalmente, muchas gracias a Kirill Malyshev, Yegor Sklyarov, Nikolai Oplachko, Andrey Moskalenko, Ivan Molodetsky, Evgeny Lyapustin, Roman Kazantsev, Alexander Yakovenko y Dmitry Klepikov por sus muchos comentarios √∫tiles y correcciones que hicieron que este texto fuera mucho mejor. <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/481844/">https://habr.com/ru/post/481844/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../481828/index.html">La historia del software educativo: sistemas de gesti√≥n del aprendizaje y el auge de la educaci√≥n en l√≠nea.</a></li>
<li><a href="../481836/index.html">Pizza como servicio: como Amazon migr√≥ a Redshift</a></li>
<li><a href="../481838/index.html">WireGuard, configurando m√∫ltiples clientes para NAT, y ¬øa d√≥nde va STUN?</a></li>
<li><a href="../481840/index.html">Proteja su API GraphQL de vulnerabilidades</a></li>
<li><a href="../481842/index.html">Mudarse al almacenamiento puro: nuestro nuevo almacenamiento</a></li>
<li><a href="../481846/index.html">Uso de GitHub CI para proyectos de elixir</a></li>
<li><a href="../481848/index.html">Formaci√≥n de personal con experiencia.</a></li>
<li><a href="../481850/index.html">La Inquisici√≥n espa√±ola y el robot para la humillaci√≥n: ¬øcu√°les son las conferencias "depredadoras" por dinero?</a></li>
<li><a href="../481852/index.html">Revisi√≥n de la impresora 3D Anet N4 // C√≥mo colorear de forma realista un personaje de Dark Souls</a></li>
<li><a href="../481854/index.html">Probar ideas a trav√©s de la creaci√≥n de prototipos del tablero</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>