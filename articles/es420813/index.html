<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üê• üëÉüèª ü§∞üèº Redes de backstage en Kubernetes üßñüèº üëºüèø ‚ò†Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota perev. : El autor del art√≠culo original, Nicolas Leiva, es un arquitecto de soluciones de Cisco que decidi√≥ compartir con sus colegas, ingenieros...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes de backstage en Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/420813/"> <i><b>Nota</b></i>  <i><b>perev.</b></i>  <i>: El autor del art√≠culo original, Nicolas Leiva, es un arquitecto de soluciones de Cisco que decidi√≥ compartir con sus colegas, ingenieros de redes, c√≥mo funciona la red Kubernetes desde adentro.</i>  <i>Para hacer esto, explora su configuraci√≥n m√°s simple en el cl√∫ster, utilizando activamente el sentido com√∫n, su conocimiento de las redes y las utilidades est√°ndar de Linux / Kubernetes.</i>  <i>Result√≥ voluminoso, pero muy claramente.</i> <br><br><img src="https://habrastorage.org/webt/gr/qw/d4/grqwd4putslwaijltw9yojfzjes.png"><br><br>  Adem√°s del hecho de que la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">gu√≠a Kubernetes The Hard Way de</a> Kelsey Hightower simplemente funciona (¬° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">incluso en AWS!</a> ), Me gust√≥ que la red se mantuviera limpia y simple;  y esta es una gran oportunidad para comprender el papel, por ejemplo, de la Interfaz de red de contenedores ( <a href="">CNI</a> ).  Dicho esto, agregar√© que la red de Kubernetes no es realmente muy intuitiva, especialmente para los principiantes ... y tambi√©n no olviden que " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">simplemente no</a> existe una red para contenedores". <a name="habracut"></a><br><br>  Aunque ya hay buenos materiales sobre este tema (ver enlaces <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> ), no pude encontrar un ejemplo tal que combinar√≠a todo lo necesario con las conclusiones de los equipos que los ingenieros de redes aman y odian, demostrando lo que realmente est√° sucediendo detr√°s de escena.  Por lo tanto, decid√≠ recopilar informaci√≥n de muchas fuentes; espero que esto ayude y que comprenda mejor c√≥mo todo est√° conectado entre s√≠.  Este conocimiento es importante no solo para probarse a s√≠ mismo, sino tambi√©n para simplificar el proceso de diagn√≥stico de problemas.  Puede seguir el ejemplo en su cl√∫ster desde <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes The Hard Way</a> : todas las direcciones IP y configuraciones se toman desde all√≠ (a partir de las confirmaciones para mayo de 2018, antes de usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">contenedores Nabla</a> ). <br><br>  Y comenzaremos desde el final, cuando tengamos tres controladores y tres nodos de trabajo: <br><br><img src="https://habrastorage.org/webt/am/vs/6j/amvs6jnhsuxzwhyoyod6vjk8cby.png"><br><br>  ¬°Puede notar que tambi√©n hay al menos tres subredes privadas aqu√≠!  Un poco de paciencia, y todos ser√°n considerados.  Recuerde que aunque nos referimos a prefijos de IP muy espec√≠ficos, simplemente est√°n tomados de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes The Hard Way</a> , por lo que solo tienen importancia local, y usted es libre de elegir cualquier otro bloque de direcciones para su entorno de acuerdo con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RFC 1918</a> .  Para el caso de IPv6, habr√° un art√≠culo de blog separado. <br><br><h2>  Red de host (10.240.0.0/24) </h2><br>  Esta es una red interna de la cual todos los nodos son parte.  Definido por el <code>--private-network-ip</code> en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GCP</a> o la <code>--private-ip-address</code> en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AWS</a> cuando se asignan recursos inform√°ticos. <br><br><h3>  Inicializando nodos de controlador en GCP </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> gcloud compute instances create controller-<span class="hljs-variable"><span class="hljs-variable">${i}</span></span> \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-network-ip 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><code>controllers_gcp.sh</code></a> ) <br><br><h3>  Inicializaci√≥n de nodos de controlador en AWS </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">declare</span></span> controller_id<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>=`aws ec2 run-instances \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-ip-address 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><code>controllers_aws.sh</code></a> ) <br><br><img src="https://habrastorage.org/webt/gt/cj/p6/gtcjp6fgkqbv1nvs2ea9d9hhueo.png"><br><br>  Cada instancia tendr√° dos direcciones IP: privadas de la red host (controladores - <code>10.240.0.1${i}/24</code> , trabajadores - <code>10.240.0.2${i}/24</code> ) y una p√∫blica, designada por el proveedor de la nube, de la que hablaremos m√°s adelante. C√≥mo llegar a <code>NodePorts</code> . <br><br><h3>  Gcp </h3><br><pre> <code class="bash hljs">$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS controller-0 us-west1-c n1-standard-1 10.240.0.10 35.231.XXX.XXX RUNNING worker-1 us-west1-c n1-standard-1 10.240.0.21 35.231.XX.XXX RUNNING ...</code> </pre> <br><br><h3>  Aws </h3><br><pre> <code class="bash hljs">$ aws ec2 describe-instances --query <span class="hljs-string"><span class="hljs-string">'Reservations[].Instances[].[Tags[?Key==`Name`].Value[],PrivateIpAddress,PublicIpAddress]'</span></span> --output text | sed <span class="hljs-string"><span class="hljs-string">'$!N;s/\n/ /'</span></span> 10.240.0.10 34.228.XX.XXX controller-0 10.240.0.21 34.173.XXX.XX worker-1 ...</code> </pre> <br>  Todos los nodos deben poder hacer ping entre s√≠ si las <a href="">pol√≠ticas de seguridad son correctas</a> (y si el <code>ping</code> instalado en el host). <br><br><h2>  Red de hogar (10.200.0.0/16) </h2><br>  Esta es la red en la que viven los pods.  Cada nodo de trabajo utiliza una subred de esta red.  En nuestro caso, <code>POD_CIDR=10.200.${i}.0/24</code> para el <code>worker-${i}</code> . <br><br><img src="https://habrastorage.org/webt/6i/yz/ih/6iyzihbxbzwugs4amvhfa9ysp5s.png"><br><br>  Para comprender c√≥mo est√° configurado todo, retroceda y mire <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el modelo de red de Kubernetes</a> , que requiere lo siguiente: <br><br><ul><li>  Todos los contenedores pueden comunicarse con cualquier otro contenedor sin usar NAT. </li><li>  Todos los nodos pueden comunicarse con todos los contenedores (y viceversa) sin usar NAT. </li><li>  La IP que ve el contenedor debe ser la misma que otros lo ven. </li></ul><br>  Todo esto se puede implementar de muchas maneras, y Kubernetes pasa la configuraci√≥n de la red al <a href="">complemento CNI</a> . <br><br><blockquote>  ‚ÄúEl complemento CNI es responsable de agregar una interfaz de <b>red al espacio de nombres de red</b> del contenedor (por ejemplo, un extremo de un <b>par de veth</b> ) y de realizar los cambios necesarios en el host (por ejemplo, conectar el segundo extremo de veth a un puente).  Luego debe asignar una interfaz IP y configurar las rutas de acuerdo con la secci√≥n Administraci√≥n de la direcci√≥n IP llamando al complemento IPAM deseado ".  <i>(de la <a href="">especificaci√≥n de interfaz de red de contenedores</a> )</i> </blockquote><br><img src="https://habrastorage.org/webt/5q/fs/vw/5qfsvwg2iuduy0q3doco11hbf-g.png"><br><br><h3>  Espacio de nombres de red </h3><br><blockquote>  ‚ÄúEl espacio de nombres envuelve el recurso del sistema global en una abstracci√≥n que es visible para los procesos en este espacio de nombres de tal manera que tengan su propia instancia aislada del recurso global.  Los cambios en el recurso global son visibles para otros procesos incluidos en este espacio de nombres, pero no son visibles para otros procesos ".  <i>( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">de la p√°gina del manual de espacios de nombres</a> )</i> </blockquote><br>  Linux proporciona siete espacios de nombres diferentes ( <code>Cgroup</code> , <code>IPC</code> , <code>Network</code> , <code>Mount</code> , <code>PID</code> , <code>User</code> , <code>UTS</code> ).  Los espacios de nombres de red ( <code>CLONE_NEWNET</code> ) definen los recursos de red que est√°n disponibles para el proceso: "Cada espacio de nombres de red tiene sus propios dispositivos de red, direcciones IP, tablas de enrutamiento IP, <code>/proc/net</code> , n√∫meros de puerto, etc." <i>( del art√≠culo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Espacios de nombres en funcionamiento</a> ")</i> . <br><br><h3>  Dispositivos virtuales Ethernet (Veth) </h3><br><blockquote>  "Un par de red virtual (veth) ofrece una abstracci√≥n en forma de" tuber√≠a ", que se puede utilizar para crear t√∫neles entre espacios de nombres de red o para crear un puente a un dispositivo de red f√≠sica en otro espacio de red.  Cuando se libera el espacio de nombres, todos los dispositivos veth en √©l se destruyen ".  <i>(desde la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">p√°gina del manual de espacios de nombres de red</a> )</i> </blockquote><br>  Baje al suelo y vea c√≥mo se relaciona todo con el cl√∫ster.  En primer lugar, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">los complementos de red</a> en Kubernetes son diversos, y los complementos CNI son uno de ellos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬øpor qu√© no CNM?</a> ).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubelet</a> en cada nodo le dice al contenedor de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tiempo de ejecuci√≥n</a> qu√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">complemento de red</a> usar.  La interfaz de red de contenedor ( <a href="">CNI</a> ) se encuentra entre el tiempo de ejecuci√≥n del contenedor y la implementaci√≥n de la red.  Y ya el complemento CNI configura la red. <br><br><blockquote>  ‚ÄúEl complemento CNI se selecciona pasando la <code>--network-plugin=cni</code> l√≠nea de comando <code>--network-plugin=cni</code> a Kubelet.  Kubelet lee el archivo desde <code>--cni-conf-dir</code> (el valor predeterminado es <code>/etc/cni/net.d</code> ) y usa la configuraci√≥n CNI de este archivo para configurar la red para cada archivo ".  <i>(de los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">requisitos del complemento de red</a> )</i> </blockquote><br>  Los binarios reales del complemento CNI est√°n en <code>-- cni-bin-dir</code> (el valor predeterminado es <code>/opt/cni/bin</code> ). <br><br>  Tenga en cuenta que los <a href=""><code>kubelet.service</code></a> llamada de <a href=""><code>kubelet.service</code></a> incluyen <code>--network-plugin=cni</code> : <br><br><pre> <code class="plaintext hljs">[Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --network-plugin=cni \\ ...</code> </pre> <br>  En primer lugar, Kubernetes crea un espacio de nombres de red para el hogar, incluso antes de llamar a cualquier complemento.  Esto se implementa utilizando el contenedor de <code>pause</code> especial, que "sirve como el" contenedor principal "para todos los contenedores de hogar" <i>(del art√≠culo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El Contenedor de pausa Todopoderoso</a> ")</i> .  Kubernetes luego ejecuta el complemento CNI para adjuntar el contenedor de <code>pause</code> a la red.  Todos los contenedores de pod utilizan el <code>netns</code> este contenedor de <code>pause</code> . <br><br><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.1", "name": "bridge", "type": "bridge", "bridge": "cnio0", "isGateway": true, "ipMasq": true, "ipam": { "type": "host-local", "ranges": [ [{"subnet": "${POD_CIDR}"}] ], "routes": [{"dst": "0.0.0.0/0"}] } }</code> </pre> <br>  La <a href="">configuraci√≥n CNI utilizada</a> indica el uso del complemento de <code>bridge</code> para configurar el puente de software de Linux (L2) en el espacio de nombres ra√≠z llamado <code>cnio0</code> (el <a href="">nombre predeterminado</a> es <code>cni0</code> ), que act√∫a como una puerta de enlace ( <code>"isGateway": true</code> ). <br><br><img src="https://habrastorage.org/webt/bo/to/jp/botojpqu0f7fascfrbk-gen27a8.png"><br><br>  Tambi√©n se configurar√° un par par para conectar el hogar al puente reci√©n creado: <br><br><img src="https://habrastorage.org/webt/-6/tt/e7/-6tte7essirvuraiuypuln_syvm.png"><br><br>  Para asignar informaci√≥n L3, como direcciones IP, se llama al <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">complemento IPAM</a> ( <code>ipam</code> ).  En este caso, se usa el tipo <code>host-local</code> , "que almacena el estado localmente en el sistema de archivos del host, lo que garantiza la unicidad de las direcciones IP en un host" <i>(de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><code> host-local</code></a> )</i> .  El complemento IPAM devuelve esta informaci√≥n al complemento anterior ( <code>bridge</code> ), de modo que todas las rutas especificadas en la configuraci√≥n se pueden configurar ( <code>"routes": [{"dst": "0.0.0.0/0"}]</code> ).  Si no se especifica <code>gw</code> , se <a href="">toma de la subred</a> .  La ruta predeterminada tambi√©n se configura en el espacio de nombres de red del hogar, apuntando al puente (que se configura como la primera subred IP del hogar). <br><br>  Y el √∫ltimo detalle importante: solicitamos enmascarar ( <code>"ipMasq": true</code> ) para el tr√°fico proveniente de la red de <code>"ipMasq": true</code> .  Realmente no necesitamos NAT aqu√≠, pero esta es la configuraci√≥n en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes The Hard Way</a> .  Por lo tanto, para completar, debo mencionar que las entradas en las <code>iptables</code> complemento de <code>bridge</code> est√°n configuradas para este ejemplo en particular.  Todos los paquetes del hogar, cuyo destinatario no est√° en el rango <code>224.0.0.0/4</code> , <a href="">estar√°n detr√°s de NAT</a> , que no cumple con el requisito "todos los contenedores pueden comunicarse con cualquier otro contenedor sin usar NAT".  Bueno, probaremos por qu√© no se necesita NAT ... <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/y-/hy/ub/y-hyubecllmzx9go5ehai4shl78.jpeg"></a> <br><br><h3>  Enrutamiento de hogar </h3><br>  Ahora estamos listos para personalizar las vainas.  Miremos todos los espacios de red de los nombres de uno de los nodos de trabajo y analicemos uno de ellos despu√©s de crear la implementaci√≥n de <code>nginx</code> <a href="">desde aqu√≠</a> .  Usaremos <code>lsns</code> con la opci√≥n <code>-t</code> para seleccionar el tipo de espacio de nombres deseado (es decir, <code>net</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo lsns -t net NS TYPE NPROCS PID USER COMMAND 4026532089 net 113 1 root /sbin/init 4026532280 net 2 8046 root /pause 4026532352 net 4 16455 root /pause 4026532426 net 3 27255 root /pause</code> </pre> <br>  Usando la opci√≥n <code>-i</code> para <code>ls</code> podemos encontrar sus n√∫meros de inodo: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ls -1i /var/run/netns 4026532352 cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af 4026532280 cni-7cec0838-f50c-416a-3b45-628a4237c55c 4026532426 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  Tambi√©n puede enumerar todos los espacios de nombres de red utilizando <code>ip netns</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip netns cni-912bcc63-712d-1c84-89a7-9e10510808a0 (id: 2) cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af (id: 1) cni-7cec0838-f50c-416a-3b45-628a4237c55c (id: 0)</code> </pre> <br>  Para ver todos los procesos que se ejecutan en el espacio de red <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ( <code>4026532426</code> ), puede ejecutar, por ejemplo, el siguiente comando: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ls -l /proc/[1-9]*/ns/net | grep 4026532426 | cut -f3 -d<span class="hljs-string"><span class="hljs-string">"/"</span></span> | xargs ps -p PID TTY STAT TIME COMMAND 27255 ? Ss 0:00 /pause 27331 ? Ss 0:00 nginx: master process nginx -g daemon off; 27355 ? S 0:00 nginx: worker process</code> </pre> <br>  Se puede ver que adem√°s de <code>pause</code> en este pod, lanzamos <code>nginx</code> .  El contenedor de <code>pause</code> comparte los espacios de nombres <code>net</code> e <code>ipc</code> con todos los dem√°s contenedores de pod.  Recuerde el PID de <code>pause</code> - 27255;  Volveremos a ello. <br><br>  Ahora veamos qu√© dice <code>kubectl</code> sobre este pod: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide | grep nginx nginx-65899c769f-wxdx6 1/1 Running 0 5d 10.200.0.4 worker-0</code> </pre> <br>  M√°s detalles: <br><br><pre> <code class="bash hljs">$ kubectl describe pods nginx-65899c769f-wxdx6</code> </pre> <br><pre> <code class="plaintext hljs">Name: nginx-65899c769f-wxdx6 Namespace: default Node: worker-0/10.240.0.20 Start Time: Thu, 05 Jul 2018 14:20:06 -0400 Labels: pod-template-hash=2145573259 run=nginx Annotations: &lt;none&gt; Status: Running IP: 10.200.0.4 Controlled By: ReplicaSet/nginx-65899c769f Containers: nginx: Container ID: containerd://4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 Image: nginx ...</code> </pre> <br>  Vemos el nombre del pod - <code>nginx-65899c769f-wxdx6</code> - y la ID de uno de sus contenedores ( <code>nginx</code> ), pero no se ha dicho nada sobre la <code>pause</code> .  Excave un nodo de trabajo m√°s profundo para que coincida con todos los datos.  Recuerde que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes The Hard Way</a> no utiliza <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Docker</a> , por lo tanto, para obtener detalles sobre el contenedor, consulte la utilidad de consola <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">containerd</a> - ctr <i>(consulte tambi√©n el art√≠culo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Integraci√≥n de containerd con Kubernetes, reemplazando Docker, listo para la producci√≥n</a> " - <b>transferencia aprox.</b> )</i> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr namespaces ls NAME LABELS k8s.io</code> </pre> <br>  Conociendo el <code>k8s.io</code> containerd ( <code>k8s.io</code> ), puede obtener el ID del contenedor <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep nginx 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 docker.io/library/nginx:latest io.containerd.runtime.v1.linux</code> </pre> <br>  ... y <code>pause</code> tambi√©n: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep pause 0866803b612f2f55e7b6b83836bde09bd6530246239b7bde1e49c04c7038e43a k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux 21640aea0210b320fd637c22ff93b7e21473178de0073b05de83f3b116fc8834 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux</code> </pre> <br>  La ID del contenedor <code>nginx</code> que termina en <code>‚Ä¶983c7</code> coincide con lo que obtuvimos de <code>kubectl</code> .  Veamos si podemos descubrir qu√© contenedor de <code>pause</code> pertenece al pod <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io task ls TASK PID STATUS ... d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 27255 RUNNING 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 27331 RUNNING</code> </pre> <br>  ¬øRecuerda que los procesos con PID 27331 y 27355 se ejecutan en el espacio de nombres de red <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ? <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"sandbox"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span>, <span class="hljs-string"><span class="hljs-string">"pod-template-hash"</span></span>: <span class="hljs-string"><span class="hljs-string">"2145573259"</span></span>, <span class="hljs-string"><span class="hljs-string">"run"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"k8s.gcr.io/pause:3.1"</span></span>, ...</code> </pre> <br>  ... y: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"container"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.container.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"docker.io/library/nginx:latest"</span></span>, ...</code> </pre> <br>  Ahora sabemos con certeza qu√© contenedores se est√°n ejecutando en este pod ( <code>nginx-65899c769f-wxdx6</code> ) y el espacio de nombres de red ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ): <br><br><ul><li>  nginx (ID: <code>4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7</code> ); </li><li>  pausa (ID: <code>d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6</code> ). </li></ul><br><img src="https://habrastorage.org/webt/3h/cx/qq/3hcxqqv-mwlrm8ax9lu9jl0fixy.png"><br><br>  ¬øC√≥mo se conecta esto a ( <code>nginx-65899c769f-wxdx6</code> ) a la red?  Usamos el PID 27255 recibido previamente desde <code>pause</code> para ejecutar comandos en su espacio de nombres de red ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns identify 27255 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  Para estos fines, usaremos <code>nsenter</code> con la opci√≥n <code>-t</code> que define el PID de destino y <code>-n</code> sin especificar un archivo para ingresar al espacio de nombres de red del proceso de destino (27255).  Esto es lo que dir√° <code>ip link show</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ip link show 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:0a:c8:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</code> </pre> <br>  ... y <code>ifconfig eth0</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ifconfig eth0 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.200.0.4 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::2097:51ff:fe39:ec21 prefixlen 64 scopeid 0x20&lt;link&gt; ether 0a:58:0a:c8:00:04 txqueuelen 0 (Ethernet) RX packets 540 bytes 42247 (42.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 177 bytes 16530 (16.5 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0</code> </pre> <br>  Esto confirma que la direcci√≥n IP obtenida anteriormente a trav√©s de <code>kubectl get pod</code> est√° configurada en la interfaz <code>eth0</code> .  Esta interfaz es parte de un <b>par veth</b> , uno de los cuales est√° en el hogar y el otro en el espacio de nombres ra√≠z.  Para descubrir la interfaz del segundo extremo, utilizamos <code>ethtool</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ethtool -S eth0 NIC statistics: peer_ifindex: 7</code> </pre> <br>  Vemos que si el <code>ifindex</code> fiesta es 7. Verifique que est√© en el espacio de nombres ra√≠z.  Esto se puede hacer usando el <code>ip link</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip link | grep <span class="hljs-string"><span class="hljs-string">'^7:'</span></span> 7: veth71f7d238@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cnio0 state UP mode DEFAULT group default</code> </pre> <br>  Para estar seguros de esto finalmente, veamos: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo cat /sys/class/net/veth71f7d238/ifindex 7</code> </pre> <br>  Genial, ahora todo est√° claro con el enlace virtual.  Usando <code>brctl</code> veamos qui√©n m√°s est√° conectado al puente de Linux: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ brctl show cnio0 bridge name bridge id STP enabled interfaces cnio0 8000.0a580ac80001 no veth71f7d238 veth73f35410 vethf273b35f</code> </pre> <br>  Entonces, la imagen es la siguiente: <br><br><img src="https://habrastorage.org/webt/yu/gc/t6/yugct6efi7ztep277en4msjuzv4.png"><br><br><h3>  Verificaci√≥n de enrutamiento </h3><br>  ¬øC√≥mo reenviamos el tr√°fico?  Veamos la tabla de enrutamiento en el pod de espacio de nombres de red: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ip route show default via 10.200.0.1 dev eth0 10.200.0.0/24 dev eth0 proto kernel scope link src 10.200.0.4</code> </pre> <br>  Al menos sabemos c√≥mo llegar al espacio de nombres ra√≠z ( <code>default via 10.200.0.1</code> ).  Ahora veamos la tabla de enrutamiento del host: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip route list default via 10.240.0.1 dev eth0 proto dhcp src 10.240.0.20 metric 100 10.200.0.0/24 dev cnio0 proto kernel scope link src 10.200.0.1 10.240.0.0/24 dev eth0 proto kernel scope link src 10.240.0.20 10.240.0.1 dev eth0 proto dhcp scope link src 10.240.0.20 metric 100</code> </pre> <br>  Sabemos c√≥mo reenviar paquetes a un enrutador VPC (VPC <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tiene un</a> enrutador "impl√≠cito", que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">generalmente tiene una segunda direcci√≥n</a> desde el espacio de la direcci√≥n IP principal de la subred).  Ahora: ¬øEl enrutador VPC sabe c√≥mo llegar a la red de cada hogar?  No, no lo hace, por lo tanto, se supone que las rutas ser√°n configuradas por el complemento CNI o <a href="">manualmente</a> (como en el manual).  Aparentemente, el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">complemento AWS CNI</a> hace exactamente eso por nosotros en AWS.  Recuerde que hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">muchos complementos CNI</a> , y estamos considerando un ejemplo de una <b>configuraci√≥n de red simple</b> : <br><br><img src="https://habrastorage.org/webt/cn/v7/v_/cnv7v_qjfkidbtuljkbgkuzuaag.png"><br><br><h3>  Inmersi√≥n profunda en NAT </h3><br>  <code>kubectl create -f busybox.yaml</code> cree dos contenedores <code>busybox</code> id√©nticos con el Controlador de replicaci√≥n: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: ReplicationController metadata: name: busybox0 labels: app: busybox0 spec: replicas: 2 selector: app: busybox0 template: metadata: name: busybox0 labels: app: busybox0 spec: containers: - image: busybox command: - sleep - "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><code>busybox.yaml</code></a> ) <br><br>  Obtenemos: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-g6pww 1/1 Running 0 4s 10.200.1.15 worker-1 busybox0-rw89s 1/1 Running 0 4s 10.200.0.21 worker-0 ...</code> </pre> <br>  Los pings de un contenedor a otro deber√≠an ser exitosos: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-rw89s -- ping -c 2 10.200.1.15 PING 10.200.1.15 (10.200.1.15): 56 data bytes 64 bytes from 10.200.1.15: seq=0 ttl=62 time=0.528 ms 64 bytes from 10.200.1.15: seq=1 ttl=62 time=0.440 ms --- 10.200.1.15 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.440/0.484/0.528 ms</code> </pre> <br>  Para comprender el movimiento del tr√°fico, puede mirar los paquetes usando <code>tcpdump</code> o <code>conntrack</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 29 src=10.200.0.21 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  La IP de origen del pod 10.200.0.21 se traduce a la direcci√≥n IP del host 10.240.0.20. <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 28 src=10.240.0.20 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  En iptables, puede ver que los recuentos est√°n aumentando: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo iptables -t nat -Z POSTROUTING -L -v Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> destination ... 5 324 CNI-be726a77f15ea47ff32947a3 all -- any any 10.200.0.0/24 anywhere /* name: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span> id: <span class="hljs-string"><span class="hljs-string">"631cab5de5565cc432a3beca0e2aece0cef9285482b11f3eb0b46c134e457854"</span></span> */ Zeroing chain `POSTROUTING<span class="hljs-string"><span class="hljs-string">'</span></span></code> </pre> <br>  Por otro lado, si elimina <code>"ipMasq": true</code> de la configuraci√≥n del complemento CNI, puede ver lo siguiente (esta operaci√≥n se realiza exclusivamente con fines educativos: ¬°no recomendamos cambiar la configuraci√≥n en un cl√∫ster de trabajo!): <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-2btxn 1/1 Running 0 16s 10.200.0.15 worker-0 busybox0-dhpx8 1/1 Running 0 16s 10.200.1.13 worker-1 ...</code> </pre> <br>  Ping a√∫n debe pasar: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-2btxn -- ping -c 2 10.200.1.13 PING 10.200.1.6 (10.200.1.6): 56 data bytes 64 bytes from 10.200.1.6: seq=0 ttl=62 time=0.515 ms 64 bytes from 10.200.1.6: seq=1 ttl=62 time=0.427 ms --- 10.200.1.6 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.427/0.471/0.515 ms</code> </pre> <br>  Y en este caso, sin usar NAT: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 29 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br>  Por lo tanto, verificamos que "todos los contenedores pueden comunicarse con cualquier otro contenedor sin usar NAT". <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 27 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br><h2>  Red de cl√∫ster (10.32.0.0/24) </h2><br>  Es posible que haya notado en el ejemplo <code>busybox</code> que las direcciones IP asignadas a <code>busybox</code> eran diferentes en cada caso.  ¬øQu√© pasar√≠a si quisi√©ramos hacer que estos contenedores est√©n disponibles para la comunicaci√≥n desde otros hogares?  Se podr√≠an tomar las direcciones IP actuales del pod, pero cambiar√°n.  Por esta raz√≥n, debe configurar el recurso del <code>Service</code> , que enviar√° las solicitudes a muchos hogares de corta duraci√≥n. <br><br><blockquote>  "El servicio en Kubernetes es una abstracci√≥n que define el conjunto l√≥gico de hogares y las pol√≠ticas mediante las cuales se puede acceder".  <i>(de la documentaci√≥n de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes Services</a> )</i> </blockquote><br>  Hay varias formas de publicar un servicio;  el tipo predeterminado es <code>ClusterIP</code> , que establece la direcci√≥n IP desde el bloque CIDR del cl√∫ster (es decir, accesible solo desde el cl√∫ster).  Un ejemplo de ello es el complemento DNS Cluster configurado en Kubernetes The Hard Way. <br><br><pre> <code class="plaintext hljs"># ... apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS" spec: selector: k8s-app: kube-dns clusterIP: 10.32.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP # ...</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><code>kube-dns.yaml</code></a> ) <br><br>  <code>kubectl</code> muestra que el <code>Service</code> recuerda puntos finales y los traduce: <br><br><pre> <code class="bash hljs">$ kubectl -n kube-system describe services ... Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.32.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 10.200.0.27:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 10.200.0.27:53 ...</code> </pre> <br>  ¬øC√≥mo exactamente? .. <code>iptables</code> nuevamente.  Veamos las reglas creadas para este ejemplo.  Su lista completa se puede ver con el comando <code>iptables-save</code> . <br><br>  Tan pronto como los paquetes son creados por el proceso ( <code>OUTPUT</code> ) o llegan a la interfaz de red ( <code>PREROUTING</code> ), pasan a trav√©s de las siguientes cadenas de <code>iptables</code> : <br><br><pre> <code class="bash hljs">-A PREROUTING -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES -A OUTPUT -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES</code> </pre> <br>  Los siguientes objetivos corresponden a paquetes TCP enviados al puerto 53 en 10.32.0.10, y se transmiten al destinatario 10.200.0.27 con el puerto 53: <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp cluster IP"</span></span> -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -j KUBE-SEP-32LPCMGYG6ODGN3H -A KUBE-SEP-32LPCMGYG6ODGN3H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -m tcp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  Lo mismo para los paquetes UDP (destinatario 10.32.0.10:53 ‚Üí 10.200.0.27:53): <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns cluster IP"</span></span> -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -j KUBE-SEP-LRUTK6XRXU43VLIG -A KUBE-SEP-LRUTK6XRXU43VLIG -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -m udp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  Hay otros tipos de <code>Services</code> en Kubernetes.  En particular, Kubernetes The Hard Way <code>NodePort</code> sobre <code>NodePort</code> ; consulte <a href="">Prueba de humo: Servicios</a> . <br><br><pre> <code class="bash hljs">kubectl expose deployment nginx --port 80 --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> NodePort</code> </pre> <br>  <code>NodePort</code> publica el servicio en la direcci√≥n IP de cada nodo, coloc√°ndolo en un puerto est√°tico (se llama <code>NodePort</code> ).  <code>NodePort</code> puede acceder desde fuera del cl√∫ster.  Puede verificar el puerto dedicado (en este caso - 31088) usando <code>kubectl</code> : <br><br><pre> <code class="bash hljs">$ kubectl describe services nginx ... Type: NodePort IP: 10.32.0.53 Port: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 80/TCP TargetPort: 80/TCP NodePort: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 31088/TCP Endpoints: 10.200.1.18:80 ...</code> </pre> <br>  Under ahora est√° disponible en Internet como <code>http://${EXTERNAL_IP}:31088/</code> .  Aqu√≠ <code>EXTERNAL_IP</code> es la direcci√≥n IP p√∫blica de <b>cualquier instancia de trabajo</b> .  En este ejemplo, utilic√© la direcci√≥n IP p√∫blica de <b>trabajador-0</b> .  La solicitud es recibida por un host con una direcci√≥n IP interna de 10.240.0.20 (el proveedor de la nube est√° comprometido en NAT p√∫blica), sin embargo, el servicio se inicia realmente en otro host ( <b>trabajador-1</b> , que puede verse en la direcci√≥n IP del punto final - 10.200.1.18): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 31088 tcp 6 86397 ESTABLISHED src=173.38.XXX.XXX dst=10.240.0.20 sport=30303 dport=31088 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=30303 [ASSURED] mark=0 use=1</code> </pre> <br>  El paquete se env√≠a desde <b>trabajador-0</b> a <b>trabajador-1</b> , donde encuentra su destinatario: <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 80 tcp 6 86392 ESTABLISHED src=10.240.0.20 dst=10.200.1.18 sport=14802 dport=80 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=14802 [ASSURED] mark=0 use=1</code> </pre> <br>  ¬øTal circuito es ideal?  Quiz√°s no, pero funciona.  En este caso, las reglas de <code>iptables</code> programadas son las siguientes: <br><br><pre> <code class="bash hljs">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp --dport 31088 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -j KUBE-SEP-UGTFMET44DQG7H7H -A KUBE-SEP-UGTFMET44DQG7H7H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp -j DNAT --to-destination 10.200.1.18:80</code> </pre> <br>  En otras palabras, la direcci√≥n del destinatario de los paquetes con el puerto 31088 se transmite el 10.200.1.18.  El puerto tambi√©n est√° transmitiendo, desde 31088 a 80. <br><br>  No mencionamos otro tipo de servicio, <code>LoadBalancer</code> , que hace que el servicio est√© disponible p√∫blicamente utilizando un equilibrador de carga del proveedor de la nube, pero el art√≠culo ya result√≥ ser extenso. <br><br><h2>  Conclusi√≥n </h2><br>  Puede parecer que hay mucha informaci√≥n, pero solo tocamos la punta del iceberg.  En el futuro voy a hablar sobre IPv6, IPVS, eBPF y un par de complementos CNI actuales e interesantes. <br><br><h2>  PD del traductor </h2><br>  Lea tambi√©n en nuestro blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Gu√≠a ilustrada para la creaci√≥n de redes en Kubernetes</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Comparaci√≥n del rendimiento de la red para Kubernetes</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Experimentos con kube-proxy y la inaccesibilidad del host en Kubernetes</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mejora de la fiabilidad de Kubernetes: c√≥mo notar r√°pidamente que un nodo ha ca√≠do</a> "; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Play with Kubernetes ‚Äî      K8s</a> ¬ª; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">   Kubernetes   </a> ¬ª <i>( ,        Kubernetes)</i> ; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Container Networking Interface (CNI) ‚Äî      Linux-</a> ¬ª. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es420813/">https://habr.com/ru/post/es420813/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es420799/index.html">MPS 2018.2: Pruebas de generador, Complemento GitHub, Aspecto VCS, Notificaciones de migraci√≥n y m√°s</a></li>
<li><a href="../es420803/index.html">Lecciones de impresi√≥n 3D. Ahorro de pl√°stico al imprimir modelos no funcionales desde 3Dtool</a></li>
<li><a href="../es420805/index.html">[Traducci√≥n] Cu√°ndo usar flujos paralelos</a></li>
<li><a href="../es420809/index.html">Semana de la seguridad 31: Cincuenta sombras de inseguridad en Android</a></li>
<li><a href="../es420811/index.html">Nueva generaci√≥n de mensajer√≠a descentralizada y red telef√≥nica</a></li>
<li><a href="../es420815/index.html">C√≥mo "decodificar el mundo digital" hizo volar la sala: los 10 mejores informes de DotNext 2018 Piter</a></li>
<li><a href="../es420819/index.html">Las 10 mejores herramientas de Python para aprendizaje autom√°tico y ciencia de datos</a></li>
<li><a href="../es420821/index.html">Regla 10: 1 en programaci√≥n y escritura</a></li>
<li><a href="../es420825/index.html">Hoy ser√° el primer partido entre los profesionales de OpenAI y Dota 2 (la gente gan√≥). Entendemos c√≥mo funciona el bot</a></li>
<li><a href="../es420827/index.html">Cree un proyecto maven simple utilizando Java EE + WildFly10 + JPA (Hibernate) + Postgresql + EJB + IntelliJ IDEA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>