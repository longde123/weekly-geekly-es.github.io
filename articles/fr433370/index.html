<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>â‡ï¸ ğŸ‘µğŸ» ğŸŒ¹ ThÃ©orie du sharding ğŸŒ± ğŸ¤˜ğŸ¿ ğŸ‘¼ğŸ½</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Il semble que nous soyons si profondÃ©ment immergÃ©s dans la jungle du dÃ©veloppement Ã  haute charge que nous ne pensons tout simplement pas aux problÃ¨me...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>ThÃ©orie du sharding</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/433370/">  Il semble que nous soyons si profondÃ©ment immergÃ©s dans la jungle du dÃ©veloppement Ã  haute charge que nous ne pensons tout simplement pas aux problÃ¨mes de base.  Prenez, par exemple, le sharding.  Que comprendre s'il est possible d'Ã©crire conditionnellement shards = n dans les paramÃ¨tres de la base de donnÃ©es et que tout sera fait par lui-mÃªme.  C'est vrai, il l'est, mais si, plutÃ´t, quand quelque chose tourne mal, les ressources commencent vraiment Ã  Ãªtre rares, j'aimerais comprendre quelle est la raison et comment y remÃ©dier. <br><br>  En bref, si vous apportiez votre implÃ©mentation alternative de hachage dans Cassandra, alors il n'y a pratiquement aucune rÃ©vÃ©lation pour vous.  Mais si la charge de vos services arrive dÃ©jÃ  et que la connaissance du systÃ¨me ne suit pas, vous Ãªtes le bienvenu.  Le grand et terrible <strong>Andrei Aksyonov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">shodan</a> ) Ã  sa maniÃ¨re habituelle dira que le <strong>partage est mauvais, pas le partage est Ã©galement mauvais</strong> , et comment il est organisÃ© Ã  l'intÃ©rieur.  Et tout Ã  fait par accident, l'une des parties de l'histoire du sharding ne concerne pas vraiment le sharding du tout, mais le diable sait quoi - comment mapper des objets sur des fragments. <br><img src="https://habrastorage.org/webt/c9/ju/s6/c9jus6tadexnz4aih4q95bl7ega.jpeg"><br>  La photo des phoques (mÃªme s'ils se sont avÃ©rÃ©s accidentellement Ãªtre des chiots) semble dÃ©jÃ  rÃ©pondre Ã  la question de savoir pourquoi c'est tout, mais commenÃ§ons dans l'ordre. <br><a name="habracut"></a><br><h2>  Qu'est-ce que le sharding? <br></h2><br>  Si vous google persistante, il se trouve qu'il existe une frontiÃ¨re assez floue entre le soi-disant partitionnement et le soi-disant sharding.  Tout le monde appelle tout ce qu'il veut que ce qu'il veut.  Certaines personnes distinguent le partitionnement horizontal et le partitionnement.  D'autres disent que le sharding est un certain type de partitionnement horizontal. <br><br>  Je n'ai trouvÃ© aucune norme terminologique qui serait approuvÃ©e par les pÃ¨res fondateurs et certifiÃ©e ISO.  Une croyance intÃ©rieure personnelle est quelque chose comme ceci: Le <strong>partitionnement,</strong> en moyenne, Â«coupe la base en morceauxÂ» de maniÃ¨re arbitraire. <br><br><ul><li>  Cloisonnement <strong>vertical</strong>  Par exemple, il y a un tableau gÃ©ant avec quelques milliards d'entrÃ©es dans 60 colonnes.  Au lieu de tenir une telle table gigantesque, nous gardons 60 tables non moins gigantesques avec 2 milliards d'enregistrements chacune - et ce n'est pas une base de donnÃ©es Ã  temps partiel, mais un partitionnement vertical (comme un exemple de terminologie). <br></li><li>  Partitionnement <strong>horizontal</strong> - nous coupons ligne par ligne, peut-Ãªtre Ã  l'intÃ©rieur du serveur. <br></li></ul><br>  Le moment gÃªnant ici est la diffÃ©rence subtile entre le partitionnement horizontal et le sharding.  Vous pouvez me couper en morceaux, mais je ne vous dirai pas avec certitude de quoi il s'agit.  On a le sentiment que le partage et le partitionnement horizontal sont Ã  peu prÃ¨s la mÃªme chose. <br><br>  Le sharding est en gÃ©nÃ©ral lorsqu'une grande table en termes de bases de donnÃ©es ou d'une collection de documents, d'objets, si vous n'avez pas de base de donnÃ©es, mais un magasin de documents, est coupÃ©e spÃ©cifiquement pour les objets.  Autrement dit, des piÃ¨ces de 2 milliards d'objets sont sÃ©lectionnÃ©es, quelle que soit leur taille.  Les objets eux-mÃªmes Ã  l'intÃ©rieur de chaque objet ne sont pas coupÃ©s en morceaux, nous ne nous dÃ©composons pas en colonnes distinctes, Ã  savoir, nous disposons des faisceaux Ã  diffÃ©rents endroits. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/xx_Lv1P_X_I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien</a> vers la prÃ©sentation pour l'exhaustivitÃ©.</i> <br><br>  De subtiles diffÃ©rences terminologiques ont dÃ©jÃ  persistÃ©.  Par exemple, relativement parlant, les dÃ©veloppeurs de Postgres peuvent dire que le partitionnement horizontal se produit lorsque toutes les tables dans lesquelles la table principale est divisÃ©e se trouvent dans le mÃªme schÃ©ma, et lorsque sur des machines diffÃ©rentes, il est partitionnÃ©. <br><br>  Dans un sens gÃ©nÃ©ral, sans Ãªtre liÃ© Ã  la terminologie d'une base de donnÃ©es spÃ©cifique et d'un systÃ¨me de gestion de donnÃ©es spÃ©cifique, on a le sentiment que le sharding ne fait que trancher ligne par ligne et ainsi de suite - et c'est tout: <br><br><blockquote>  Partitionnement (~ =, \ in ...) Le partitionnement horizontal == est typique. <br></blockquote><br>  J'insiste, gÃ©nÃ©ralement.  En ce sens que nous faisons tout cela non seulement pour couper 2 milliards de documents en 20 tableaux, chacun Ã©tant plus facile Ã  gÃ©rer, mais pour le distribuer dans de nombreux cÅ“urs, de nombreux disques ou de nombreux serveurs physiques ou virtuels diffÃ©rents . <br><br>  Il est entendu que nous procÃ©dons de telle sorte que chaque fragment - chaque shatka de donnÃ©es - soit rÃ©pliquÃ© plusieurs fois.  Mais en fait, non. <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs00 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">0</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs15 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">15</span></span></code> </pre> <br>  En fait, si vous effectuez un tel dÃ©coupage de donnÃ©es, et Ã  partir d'une table SQL gÃ©ante sur MySQL, vous gÃ©nÃ©rerez 16 petites tables sur votre vaillant ordinateur portable, sans aller au-delÃ  d'un seul ordinateur portable, pas d'un seul schÃ©ma, pas d'une seule base de donnÃ©es, etc.  etc.  - tout, vous avez dÃ©jÃ  du sharding. <br><br>  En se souvenant de l'illustration avec des chiots, cela conduit Ã  ce qui suit: <br><br><ul><li>  La bande passante augmente. <br></li><li>  La latence ne change pas, c'est-Ã -dire que chacun, pour ainsi dire, travailleur ou consommateur dans ce cas, obtient le sien.  On ne sait pas ce que les chiots obtiennent sur la photo, mais les demandes sont servies Ã  peu prÃ¨s en mÃªme temps, comme si le chiot Ã©tait seul. </li><li>  Ou Ã  la fois cela, et un autre, et toujours une haute disponibilitÃ© (rÃ©plication). <br></li></ul><br>  <strong>Pourquoi la bande passante?</strong>  Parfois, nous pouvons avoir de tels volumes de donnÃ©es qui ne correspondent pas - on ne sait pas oÃ¹, mais ils ne correspondent pas - par 1 {core |  conduire |  serveur |  ...}.  Il n'y a tout simplement pas assez de ressources et c'est tout.  Pour travailler avec ce grand ensemble de donnÃ©es, vous devez le couper. <br><br>  <strong>Pourquoi la latence?</strong>  Sur un cÅ“ur, l'analyse d'une table de 2 milliards de lignes est 20 fois plus lente que l'analyse de 20 tables sur 20 noyaux, en parallÃ¨le.  Les donnÃ©es sont traitÃ©es trop lentement sur une seule ressource. <br><br>  <strong>Pourquoi une haute disponibilitÃ©?</strong>  Ou nous coupons les donnÃ©es afin de faire l'une et l'autre en mÃªme temps, et en mÃªme temps plusieurs copies de chaque fragment - la rÃ©plication fournit une haute disponibilitÃ©. <br><br><h2>  Un exemple simple de "comment le faire avec vos mains" <br></h2><br>  Le partitionnement conditionnel peut Ãªtre dÃ©coupÃ© en utilisant la table de test test.documents pour 32 documents, et en gÃ©nÃ©rant Ã  partir de cette table 16 tables de test pour environ 2 documents test.docs00, 01, 02, ..., 15 chacun. <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs00 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">0</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs15 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">15</span></span></code> </pre><br>  Pourquoi?  Parce que a priori nous ne savons pas comment est distribuÃ© id, si de 1 Ã  32 inclus, alors il y aura exactement 2 documents chacun, sinon non. <br><br>  <strong>Nous faisons cela pour quoi.</strong>  AprÃ¨s avoir fait 16 tableaux, nous pouvons Â«attraperÂ» 16 de ce dont nous avons besoin.  IndÃ©pendamment de ce sur quoi nous nous appuyions, nous pouvons parallÃ©liser ces ressources.  Par exemple, s'il n'y a pas assez d'espace disque, il sera judicieux de dÃ©composer ces tables en disques distincts. <br><br>  Tout cela, malheureusement, n'est pas gratuit.  Je soupÃ§onne que dans le cas du standard SQL canonique (je n'ai pas relu le standard SQL depuis longtemps, il n'a peut-Ãªtre pas Ã©tÃ© mis Ã  jour depuis longtemps), il n'y a pas de syntaxe standardisÃ©e officielle pour dire Ã  un serveur SQL: Â«Cher serveur SQL, faites-moi 32 fragments et les mettre sur 4 disques. "  Mais dans les implÃ©mentations individuelles, il existe souvent une syntaxe spÃ©cifique afin de faire la mÃªme chose en principe.  PostgreSQL a des mÃ©canismes de partitionnement, MySQL MariaDB l'a, Oracle a probablement fait tout cela il y a longtemps. <br><br>  NÃ©anmoins, si nous le faisons Ã  la main, sans support de base de donnÃ©es et dans le cadre de la norme, nous <strong>payons conditionnellement la complexitÃ© de l'accÃ¨s aux donnÃ©es</strong> .  LÃ  oÃ¹ il y avait un simple SELECT * FROM documents WHERE id = 123, maintenant 16 x SELECT * FROM docsXX.  Et bien, si on essayait d'obtenir le record par clÃ©.  Significativement plus intÃ©ressant si nous essayions d'obtenir une premiÃ¨re gamme de disques.  Maintenant (si, je le souligne, comme si c'Ã©tait un imbÃ©cile, et que je reste dans la norme), les rÃ©sultats de ces 16 SELECT * FROM devront Ãªtre combinÃ©s dans l'application. <br><br>  <strong>Quel changement de performance attendre?</strong> <br><br><ul><li>  Intuitivement linÃ©aire. </li><li>  ThÃ©oriquement - sublinÃ©aire, parce que la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">loi d'Amdahl</a> . </li><li>  En pratique - peut-Ãªtre presque linÃ©airement, peut-Ãªtre pas. </li></ul><br>  En fait, la bonne rÃ©ponse est inconnue.  En appliquant intelligemment la technique de sharding, vous pouvez obtenir une dÃ©tÃ©rioration super-linÃ©aire significative des performances de votre application, et mÃªme le DBA fonctionnera avec un poker chaud. <br><br>  Voyons comment cela peut Ãªtre rÃ©alisÃ©.  Il est clair que simplement dÃ©finir le paramÃ¨tre sur les fragments PostgreSQL = 16, puis il s'est dÃ©collÃ© - ce n'est pas intÃ©ressant.  RÃ©flÃ©chissons Ã  la faÃ§on dont nous pourrions rÃ©aliser que <em>nous ralentirions Ã  32 fois aprÃ¨s le partage</em> , ce qui est intÃ©ressant du point de vue de la faÃ§on de ne pas le faire. <br><br>  Nos tentatives d'accÃ©lÃ©ration ou de ralentissement reposeront toujours contre les classiques - la bonne vieille loi d'Amdahl, qui dit qu'il n'y a pas de parallÃ©lisation parfaite de toute demande, il y a toujours une partie cohÃ©rente. <br><br><h2>  Loi d'Amdahl <br></h2><br><blockquote>  <strong><em>Il</em></strong> y a <strong><em>toujours</em></strong> une partie sÃ©rialisÃ©e. <br></blockquote><br>  Il y a toujours une partie de l'exÃ©cution de la requÃªte qui est parallÃ©lisÃ©e, et il y a toujours une partie qui n'est pas parallÃ¨le.  MÃªme s'il vous semble qu'une requÃªte parfaitement parallÃ¨le, collectant au moins une ligne du rÃ©sultat que vous allez envoyer au client, Ã  partir des lignes reÃ§ues de chaque fragment, il y en a toujours, et elle est toujours cohÃ©rente. <br><br>  Il y a toujours une sorte de partie sÃ©quentielle.  Il peut Ãªtre minuscule, absolument invisible dans le contexte gÃ©nÃ©ral, il peut Ãªtre gigantesque et, par consÃ©quent, affecter fortement la parallÃ©lisation, mais il est toujours lÃ . <br><br>  De plus, son influence est en <strong><em>train de changer</em></strong> et peut augmenter considÃ©rablement, par exemple, si nous rÃ©duisons notre table - augmentons les taux - de 64 enregistrements Ã  16 tables de 4 enregistrements, cette partie changera.  Bien sÃ»r, Ã  en juger par ces quantitÃ©s gigantesques de donnÃ©es, nous travaillons sur un tÃ©lÃ©phone mobile et un processeur Ã  86 MHz, nous n'avons pas assez de fichiers qui peuvent Ãªtre ouverts en mÃªme temps.  Apparemment, avec une telle entrÃ©e, nous ouvrons un fichier Ã  la fois. <br><br><ul><li>  C'Ã©tait <strong>Total =</strong> <strong>Serial +</strong> <strong>Parallel</strong> .  Par exemple, oÃ¹ parallÃ¨le est tout le travail Ã  l'intÃ©rieur de la base de donnÃ©es et sÃ©rie envoie le rÃ©sultat au client. <br></li><li>  Il est devenu <strong>Total2 = Serial + Parallel / N + Xserial.</strong>  Par exemple, lorsque le gÃ©nÃ©ral ORDER BY, Xserial&gt; 0. <br></li></ul><br>  Avec cet exemple simple, j'essaie de montrer que certains Xserial apparaissent.  En plus du fait qu'il y a toujours une partie sÃ©rialisÃ©e, et du fait que nous essayons de travailler avec des donnÃ©es en parallÃ¨le, une partie supplÃ©mentaire apparaÃ®t pour assurer ce dÃ©coupage des donnÃ©es.  En gros, nous pouvons avoir besoin de: <br><br><ul><li>  trouver ces 16 tables dans le dictionnaire de base de donnÃ©es interne; </li><li>  ouvrir des fichiers; </li><li>  allouer de la mÃ©moire; </li><li>  dÃ©placer la mÃ©moire; </li><li>  tacher les rÃ©sultats; </li><li>  synchroniser entre les cÅ“urs; </li></ul><br>  Tous les effets dÃ©synchronisÃ©s apparaissent toujours.  Ils peuvent Ãªtre insignifiants et occuper un milliardiÃ¨me du temps total, mais ils sont toujours non nuls et existent toujours.  Avec leur aide, nous pouvons considÃ©rablement perdre en productivitÃ© aprÃ¨s le sharding. <br><br><img src="https://habrastorage.org/webt/fh/mx/yh/fhmxyh9tozfrbd4yszxj2va9a1g.jpeg"><br><br>  Ceci est une image standard de la loi d'Amdahl.  Ce n'est pas trÃ¨s lisible, mais il est important que les lignes, qui devraient idÃ©alement Ãªtre droites et croÃ®tre linÃ©airement, butent sur l'asymptote.  Mais comme le graphique sur Internet est illisible, j'ai fait, Ã  mon avis, des tableaux plus visuels avec des chiffres. <br><br>  Supposons que nous ayons une partie sÃ©rialisÃ©e du traitement de la demande, ce qui ne prend que 5%: <strong>sÃ©rie = 0,05 = 1/20.</strong> <br><br>  Intuitivement, il semblerait qu'avec la partie sÃ©rialisÃ©e, qui ne prend que 1/20 du traitement de la demande, si nous parallÃ©lisons le traitement de la demande Ã  20 cÅ“urs, elle deviendra environ 20, dans le pire des cas, 18 fois plus rapide. <br><br>  En fait, les <b>mathÃ©matiques sont une chose sans cÅ“ur</b> : <br><br> <code>wall = 0.05 + 0.95/num_cores, speedup = 1 / (0.05 + 0.95/num_cores)</code> <br> <br>  Il s'avÃ¨re que si vous calculez soigneusement, avec une partie sÃ©rialisÃ©e de 5%, l'accÃ©lÃ©ration sera 10 fois (10,3), et c'est 51% par rapport Ã  l'idÃ©al thÃ©orique. <br><br><table><tbody><tr><td>  8 noyaux </td><td>  = 5,9 </td><td>  <font color="#c45911">= 74%</font> </td></tr><tr><td>  10 cÅ“urs </td><td>  = 6,9 </td><td>  <font color="#c45911">= 69%</font> </td></tr><tr><td>  <strong>20 noyaux</strong> </td><td>  <strong>= 10,3</strong> </td><td>  <strong><font color="#c45911">= 51%</font></strong> </td></tr><tr><td>  40 noyaux </td><td>  = 13,6 </td><td>  <font color="#ff0000">= 34%</font> </td></tr><tr><td>  128 cÅ“urs </td><td>  = 17,4 </td><td>  <font color="#ff0000">= 14%</font> </td></tr></tbody></table><br>  En utilisant 20 cÅ“urs (20 disques, si vous le souhaitez) pour la tÃ¢che sur laquelle on a travaillÃ© auparavant, nous n'obtiendrons jamais thÃ©oriquement une accÃ©lÃ©ration plus de 20 fois, mais pratiquement beaucoup moins.  De plus, avec une augmentation du nombre de parallÃ¨les, l'inefficacitÃ© augmente rapidement. <br><br>  Lorsqu'il ne reste que 1% du travail sÃ©rialisÃ© et 99% en parallÃ¨le, les valeurs d'accÃ©lÃ©ration sont quelque peu amÃ©liorÃ©es: <br><br><table><tbody><tr><td>  8 noyaux </td><td>  = 7,5 </td><td>  <font color="#538135">= 93%</font> </td></tr><tr><td>  16 cÅ“urs </td><td>  = 13,9 </td><td>  <font color="#538135">= 87%</font> </td></tr><tr><td>  32 coeurs </td><td>  = 24,4 </td><td>  <font color="#c45911">= 76%</font> </td></tr><tr><td>  64 coeurs </td><td>  = 39,3 </td><td>  <font color="#c45911">= 61%</font> </td></tr></tbody></table><br>  Pour une requÃªte complÃ¨tement thermonuclÃ©aire, qui dure naturellement des heures, et les travaux prÃ©paratoires et l'assemblage du rÃ©sultat prennent trÃ¨s peu de temps (sÃ©rie = 0,001), on verra dÃ©jÃ  une bonne efficacitÃ©: <br><br><table><tbody><tr><td>  8 noyaux </td><td>  = 7,94 </td><td>  <font color="#538135">= 99%</font> </td></tr><tr><td>  16 cÅ“urs </td><td>  = 15,76 </td><td>  <font color="#538135">= 99%</font> </td></tr><tr><td>  32 coeurs </td><td>  = 31,04 </td><td>  <font color="#538135">= 97%</font> </td></tr><tr><td>  64 coeurs </td><td>  = 60,20 </td><td>  <font color="#538135">= 94%</font> </td></tr></tbody></table><br>  Veuillez noter que <strong>nous ne verrons jamais 100%</strong> .  Dans des cas particuliÃ¨rement bons, vous pouvez voir, par exemple, 99,999%, mais pas exactement 100%. <br><br><h2>  Comment mÃ©langer et casser en N fois? <br></h2><br>  Vous pouvez mÃ©langer et casser exactement N fois: <br><br><ol><li>  Envoyez les demandes docs00 ... docs15 de <strong>maniÃ¨re sÃ©quentielle</strong> , pas en parallÃ¨le. </li><li>  Dans les requÃªtes simples, ne sÃ©lectionnez <strong>pas</strong> <strong>par clÃ©</strong> , O something quelque chose = 234. </li></ol><br>  Dans ce cas, la partie sÃ©rialisÃ©e (sÃ©rie) occupe non pas 1% et non 5%, mais environ 20% dans les bases de donnÃ©es modernes.  Vous pouvez obtenir 50% de la partie sÃ©rialisÃ©e si vous accÃ©dez Ã  la base de donnÃ©es Ã  l'aide d'un protocole binaire extrÃªmement efficace ou si vous la liez en tant que bibliothÃ¨que dynamique Ã  un script Python. <br><br>  Le reste du temps de traitement d'une simple demande sera occupÃ© par des opÃ©rations non parallÃ©lisÃ©es d'analyse de la demande, de prÃ©paration du plan, etc.  Autrement dit, il ralentit de ne pas lire le dossier. <br><br>  Si nous divisons les donnÃ©es en 16 tables et les exÃ©cutons sÃ©quentiellement, comme c'est le cas dans le langage de programmation PHP, par exemple, (il ne sait pas trÃ¨s bien comment exÃ©cuter les processus asynchrones), nous obtenons le ralentissement de 16 fois.  Et peut-Ãªtre mÃªme davantage, car des allers-retours sur le rÃ©seau seront Ã©galement ajoutÃ©s. <br><br><blockquote>  Du coup lors du sharding, le choix d'un langage de programmation est important. <br></blockquote><br>  Nous nous souvenons du choix d'un langage de programmation, car si vous envoyez des requÃªtes Ã  la base de donnÃ©es (ou au serveur de recherche) sÃ©quentiellement, d'oÃ¹ vient l'accÃ©lÃ©ration?  Au contraire, un ralentissement apparaÃ®tra. <br><br><h3>  VÃ©lo de la vie <br></h3><br>  Si vous choisissez C ++, <strong>Ã©crivez dans les threads POSIX</strong> , pas Boost I / O.  J'ai vu une excellente bibliothÃ¨que de dÃ©veloppeurs expÃ©rimentÃ©s d'Oracle et MySQL lui-mÃªme, qui ont Ã©crit la communication avec le serveur MySQL sur Boost.  Apparemment, ils ont Ã©tÃ© forcÃ©s d'Ã©crire en C pur au travail, mais ils ont ensuite rÃ©ussi Ã  faire demi-tour, Ã  prendre Boost avec des E / S asynchrones, etc.  Un problÃ¨me - cette E / S asynchrone, qui thÃ©oriquement aurait dÃ» conduire 10 demandes en parallÃ¨le, pour une raison quelconque, avait un point de synchronisation invisible Ã  l'intÃ©rieur.  Lors du dÃ©marrage de 10 requÃªtes en parallÃ¨le, elles ont Ã©tÃ© exÃ©cutÃ©es exactement 20 fois plus lentement qu'une, car 10 fois pour les requÃªtes elles-mÃªmes et une fois pour le point de synchronisation. <br><br>  <strong>Conclusion:</strong> Ã©crivez dans des langages qui implÃ©mentent l'exÃ©cution parallÃ¨le et attendent bien les diffÃ©rentes requÃªtes.  Pour Ãªtre honnÃªte, je ne sais pas exactement quoi conseiller Ã  part Go.  Non seulement parce que j'aime vraiment Go, mais parce que je ne sais rien de mieux. <br><br>  <strong>N'Ã©crivez pas dans des langues inappropriÃ©es</strong> dans lesquelles vous ne pouvez pas exÃ©cuter 20 requÃªtes parallÃ¨les dans la base de donnÃ©es.  Ou Ã  chaque occasion, ne faites pas tout avec vos mains - comprenez comment cela fonctionne, mais ne le faites pas manuellement. <br><br><h2>  VÃ©lo d'essai A / B <br></h2><br>  Parfois, vous pouvez ralentir parce que vous Ãªtes habituÃ© au fait que tout fonctionne et que vous nâ€™avez pas remarquÃ© que la partie sÃ©rialisÃ©e, premiÃ¨rement, est, deuxiÃ¨mement, une grosse. <br><br><ul><li>  ImmÃ©diatement ~ 60 fragments d'index de recherche, catÃ©gories </li><li>  Ce sont des fragments corrects et corrects, sous un domaine. </li><li>  Il y avait jusqu'Ã  1 000 documents et 50 000 documents. </li></ul><br>  Il s'agit d'un vÃ©lo de production, lorsque les requÃªtes de recherche ont Ã©tÃ© lÃ©gÃ¨rement modifiÃ©es et ils ont commencÃ© Ã  sÃ©lectionner beaucoup plus de documents parmi 60 fragments de l'index de recherche.  Tout a fonctionnÃ© rapidement et sur le principe: "Ã§a marche - ne le touchez pas", ils l'ont tous oubliÃ©, ce qui est en fait Ã  l'intÃ©rieur de 60 Ã©clats.  Nous avons augmentÃ© la limite d'Ã©chantillonnage pour chaque fragment de mille Ã  50 000 documents.  Soudain, il a commencÃ© Ã  ralentir et le parallÃ©lisme a cessÃ©.  Les demandes elles-mÃªmes, qui ont Ã©tÃ© exÃ©cutÃ©es en fonction des tessons, ont plutÃ´t bien volÃ© et l'Ã©tape a Ã©tÃ© ralentie lorsque 50 000 documents ont Ã©tÃ© collectÃ©s auprÃ¨s de 60 tessons.  Ces 3 millions de documents finaux sur un core ont fusionnÃ©, triÃ©s, le top 3 millions a Ã©tÃ© sÃ©lectionnÃ© et remis au client.  La mÃªme partie en sÃ©rie a ralenti, la mÃªme loi impitoyable d'Amdal a fonctionnÃ©. <br><br>  <em>Alors peut-Ãªtre que tu ne devrais pas faire du sharding avec tes mains, mais juste humainement</em> <em><br></em>  <em>dites Ã  la base de donnÃ©es: "Faites-le!"</em> <em><br></em> <br>  <strong>Avertissement:</strong> je ne sais pas vraiment comment faire quelque chose de bien.  Je suis du mauvais Ã©tage !!! <br><br>  J'ai fait la promotion d'une religion appelÃ©e Â«fondamentalisme algorithmiqueÂ» tout au long de ma vie consciente.  Il est briÃ¨vement formulÃ© trÃ¨s simplement: <br><br><blockquote>  Vous ne voulez vraiment rien faire avec vos mains, mais il est extrÃªmement utile de savoir comment cela est organisÃ© Ã  l'intÃ©rieur.  De sorte qu'au moment oÃ¹ quelque chose ne va pas dans la base de donnÃ©es, vous comprenez au moins ce qui s'est mal passÃ© lÃ -bas, comment il est organisÃ© Ã  l'intÃ©rieur et Ã  peu prÃ¨s comment il peut Ãªtre rÃ©parÃ©. <br></blockquote><br>  Examinons les options: <br><br><ol><li>  <strong>"Mains</strong> . <strong>"</strong>  Plus tÃ´t, nous avons fragmentÃ© manuellement les donnÃ©es en 16 tables virtuelles et rÃ©Ã©crit toutes les requÃªtes avec nos mains - c'est extrÃªmement inconfortable Ã  faire.  <strong>S'il est possible de ne pas mÃ©langer les mains - ne mÃ©langez pas les mains!</strong>  Mais parfois, ce n'est pas possible, par exemple, vous avez MySQL 3.23, puis vous devez. </li><li>  <strong>"Automatique".</strong>  Il arrive que vous puissiez mÃ©langer automatiquement ou presque automatiquement, lorsque la base de donnÃ©es peut distribuer les donnÃ©es elle-mÃªme, il vous suffit d'Ã©crire Ã  peu prÃ¨s quelque part un paramÃ¨tre spÃ©cifique.  Il y a beaucoup de bases et elles ont beaucoup de rÃ©glages diffÃ©rents.  Je suis sÃ»r que dans chaque base de donnÃ©es dans laquelle il est possible d'Ã©crire shards = 16 (quelle que soit la syntaxe), de nombreux autres paramÃ¨tres sont collÃ©s Ã  ce cas par le moteur. </li><li>  <strong>"Semi-automatique"</strong> - un mode complÃ¨tement cosmique, Ã  mon avis, et brutal.  Autrement dit, la base elle-mÃªme ne semble pas pouvoir le faire, mais il existe des correctifs supplÃ©mentaires externes. </li></ol><br>  Il est difficile de dire quelque chose sur la machine, sauf pour l'envoyer Ã  la documentation sur la base de donnÃ©es appropriÃ©e (MongoDB, Elastic, Cassandra, ... en gÃ©nÃ©ral, le soi-disant NoSQL).  Si vous Ãªtes chanceux, alors vous tirez simplement sur l'interrupteur Â«faites-moi 16 Ã©clatsÂ» et tout fonctionnera.  Ã€ ce moment, quand cela ne fonctionne pas, le reste de l'article peut Ãªtre nÃ©cessaire. <br><br><h2>  Ã€ propos du dispositif semi-automatique <br></h2><br>  Dans certains endroits, les technologies de l'information sophistiquÃ©es inspirent l'horreur chthonique.  Par exemple, MySQL prÃªt Ã  l'emploi n'avait aucune implÃ©mentation de partitionnement vers certaines versions, bien sÃ»r, nÃ©anmoins, la taille des bases utilisÃ©es au combat atteignait des valeurs indÃ©centes. <br><br>  La souffrance de l'humanitÃ© face aux administrateurs de base de donnÃ©es individuels est tourmentÃ©e depuis des annÃ©es et Ã©crit plusieurs mauvaises solutions de partitionnement conÃ§ues sans raison.  AprÃ¨s cela, une solution de sharding plus ou moins dÃ©cente est Ã©crite appelÃ©e ProxySQL (MariaDB / Spider, PG / pg_shard / Citus, ...).  Ceci est un exemple bien connu de ce mÃªme manteau. <br><br>  ProxySQL dans son ensemble, bien sÃ»r, est une solution complÃ¨te de classe entreprise pour l'open source, pour le routage et plus encore.  Mais l'une des tÃ¢ches Ã  rÃ©soudre est le partitionnement d'une base de donnÃ©es, qui en elle-mÃªme ne sait pas comment fragmenter humainement.  Vous voyez, il n'y a pas de commutateur Â«shards = 16Â», soit vous devez rÃ©Ã©crire chaque demande dans l'application, et il y en a beaucoup, soit mettre une couche intermÃ©diaire entre l'application et la base de donnÃ©es qui ressemble Ã : Â«Hmm ... SELECT * FROM documents?  Oui, il doit Ãªtre dÃ©chirÃ© en 16 petits SELECT * FROM server1.document1, SELECT * FROM server2.document2 - Ã  ce serveur avec ce nom d'utilisateur / mot de passe, Ã  ceci avec un autre.  Si on ne rÃ©pond pas, alors ... "etc. <br><br>  Cela peut Ãªtre fait exactement par des correctifs intermÃ©diaires.  Ils sont lÃ©gÃ¨rement infÃ©rieurs Ã  ceux de toutes les bases de donnÃ©es.  Pour PostgreSQL, si je comprends bien, il existe des solutions intÃ©grÃ©es en mÃªme temps (PostgresForeign Data Wrappers, Ã  mon avis, est intÃ©grÃ© Ã  PostgreSQL lui-mÃªme), il existe des correctifs externes. <br><br>  La configuration de chaque correctif spÃ©cifique est un sujet gÃ©ant distinct qui ne rentrera pas dans un seul rapport, nous ne discuterons donc que des concepts de base. <br><br>  Mieux vaut parler un peu de la thÃ©orie du buzz. <br><br><h2>  Une automatisation parfaite absolue? <br></h2><br>  Toute la thÃ©orie du buzz dans le cas du sharding dans cette lettre F (), le principe de base est <strong>toujours</strong> le mÃªme brut: <code>shard_id = F(object).</code> <br><br>  Le sharding, c'est gÃ©nÃ©ralement quoi?  Nous avons 2 milliards d'enregistrements (ou 64).  Nous voulons les diviser en plusieurs morceaux.  Une question inattendue se pose - comment?  Selon quel principe dois-je rÃ©partir mes 2 milliards d'enregistrements (ou 64) sur 16 serveurs Ã  ma disposition? <br><br>  Le mathÃ©maticien latent en nous devrait suggÃ©rer qu'Ã  la fin il y a toujours une certaine fonction magique qui, pour chaque document (objet, ligne, etc.), dÃ©terminera dans quelle piÃ¨ce le mettre. <br><br>  Si nous approfondissons les mathÃ©matiques, cette fonction dÃ©pend toujours non seulement de l'objet lui-mÃªme (la ligne elle-mÃªme), mais Ã©galement de paramÃ¨tres externes tels que le nombre total de fragments.  La fonction, qui pour chaque objet doit indiquer oÃ¹ la placer, ne peut pas renvoyer une valeur de plus qu'il n'y a de serveurs sur le systÃ¨me.  Et les fonctions sont un peu diffÃ©rentes: <br><br><ul><li>  shard_func = <strong>F1</strong> (objet); <br></li><li>  shard_id = <strong>F2</strong> (shard_func, ...); </li><li>  shard_id = <strong>F2</strong> ( <strong>F1</strong> (objet), current_num_shards, ...). </li></ul><br>  Mais plus loin nous ne creuserons pas dans ces jungles de fonctions individuelles, nous parlons juste de ce que sont les fonctions magiques F (). <br><br><h2>  Que sont F ()? <br></h2><br>  Ils peuvent proposer de nombreux mÃ©canismes de mise en Å“uvre diffÃ©rents et trÃ¨s diffÃ©rents.  Exemple de rÃ©sumÃ©: <br><br><ul><li>  F = <strong>rand</strong> ()% nums_shards </li><li>  F = <strong>somehash</strong> ( <strong>object.id</strong> )% num_shards </li><li>  F = object.date% num_shards </li><li>  F = object.user_id% num_shards </li><li>  ... </li><li>  F = shard_table [somehash () | ... object.date | ...] </li></ul><br>  Un fait intÃ©ressant - vous pouvez naturellement disperser toutes les donnÃ©es au hasard - nous jetons l'enregistrement suivant sur un serveur arbitraire, sur un noyau arbitraire, dans une table arbitraire.  Il n'y aura pas beaucoup de bonheur, mais cela fonctionnera. <br><br>  Il existe des mÃ©thodes lÃ©gÃ¨rement plus intelligentes d'escroquerie pour des fonctions de hachage reproductibles ou mÃªme cohÃ©rentes, ou d'escroquerie pour certains attributs.  Passons en revue chaque mÃ©thode. <br><br><h3>  F = rand () <br></h3><br>  La dispersion n'est pas une mÃ©thode trÃ¨s correcte.  Un problÃ¨me: nous avons dispersÃ© nos 2 milliards d'enregistrements pour mille serveurs au hasard, et nous ne savons pas oÃ¹ se trouve l'enregistrement.  Nous devons extraire user_1, mais nous ne savons pas oÃ¹ il se trouve.  Nous allons sur un millier de serveurs et trions tout - d'une maniÃ¨re ou d'une autre, c'est inefficace. <br><br><h3>  F = somehash () <br></h3><br>  RÃ©partissons les utilisateurs de maniÃ¨re adulte: lisez la fonction de hachage reproduite de user_id, prenez le reste de la division par le nombre de serveurs et contactez immÃ©diatement le serveur souhaitÃ©. <br><br>  <em>Pourquoi on fait Ã§a?</em>  <em>Et puis, nous avons une charge Ã©levÃ©e et nous n'introduisons rien dans un seul serveur.</em>  <em>Si elle s'entremÃªlait, la vie serait si simple.</em> <br><br>  Eh bien, la situation s'est dÃ©jÃ  amÃ©liorÃ©e, pour obtenir un enregistrement, nous allons sur un serveur bien connu.  Mais si nous avons une plage de clÃ©s, dans toute cette plage, nous devons trier toutes les valeurs de clÃ© et, dans la limite, aller soit Ã  autant de fragments que nous avons de clÃ©s dans la plage, soit Ã  chaque serveur en gÃ©nÃ©ral.  Bien sÃ»r, la situation s'est amÃ©liorÃ©e, mais pas pour toutes les demandes.  Certaines demandes ont Ã©tÃ© affectÃ©es. <br><br><h3>  Partage naturel (F = object.date% num_shards) <br></h3><br>  Parfois, c'est souvent 95% du trafic et 95% de la charge sont des demandes qui ont une sorte de partage naturel. , 95%  -       1 , 3 , 7 ,   5%     .  95% ,  ,    ,        . <br><br>        , ,   ,         -           . <br><br>   â€”        ,      .       ,    , , ,    .        5 %  . <br><br>       ,    : <br><br><ol><li>      ,  95%     . </li><li>  95%    ,       ,     .   ,           .     ,     . </li></ol><br>  ,      â€”    ,         - . <br><br>   ,   ,         ,     ,         .       Â«   -      Â». <br><br> <strong>     Â«Â».</strong> ,            . <br><br><h3> 1.  :   <br></h3><br>    ,      ,  . <br><br><ul><li>    ,   ! </li><li> <strong><em></em></strong>  () . </li></ul><br>   , /  , ,  , PM    (       ,  PM   ),     .     . <br><br>  ,    .      ,       ,    100   .        . <br><br>   ,  ,   ,            ,    - . <br><br><h3> 2. Â«Â» : , join <br></h3><br>   ,             ? <br><br><ul><li>  Â«Â» â€¦ WHERE randcol BETWEEN aaa AND bbb? <br></li><li>  Â«Â» â€¦ users_32shards JOIN posts_1024 shards? </li></ul><br>  : , ! <br><br>           ,    ,       ,           .      .       (, , document store    ),     ,     . <br><br>   â€” <strong>-       </strong> .     .  ,          .     ,       ,    ,   .       - , ,         ,   ,         â€”    . <br><br>       ,             . <br><br><h3> 3. / :  <br></h3><br> :         ,          . <br><br><blockquote>    ,   . <br></blockquote><br>      ,  , ,  .     ,     ,   ,    10 , -        30,       100   .    .          â€”       ,  -   â€”  , -  . <br><br> ,      :  16 -,  32. ,   17,  23 â€”    .      ,  ,    -  ? <br><br>  : ,    ,     . <br><br>  ,    Â«Â»,   Â« Â». <br><br><h4>   #1.   <br></h4><br><ul><li>     NewF(object),    . </li><li>   NewF()=OldF() . </li><li>   <strong> .</strong> </li><li>  Ouch. </li></ul><br>  ,    2       ,  ,  .   :  17 ,  6   ,  2  ,    17   23 .   10  , ,    .      . <br><br><h4>   #2.   <br></h4><br>    â€”       â€”  17    23,     16   32 !         ,        . <br><br><ul><li>     NewF(object),    . </li><li> <strong>  2^N,   2^(N+1) .</strong> </li><li>   NewF()=OldF()  0,5. </li><li>   50% . </li><li> ,   <strong>   .</strong> </li></ul><br>  ,  ,         .   ,   ,  . <br><br>  ,            .   ,  16     16,      â€”    . <br><br> ,        â€”     . <br><br><h4>  #3. Consistent hashing <br></h4><br> ,       consistent hashing <br><img src="https://habrastorage.org/webt/il/ml/rt/ilmlrt9xy-c3wuyfaafntagufay.jpeg"><br><br>   Â«consistent hashingÂ»,    ,    . <br><br> :    ()   ,      .    ,     ,  ,      (  ,     ), . <br><br><ul><li>   :  <strong><em> </em></strong> ,   2 Â«Â»,    1/n. <br></li><li>   :    ,   .  . </li></ul><br>          ,         .  ,      ,      ,     :     ,          . <br><br>        .  ,        .  ,   ..,    .  ,   - , ,        . <br><br>       ,  , ,  Cassandra   .  ,         , ,      , ,  . <br><br>   ,        â€”     /    ,   ,    . <br><br> , :    ?       ? â€” ,  ! <br><br><h4>  #4. Rendezvous/HRW <br></h4><br>    (  ,   ): <strong>shard_id = arg max hash(object_id, shard_id).</strong> <br><br>    Rendezvous hashing,   ,  ,    Highest Random Weight.      : <br><img src="https://habrastorage.org/webt/0t/dt/rm/0tdtrm0iftxxb5ors5a2wxcex8s.jpeg"><br>   , , 16 .    (),   - ,  16 ,      .      -,   . <br><br>    HRW-hashing,   Rendezvous hashing.       , -,        ,   . <br><br>    ,       .  ,        - -        .      . <br><br>   ,       . <br><br><h4>  #5.   <br></h4><br> ,        Google    -   : <br><br><ul><li> Jump Hash â€” Google '2014. </li><li> Multi Probe â€”Google '2015. </li><li> Maglev â€” Google '2016. </li></ul><br>    ,    .      ,   ,    , -,       .      . <br><br><h4>  #6.  <br></h4><br>      â€”  .     ?   ,     2  ,          object_id  2  ,     . <br><br>  ,       ?    ? <br><br>     . ,   -     ,   ,  .  ,      , ,  ,     . <br><br> : <br><br><ul><li>  1  . </li><li>      /  /  /       : min/max_id =&gt; shard_id. </li><li>    8    4    (4      !) â€”  20    . </li><li>      -   ,        20  â€”     . </li><li> 20  â€”                 . </li></ul><br>     2     -    16  â€”   100   -   .       : ,         ,   â€”  1 .     ,  ,   . <br><br> ,    ,     ,    - ,     . <br><br><h1>  Conclusions <br></h1><br>            : Â«  ,   !Â».       ,     20 . <br><br>   ,   ,     .   ,  <strong>   </strong> â€”   .     100$        ,     .          -,    .     â€”   . <br><br> <strong>    </strong> , ,  Â«Â» (, DFS, ...)   .   ,   , highload   -   .  ,        ,     - .     â€” <strong> ,    </strong> . <br><br>     <strong> </strong> <strong>F()</strong> ,   , ,  ..  , ,    2   <strong>     </strong> . <br><br><h2>   <br></h2><br> ,      ,        .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> HighLoad++</a> ,  ,     â€”Sphinxâ€”highload  ,   . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/qpGljUyIht8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>    <br></h2><br>         Highload User Group.  ,    . <br><br>  , ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HighLoad++</a>     .         , ,  .  ,            , .     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a>   highload-,   . <br><br>        ,  ,     ,  . ,           , ,        . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> <strong>24   -</strong>      Â«Â», Â« Â».  ,        .      ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> . <br><br><blockquote>         , ,  <strong>8  9   -  </strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>HighLoad++</strong></a>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> early bird . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr433370/">https://habr.com/ru/post/fr433370/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr433360/index.html">Les scientifiques ont essayÃ© de prÃ©dire quand les avions Ã©lectriques deviendront rÃ©alitÃ©</a></li>
<li><a href="../fr433362/index.html">9 principes de beautÃ©, de simplicitÃ© et de soins en UX</a></li>
<li><a href="../fr433364/index.html">LDraw + Unity. Comment j'ai gÃ©nÃ©rÃ© Lego</a></li>
<li><a href="../fr433366/index.html">Utilisation de ressources externes dans Unity3D</a></li>
<li><a href="../fr433368/index.html">Comment appliquer la pensÃ©e Ã©picerie au monde: un exemple de sweat-shirt</a></li>
<li><a href="../fr433372/index.html">VÃ©lo de voiture</a></li>
<li><a href="../fr433374/index.html">Toute la vÃ©ritÃ© sur RTOS. Article # 26. Canaux: services auxiliaires et structures de donnÃ©es</a></li>
<li><a href="../fr433376/index.html">Cours MIT "SÃ©curitÃ© des systÃ¨mes informatiques". ConfÃ©rence 21: Suivi des donnÃ©es, partie 1</a></li>
<li><a href="../fr433378/index.html">Cours MIT "SÃ©curitÃ© des systÃ¨mes informatiques". ConfÃ©rence 21: Suivi des donnÃ©es, partie 2</a></li>
<li><a href="../fr433380/index.html">Cours MIT "SÃ©curitÃ© des systÃ¨mes informatiques". ConfÃ©rence 21: Suivi des donnÃ©es, partie 3</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>