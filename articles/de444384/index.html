<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßíüèΩ üñïüèª ‚ö±Ô∏è Buch "Angewandte Textdatenanalyse in Python" üë®‚Äçüè≠ üëäüèæ üîì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Textanalysetechnologie √§ndert sich unter dem Einfluss des maschinellen Lernens rasant. Neuronale Netze aus der theoretischen wissenschaftlichen Fo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Buch "Angewandte Textdatenanalyse in Python"</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/444384/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/xl/rp/2m/xlrp2mz88qpo576nl7wcvkhpaek.jpeg" align="left" alt="Bild"></a>  Die Textanalysetechnologie √§ndert sich unter dem Einfluss des maschinellen Lernens rasant.  Neuronale Netze aus der theoretischen wissenschaftlichen Forschung sind in die Realit√§t √ºbergegangen, und die Textanalyse wird aktiv in Softwarel√∂sungen integriert.  Neuronale Netze sind in der Lage, die komplexesten Aufgaben der Verarbeitung nat√ºrlicher Sprache zu l√∂sen. Niemand ist √ºberrascht von maschineller √úbersetzung, ‚ÄûKonversation‚Äú mit einem Roboter in einem Online-Shop, Umformulierung, Beantwortung von Fragen und Aufrechterhaltung eines Dialogs.  Warum wollen Siri, Alexa und Alice uns nicht verstehen, Google findet nicht, wonach wir suchen, und Maschinen√ºbersetzer am√ºsieren uns mit Beispielen f√ºr ‚Äû√úbersetzungsschwierigkeiten‚Äú vom Chinesischen ins Albanische?  Die Antwort liegt im Detail - in Algorithmen, die theoretisch korrekt funktionieren, aber in der Praxis schwer zu implementieren sind.  Erfahren Sie, wie Sie mithilfe von Techniken des maschinellen Lernens Text in realen Aufgaben mithilfe von Python-Funktionen und -Bibliotheken analysieren.  Von der Modellsuche bis zur Datenvorverarbeitung gehen Sie zu den Methoden zum Klassifizieren und Clustering von Texten √ºber, fahren dann mit der visuellen Interpretation und der Diagrammanalyse fort und lernen, nachdem Sie sich mit Skalierungstechniken vertraut gemacht haben, Deep Learning zum Analysieren von Text zu verwenden. <br><br><a name="habracut"></a><br><h3>  Was ist in diesem Buch beschrieben? </h3><br>  In diesem Buch wird die Verwendung von Methoden des maschinellen Lernens zum Analysieren von Text mithilfe der gerade aufgelisteten Python-Bibliotheken beschrieben.  Der angewandte Charakter des Buches legt nahe, dass wir uns nicht auf akademische Linguistik oder statistische Modelle konzentrieren, sondern auf den effektiven Einsatz von textgeschulten Modellen innerhalb der Anwendung. <br><br>  Unser vorgeschlagenes Modell der Textanalyse steht in direktem Zusammenhang mit dem Prozess des maschinellen Lernens - der Suche nach einem Modell, das aus Merkmalen, einem Algorithmus und Hyperparametern besteht und die besten Ergebnisse f√ºr Trainingsdaten liefert, um unbekannte Daten auszuwerten.  Dieser Prozess beginnt mit der Erstellung eines Trainingsdatensatzes, der im Bereich der Textanalyse als Korpus bezeichnet wird.  Anschlie√üend untersuchen wir die Methoden zum Extrahieren von Attributen und zur Vorverarbeitung, um den Text in Form von numerischen Daten darzustellen, die f√ºr Methoden des maschinellen Lernens verst√§ndlich sind.  Nachdem wir uns mit einigen Grundlagen vertraut gemacht haben, werden wir uns mit den Methoden der Klassifizierung und Gruppierung von Texten befassen, deren Geschichte die ersten Kapitel des Buches vervollst√§ndigt. <br><br>  In den folgenden Kapiteln liegt der Schwerpunkt auf der Erweiterung von Modellen mit umfangreicheren Attributen und der Erstellung von Textanalyseanwendungen.  Zuerst werden wir untersuchen, wie es m√∂glich ist, einen Kontext in Form von Zeichen darzustellen und zu implementieren, und dann werden wir zu einer visuellen Interpretation √ºbergehen, um den Prozess der Modellauswahl zu steuern.  Anschlie√üend werden wir untersuchen, wie komplexe Beziehungen, die aus Text extrahiert wurden, mithilfe von Diagrammanalysetechniken analysiert werden.  Danach wenden wir uns interaktiven Agenten zu und vertiefen unser Verst√§ndnis der syntaktischen und semantischen Analyse des Textes.  Abschlie√üend wird das Buch eine praktische Diskussion der Skalierungstechniken f√ºr die Textanalyse in Multiprozessorsystemen mit Spark pr√§sentieren, und schlie√ülich werden wir die n√§chste Stufe der Textanalyse betrachten: Deep Learning. <br><br><h3>  F√ºr wen ist dieses Buch? </h3><br>  Dieses Buch richtet sich an Python-Programmierer, die an der Verwendung von Methoden zur Verarbeitung nat√ºrlicher Sprache und zum maschinellen Lernen in ihren Softwareprodukten interessiert sind.  Wir gehen nicht davon aus, dass unsere Leser √ºber spezielle akademische oder mathematische Kenntnisse verf√ºgen, sondern konzentrieren sich eher auf Werkzeuge und Techniken als auf langwierige Erkl√§rungen.  In diesem Buch wird zun√§chst die Analyse von Texten auf Englisch behandelt, sodass die Leser mindestens Grundkenntnisse √ºber grammatikalische Entit√§ten wie Substantive, Verben, Adverbien und Adjektive sowie deren Beziehung ben√∂tigen.  Leser ohne Erfahrung in maschinellem Lernen und Linguistik, aber mit Programmierkenntnissen in Python, werden sich nicht verloren f√ºhlen, wenn sie die Konzepte lernen, die wir vorstellen werden. <br><br><h3>  Auszug.  Extrahieren Sie Grafiken aus dem Text </h3><br>  Das Extrahieren eines Diagramms aus Text ist eine schwierige Aufgabe.  Die L√∂sung h√§ngt normalerweise vom Themenbereich ab, und im Allgemeinen wird die Suche nach strukturierten Elementen in unstrukturierten oder halbstrukturierten Daten durch kontextsensitive analytische Fragen bestimmt. <br><br>  Wir schlagen vor, diese Aufgabe in kleinere Schritte zu unterteilen, indem wir einen einfachen Graphanalyseprozess organisieren, wie in Abb.  9.3. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2k/f4/ce/2kf4cel7ri9mnaguprrfabt_qus.png" alt="Bild"></div><br>  In diesem Prozess bestimmen wir zun√§chst die Entit√§ten und Beziehungen zwischen ihnen basierend auf der Beschreibung der Aufgabe.  Auf der Grundlage dieses Schemas bestimmen wir au√üerdem die Methode zur Auswahl eines Diagramms aus dem Korpus unter Verwendung von Metadaten, Dokumenten im Korpus und Phrasen oder Token in Dokumenten, um Daten und die Beziehungen zwischen ihnen zu extrahieren.  Die Technik zur Auswahl eines Diagramms ist ein zyklischer Prozess, der auf den K√∂rper angewendet, ein Diagramm generiert und dieses Diagramm zur weiteren analytischen Verarbeitung auf der Festplatte oder im Speicher gespeichert werden kann. <br><br>  In der Analysephase werden Berechnungen f√ºr das extrahierte Diagramm durchgef√ºhrt, z. B. Clustering, Strukturanalyse, Filterung oder Bewertung, und es wird ein neues Diagramm erstellt, das in Anwendungen verwendet wird.  Basierend auf den Ergebnissen der Analysephase k√∂nnen wir zum Beginn des Zyklus zur√ºckkehren, die Methodik und das Schema verfeinern, Gruppen von Knoten oder Kanten extrahieren oder reduzieren, um genauere Ergebnisse zu erzielen. <br><br><h3>  Erstellen eines sozialen Diagramms </h3><br>  Betrachten Sie unsere zahlreichen Nachrichtenartikel und die Aufgabe, die Beziehungen zwischen verschiedenen Entit√§ten im Text zu modellieren.  Wenn wir das Problem der unterschiedlichen Berichterstattung zwischen verschiedenen Nachrichtenagenturen ber√ºcksichtigen, k√∂nnen Sie aus den Elementen, die die Namen von Ver√∂ffentlichungen, Namen von Autoren und Informationsquellen darstellen, ein Diagramm erstellen.  Und wenn das Ziel darin besteht, in vielen Artikeln zus√§tzlich zu demografischen Details Verweise auf eine Entit√§t zu kombinieren, k√∂nnen unsere Netzwerke die Form der Berufung (respektvoll und andere) festlegen.  F√ºr uns interessante Stellen k√∂nnen sich in der Struktur der Dokumente selbst befinden oder direkt im Text enthalten sein. <br><br>  Nehmen wir an, unser Ziel ist es, in unseren Dokumenten Menschen, Orte und alles andere herauszufinden, was miteinander zu tun hat.  Mit anderen Worten, wir m√ºssen ein soziales Netzwerk aufbauen, indem wir eine Reihe von Transformationen durchf√ºhren, wie in Abb. 2 gezeigt.  9.4.  Wir beginnen mit der Erstellung des Diagramms mithilfe der in Kapitel 7 erstellten EntityExtractor-Klasse. Anschlie√üend f√ºgen wir die Transformatoren hinzu, von denen einer nach Paaren verwandter Entit√§ten sucht, und der zweite konvertiert diese Paare in ein Diagramm. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1f/uf/m7/1fufm7sghprbu87dmxa-zpllt3k.png" alt="Bild"></div><br>  <b>Suchen Sie nach Entit√§tspaaren</b> <br><br>  Unser n√§chster Schritt ist das Erstellen der EntityPairs-Klasse, die Dokumente in Form von Entit√§tslisten empf√§ngt (erstellt von der EntityExtractor-Klasse aus Kapitel 7).  Diese Klasse sollte als Konverter in der Pipeline-Pipeline von Scikit-Learn fungieren und daher die Klassen BaseEstimator und TransformerMixin erben, wie in Kapitel 4 beschrieben. Es wird davon ausgegangen, dass die Entit√§ten im selben Dokument unbedingt miteinander verbunden sind. Daher f√ºgen wir die Pair-Methode mithilfe der Funktion itertools hinzu .permutationen zum Erstellen aller m√∂glichen Entit√§tspaare in einem Dokument.  Unsere Transformationsmethode ruft Paare f√ºr jedes Dokument im Hauptteil auf: <br><br><pre><code class="plaintext hljs">import itertools from sklearn.base import BaseEstimator, TransformerMixin class EntityPairs(BaseEstimator, TransformerMixin): def __init__(self): super(EntityPairs, self).__init__() def pairs(self, document): return list(itertools.permutations(set(document), 2)) def fit(self, documents, labels = None): return self def transform(self, documents): return [self.pairs(document) for document in documents]</code> </pre> <br>  Jetzt k√∂nnen Sie Entit√§ten nacheinander aus Dokumenten extrahieren und koppeln.  Wir k√∂nnen jedoch noch keine h√§ufig vorkommenden Entit√§tspaare von nur einmal vorkommenden Paaren unterscheiden.  Wir m√ºssen irgendwie das Gewicht der Beziehung zwischen den Entit√§ten in jedem Paar kodieren, worauf wir im n√§chsten Abschnitt eingehen werden. <br><br><h3>  Eigenschaftsdiagramme </h3><br>  Das mathematische Modell des Graphen definiert nur S√§tze von Knoten und Kanten und kann als Adjazenzmatrix dargestellt werden, die in einer Vielzahl von Berechnungen verwendet werden kann.  Es wird jedoch kein Mechanismus zum Modellieren von St√§rke oder Arten von Beziehungen unterst√ºtzt.  Werden zwei Entit√§ten nur in einem Dokument oder in vielen angezeigt?  Treffen sie sich in Artikeln eines bestimmten Genres?  Um diese √úberlegungen zu unterst√ºtzen, ben√∂tigen wir eine M√∂glichkeit, sinnvolle Eigenschaften in den Knoten und Kanten des Diagramms zu speichern. <br><br>  Mit dem Eigenschaftsdiagrammmodell k√∂nnen Sie weitere Informationen in das Diagramm einbetten und so unsere Funktionen erweitern.  Im Eigenschaftsdiagramm sind Knoten Objekte mit eingehenden und ausgehenden Kanten, die in der Regel ein Typfeld enthalten, das einer Tabelle in einer relationalen Datenbank √§hnelt.  Rippen sind Objekte, die den Start- und Endpunkt definieren.  Diese Objekte enthalten normalerweise ein Beschriftungsfeld, das den Verbindungstyp angibt, und ein Gewichtsfeld, das die St√§rke der Verbindung definiert.  Bei der Verwendung von Diagrammen f√ºr die Textanalyse verwenden wir h√§ufig Substantive als Knoten und Verben als Kanten.  Nachdem Sie mit dem Modellierungsschritt fortgefahren sind, k√∂nnen Sie die Knotentypen, Verkn√ºpfungsbezeichnungen und die vorgeschlagene Diagrammstruktur beschreiben. <br><br><h3>  √úber Autoren </h3><br>  Benjamin Bengfort ist ein in Washington, DC, ans√§ssiger Spezialist f√ºr Datenwissenschaft, der die Politik (eine f√ºr den District of Columbia √ºbliche Sache) v√∂llig ignoriert und Technologie bevorzugt.  Derzeit arbeitet er an seiner Doktorarbeit an der University of Maryland, wo er maschinelles Lernen und verteiltes Rechnen studiert.  In seinem Labor gibt es Roboter (obwohl dies nicht sein Lieblingsgebiet ist), und zu seinem Leidwesen statten seine Assistenten diese Roboter st√§ndig mit Messern und Werkzeugen aus, wahrscheinlich mit dem Ziel, einen kulinarischen Wettbewerb zu gewinnen.  Benjamin beobachtet einen Roboter, der versucht, eine Tomate zu hacken, und bevorzugt es, die K√ºche selbst zu hosten, in der er franz√∂sische und hawaiianische Gerichte sowie Grillgerichte und Grillgerichte aller Art kocht.  Benjamin ist ein professioneller Ausbildungsprogrammierer und Berufungsdatenforscher. Er schreibt h√§ufig Artikel zu einer Vielzahl von Themen, von der Verarbeitung nat√ºrlicher Sprache √ºber die Datenanalyse in Python bis hin zur Verwendung von Hadoop und Spark in der Analytik. <br><br>  Dr. Rebecca Bilbro - Spezialistin f√ºr Datenwissenschaft, Python-Programmiererin, Lehrerin, Dozentin und Autorin von Artikeln;  lebt in Washington, DC.  Es ist spezialisiert auf die visuelle Bewertung von Ergebnissen des maschinellen Lernens: von der Merkmalsanalyse √ºber die Modellauswahl bis hin zu Hyperparametereinstellungen.  Er forscht auf dem Gebiet der Verarbeitung nat√ºrlicher Sprache, baut semantische Netzwerke auf, l√∂st Entit√§ten auf und verarbeitet Informationen mit einer Vielzahl von Dimensionen.  Als aktive Teilnehmerin in der Community von Benutzern und Entwicklern von Open Source-Software freut sich Rebecca, mit anderen Entwicklern an Projekten wie Yellowbrick (einem Python-Paket zur Vorhersage der Black-Box-Modellierung) zu arbeiten.  In seiner Freizeit f√§hrt er oft mit seiner Familie Fahrrad oder √ºbt Ukulele.  Sie promovierte an der University of Illinois in Urbana-Champaign, wo sie praktische Kommunikations- und Visualisierungstechniken studierte. <br><br>  ¬ªWeitere Informationen zum Buch finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website des Herausgebers</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalt</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszug</a> <br><br>  20% Rabatt-Gutschein f√ºr Stra√üenh√§ndler - <b>Python</b> <br><br>  Nach Zahlung der Papierversion des Buches wird eine elektronische Version des Buches per E-Mail verschickt. <br><br>  PS: 7% der Kosten des Buches flie√üen in die √úbersetzung neuer Computerb√ºcher. Die Liste der B√ºcher, die der Druckerei √ºbergeben wurden, finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de444384/">https://habr.com/ru/post/de444384/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de444372/index.html">Maschinenorientierung √ºber gro√üe Entfernungen durch verst√§rktes Lernen</a></li>
<li><a href="../de444374/index.html">Hipster-Effekt: Warum Nonkonformisten oft gleich aussehen</a></li>
<li><a href="../de444376/index.html">Die Wirtschaft der Aufmerksamkeit ist fast tot</a></li>
<li><a href="../de444378/index.html">USPACE - Single Space f√ºr bemannte und unbemannte Flugzeuge</a></li>
<li><a href="../de444382/index.html">So besuchen Sie die Korea University mit dem Network File System</a></li>
<li><a href="../de444386/index.html">Mondmission "Bereshit" - das vierte Man√∂ver wurde erfolgreich abgeschlossen, die Vorbereitungen f√ºr den Eintritt in die Mondumlaufbahn laufen</a></li>
<li><a href="../de444388/index.html">Legend√§re Modems der Vergangenheit: die besten Verbindungsinhaber unter den Bedingungen der inl√§ndischen B√∂rsen</a></li>
<li><a href="../de444390/index.html">DeviceLock 8.3 DLP-System: Ein Jahr ist vergangen, Billy, aber Sie haben sich √ºberhaupt nicht ge√§ndert</a></li>
<li><a href="../de444392/index.html">Strahlung: Risiken, Sicherheit, Schutz</a></li>
<li><a href="../de444394/index.html">Die Linux Foundation startet mit Jenkins und Spinnaker ein neues DevOps-Projekt</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>