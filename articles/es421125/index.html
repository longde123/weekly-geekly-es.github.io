<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äçüè´ üåé üóø Segmentamos 600 millones de usuarios en tiempo real todos los d√≠as. ü§∏üèø üñåÔ∏è üë©üèæ‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Todos los d√≠as, los usuarios comprometen millones de actividades en l√≠nea. El proyecto FACETz DMP necesita estructurar estos datos y segmentarlos para...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Segmentamos 600 millones de usuarios en tiempo real todos los d√≠as.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jugru/blog/421125/">  Todos los d√≠as, los usuarios comprometen millones de actividades en l√≠nea.  El proyecto FACETz DMP necesita estructurar estos datos y segmentarlos para identificar las preferencias del usuario.  En el art√≠culo, hablaremos sobre c√≥mo el equipo segment√≥ una audiencia de 600 millones de personas, proces√≥ 5 mil millones de eventos diariamente y trabaj√≥ con estad√≠sticas usando Kafka y HBase. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/5Ybt_k53CIE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  El material se basa en una transcripci√≥n de un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">informe de Artyom Marinov</a> , especialista en big data de Directual, de la conferencia SmartData 2017. <br><a name="habracut"></a><br>  Mi nombre es Artyom Marinov, quiero hablar sobre c√≥mo redise√±amos la arquitectura del proyecto FACETz DMP cuando trabaj√© en Data Centric Alliance.  Por qu√© lo hicimos, a qu√© condujo, hacia d√≥nde nos dirigimos y qu√© problemas encontramos. <br><br>  DMP (Data Management Platform) es una plataforma para recopilar, procesar y agregar datos de usuario.  Los datos son muchas cosas diferentes.  La plataforma tiene alrededor de 600 millones de usuarios.  Son millones de cookies que se conectan a Internet y realizan diversos eventos.  En general, un d√≠a en promedio se ve m√°s o menos as√≠: vemos alrededor de 5.5 mil millones de eventos por d√≠a, de alguna manera se extienden por d√≠a, y en el pico alcanzan aproximadamente 100 mil eventos por segundo. <img src="https://habrastorage.org/getpro/habr/post_images/f66/f4d/915/f66f4d9154b1ddad3c3bb8af7e5ba860.png">  Los eventos son varias se√±ales de usuario.  Por ejemplo, una visita a un sitio: vemos desde qu√© navegador va el usuario, su agente de uso y todo lo que podemos extraer.  A veces vemos c√≥mo y para qu√© consultas de b√∫squeda lleg√≥ al sitio.  Tambi√©n pueden ser varios datos del mundo fuera de l√≠nea, por ejemplo, lo que paga con cupones de descuento, etc. <br><br>  Necesitamos guardar estos datos y marcar al usuario en los llamados grupos de segmentos de audiencia.  Por ejemplo, los segmentos pueden ser una "mujer" que "ama a los gatos" y est√° buscando "servicio de autom√≥vil", ella "tiene un autom√≥vil de m√°s de tres a√±os". <br><br>  ¬øPor qu√© segmentar a un usuario?  Hay muchas aplicaciones para esto, por ejemplo, publicidad.  Varias redes publicitarias pueden optimizar los algoritmos de publicaci√≥n de anuncios.  Si est√° anunciando su servicio de autom√≥vil, puede configurar una campa√±a de tal manera que solo las personas que tienen un autom√≥vil antiguo muestren informaci√≥n, excluyendo a los propietarios de otros nuevos.  Puede cambiar din√°micamente el contenido del sitio, puede usar los datos para la puntuaci√≥n: hay muchas aplicaciones. <br><br>  Los datos se obtienen de muchos lugares completamente diferentes.  Puede ser una configuraci√≥n directa de p√≠xeles: si el cliente quiere analizar su audiencia, coloca el p√≠xel en el sitio, una imagen invisible que se descarga de nuestro servidor.  La conclusi√≥n es que vemos la visita del usuario a este sitio: puede guardarlo, comenzar a analizar y comprender el retrato del usuario, toda esta informaci√≥n est√° disponible para nuestro cliente. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad9/ebf/849/ad9ebf84913e17fb9e84a947b256a810.png"><br>  Se pueden obtener datos de varios socios que ven muchos datos y desean monetizarlos de varias maneras.  Los socios pueden suministrar datos en tiempo real y realizar cargas peri√≥dicas en forma de archivos. <br><br>  Requisitos clave: <br><br><ul><li>  Escalabilidad horizontal; </li><li>  Evaluaci√≥n del volumen de la audiencia; </li><li>  Conveniencia de monitoreo y desarrollo; </li><li>  Buena velocidad de reacci√≥n a los eventos. </li></ul><br>  Uno de los requisitos clave del sistema es la escalabilidad horizontal.  Hay un momento en el que cuando est√° desarrollando un portal o una tienda en l√≠nea, puede estimar la cantidad de usuarios (c√≥mo crecer√°, c√≥mo cambiar√°) y comprender aproximadamente cu√°ntos recursos se necesitan y c√≥mo la tienda vivir√° y se desarrollar√° con el tiempo. <br><br>  Cuando desarrolle una plataforma similar a DMP, debe estar preparado para el hecho de que cualquier sitio grande, el Amazon condicional, puede poner su p√≠xel en √©l, y tendr√° que trabajar con el tr√°fico de todo este sitio, mientras no debe caer, y los indicadores los sistemas no deber√≠an cambiar de alguna manera. <br><br>  Tambi√©n es bastante importante poder comprender el volumen de cierta audiencia para que un anunciante potencial u otra persona pueda elaborar un plan de medios.  Por ejemplo, una persona acude a usted y le pide que averig√ºe cu√°ntas mujeres embarazadas de Novosibirsk est√°n buscando una hipoteca para evaluar si tiene sentido apuntarlas o no. <br><br>  Desde el punto de vista del desarrollo, debe ser capaz de monitorear fr√≠amente todo lo que sucede en su sistema, depurar parte del tr√°fico real, etc. <br><br>  Uno de los requisitos del sistema m√°s importantes es una buena velocidad de reacci√≥n a los eventos.  Cuanto m√°s r√°pido respondan los sistemas a los eventos, mejor ser√° obvio.  Si est√° buscando entradas para el teatro, si ve alg√∫n tipo de oferta de descuento despu√©s de un d√≠a, dos d√≠as o incluso una hora, esto puede ser irrelevante, ya que ya podr√≠a comprar entradas o asistir a una actuaci√≥n.  Cuando est√° buscando un taladro, lo est√° buscando, encuentre, compre, cuelgue un estante, y despu√©s de un par de d√≠as comienza el bombardeo: "¬°Compre un taladro!". <br><br><h3>  Como era antes </h3><br>  El art√≠culo en su conjunto trata sobre el reciclaje de la arquitectura.  Me gustar√≠a decirle cu√°l fue nuestro punto de partida, c√≥mo funcion√≥ todo antes de los cambios. <br><br>  Todos los datos que ten√≠amos, ya sea un flujo directo de datos o registros, se almacenaron en HDFS - almacenamiento de archivos distribuidos.  Luego hubo un cierto proceso que se inici√≥ peri√≥dicamente, tom√≥ todos los archivos no procesados ‚Äã‚Äãde HDFS y los convirti√≥ en solicitudes de enriquecimiento de datos en HBase ("solicitudes PUT"). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3c6/902/3eb/3c69023eb0fc851d8acc327a7b57fb22.png"><br><br><h3>  ¬øC√≥mo almacenamos datos en HBase? </h3><br>  Esta es una base de datos columnar de series temporales.  Ella tiene el concepto de una clave de fila: esta es la clave con la que almacena sus datos.  Usamos el ID de usuario como clave, el ID de usuario, que generamos cuando vemos al usuario por primera vez.  Dentro de cada clave, los datos se dividen en Familia de columnas: entidades a cuyo nivel puede administrar la metainformaci√≥n de sus datos.  Por ejemplo, puede almacenar mil versiones de registros para los "datos" de la familia de columnas y almacenarlos durante dos meses, y para la familia de columnas "sin procesar": un a√±o, como opci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/89f/fb5/46e/89ffb546efcad40cd08d140df54ac6a4.png"><br>  Dentro de la familia de columnas, hay muchos calificadores de columna (en adelante columna).  Utilizamos varios atributos de usuario como columna.  Podr√≠a ser la URL a la que fue, direcci√≥n IP, consulta de b√∫squeda.  Y lo m√°s importante, se almacena mucha informaci√≥n dentro de cada columna.  Dentro de la columna URL se puede indicar que el usuario fue a smartdataconf.ru, luego a algunos otros sitios.  Y la marca de tiempo se usa como la versi√≥n: ve un historial ordenado de visitas de usuarios.  En nuestro caso, podemos determinar que el usuario lleg√≥ al sitio web de smartdataconf con la palabra clave "conferencia", porque tienen la misma marca de tiempo. <br><br><h3>  Trabajar con HBase </h3><br>  Hay varias opciones para trabajar con HBase.  Pueden ser solicitudes PUT (solicitud de cambio de datos), solicitud GET ("dame todos los datos sobre el usuario Vasya", etc.).  Puede ejecutar solicitudes SCAN: exploraci√≥n secuencial de subprocesos m√∫ltiples de todos los datos en HBase.  Usamos esto antes para marcar en segmentos de audiencia. <br><br>  Hab√≠a una tarea llamada Motor de an√°lisis, se ejecutaba una vez al d√≠a y escaneaba HBase en varios subprocesos.  Para cada usuario, ella levant√≥ toda la historia de HBase y la revis√≥ a trav√©s de un conjunto de scripts anal√≠ticos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fba/b4c/674/fbab4c674fabf2f07e0dc8553e8b6cfc.png"><br>  ¬øQu√© es un script anal√≠tico?  Este es un tipo de caja negra (clase java), que recibe todos los datos del usuario como entrada y proporciona un conjunto de segmentos que considera adecuados como salida.  Le damos todo al script que vemos: IP, visitas, UserAgent, etc., y en el resultado los scripts dan: "esta es una mujer, ama a los gatos, no le gustan los perros". <br><br>  Estos datos se entregaron a los socios, se consideraron las estad√≠sticas.  Era importante para nosotros entender cu√°ntas mujeres hay en general, cu√°ntos hombres, cu√°ntas personas aman a los gatos, cu√°ntas tienen o no tienen autom√≥vil, etc. <br><br>  Almacenamos estad√≠sticas en MongoDB y escribimos incrementando un contador de segmento espec√≠fico para cada d√≠a.  Ten√≠amos una gr√°fica del volumen de cada segmento para cada d√≠a. <br><br>  Este sistema fue bueno para su tiempo.  Permit√≠a escalar horizontalmente, crecer, permit√≠a estimar el volumen de la audiencia, pero ten√≠a una serie de inconvenientes. <br><br>  No siempre era posible entender lo que estaba sucediendo en el sistema, mirar los registros.  Mientras est√°bamos en el hoster anterior, la tarea a menudo cay√≥ por varias razones.  Hab√≠a un cl√∫ster Hadoop de m√°s de 20 servidores, una vez al d√≠a uno de los servidores se bloqueaba de manera estable.  Esto llev√≥ al hecho de que la tarea podr√≠a caer parcialmente y no calcular los datos.  Era necesario tener tiempo para reiniciarlo y, dado que funcion√≥ durante varias horas, hubo una serie de ciertos matices. <br><br>  Lo m√°s b√°sico que la arquitectura existente no cumpli√≥ fue que el tiempo de reacci√≥n al evento fue demasiado largo.  Incluso hay una historia sobre este tema.  Hab√≠a una compa√±√≠a que emit√≠a micropr√©stamos a la poblaci√≥n en las regiones, y nos asociamos con ellos.  Su cliente llega al sitio, llena una solicitud de microcr√©dito, la empresa debe responder en 15 minutos: ¬øest√°n listos para otorgar un pr√©stamo o no?  Si est√° listo, inmediatamente transfirieron dinero a la tarjeta. <br><br>  Todo funcion√≥ bastante bien.  El cliente decidi√≥ verificar c√≥mo sucede generalmente: tomaron una computadora port√°til separada, instalaron un sistema limpio, visitaron muchas p√°ginas en Internet y fueron a su sitio.  Ven que hay una solicitud y, en respuesta, decimos que todav√≠a no hay datos.  El cliente pregunta: "¬øPor qu√© no hay datos?" <br><br>  Explicamos: hay un cierto retraso antes de que el usuario realice una acci√≥n.  Los datos se env√≠an a HBase, se procesan y solo entonces el cliente recibe el resultado.  Parecer√≠a que si el usuario no vio el anuncio, todo est√° en orden, no pasar√° nada malo.  Pero en esta situaci√≥n, el usuario podr√≠a no recibir un pr√©stamo debido al retraso. <br><br>  Este no es un caso aislado, y fue necesario cambiar a un sistema en tiempo real.  ¬øQu√© queremos de ella? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad8/103/d08/ad8103d08bb70410bcc51c8fdd99b3f5.png"><br>  Queremos escribir datos en HBase tan pronto como lo veamos.  Vimos una visita, enriquecimos todo lo que sabemos y lo enviamos a Storage.  Tan pronto como los datos en Almacenamiento hayan cambiado, debe ejecutar de inmediato todo el conjunto de scripts anal√≠ticos que tenemos.  Queremos la conveniencia de monitoreo y desarrollo, la capacidad de escribir nuevos scripts, depurarlos en fragmentos de tr√°fico real.  Queremos entender en qu√© est√° ocupado el sistema en este momento. <br><br>  Lo primero con lo que comenzamos es resolver el segundo problema: segmentar al usuario inmediatamente despu√©s de cambiar los datos sobre √©l en HBase.  Inicialmente, ten√≠amos nodos de trabajo (se iniciaron tareas de reducci√≥n de mapas en ellos) ubicados en el mismo lugar que HBase.  En varios casos, fue muy bueno: los c√°lculos se realizan junto a los datos, las tareas funcionan con bastante rapidez y pasa poco tr√°fico por la red.  Est√° claro que la tarea consume algunos recursos, ya que ejecuta scripts anal√≠ticos complejos. <br><br>  Cuando vamos a trabajar en tiempo real, la naturaleza de la carga en HBase cambia.  Pasamos a lecturas aleatorias en lugar de lecturas secuenciales.  Es importante que se espere la carga en HBase: no podemos permitir que alguien ejecute la tarea en el cl√∫ster de Hadoop y estropee el rendimiento de HBase. <br><br>  Lo primero que hicimos fue mover HBase a servidores separados.  Tambi√©n modific√≥ BlockCache y BloomFilter.  Luego hicimos un buen trabajo sobre c√≥mo almacenar datos en HBase.  Pr√°cticamente reelaboraron el sistema del que habl√© al principio y cosecharon los datos en s√≠. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/191/f68/3c9/191f683c9cddf8d90f43cafc5c1163a3.png"><br>  De lo obvio: almacenamos IP como una cadena y nos volvimos largos en n√∫meros.  Se clasificaron algunos datos, se llevaron a cabo actividades de vocabulario, etc.  La conclusi√≥n es que debido a esto, pudimos sacudir HBase aproximadamente dos veces, de 10 TB a 5 TB.  HBase tiene un mecanismo similar a los desencadenantes en una base de datos normal.  Este es un mecanismo coprocesador.  Escribimos un coprocesador que, cuando un usuario cambia a HBase, env√≠a la identificaci√≥n de usuario a Kafka. <br><br>  La identificaci√≥n de usuario est√° en Kafka.  Adem√°s hay un cierto servicio "segmentador".  Lee el flujo de identificadores de usuario y ejecuta en ellos los mismos scripts que antes, solicitando datos de HBase.  El proceso se lanz√≥ en el 10% del tr√°fico, observamos c√≥mo funciona.  Todo estuvo bastante bien. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8c1/589/928/8c15899287c8a623bb63df3f85ba84e6.png"><br>  Luego, comenzamos a aumentar la carga y vimos una serie de problemas.  Lo primero que vimos fue que el servicio funciona, se segmenta y luego se cae de Kafka, se conecta y comienza a funcionar nuevamente.  Varios servicios: se ayudan mutuamente.  Luego, el siguiente se cae, otro y as√≠ sucesivamente en un c√≠rculo.  Al mismo tiempo, la alineaci√≥n de usuarios para la segmentaci√≥n casi no se est√° clasificando. <br><br>  Esto se debi√≥ a la peculiaridad del mecanismo de los latidos del coraz√≥n en Kafka, entonces todav√≠a era la versi√≥n 0.8.  Heartbeat es cuando los consumidores le dicen al corredor si est√°n vivos o no, en nuestro caso, informa el segmentador.  Sucedi√≥ lo siguiente: recibimos un paquete de datos bastante grande y lo enviamos para su procesamiento.  Por un tiempo funcion√≥, mientras funcion√≥, no se envi√≥ ning√∫n latido.  Los corredores cre√≠an que el consumidor estaba muerto y lo apagaron. <br><br>  El consumidor trabaj√≥ hasta el final, desperdiciando preciosas CPU, trat√≥ de decir que el paquete de datos estaba resuelto y que el siguiente podr√≠a ser tomado, pero fue rechazado porque el otro le quit√≥ lo que estaba trabajando.  Lo arreglamos haciendo que nuestro fondo latiera, luego la verdad lleg√≥ una versi√≥n m√°s nueva de Kafka donde solucionamos este problema. <br><br>  Entonces surgi√≥ la pregunta: ¬øen qu√© tipo de hardware deber√≠an instalar nuestros segmentadores?  La segmentaci√≥n es un proceso intensivo en recursos (vinculado a la CPU).  Es importante que el servicio no solo consuma mucha CPU, sino que tambi√©n cargue la red.  Ahora el tr√°fico alcanza los 5 Gbit / seg.  La pregunta era: d√≥nde colocar los servicios, en muchos servidores peque√±os o un poco grandes. <br><br>  En ese momento, ya nos mudamos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">server.com</a> en metal desnudo.  Hablamos con los chicos de los servidores, nos ayudaron, hicieron posible probar el trabajo de nuestra soluci√≥n tanto en un peque√±o n√∫mero de servidores caros como en muchos de bajo costo con CPU potentes.  Elegimos la opci√≥n adecuada, calculando el costo unitario de procesar un evento por segundo.  Por cierto, la elecci√≥n recay√≥ en Dell R230 suficientemente potente y al mismo tiempo extremadamente asequible, lo lanzaron, todo funcion√≥. <br><br>  Es importante que despu√©s de que el segmentador haya marcado al usuario en segmentos, el resultado de su an√°lisis recaiga en Kafka, en un determinado tema Resultado de segmentaci√≥n. <br><br>  Adem√°s, podemos conectarnos de manera independiente a estos datos por diferentes consumidores que no interferir√°n entre s√≠.  Esto nos permite proporcionar datos de forma independiente a cada socio, ya sean algunos socios externos, DSP interno, Google, estad√≠sticas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1e7/051/526/1e705152621996ed538a6a3910e8db47.png"><br>  Con las estad√≠sticas, tambi√©n hay un punto interesante: antes podr√≠amos aumentar el valor de los contadores en MongoDB, cu√°ntos usuarios estaban en un segmento determinado durante un d√≠a determinado.  Ahora, esto no se puede hacer porque ahora analizamos a cada usuario despu√©s de que completa un evento, es decir,  Varias veces al d√≠a. <br><br>  Por lo tanto, tuvimos que resolver el problema de contar el n√∫mero √∫nico de usuarios en la transmisi√≥n.  Para hacer esto, utilizamos la estructura de datos HyperLogLog y su implementaci√≥n en Redis.  La estructura de datos es probabil√≠stica.  Esto significa que puede agregar identificadores de usuario all√≠, los identificadores en s√≠ no se almacenar√°n, por lo que puede almacenar millones de identificadores √∫nicos en HyperLogLog extremadamente compacto, y esto tomar√° hasta 12 kilobytes por clave. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2a6/572/614/2a6572614da5ccffd52271292e646e9d.png"><br><br>  No puede obtener los identificadores usted mismo, pero puede averiguar el tama√±o de este conjunto.  Dado que la estructura de datos es probabil√≠stica, hay alg√∫n error.  Por ejemplo, si tiene un segmento "le gustan los gatos", al solicitar el tama√±o de este segmento para un d√≠a determinado, recibir√° 99,2 millones y esto significar√° algo as√≠ como "de 99 millones a 100 millones". <br><br>  Tambi√©n en HyperLogLog puede obtener el tama√±o de la uni√≥n de varios conjuntos.  Digamos que tiene dos segmentos: "ama a las focas" y "ama a los perros".  Digamos los primeros 100 millones, el segundo 1 mill√≥n. Uno puede preguntarse: "¬øCu√°ntos animales les gustan?"  y obtenga la respuesta "aproximadamente 101 millones" con un error del 1%.  Ser√≠a interesante calcular cu√°nto se ama tanto a los gatos como a los perros al mismo tiempo, pero hacerlo es bastante dif√≠cil. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/380/74d/500/38074d5004513c86015c8d2770047f56.png"><br>  Por un lado, puede averiguar el tama√±o de cada conjunto, conocer el tama√±o de la uni√≥n, sumar, restar uno del otro y obtener la intersecci√≥n.  Pero debido al hecho de que el tama√±o del error puede ser mayor que el tama√±o de la intersecci√≥n final, el resultado final puede tener la forma "de -50 a 50 mil". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/08d/3ba/520/08d3ba520f5c91efd84822d63f299c83.png"><br>  Hemos trabajado bastante sobre c√≥mo aumentar el rendimiento al escribir datos en Redis.  Inicialmente, alcanzamos 200 mil operaciones por segundo.  Pero cuando cada usuario tiene m√°s de 50 segmentos, registrando informaci√≥n sobre cada usuario, 50 operaciones.  Resulta que tenemos un ancho de banda bastante limitado y, en este ejemplo, no podemos escribir informaci√≥n sobre m√°s de 4 mil usuarios por segundo, esto es varias veces menos de lo que necesitamos. <br><br>  Hicimos un "procedimiento almacenado" por separado en Redis a trav√©s de Lua, lo cargamos all√≠ y comenzamos a pasarle una cadena con la lista completa de segmentos de un usuario.  El procedimiento interno cortar√° la cadena pasada en las actualizaciones necesarias de HyperLogLog y guardar√° los datos, por lo que llegamos a aproximadamente 1 mill√≥n de actualizaciones por segundo. <br><br>  Un poco duro: Redis es de un solo subproceso, puede fijarlo a un n√∫cleo de procesador y una tarjeta de red a otro y lograr otro 15% de rendimiento, ahorrando en el cambio de contexto.  Adem√°s de esto, el punto importante es que no puede simplemente agrupar la estructura de datos, porque las operaciones para obtener el poder de las uniones de conjuntos no est√°n agrupadas <br><br><h3>  Kafka es una gran herramienta </h3><br>  Usted ve que Kafka es nuestra principal herramienta de transporte en el sistema. <br>  Tiene la esencia del "tema".  Aqu√≠ es donde escribe los datos, pero en esencia: la cola.  En nuestro caso, hay varias colas.  Uno de ellos son los identificadores de usuarios a los que es necesario segmentar.  El segundo es la segmentaci√≥n de resultados. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f35/eb3/8c2/f35eb38c205fec791ea2f42d41a8c875.png"><br>  Un tema es un conjunto de particiones.  Se divide en algunas piezas.  Cada partici√≥n es un archivo en el disco duro.  Cuando sus productores escriben datos, escriben fragmentos de texto al final de la partici√≥n.  Cuando sus consumidores leen los datos, simplemente leen desde estas particiones. <br><br>  Lo importante es que puede conectar de forma independiente varios grupos de consumidores, ya que consumir√°n datos sin interferir entre s√≠.  Esto se determina por el nombre del grupo de consumidores y se logra de la siguiente manera. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/157/5f7/23d/1575f723d77042fd058b9c30bc050247.png"><br>  Existe una compensaci√≥n, la posici√≥n donde el grupo de consumidores se encuentra ahora en cada partici√≥n.  Por ejemplo, el grupo A consume el s√©ptimo mensaje de la partici√≥n1 y el quinto de la partici√≥n2.  El grupo B, independiente de A, tiene otro desplazamiento. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f8a/10f/1a0/f8a10f1a04b8614f6b9a437142b0db7b.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Puede escalar su grupo de consumidores horizontalmente, agregar otro proceso o servidor. Esto suceder√° la reasignaci√≥n de la partici√≥n (el corredor de Kafka asignar√° a cada consumidor una lista de particiones para el consumo) Esto significa que el primer grupo de consumidores comenzar√° a consumir solo la partici√≥n 1, y el segundo solo consumir√° la partici√≥n 2. Si algunos de los consumidores mueren (por ejemplo, el latido no llega), se produce una nueva reasignaci√≥n , cada consumidor recibe una lista de particiones actualizada para su procesamiento.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/56b/40b/bde/56b40bbde5b1890d35f67a481c4a6462.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es bastante conveniente. Primero, puede manipular el desplazamiento para cada grupo de consumidores. Imagine que hay un socio al que transfiere datos de este tema con los resultados de la segmentaci√≥n. √âl escribe que perdi√≥ accidentalmente el √∫ltimo d√≠a de datos como resultado de un error. Y usted, para el grupo de consumidores de este cliente, simplemente retroceda un d√≠a y vierta todo el d√≠a de datos en √©l. Tambi√©n podemos tener nuestro propio grupo de consumidores, conectarnos con el tr√°fico de producci√≥n, ver lo que sucede y depurar datos reales. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces, hemos logrado que comenzamos a segmentar a los usuarios cuando cambian, podemos conectar de manera independiente a los nuevos consumidores, escribimos estad√≠sticas y podemos verlo. Ahora debe obtener los datos escritos en HBase inmediatamente despu√©s de que nos lleguen.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c4e/eec/d6c/c4eeecd6ce987e902fee723a089ab780.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C√≥mo lo hicimos Sol√≠a ‚Äã‚Äãhaber carga de datos por lotes. Hab√≠a un Batch Loader, procesaba archivos de registro de actividad del usuario: si un usuario realizaba 10 visitas, el lote ven√≠a para 10 eventos, se registraba en HBase en una operaci√≥n. Solo hubo un evento por segmentaci√≥n. Ahora queremos escribir cada evento por separado en el almacenamiento. Aumentaremos en gran medida la secuencia de escritura y la secuencia de lectura. El n√∫mero de eventos por segmentaci√≥n tambi√©n aumentar√°.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/79a/849/910/79a8499101ac2cb58ccf272abb668f6f.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lo primero que hicimos fue portar HBase al SSD. Por medios est√°ndar, esto no se hace particularmente. Esto se hizo usando HDFS. Puede decir que un directorio espec√≠fico en HDFS debe estar en dicho grupo de discos. Hubo un problema genial con el hecho de que cuando llevamos HBase a la SSD y lo apagamos, todas las instant√°neas llegaron all√≠, y nuestras SSD terminaron bastante r√°pido. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esto tambi√©n se solucion√≥, comenzamos a exportar instant√°neamente instant√°neas a un archivo, escribir en otro directorio HDFS y eliminar toda la metainformaci√≥n sobre las instant√°neas. Si necesita restaurar, tome el archivo guardado, importe y restaure. Esta operaci√≥n es muy poco frecuente, afortunadamente.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tambi√©n en el SSD sacaron Write Ahead Log, retorcieron MemStore, activaron el bloque de cach√© en la opci√≥n de escritura. Le permite colocarlos de inmediato en el cach√© de bloque al grabar datos. Esto es muy conveniente porque en nuestro caso, si registramos los datos, es muy probable que se lean de inmediato. Esto tambi√©n dio algunas ventajas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Luego, cambiamos todas nuestras fuentes de datos para escribir datos en Kafka. Ya desde Kafka registramos datos en HDFS para mantener la compatibilidad con versiones anteriores, incluso para que nuestros analistas puedan trabajar con datos, ejecutar tareas de MapReduce y analizar sus resultados. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conectamos un grupo de consumidores separado que escribe datos en HBase. Este es, de hecho, un contenedor que lee de Kafka y forma los PUT en HBase.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d53/267/0b3/d532670b344a70dadf97c5c4674b1596.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lanzamos dos circuitos en paralelo para no romper la compatibilidad con versiones anteriores y no degradar el rendimiento del sistema. Se lanz√≥ un nuevo esquema solo con un cierto porcentaje de tr√°fico. Con un 10%, todo fue genial. Pero a mayor carga, los segmentadores no pod√≠an hacer frente al flujo de segmentaci√≥n. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d20/3fa/78c/d203fa78c3d0b80ee7c7a5dce34f558b.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recopilamos la m√©trica "cu√°ntos mensajes hab√≠a en Kafka antes de que se leyera desde all√≠". Esta es una buena m√©trica. Inicialmente, recopilamos la m√©trica "cu√°ntos mensajes en bruto hay ahora", pero no dice nada especial. Usted mira: "Tengo un mill√≥n de mensajes en bruto", ¬øy qu√©? Para interpretar este mill√≥n, necesita saber qu√© tan r√°pido est√° funcionando el segmentador (consumidor), lo que no siempre est√° claro.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Con esta m√©trica, ver√° de inmediato que los datos se escriben en la cola, que se tomar√°n de ella, y ver√° cu√°nto esperan que se procesen. </font><font style="vertical-align: inherit;">Vimos que no ten√≠amos tiempo para segmentar, y el mensaje estaba en la cola varias horas antes de leerlo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Simplemente podr√≠a agregar capacidad, pero ser√≠a demasiado </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">costoso</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Por lo tanto, tratamos de optimizar.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Autoescalado </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tenemos HBase El usuario est√° cambiando, su identificador est√° volando en Kafka. El tema se divide en particiones, la partici√≥n de destino se selecciona por ID de usuario. Esto significa que cuando ve al usuario "Vasya" - √©l va a la partici√≥n 1. Cuando ve "Petya" - a la partici√≥n 2. Esto es conveniente - puede lograr que ver√° un consumidor en una instancia de su servicio, y el segundo - por el otro </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/299/573/aef/299573aefcae8b909623e246d4a2cf80.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comenzamos a ver lo que estaba sucediendo. Un comportamiento t√≠pico del usuario en Internet es ir a alg√∫n sitio web y abrir varias pesta√±as de fondo. El segundo es ir al sitio y hacer unos pocos clics para llegar a la p√°gina de destino.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Observamos la cola de segmentaci√≥n y vemos lo siguiente: el usuario A visit√≥ la p√°gina. 5 m√°s eventos provienen de este usuario, cada uno significa una apertura de p√°gina. Procesamos cada evento del usuario. Pero, de hecho, los datos en HBase contienen las 5 visitas. Procesamos las 5 visitas por primera vez, la segunda vez, etc., estamos desperdiciando recursos de la CPU. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6df/eba/fbf/6dfebafbf435cfb43e5b71cc7d2016fd.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por lo tanto, comenzamos a almacenar un cierto cach√© local en cada uno de los segmentadores con la fecha en que analizamos por √∫ltima vez a este usuario. Es decir, lo procesamos, escribimos su ID de usuario y marca de tiempo en el cach√©. Cada mensaje kafka tambi√©n tiene una marca de tiempo, simplemente la comparamos: si la marca de tiempo en la cola es menor que la fecha de la √∫ltima segmentaci√≥n, ya hemos analizado al usuario para estos datos, y simplemente puede omitir este evento.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los eventos de usuario (Rojo A) pueden ser diferentes y se salen de orden. El usuario puede abrir varias pesta√±as de fondo, abrir varios enlaces seguidos, tal vez el sitio tenga varios de nuestros socios a la vez, cada uno de los cuales env√≠a estos datos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nuestro p√≠xel puede ver la visita del usuario, y luego alguna otra acci√≥n: nos enviaremos su casco a nosotros mismos. Llegan cinco eventos, estamos procesando la primera A roja. Si el evento ha llegado, entonces ya est√° en HBase. Vemos eventos, ejecutamos un conjunto de scripts. Vemos el siguiente evento, y todos los mismos eventos, porque ya est√°n grabados. Lo ejecutamos nuevamente y guardamos el cach√© con la fecha, lo comparamos con la marca de tiempo del evento.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f5/a41/48b/3f5a4148b5b8f14067816cda7b7bfade.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gracias a esto, el sistema obtuvo la propiedad de autoescalabilidad. El eje y es el porcentaje de lo que hacemos con las ID de usuario cuando nos llegan. Verde: el trabajo que realizamos lanz√≥ el script de segmentaci√≥n. Amarillo: no hicimos esto porque Ya segmentado exactamente estos datos. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/902/e54/03d/902e5403d2dc07b5f2a4ceaf57ea3e47.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se puede ver que hay recursos por la noche, hay menos flujo de datos y puede segmentar cada segundo evento. Un d√≠a de recursos m√°s peque√±o, y segmentamos solo el 20% de los eventos. Un salto al final del d√≠a: el socio subi√≥ archivos de datos que no hab√≠amos visto antes, y tuvieron que ser segmentados "honestamente".</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El sistema en s√≠ se adapta al crecimiento de la carga. Si tenemos un socio muy grande, procesamos los mismos datos pero con un poco menos de frecuencia. En este caso, las caracter√≠sticas del sistema se deteriorar√°n en la noche, la segmentaci√≥n se retrasar√° no por 2-3 segundos, sino por un minuto. Por la ma√±ana, agregue los servidores y vuelva a los resultados deseados. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por lo tanto, ahorramos aproximadamente 5 veces en los servidores. Ahora trabajamos en 10 servidores, por lo que tomar√≠a 50-60.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La peque√±a cosa azul en la parte superior son los bots. Esta es la parte m√°s dif√≠cil de la segmentaci√≥n. Tienen una gran cantidad de visitas, crean una carga muy grande en la plancha. Vemos cada bot en un servidor separado. Podemos recopilar en √©l un cach√© local con una lista negra de bots. Introdujo un simple antifraude: si un usuario realiza demasiadas visitas durante un tiempo determinado, entonces algo est√° mal con √©l, lo agregamos a la lista negra por un tiempo. Esta es una peque√±a franja azul, alrededor del 5%. Nos dieron otro 30% de ahorro en CPU. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por lo tanto, hemos logrado lo que vemos en todo el proceso de procesamiento de datos en cada etapa. Vemos m√©tricas de cu√°nto fue el mensaje en Kafka. Por la noche, algo apagado en alguna parte, el tiempo de procesamiento aument√≥ a un minuto, luego se solt√≥ y volvi√≥ a la normalidad.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/58e/fed/411/58efed411c7c5a4348b9280b3c963c16.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Podemos monitorear c√≥mo nuestras acciones con el sistema afectan su rendimiento, podemos ver cu√°nto se ejecuta el script, d√≥nde es necesario optimizar y cu√°nto se puede guardar. </font><font style="vertical-align: inherit;">Podemos ver el tama√±o de los segmentos, la din√°mica del tama√±o de los segmentos, evaluar su asociaci√≥n e intersecci√≥n. </font><font style="vertical-align: inherit;">Esto se puede hacer para m√°s o menos los mismos tama√±os de segmento.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ¬øQu√© te gustar√≠a refinar? </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tenemos un cl√∫ster Hadoop con algunos recursos inform√°ticos. Est√° ocupado: los analistas trabajan en √©l durante el d√≠a, pero por la noche es pr√°cticamente libre. En general, podemos contenerizar y ejecutar el segmentador como un proceso separado dentro de nuestro cl√∫ster. Queremos almacenar estad√≠sticas con mayor precisi√≥n para calcular con mayor precisi√≥n el volumen de la intersecci√≥n. Tambi√©n necesitamos optimizaci√≥n en la CPU. Esto afecta directamente el costo de la decisi√≥n. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para resumir: Kafka es bueno, pero, como con cualquier otra tecnolog√≠a, debe comprender c√≥mo funciona por dentro y qu√© le sucede. Por ejemplo, la garant√≠a de prioridad de mensajes solo funciona dentro de la partici√≥n. Si env√≠a un mensaje que va a diferentes particiones, no est√° claro en qu√© orden se procesar√°n.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los datos reales son muy importantes. </font><font style="vertical-align: inherit;">Si no hubi√©ramos probado el tr√°fico real, lo m√°s probable es que no hubi√©ramos visto problemas con los bots, con las sesiones de los usuarios. </font><font style="vertical-align: inherit;">Desarrollar√≠a algo en el vac√≠o, correr√≠a y se acostar√≠a. </font><font style="vertical-align: inherit;">Es importante monitorear lo que considera necesario monitorear, y no monitorear lo que no piensa.</font></font><br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Minuto de publicidad. </font><font style="vertical-align: inherit;">Si le gust√≥ este informe de la conferencia SmartData, tenga en cuenta que </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmartData 2018</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> se llevar√° a cabo en San Petersburgo el 15 de octubre, una </font><font style="vertical-align: inherit;">conferencia para aquellos que est√°n inmersos en el mundo del aprendizaje autom√°tico, el an√°lisis y el procesamiento de datos. </font><font style="vertical-align: inherit;">El programa tendr√° muchas cosas interesantes, el sitio ya tiene sus primeros oradores e informes.</font></font></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es421125/">https://habr.com/ru/post/es421125/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es421113/index.html">Presentamos DJI Mavic 2 Pro / Zoom</a></li>
<li><a href="../es421115/index.html">Contexto en una aplicaci√≥n de Android</a></li>
<li><a href="../es421119/index.html">Rastrillo submarino Desarrollo de SmartTV</a></li>
<li><a href="../es421121/index.html">Transmisi√≥n de video a trav√©s de un navegador con latencia ultra baja (¬°y WebRTC!)</a></li>
<li><a href="../es421123/index.html">Resumen de eventos de TI para septiembre</a></li>
<li><a href="../es421127/index.html">Seminarios web de Skillbox Friday: dise√±o y desarrolladores</a></li>
<li><a href="../es421129/index.html">C√≥mo reducir la revisi√≥n de c√≥digo de dos semanas a varias horas. La experiencia del equipo Yandex.Market</a></li>
<li><a href="../es421131/index.html">Vulnerabilidad cr√≠tica de los servidores 1Cloud</a></li>
<li><a href="../es421133/index.html">LINKa. Teclado de papel Botones extra grandes</a></li>
<li><a href="../es421135/index.html">Au / Ni / MgO: transferencia de calor a nanoescala</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>