<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143967986-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-143967986-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üÜé üë©üèº‚Äçüé§ üíô Nano-neuron - 7 simple JavaScript functions showing how the machine can "learn" ü§µüèª üë®‚Äçüë©‚Äçüëß‚Äçüëß üçé</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A nano-neuron is a simplified version of a neuron from the concept of a neural network. Nano-neuron performs the simplest task and is trained to conve...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nano-neuron - 7 simple JavaScript functions showing how the machine can "learn"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/479220/"><p> <a href="https://github.com/trekhleb/nano-neuron" rel="nofollow"><strong>A nano-neuron</strong></a> is a <em>simplified</em> version of a neuron from the concept of a neural network.  Nano-neuron performs the simplest task and is trained to convert temperature from degrees Celsius to degrees Fahrenheit. </p><br><p>  The <a href="" rel="nofollow"><strong>NanoNeuron.js</strong></a> code consists of 7 simple JavaScript functions involving learning, training, predicting, and direct and backward propagation of the model signal.  The purpose of writing these functions was to give the reader a minimal, basic explanation (intuition) of how, after all, a machine can ‚Äúlearn‚Äù.  The code does not use third-party libraries.  As the saying goes, only simple "vanilla" JavaScript functions. </p><br><p>  These functions are by <strong>no means</strong> an exhaustive guide to machine learning.  Many machine learning concepts are missing or simplified!  This simplification is allowed for the sole purpose - to give the reader the most <strong>basic</strong> understanding and intuition about how a machine can ‚Äúlearn‚Äù in principle, so that as a result, ‚ÄúMAGIC of machine learning‚Äù sounds more and more to the reader as ‚ÄúMATHEMATICS of machine learning‚Äù. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/98d/6c4/69e/98d6c469e1facbf97154fe29f698cd12.png" alt="Nanoneuron"></p><a name="habracut"></a><br><h2 id="chto-vyuchit-nash-nano-neyron">  What our nano-neuron will ‚Äúlearn‚Äù </h2><br><p>  You may have heard of neurons in the context of <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D1%2581%25D0%25BA%25D1%2583%25D1%2581%25D1%2581%25D1%2582%25D0%25B2%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">neural networks</a> .  A nano-neuron is a simplified version of that same neuron.  In this example, we will write its implementation from scratch.  For simplicity, we will not build a network of nano-neurons.  We will focus on creating one single nano-neuron and try to teach him how to convert temperature from degrees Celsius to degrees Fahrenheit.  In other words, we will teach him to <strong>predict the</strong> temperature in degrees Fahrenheit based on the temperature in degrees Celsius. </p><br><p>  By the way, the formula for converting degrees Celsius to degrees Fahrenheit is as follows: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9fa/2e8/8b5/9fa2e88b5a7324c8b9fc359b274ba091.png" alt="Celsius to fahrenheit"></p><br><p>  But at the moment, our nano-neuron knows nothing about this formula ... </p><br><h3 id="model-nano-neyrona">  Nano-neuron model </h3><br><p> Let's start by creating a function that describes the model of our nano-neuron.  This model is a simple linear relationship between <code>x</code> and <code>y</code> , which looks like this: <code>y = w * x + b</code> .  Simply put, our nano-neuron is a child who can draw a straight line in the <code>XY</code> coordinate system. </p><br><p>  Variables <code>w</code> and <code>b</code> are model <strong>parameters</strong> .  A nano-neuron knows only these two parameters of a linear function.  These parameters are precisely what our nano-neuron will learn during the training process. </p><br><p>  The only thing that a nano-neuron can do at this stage is to simulate linear relationships.  He does this in the <code>predict()</code> method, which takes a variable <code>x</code> at the input and predicts the variable <code>y</code> at the output.  No magic. </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">NanoNeuron</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">w, b</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.w = w; <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.b = b; <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.predict = <span class="hljs-function"><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">x</span></span></span><span class="hljs-function">) =&gt;</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x * <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.w + <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.b; } }</code> </pre> <br><p>  _ (... wait ... <a href="https://en.wikipedia.org/wiki/Linear_regression" rel="nofollow">linear regression</a> is you, or what?) _ </p><br><h3 id="konvertaciya-gradusov-celsiya-v-gradusy-farengeyta">  Convert degrees Celsius to degrees Fahrenheit </h3><br><p>  The temperature in degrees Celsius can be converted to degrees Fahrenheit according to the formula: <code>f = 1.8 * c + 32</code> , where <code>c</code> is the temperature in degrees Celsius and <code>f</code> is the temperature in degrees Fahrenheit. </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">celsiusToFahrenheit</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">c</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> w = <span class="hljs-number"><span class="hljs-number">1.8</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> b = <span class="hljs-number"><span class="hljs-number">32</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> f = c * w + b; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> f; };</code> </pre> <br><p>  As a result, we want our nano-neuron to be able to simulate this particular function.  He will have to guess (learn) that the parameter <code>w = 1.8</code> and <code>b = 32</code> without knowing it in advance. </p><br><p>  This is how the conversion function looks on the chart.  That is what our nano-neural ‚Äúbaby‚Äù must learn to ‚Äúdraw‚Äù: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/68b/0d6/8bc/68b0d68bcc7be00ec9526867b2fcecf3.png" alt="Celsius to fahrenheit conversion"></p><br><h3 id="generirovanie-dannyh">  Data generation </h3><br><p>  In classical programming, we know the input data ( <code>x</code> ) and the algorithm for converting this data (parameters <code>w</code> and <code>b</code> ), but the output data ( <code>y</code> ) is unknown.  The output is calculated based on the input using a known algorithm.  In machine learning, on the contrary, only the input and output data ( <code>x</code> and <code>y</code> ) are known, but the algorithm for switching from <code>x</code> to <code>y</code> unknown (parameters <code>w</code> and <code>b</code> ). </p><br><p>  It is the generation of input and output that we are now going to do.  We need to generate data for <strong>training</strong> our model and data for <strong>testing the</strong> model.  The <code>celsiusToFahrenheit()</code> helper function will help us with this.  Each of the training and test data sets is a set of pairs <code>x</code> and <code>y</code> .  For example, if <code>x = 2</code> , then <code>y = 35,6</code> and so on. </p><br><blockquote>  In the real world, most of the data is likely to be <em>collected</em> , not <em>generated</em> .  For example, such collected data may be a set of pairs of ‚Äúface photos‚Äù -&gt; ‚Äúperson‚Äôs name‚Äù. </blockquote><p>  We will use the TRAINING dataset to train our nano-neuron.  Before he grows up and is able to make decisions on his own, we must teach him what is ‚Äútrue‚Äù and what is ‚Äúfalse‚Äù using ‚Äúcorrect‚Äù data from a training set. </p><br><blockquote>  By the way, here the life principle ‚Äúgarbage at the entrance - garbage at the exit‚Äù is clearly traced.  If a nano-neuron throws a ‚Äúlie‚Äù into the training kit that 5 ¬∞ C is converted to 1000 ¬∞ F, then after many iterations of training, he will believe this and will correctly convert all temperature values <strong>except</strong> 5 ¬∞ C.  We need to be very careful with the training data that we load every day into our brain neural network. </blockquote><p>  Distracted.  Let's continue. </p><br><p>  We will use the TEST dataset to evaluate how well our nano-neuron has trained and can make correct predictions on new data that he did not see during his training. </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generateDataSets</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>) </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// xTrain -&gt; [0, 1, 2, ...], // yTrain -&gt; [32, 33.8, 35.6, ...] const xTrain = []; const yTrain = []; for (let x = 0; x &lt; 100; x += 1) { const y = celsiusToFahrenheit(x); xTrain.push(x); yTrain.push(y); } // xTest -&gt; [0.5, 1.5, 2.5, ...] // yTest -&gt; [32.9, 34.7, 36.5, ...] const xTest = []; const yTest = []; //   0.5    1,       //   ,       . for (let x = 0.5; x &lt; 100; x += 1) { const y = celsiusToFahrenheit(x); xTest.push(x); yTest.push(y); } return [xTrain, yTrain, xTest, yTest]; }</span></span></code> </pre> <br><h3 id="ocenka-pogreshnosti-predskazaniy">  Prediction error estimation </h3><br><p>  We need a certain metric (measurement, number, rating) that will show how close the prediction of a nano-neuron is to true.  In other words, this number / metric / function should show how right or wrong the nano neuron is.  It's like in school, a student can get a grade of <code>5</code> or <code>2</code> for his control. </p><br><p>  In the case of a nano-neuron, its error (error) between the true value of <code>y</code> and the predicted value of <code>prediction</code> will be produced by the formula: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/8d8/e50/ac1/8d8e50ac12d03614e65975f7b5d36931.png" alt="Prediction cost"></p><br><p>  As can be seen from the formula, we will consider the error as a simple difference between the two values.  The closer the values ‚Äã‚Äãare to each other, the smaller the difference.  We use squaring here to get rid of the sign, so that in the end <code>(1 - 2) ^ 2</code> equivalent to <code>(2 - 1) ^ 2</code> .  Division by <code>2</code> occurs solely in order to simplify the meaning of the derivative of this function in the formula for back propagation of a signal (more on this below). </p><br><p>  The error function in this case will look like this: </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predictionCost</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">y, prediction</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (y - prediction) ** <span class="hljs-number"><span class="hljs-number">2</span></span> / <span class="hljs-number"><span class="hljs-number">2</span></span>; <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 235.6 }</span></span></code> </pre> <br><h3 id="pryamoe-rasprostranenie-signala">  Direct signal propagation </h3><br><p>  Direct signal propagation through our model means making predictions for all pairs from the <code>xTrain</code> and <code>yTrain</code> training dataset and calculating the average error (error) of these predictions. </p><br><p>  We just let our nano-neuron ‚Äúspeak out‚Äù, allowing it to make predictions (convert temperature).  At the same time, a nano-neuron at this stage can be very wrong.  The average value of the prediction error will show us how far our model is / is close to the truth at the moment.  The error value is very important here, since by changing the parameters <code>w</code> and <code>b</code> and direct signal propagation again, we can evaluate whether our nano-neuron has become ‚Äúsmarter‚Äù with new parameters or not. </p><br><p>  The average prediction error of a nano-neuron will be performed using the following formula: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/575/db3/e0a/575db3e0a0c872b29582147e41231344.png" alt="Average cost"></p><br><p>  Where <code>m</code> is the number of training copies (in our case, we have <code>100</code> data pairs). </p><br><p>  Here's how we can implement this in code: </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forwardPropagation</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">model, xTrain, yTrain</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> m = xTrain.length; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> predictions = []; <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> cost = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">let</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; m; i += <span class="hljs-number"><span class="hljs-number">1</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> prediction = nanoNeuron.predict(xTrain[i]); cost += predictionCost(yTrain[i], prediction); predictions.push(prediction); } <span class="hljs-comment"><span class="hljs-comment">//     . cost /= m; return [predictions, cost]; }</span></span></code> </pre> <br><h3 id="obratnoe-rasprostranenie-signala">  Signal Reverse Propagation </h3><br><p>  Now that we know how our nano-neuron is right or wrong in its predictions (based on the average value of the error), how can we make the predictions more accurate? </p><br><p>  Reverse signal propagation will help us with this.  Signal back propagation is the process of evaluating the error of a nano-neuron and then adjusting its parameters <code>w</code> and <code>b</code> so that the next predictions of the nano-neuron for the entire set of training data become a little more accurate. </p><br><p>  This is where machine learning becomes like magic.  The key concept here is a <strong>derivative of the function</strong> , which shows what size step and which way we need to take in order to approach the minimum of the function (in our case, the minimum of the error function). </p><br><p>  The ultimate goal of training a nano-neuron is to find the minimum of the error function (see function above).  If we can find such values ‚Äã‚Äãof <code>w</code> and <code>b</code> at which the average value of the error function is small, then this will mean that our nano-neuron copes well with temperature predictions in degrees Fahrenheit. </p><br><p>  Derivatives are a large and separate topic that we will not cover in this article.  <a href="https://www.mathsisfun.com/calculus/derivatives-introduction.html" rel="nofollow">MathIsFun</a> is a great resource that can provide a basic understanding of derivatives. </p><br><p>  One thing that we must learn from the essence of a derivative and which will help us understand how the back propagation of a signal works is that the derivative of a function at a specific point <code>x</code> and <code>y</code> , by definition, is a tangent line to the curve of this function at <code>x</code> and <code>y</code> and <em>indicates us the direction to the minimum of the function</em> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/66d/bfd/49a/66dbfd49aaf1ced48d7f6b5917fddb12.svg" alt="Derivative slope"></p><br><p>  <em>Image taken from <a href="https://www.mathsisfun.com/calculus/derivatives-introduction.html" rel="nofollow">MathIsFun</a></em> </p><br><p>  For example, in the graph above, you see that at the point <code>(x=2, y=4)</code> slope of the tangent shows us that we need to move <code></code> and <code></code> to get to the minimum of the function.  Also note that the greater the slope of the tangent, the faster we must move to the minimum point. </p><br><p>  The derivatives of our average error function <code>averageCost</code> with <code>averageCost</code> to the parameters <code>w</code> and <code>b</code> will look like this: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/4cc/bda/ba1/4ccbdaba120c399c1528e2bc38cf0efd.png" alt="dW"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e02/0cb/125/e020cb125449849009a9f565a32ff46f.png" alt="dB"></p><br><p>  Where <code>m</code> is the number of training copies (in our case, we have <code>100</code> data pairs). </p><br><p>  <em>You can read in more detail about how to take the derivative of complex functions <a href="https://www.mathsisfun.com/calculus/derivatives-rules.html" rel="nofollow">here</a> .</em> </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backwardPropagation</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">predictions, xTrain, yTrain</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> m = xTrain.length; <span class="hljs-comment"><span class="hljs-comment">//           'w'  'b'. //      0. let dW = 0; let dB = 0; for (let i = 0; i &lt; m; i += 1) { dW += (yTrain[i] - predictions[i]) * xTrain[i]; dB += yTrain[i] - predictions[i]; } //    . dW /= m; dB /= m; return [dW, dB]; }</span></span></code> </pre> <br><h3 id="trenirovka-modeli">  Model training </h3><br><p>  Now we know how to estimate the error / error of the predictions of our nano-neuron model for all training data (direct signal propagation).  We also know how to adjust the parameters <code>w</code> and <code>b</code> the nano-neuron model (back propagation of the signal) in order to improve the accuracy of the predictions.  The problem is that if we perform forward and backward propagation of the signal only once, then this will not be enough for our model to identify and learn the dependencies and laws in the training data.  You can compare this to a student's one-day school visit.  He / she must go to school regularly, day after day, year after year, in order to learn all the material. </p><br><p>  So, we must <em>repeat the</em> forward and backward propagation of the signal many times.  This is <code>trainModel()</code> function <code>trainModel()</code> .  She‚Äôs like a ‚Äúteacher‚Äù for the model of our nano-neuron: </p><br><ul><li>  she will spend some time ( <code>epochs</code> ) with our still silly nano-neuron, trying to train him, </li><li>  she will use special books ( <code>xTrain</code> and <code>yTrain</code> datasets) for training, </li><li>  it encourages our ‚Äústudent‚Äù to study more diligently (faster) using the <code>alpha</code> parameter, which essentially controls the speed of learning. </li></ul><br><p>  A few words about the <code>alpha</code> parameter.  This is just a coefficient (multiplier) for the values ‚Äã‚Äãof the variables <code>dW</code> and <code>dB</code> , which we calculate during the back propagation of the signal.  So, the derivative showed us the direction to the minimum of the error function (the signs of the values ‚Äã‚Äãof <code>dW</code> and <code>dB</code> tell us this).  The derivative also showed us how quickly we need to move towards the minimum of the function (the absolute values ‚Äã‚Äãof <code>dW</code> and <code>dB</code> tell us this).  Now we need to multiply the step size by <code>alpha</code> in order to adjust the speed of our approach to a minimum (the total step size).  Sometimes, if we use large values ‚Äã‚Äãfor <code>alpha</code> , we can go in such large steps that we can simply <em>step over the</em> minimum of the function, thereby skipping it. </p><br><p>  By analogy with the ‚Äúteacher‚Äù, the stronger she would force our ‚Äúnano-student‚Äù to learn, the faster he would learn, BUT, if you force and put pressure on him very hard, then our ‚Äúnano-student‚Äù may experience a nervous breakdown and complete apathy and he won‚Äôt learn anything at all. </p><br><p>  We will update the parameters of our model <code>w</code> and <code>b</code> as follows: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c7b/db8/84f/c7bdb884f2a940d62332246cdbcb44bc.png" alt="w"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b57/622/0ab/b576220ab6515d44255ef56699077bab.png" alt="b"></p><br><p>  And this is how the training itself looks: </p><br><pre> <code class="javascript hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">trainModel</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">{model, epochs, alpha, xTrain, yTrain}</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-comment"><span class="hljs-comment">//     -.  . const costHistory = []; //    ()  for (let epoch = 0; epoch &lt; epochs; epoch += 1) { //   . const [predictions, cost] = forwardPropagation(model, xTrain, yTrain); costHistory.push(cost); //   . const [dW, dB] = backwardPropagation(predictions, xTrain, yTrain); //    -,    . nanoNeuron.w += alpha * dW; nanoNeuron.b += alpha * dB; } return costHistory; }</span></span></code> </pre> <br><h3 id="soberem-vse-funkcii-vmeste">  Putting all the features together </h3><br><p>  Time to use all previously created functions together. </p><br><p>  Create an instance of the nano-neuron model.  At the moment, the nano-neuron does not know anything about what the parameters <code>w</code> and <code>b</code> should be.  So let's set <code>w</code> and <code>b</code> randomly. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> w = <span class="hljs-built_in"><span class="hljs-built_in">Math</span></span>.random(); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 0.9492 const b = Math.random(); // ie -&gt; 0.4570 const nanoNeuron = new NanoNeuron(w, b);</span></span></code> </pre> <br><p>  We generate training and test data sets. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [xTrain, yTrain, xTest, yTest] = generateDataSets();</code> </pre> <br><p>  Now let's try to train our model using small steps ( <code>0.0005</code> ) for <code>70000</code> eras.  You can experiment with these parameters, they are determined empirically. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> epochs = <span class="hljs-number"><span class="hljs-number">70000</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> alpha = <span class="hljs-number"><span class="hljs-number">0.0005</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> trainingCostHistory = trainModel({<span class="hljs-attr"><span class="hljs-attr">model</span></span>: nanoNeuron, epochs, alpha, xTrain, yTrain});</code> </pre> <br><p>  Let's check how the error value of our model changed during training.  We expect that the error value after training should be significantly less than before training.  This would mean that our nano-neuron wiser.  The opposite option is also possible, when after training, the error of predictions only increased (for example, large values ‚Äã‚Äãof the learning step <code>alpha</code> ). </p><br><pre> <code class="javascript hljs"><span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">'  :'</span></span>, trainingCostHistory[<span class="hljs-number"><span class="hljs-number">0</span></span>]); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 4694.3335043 console.log('  :', trainingCostHistory[epochs - 1]); // ie -&gt; 0.0000024</span></span></code> </pre> <br><p>  And here is how the value of the model error changed during training.  On the <code>x</code> axis are epochs (in thousands).  We expect that the chart will be decreasing. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/191/860/d6f/191860d6f0cd8cf7d24127f04f779462.png" alt="Training process"></p><br><p>  Let's look at what parameters our nano-neuron ‚Äúlearned‚Äù.  We expect that the parameters <code>w</code> and <code>b</code> will be similar to the parameters of the same name from the <code>celsiusToFahrenheit()</code> function ( <code>w = 1.8</code> and <code>b = 32</code> ), because it was her nano-neuron that I tried to simulate. </p><br><pre> <code class="javascript hljs"><span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">' -:'</span></span>, {<span class="hljs-attr"><span class="hljs-attr">w</span></span>: nanoNeuron.w, <span class="hljs-attr"><span class="hljs-attr">b</span></span>: nanoNeuron.b}); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; {w: 1.8, b: 31.99}</span></span></code> </pre> <br><p>  As you can see, the nano-neuron is very close to the <code>celsiusToFahrenheit()</code> function. </p><br><p>  Now let's see how accurate the predictions of our nano-neuron are for test data that he did not see during training.  The prediction error for the test data should be close to the prediction error for the training data.  This will mean that the nano-neuron has learned the correct dependencies and can correctly abstract its experience from previously unknown data (this is the whole value of the model). </p><br><pre> <code class="javascript hljs">[testPredictions, testCost] = forwardPropagation(nanoNeuron, xTest, yTest); <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">'   :'</span></span>, testCost); <span class="hljs-comment"><span class="hljs-comment">// ie -&gt; 0.0000023</span></span></code> </pre> <br><p>  Now, since our "nano-baby" was well trained in the "school" and now knows how to accurately convert degrees Celsius to degrees Fahrenheit even for data that he did not see, we can call him reasonably smart.  Now we can even ask him for advice on temperature conversion, and that was the purpose of the whole training. </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> tempInCelsius = <span class="hljs-number"><span class="hljs-number">70</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> customPrediction = nanoNeuron.predict(tempInCelsius); <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(<span class="hljs-string"><span class="hljs-string">`- "",  </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${tempInCelsius}</span></span></span><span class="hljs-string">¬∞C   :`</span></span>, customPrediction); <span class="hljs-comment"><span class="hljs-comment">// -&gt; 158.0002 console.log('  :', celsiusToFahrenheit(tempInCelsius)); // -&gt; 158</span></span></code> </pre> <br><p>  Very close!  Like people, our nano-neuron is good, but not perfect :) </p><br><p>  Successful coding! </p><br><h2 id="kak-zapustit-i-protestirovat-nano-neyron">  How to run and test a nano-neuron </h2><br><p>  You can clone the repository and run the nano neuron locally: </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/trekhleb/nano-neuron.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> nano-neuron</code> </pre> <br><pre> <code class="bash hljs">node ./NanoNeuron.js</code> </pre> <br><h2 id="upuschennye-koncepcii">  Missed concepts </h2><br><p>  The following machine learning concepts have been omitted or simplified for ease of explanation. </p><br><p>  <strong>Separation of training and test data sets</strong> </p><br><p>  Usually you have one big data set.  Depending on the number of copies in this set, its division into training and test sets can be carried out in the proportion of 70/30.  The data in the set must be randomly mixed before being split.  If the amount of data is large (for example, millions), then the division into test and training sets can be carried out in proportions close to 90/10 or 95/5. </p><br><p>  <strong>Online power</strong> </p><br><p>  Usually you will not find cases when only one neuron is used.  Strength is in the <a href="https://en.wikipedia.org/wiki/Neural_network" rel="nofollow">network of</a> such neurons.  A neural network can learn much more complex dependencies. </p><br><p>  Also in the example above, our nano-neuron may look more like a simple <a href="https://en.wikipedia.org/wiki/Linear_regression" rel="nofollow">linear regression</a> than a neural network. </p><br><p>  <strong>Input Normalization</strong> </p><br><p>  Before training, it is customary to <a href="https://www.jeremyjordan.me/batch-normalization/" rel="nofollow">normalize the input data</a> . </p><br><p>  <strong>Vector implementation</strong> </p><br><p>  For neural networks, vector (matrix) calculations are much faster than calculations in <code>for</code> loops.  Usually the direct and reverse signal propagation is performed using matrix operations using, for example, the Python <a href="https://numpy.org/" rel="nofollow">Numpy</a> library. </p><br><p>  <strong>Minimum Error Function</strong> </p><br><p>  The error function that we used for the nano neuron is very simplified.  It should contain <a href="https://stackoverflow.com/questions/32986123/why-the-cost-function-of-logistic-regression-has-a-logarithmic-expression/32998675" rel="nofollow">logarithmic components</a> .  A change in the formula for the error function will also entail a change in the formulas for the forward and backward propagation of the signal. </p><br><p>  <strong>Activation function</strong> </p><br><p>  Usually the output value of the neuron passes through the activation function.  For activation, functions such as <a href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="nofollow">Sigmoid</a> , <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="nofollow">ReLU</a> and others can be used. </p></div></div><p>Source: <a href="https://habr.com/ru/post/479220/">https://habr.com/ru/post/479220/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../479202/index.html">C ++ and Numerical Methods: Approximate Newton-Cotes Integration</a></li>
<li><a href="../479210/index.html">What will happen to purchases in foreign online stores from January 1, 2020</a></li>
<li><a href="../479214/index.html">A selection of upcoming free events for developers in Moscow # 2</a></li>
<li><a href="../479216/index.html">Second wind Pandora DXL 3000 or how I screwed my own telemetry</a></li>
<li><a href="../479218/index.html">How to make a bot that turns a photo into a comic book: step-by-step instructions for dummies</a></li>
<li><a href="../479222/index.html">The digest of interesting materials for the mobile # 325 developer (on December 2 - 8)</a></li>
<li><a href="../479226/index.html">Habr-analysis: what users order as a gift from Habr</a></li>
<li><a href="../479232/index.html">MQ JMS application development on Spring Boot</a></li>
<li><a href="../479234/index.html">News from the world of OpenStreetMap No. 488 (11/19/2019 - 11/25/2019)</a></li>
<li><a href="../479236/index.html">Kivy. Build packages for Android and no magic</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter54458986 = new Ya.Metrika({
                  id:54458986,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/54458986" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-143967986-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=IU0EG0jaqnehka2lu5TyzAcchrZXI4Yb1QXKQvJxpqE&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>