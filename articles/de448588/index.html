<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë≤ ‚ùÑÔ∏è üèáüèø Die Architektur des Netzwerklastenausgleichs in Yandex.Cloud üå≠ ‚ÄºÔ∏è ü§òüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo, ich bin Sergey Elantsev und entwickle einen Netzwerklastenausgleich in Yandex.Cloud. Zuvor leitete ich die Entwicklung des L7-Balancers des Yan...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Architektur des Netzwerklastenausgleichs in Yandex.Cloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/448588/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/8p/s5/_f/8ps5_f3kanb-pmkeuze4rdd971i.png" width="430"></div><br>  Hallo, ich bin Sergey Elantsev und entwickle einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerklastenausgleich</a> in Yandex.Cloud.  Zuvor leitete ich die Entwicklung des L7-Balancers des Yandex-Portals - meine Kollegen scherzen, dass ich unabh√§ngig von meiner T√§tigkeit einen Balancer bekomme.  Ich werde den Lesern von Habr erkl√§ren, wie die Last in der Cloud-Plattform verwaltet wird, wie wir das ideale Tool zur Erreichung dieses Ziels sehen und wie wir dieses Tool entwickeln. <a name="habracut"></a><br><br>  Zun√§chst stellen wir einige Begriffe vor: <br><br><ul><li>  VIP (Virtual IP) - Balancer-IP-Adresse </li><li>  Server, Backend, Instanz - eine virtuelle Maschine, auf der eine Anwendung ausgef√ºhrt wird </li><li>  RIP (Real IP) - Server-IP-Adresse </li><li>  Healthcheck - Serververf√ºgbarkeitspr√ºfung </li><li>  Availability Zone, AZ - isolierte Infrastruktur im Rechenzentrum </li><li>  Region - die Vereinigung verschiedener AZ </li></ul><br>  Load Balancer l√∂sen drei Hauptaufgaben: Sie f√ºhren den Ausgleich selbst durch, verbessern die Fehlertoleranz des Dienstes und vereinfachen dessen Skalierung.  Die Fehlertoleranz wird durch die automatische Verkehrssteuerung sichergestellt: Der Balancer √ºberwacht den Status der Anwendung und schlie√üt Instanzen vom Balancing aus, die den Lebendigkeitstest nicht bestehen.  Die Skalierung wird durch eine gleichm√§√üige Lastverteilung auf die Instanzen sowie durch die Aktualisierung der Liste der Instanzen im laufenden Betrieb sichergestellt.  Wenn der Ausgleich nicht einheitlich genug ist, erhalten einige Instanzen eine Last, die ihre Arbeitskapazit√§tsgrenze √ºberschreitet, und der Dienst wird weniger zuverl√§ssig. <br><br>  Der Load Balancer wird h√§ufig nach Protokollebene aus dem OSI-Modell klassifiziert, auf dem er ausgef√ºhrt wird.  Der Cloud Balancer arbeitet auf der TCP-Ebene, die der vierten Ebene, L4, entspricht. <br><br>  Kommen wir zu einem √úberblick √ºber die Cloud-Balancer-Architektur.  Wir werden den Detaillierungsgrad schrittweise erh√∂hen.  Wir teilen die Balancer-Komponenten in drei Klassen ein.  Die Konfigurationsebenenklasse ist f√ºr die Benutzerinteraktion verantwortlich und speichert den Zielstatus des Systems.  Die Steuerebene speichert den aktuellen Status des Systems und verwaltet Systeme aus der Datenebenenklasse, die direkt f√ºr die √úbermittlung des Datenverkehrs von Clients an Ihre Instanzen verantwortlich sind. <br><br><h3>  Datenebene </h3><br>  Der Datenverkehr f√§llt auf teure Ger√§te, die als Grenzrouter bezeichnet werden.  Um die Fehlertoleranz zu erh√∂hen, arbeiten mehrere solcher Ger√§te gleichzeitig in einem Rechenzentrum.  Anschlie√üend wird der Datenverkehr an Balancer weitergeleitet, die allen AZs √ºber BGP eine Anycast-IP-Adresse f√ºr Clients mitteilen. <br><br><img src="https://habrastorage.org/webt/pc/jl/-n/pcjl-nh1bq9egjh1d0iv3o4u9bc.jpeg"><br><br>  Der Datenverkehr wird √ºber ECMP √ºbertragen. Dies ist eine Routing-Strategie, nach der es mehrere gleich gute Routen zum Ziel geben kann (in unserem Fall ist das Ziel die Ziel-IP-Adresse) und Pakete an eine von ihnen gesendet werden k√∂nnen.  Wir unterst√ºtzen auch die Arbeit in mehreren Zugangszonen nach folgendem Schema: Wir geben die Adresse in jeder der Zonen bekannt, der Verkehr f√§llt in die n√§chste und geht nicht bereits dar√ºber hinaus.  Weiter unten in der Post werden wir genauer untersuchen, was mit dem Verkehr passiert. <br><br><h3>  Konfigurationsflugzeug </h3><br>  Die Schl√ºsselkomponente der Konfigurationsebene ist die API, √ºber die die grundlegenden Operationen mit Balancern ausgef√ºhrt werden: Erstellen, L√∂schen, √Ñndern der Zusammensetzung von Instanzen, Abrufen von Healthcheck-Ergebnissen usw. Einerseits handelt es sich um eine REST-API, andererseits verwenden wir h√§ufig das Framework in der Cloud gRPC, also "√ºbersetzen" wir REST in gRPC und verwenden dann nur gRPC.  Jede Anforderung f√ºhrt zur Erstellung einer Reihe von asynchronen idempotenten Aufgaben, die in einem gemeinsam genutzten Pool von Yandex.Cloud-Mitarbeitern ausgef√ºhrt werden.  Aufgaben werden so geschrieben, dass sie jederzeit angehalten und anschlie√üend neu gestartet werden k√∂nnen.  Dies bietet Skalierbarkeit, Wiederholbarkeit und Protokollierungsvorg√§nge. <br><br><img src="https://habrastorage.org/webt/yb/ru/-c/ybru-c8q6hucu-tdqfybedsjrcu.jpeg"><br><br>  Infolgedessen sendet die Aufgabe von der API eine Anforderung an den Balancer-Service-Controller, der in Go geschrieben ist.  Er kann Balancer hinzuf√ºgen und entfernen, die Zusammensetzung von Backends und Einstellungen √§ndern. <br><br><img src="https://habrastorage.org/webt/wr/v3/v7/wrv3v7dfm1_k48ztxc9wfzvvq48.jpeg"><br><br>  Der Dienst speichert seinen Status in der Yandex-Datenbank - einer verteilten verwalteten Datenbank, die Sie auch bald verwenden k√∂nnen.  In Yandex.Cloud funktioniert, wie bereits erw√§hnt, das Hundefutterkonzept: Wenn wir unsere Dienstleistungen selbst nutzen, werden unsere Kunden sie auch gerne nutzen.  Die Yandex-Datenbank ist ein Beispiel f√ºr die Implementierung eines solchen Konzepts.  Wir speichern alle unsere Daten in YDB und m√ºssen nicht √ºber die Pflege und Skalierung der Datenbank nachdenken. Diese Probleme sind f√ºr uns gel√∂st. Wir verwenden die Datenbank als Service. <br><br>  Wir kehren zum Balancer-Controller zur√ºck.  Seine Aufgabe besteht darin, Informationen √ºber den Balancer zu speichern und die Bereitschaft zur √úberpr√ºfung der Bereitschaft der virtuellen Maschine an den Healthcheck-Controller zu senden. <br><br><h3>  Healthcheck-Controller </h3><br>  Es empf√§ngt Anforderungen zum √Ñndern von Inspektionsregeln, speichert sie in YDB, verteilt Aufgaben an Healtcheck-Knoten und aggregiert die Ergebnisse, die dann in der Datenbank gespeichert und an den Loadbalancer-Controller gesendet werden.  Er sendet seinerseits eine Anfrage, um die Zusammensetzung des Clusters in der Datenebene an den Loadbalancer-Knoten zu √§ndern, worauf ich weiter unten eingehen werde. <br><br><img src="https://habrastorage.org/webt/qx/p2/ll/qxp2llhomz9slemwamgsylotd4k.jpeg" width="600"><br><br>  Lassen Sie uns mehr √ºber Gesundheitschecks sprechen.  Sie k√∂nnen in mehrere Klassen unterteilt werden.  Audits haben unterschiedliche Erfolgskriterien.  TCP-Pr√ºfungen m√ºssen in einer festgelegten Zeit erfolgreich eine Verbindung herstellen.  HTTP-Pr√ºfungen erfordern sowohl eine erfolgreiche Verbindung als auch eine Antwort mit einem Statuscode von 200. <br><br>  Au√üerdem unterscheiden sich die Pr√ºfungen in der Aktionsklasse - sie sind aktiv und passiv.  Passive √úberpr√ºfungen √ºberwachen einfach, was mit dem Verkehr passiert, ohne besondere Ma√ünahmen zu ergreifen.  Dies funktioniert bei L4 nicht sehr gut, da es von der Logik der √ºbergeordneten Protokolle abh√§ngt: Bei L4 gibt es keine Informationen dar√ºber, wie lange der Vorgang gedauert hat und ob die Verbindung gut oder schlecht war.  Bei aktiven √úberpr√ºfungen muss der Balancer Anforderungen an jede Serverinstanz senden. <br><br>  Die meisten Load Balancer f√ºhren ihre Lebendigkeitspr√ºfungen selbst durch.  Wir bei Cloud haben beschlossen, diese Teile des Systems zu trennen, um die Skalierbarkeit zu verbessern.  Dieser Ansatz erm√∂glicht es uns, die Anzahl der Balancer zu erh√∂hen und gleichzeitig die Anzahl der Healthcheck-Anforderungen an den Service beizubehalten.  √úberpr√ºfungen werden von separaten Healthcheck-Knoten durchgef√ºhrt, die zum Sharding und Replizieren von Testzielen verwendet werden.  Es ist unm√∂glich, √úberpr√ºfungen von einem Host aus durchzuf√ºhren, da dies fehlschlagen kann.  Dann erhalten wir nicht den Status der Instanzen, die er √ºberpr√ºft hat.  Wir f√ºhren √úberpr√ºfungen f√ºr jede Instanz von mindestens drei Healthcheck-Knoten durch.  Die Ziele von √úberpr√ºfungen werden zwischen Knoten mithilfe konsistenter Hashing-Algorithmen aufgeteilt. <br><br><img src="https://habrastorage.org/webt/6u/r4/pp/6ur4pp9sk-nqulkepwvqhvb-kdg.jpeg"><br><br>  Die Trennung von Balancing und Healthcheck kann zu Problemen f√ºhren.  Wenn der Healthcheck-Knoten unter Umgehung des Balancers (der derzeit keinen Datenverkehr bedient) Anforderungen an die Instanz stellt, tritt eine seltsame Situation auf: Die Ressource scheint aktiv zu sein, aber der Datenverkehr erreicht sie nicht.  Wir l√∂sen dieses Problem auf diese Weise: Wir erhalten garantiert Healthcheck-Verkehr durch Balancer.  Mit anderen Worten, das Schema des Verschiebens von Paketen mit Datenverkehr von Clients und von Integrit√§tspr√ºfungen unterscheidet sich minimal: In beiden F√§llen werden Pakete an die Balancer gesendet, die sie an die Zielressourcen liefern. <br><br>  Der Unterschied besteht darin, dass Kunden Anfragen nach VIPs stellen und sich die Gesundheitspr√ºfungen auf jeden einzelnen RIP beziehen.  Hier ergibt sich ein interessantes Problem: Wir geben unseren Benutzern die M√∂glichkeit, Ressourcen in grauen IP-Netzwerken zu erstellen.  Stellen Sie sich vor, es gibt zwei verschiedene Cloud-Besitzer, die ihre Dienste f√ºr Balancer versteckt haben.  Jeder von ihnen verf√ºgt au√üerdem √ºber Ressourcen im Subnetz 10.0.0.1/24 mit denselben Adressen.  Sie m√ºssen in der Lage sein, sie auf irgendeine Weise zu unterscheiden, und hier m√ºssen Sie in das Ger√§t des virtuellen Netzwerks Yandex.Cloud eintauchen.  Weitere Informationen finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Video zum Ereignis about: cloud.</a> F√ºr uns ist es jetzt wichtig, dass das Netzwerk mehrschichtig ist und √ºber Tunnel verf√ºgt, die anhand der Subnetz-ID unterschieden werden k√∂nnen. <br><br>  Healthcheck-Knoten greifen √ºber sogenannte Quasi-IPv6-Adressen auf Balancer zu.  Eine Quasi-Adresse ist eine IPv6-Adresse, innerhalb derer die IPv4-Adresse und die Benutzer-Subnetz-ID gesch√ºtzt sind.  Der Datenverkehr f√§llt auf den Balancer, er extrahiert die IPv4-Adresse der Ressource daraus, ersetzt IPv6 durch IPv4 und sendet das Paket an das Netzwerk des Benutzers. <br><br>  Der umgekehrte Datenverkehr verl√§uft auf die gleiche Weise: Der Balancer erkennt anhand von Integrit√§tspr√ºfungen, dass das Ziel ein graues Netzwerk ist, und konvertiert IPv4 in IPv6. <br><br><h3>  VPP - das Herz der Datenebene </h3><br>  Der Balancer basiert auf der Technologie der Vector Packet Processing (VPP) - einem Framework von Cisco f√ºr die Paketverarbeitung des Netzwerkverkehrs.  In unserem Fall l√§uft das Framework auf der Bibliothek zur Verwaltung des Benutzerraums von Netzwerkger√§ten - Data Plane Development Kit (DPDK).  Dies bietet eine hohe Paketverarbeitungsleistung: Es gibt viel weniger Unterbrechungen im Kernel, es gibt keine Kontextwechsel zwischen Kernelraum und Benutzerraum. <br><br>  VPP geht noch weiter und bringt noch mehr Leistung aus dem System, indem Pakete zu Stapeln kombiniert werden.  Eine erh√∂hte Produktivit√§t ist auf den aggressiven Einsatz von Caches moderner Prozessoren zur√ºckzuf√ºhren.  Beide Datencaches werden verwendet (Pakete werden von ‚ÄûVektoren‚Äú verarbeitet, Daten liegen nahe beieinander) und Befehls-Caches: In VPP folgt die Paketverarbeitung einem Diagramm, in dessen Knoten Funktionen vorhanden sind, die eine Aufgabe ausf√ºhren. <br><br>  Beispielsweise wird die Verarbeitung von IP-Paketen in VPP in der folgenden Reihenfolge fortgesetzt: Zuerst werden Paket-Header im Analyseknoten analysiert und dann an den Knoten gesendet, der die Pakete gem√§√ü den Routing-Tabellen weiterleitet. <br><br>  Ein bisschen Hardcore.  VPP-Autoren gehen bei der Verwendung von Prozessor-Caches keine Kompromisse ein, daher enth√§lt ein typischer Paketvektor-Verarbeitungscode eine manuelle Vektorisierung: Es gibt einen Verarbeitungszyklus, in dem die Situation wie "Wir haben vier Pakete in der Warteschlange" verarbeitet wird, dann dieselbe f√ºr zwei, dann - f√ºr einen.  Oft werden Prefetch-Anweisungen verwendet, die Daten in Caches laden, um den Zugriff auf sie bei den folgenden Iterationen zu beschleunigen. <br><br><pre><code class="cpp hljs">n_left_from = frame-&gt;n_vectors; <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (n_left_from &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) { vlib_get_next_frame (vm, node, next_index, to_next, n_left_to_next); <span class="hljs-comment"><span class="hljs-comment">// ... while (n_left_from &gt;= 4 &amp;&amp; n_left_to_next &gt;= 2) { // processing multiple packets at once u32 next0 = SAMPLE_NEXT_INTERFACE_OUTPUT; u32 next1 = SAMPLE_NEXT_INTERFACE_OUTPUT; // ... /* Prefetch next iteration. */ { vlib_buffer_t *p2, *p3; p2 = vlib_get_buffer (vm, from[2]); p3 = vlib_get_buffer (vm, from[3]); vlib_prefetch_buffer_header (p2, LOAD); vlib_prefetch_buffer_header (p3, LOAD); CLIB_PREFETCH (p2-&gt;data, CLIB_CACHE_LINE_BYTES, STORE); CLIB_PREFETCH (p3-&gt;data, CLIB_CACHE_LINE_BYTES, STORE); } // actually process data /* verify speculative enqueues, maybe switch current next frame */ vlib_validate_buffer_enqueue_x2 (vm, node, next_index, to_next, n_left_to_next, bi0, bi1, next0, next1); } while (n_left_from &gt; 0 &amp;&amp; n_left_to_next &gt; 0) { // processing packets by one } // processed batch vlib_put_next_frame (vm, node, next_index, n_left_to_next); }</span></span></code> </pre> <br>  Healthchecks wandeln also IPv6 an VPP um, wodurch sie in IPv4 umgewandelt werden.  Dies geschieht durch den Graphknoten, den wir algorithmisches NAT nennen.  F√ºr den umgekehrten Verkehr (und die Konvertierung von IPv6 nach IPv4) gibt es denselben Knoten f√ºr algorithmisches NAT. <br><br><img src="https://habrastorage.org/webt/ug/ju/n4/ugjun48y3qurodsxpuia5lfbhn0.jpeg" width="400"><br><br>  Der direkte Datenverkehr von den Balancer-Clients durchl√§uft die Knoten des Diagramms, die den Ausgleich selbst durchf√ºhren. <br><br><img src="https://habrastorage.org/webt/p4/eq/uv/p4equvplkheuowkh-ddthjgxeou.jpeg"><br><br>  Der erste Knoten sind Sticky Sessions.  Es speichert einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">5-Tupel-</a> Hash f√ºr etablierte Sitzungen.  5-Tupel enth√§lt die Adresse und den Port des Clients, von dem Informationen √ºbertragen werden, die Adresse und die Ports der Ressourcen, die f√ºr den Empfang von Verkehr verf√ºgbar sind, sowie das Netzwerkprotokoll. <br><br>  Der 5-Tupel-Hash hilft uns, weniger Berechnungen im nachfolgenden konsistenten Hash-Knoten durchzuf√ºhren und auch die √Ñnderung in der Liste der Ressourcen hinter dem Balancer besser zu handhaben.  Wenn ein Paket beim Balancer ankommt, f√ºr das es keine Sitzung gibt, wird es an den konsistenten Hashing-Knoten gesendet.  Hier erfolgt der Ausgleich mithilfe von konsistentem Hashing: Wir w√§hlen eine Ressource aus der Liste der verf√ºgbaren "Live" -Ressourcen aus.  Anschlie√üend werden die Pakete an den NAT-Knoten gesendet, der die Zieladresse ersetzt und die Pr√ºfsummen neu berechnet.  Wie Sie sehen k√∂nnen, befolgen wir die Regeln von VPP - √§hnlich wie bei √§hnlichen, gruppen√§hnlichen Berechnungen, um die Effizienz von Prozessor-Caches zu erh√∂hen. <br><br><h3>  Konsequentes Hashing </h3><br>  Warum haben wir ihn gew√§hlt und worum geht es?  Betrachten Sie zun√§chst die vorherige Aufgabe - Auswahl einer Ressource aus der Liste. <br><br><img src="https://habrastorage.org/webt/ab/ez/jl/abezjle6p5u8g54jthaylj5duhi.jpeg"><br><br>  Bei nicht konsistentem Hashing wird der Hash aus dem eingehenden Paket berechnet und die Ressource aus der Liste ausgew√§hlt, indem der Rest dieses Hash durch die Anzahl der Ressourcen dividiert wird.  Solange die Liste unver√§ndert bleibt, funktioniert ein solches Schema gut: Wir senden immer Pakete mit demselben 5-Tupel an dieselbe Instanz.  Wenn beispielsweise eine Ressource nicht mehr auf Integrit√§tspr√ºfungen reagiert, √§ndert sich f√ºr einen erheblichen Teil der Hashes die Auswahl.  TCP-Verbindungen werden auf dem Client unterbrochen: Ein Paket, das zuvor an Instanz A gesendet wurde, f√§llt m√∂glicherweise auf Instanz B, die mit der Sitzung f√ºr dieses Paket nicht vertraut ist. <br><br>  Konsistentes Hashing l√∂st das beschriebene Problem.  Der einfachste Weg, dieses Konzept zu erkl√§ren, ist folgender: Stellen Sie sich vor, Sie haben einen Ring, in den Sie Ressourcen per Hash (z. B. per IP: Port) verteilen.  Die Wahl einer Ressource ist die Drehung des Rades um einen Winkel, der durch den Hash aus dem Paket bestimmt wird. <br><br><img src="https://habrastorage.org/webt/qg/rt/yq/qgrtyq_obonzlczzouq4tcshaqq.jpeg"><br><br>  Dies minimiert die Umverteilung des Datenverkehrs, wenn die Zusammensetzung der Ressourcen ge√§ndert wird.  Das L√∂schen einer Ressource wirkt sich nur auf den Teil des konsistenten Hash-Rings aus, auf dem sich die angegebene Ressource befand.  Das Hinzuf√ºgen einer Ressource √§ndert auch die Verteilung, aber wir haben einen Sticky-Sessions-Knoten, mit dem wir bereits eingerichtete Sitzungen nicht auf neue Ressourcen umstellen k√∂nnen. <br><br>  Wir haben untersucht, was mit dem direkten Verkehr zwischen dem Balancer und den Ressourcen passiert.  Lassen Sie uns nun den umgekehrten Verkehr behandeln.  Es folgt dem gleichen Muster wie der Verifizierungsverkehr - durch algorithmisches NAT, dh durch umgekehrtes NAT 44 f√ºr den Clientverkehr und √ºber NAT 46 f√ºr den Healthchecks-Verkehr.  Wir halten uns an unser eigenes Schema: Wir vereinen den Healthchecks-Verkehr und den realen Benutzerverkehr. <br><br><h3>  Baugruppe aus Loadbalancer-Knoten und Komponenten </h3><br>  Die Zusammensetzung der Balancer und Ressourcen in VPP wird vom lokalen Dienst - Loadbalancer-Node - gemeldet.  Er abonniert den Ereignisfluss vom Loadbalancer-Controller und kann die Differenz zwischen dem aktuellen Status des VPP und dem vom Controller empfangenen Zielstatus erstellen.  Wir erhalten ein geschlossenes System: Ereignisse von der API kommen zum Balancer-Controller, der die Healthcheck-Controller-Aufgaben festlegt, um die "Lebendigkeit" der Ressourcen zu √ºberpr√ºfen.  Dadurch werden wiederum Aufgaben im Healthcheck-Knoten festgelegt und die Ergebnisse aggregiert. Anschlie√üend werden sie an den Balancer-Controller zur√ºckgesendet.  Der Loadbalancer-Knoten abonniert Ereignisse vom Controller und √§ndert den Status des VPP.  In einem solchen System wei√ü jeder Dienst nur, was er √ºber benachbarte Dienste ben√∂tigt.  Die Anzahl der Verbindungen ist begrenzt und wir haben die M√∂glichkeit, die verschiedenen Segmente unabh√§ngig voneinander zu nutzen und zu skalieren. <br><br><img src="https://habrastorage.org/webt/rk/a0/vi/rka0viu8dbcd7irrdx5m0quwfpw.jpeg"><br><br><h3>  Welche Fragen wurden vermieden </h3><br>  Alle unsere Dienste in der Steuerebene sind in Go geschrieben und verf√ºgen √ºber gute Skalierungs- und Zuverl√§ssigkeitsfunktionen.  Go verf√ºgt √ºber viele Open Source-Bibliotheken zum Erstellen verteilter Systeme.  Wir verwenden GRPC aktiv, alle Komponenten enthalten eine Open-Source-Implementierung der Serviceerkennung - unsere Services √ºberwachen die Leistung des anderen, k√∂nnen ihre Zusammensetzung dynamisch √§ndern und haben sie mit dem GRPC-Balancing verkn√ºpft.  F√ºr Metriken verwenden wir auch eine Open Source-L√∂sung.  In der Datenebene haben wir eine anst√§ndige Leistung und eine gro√üe Ressourcenreserve: Es stellte sich als sehr schwierig heraus, einen Stand aufzubauen, auf dem man sich auf die Leistung von VPP st√ºtzen konnte, und nicht auf eine eiserne Netzwerkkarte. <br><br><h3>  Probleme und L√∂sungen </h3><br>  Was hat nicht sehr gut funktioniert?  In Go erfolgt die Speicherverwaltung automatisch, es treten jedoch Speicherlecks auf.  Der einfachste Weg, mit ihnen umzugehen, besteht darin, Goroutinen zu starten und nicht zu vergessen, sie zu vervollst√§ndigen.  Fazit: √úberwachen Sie den Speicherverbrauch von Go-Programmen.  Oft ist ein guter Indikator die Menge an Goroutine.  Diese Geschichte hat ein Plus: In Go ist es einfach, Daten zur Laufzeit abzurufen - zum Speicherverbrauch, zur Anzahl der gestarteten Goroutinen und zu vielen anderen Parametern. <br><br>  Dar√ºber hinaus ist Go m√∂glicherweise nicht die beste Wahl f√ºr Funktionstests.  Sie sind ziemlich ausf√ºhrlich und der Standardansatz ‚ÄûAlles in CI-Paketen ausf√ºhren‚Äú ist f√ºr sie nicht sehr geeignet.  Tatsache ist, dass Funktionstests h√∂here Anforderungen an die Ressourcen stellen und bei ihnen echte Zeit√ºberschreitungen auftreten.  Aus diesem Grund k√∂nnen Tests fehlschlagen, da die CPU mit Komponententests besch√§ftigt ist.  Schlussfolgerung: F√ºhren Sie nach M√∂glichkeit ‚Äûschwere‚Äú Tests getrennt von Unit-Tests durch. <br><br>  Die Microservice-Ereignisarchitektur ist komplizierter als ein Monolith: Das Abrufen von Protokollen auf Dutzenden verschiedener Computer ist nicht sehr praktisch.  Fazit: Wenn Sie Microservices durchf√ºhren, denken Sie sofort an die R√ºckverfolgung. <br><br><h3>  Unsere Pl√§ne </h3><br>  Wir werden den internen Balancer IPv6-Balancer starten, Unterst√ºtzung f√ºr Kubernetes-Skripte hinzuf√ºgen, unsere Dienste weiterhin sharden (jetzt sind nur noch Healthcheck-Node und Healthcheck-Ctrl schattiert), neue Healthchecks hinzuf√ºgen und auch die Smart-Check-Aggregation implementieren.  Wir erw√§gen die M√∂glichkeit, unsere Dienste noch unabh√§ngiger zu gestalten, damit sie nicht direkt miteinander kommunizieren, sondern eine Nachrichtenwarteschlange verwenden.  Der SQS-kompatible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yandex Message Queue-</a> Dienst wurde k√ºrzlich in der Cloud ver√∂ffentlicht. <br><br>  Vor kurzem wurde Yandex Load Balancer √∂ffentlich ver√∂ffentlicht.  Lesen Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> f√ºr den Service, verwalten Sie die Balancer auf eine f√ºr Sie bequeme Weise und erh√∂hen Sie die Fehlertoleranz Ihrer Projekte! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de448588/">https://habr.com/ru/post/de448588/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de448574/index.html">Eine Aufgabe aus der SEO-Routine: 3-Schritt-L√∂sung</a></li>
<li><a href="../de448576/index.html">Transistorgeschichte Teil 2: Aus dem Schmelztiegel des Krieges</a></li>
<li><a href="../de448580/index.html">CQ CQ CQ Frohe Feiertage, Radio Amateur! #WorldAmateurRadioDay</a></li>
<li><a href="../de448582/index.html">Erstellen eines Trinkgeldrechners auf Kotlin: Wie funktioniert das?</a></li>
<li><a href="../de448584/index.html">7 h√§ufige Fehler bei der Verwendung von Pr√§positionen in Englisch und wie man sie vermeidet</a></li>
<li><a href="../de448590/index.html">Bekannte Fremde oder noch einmal √ºber die Verwendung von Designmustern</a></li>
<li><a href="../de448594/index.html">Kostenlose Antiviren- und Firewalls (UTM, NGFW) von Sophos</a></li>
<li><a href="../de448596/index.html">Tablet-Halter auf dem Laufband oder suchen Sie nach freien Schritten</a></li>
<li><a href="../de448602/index.html">Ist die √úberwachung tot? - Es lebe die √úberwachung</a></li>
<li><a href="../de448604/index.html">Gameboy in C #</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>