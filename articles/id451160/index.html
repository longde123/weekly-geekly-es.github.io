<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÜüèª üßëüèº üëêüèª Apache Kafka dan Streaming dengan Spark Streaming ü§¥üèæ ‚öúÔ∏è üèÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! Hari ini kita akan membangun sistem yang akan menggunakan Apark Kafka untuk memproses aliran pesan menggunakan Spark Streaming dan menulis...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Kafka dan Streaming dengan Spark Streaming</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/451160/">  Halo, Habr!  Hari ini kita akan membangun sistem yang akan menggunakan Apark Kafka untuk memproses aliran pesan menggunakan Spark Streaming dan menulis hasil pemrosesan ke basis data cloud AWS RDS. <br><br>  Bayangkan bahwa lembaga kredit tertentu telah menetapkan tugas memproses transaksi masuk dengan cepat di semua cabangnya.  Ini dapat dilakukan untuk menghitung dengan cepat posisi mata uang terbuka untuk Perbendaharaan, batasan atau hasil keuangan pada transaksi, dll. <br><br>  Bagaimana menerapkan kasus ini tanpa menggunakan sihir dan mantra sihir - kita baca di bawah!  Ayo pergi! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5w/sb/8v/5wsb8vvncrzhysct-pd6oqraqky.jpeg"></div><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">(Sumber gambar)</a> <br><a name="habracut"></a><br><h2>  Pendahuluan </h2><br>  Tentu saja, memproses array data yang besar secara real time memberikan banyak peluang untuk digunakan dalam sistem modern.  Salah satu kombinasi paling populer untuk ini adalah Apache Kafka dan Spark Streaming tandem, di mana Kafka menciptakan aliran paket pesan masuk, dan Spark Streaming memproses paket-paket ini pada interval waktu tertentu. <br><br>  Untuk meningkatkan toleransi kesalahan aplikasi, kami akan menggunakan pos pemeriksaan - pos pemeriksaan.  Menggunakan mekanisme ini, ketika modul Spark Streaming perlu memulihkan data yang hilang, hanya perlu kembali ke titik kontrol terakhir dan melanjutkan perhitungan dari itu. <br><br><h2>  Arsitektur sistem dalam pengembangan </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/od/ef/zc/odefzciug8ckvim4-ei6pdg49tw.png"></div><br><br>  Komponen yang Digunakan: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><b>Apache Kafka</b></a> adalah sistem pesan terdistribusi dengan menerbitkan dan berlangganan.  Cocok untuk konsumsi pesan offline dan online.  Untuk mencegah kehilangan data, pesan Kafka disimpan di disk dan direplikasi di dalam cluster.  Sistem Kafka dibangun di atas layanan sinkronisasi ZooKeeper; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><b>Apache Spark Streaming</b></a> - Komponen Spark untuk memproses data streaming.  Modul Spark Streaming dibangun menggunakan arsitektur mikro-batch, ketika aliran data ditafsirkan sebagai urutan berkelanjutan dari paket data kecil.  Spark Streaming menerima data dari berbagai sumber dan menggabungkannya ke dalam paket kecil.  Paket-paket baru dibuat secara berkala.  Pada awal setiap interval waktu, paket baru dibuat, dan data apa pun yang diterima selama interval ini termasuk dalam paket.  Pada akhir interval, pertumbuhan paket berhenti.  Ukuran interval ditentukan oleh parameter yang disebut interval batch; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><b>Apache Spark SQL</b></a> - Menggabungkan pemrosesan relasional dengan pemrograman fungsional Spark.  Data terstruktur mengacu pada data yang memiliki skema, yaitu, satu set bidang untuk semua catatan.  Spark SQL mendukung input dari berbagai sumber data terstruktur dan, berkat ketersediaan informasi tentang skema tersebut, Spark secara efisien hanya dapat mengambil bidang rekaman yang diperlukan, dan juga menyediakan API DataFrame; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><b>AWS RDS</b></a> adalah basis data relasional berbasis cloud yang relatif murah, layanan web yang menyederhanakan konfigurasi, operasi, dan penskalaan, dan secara langsung dikelola oleh Amazon. </li></ul><br><h2>  Instal dan mulai server Kafka </h2><br>  Sebelum menggunakan Kafka secara langsung, Anda perlu memastikan Java tersedia, sebagai  JVM digunakan untuk bekerja: <br><br><pre><code class="bash hljs">sudo apt-get update sudo apt-get install default-jre java -version</code> </pre> <br>  Buat pengguna baru untuk bekerja dengan Kafka: <br><br><pre> <code class="bash hljs">sudo useradd kafka -m sudo passwd kafka sudo adduser kafka sudo</code> </pre><br>  Selanjutnya, unduh distribusi dari situs web resmi Apache Kafka: <br><br><pre> <code class="bash hljs">wget -P /YOUR_PATH <span class="hljs-string"><span class="hljs-string">"http://apache-mirror.rbc.ru/pub/apache/kafka/2.2.0/kafka_2.12-2.2.0.tgz"</span></span></code> </pre> <br>  Buka paket arsip yang diunduh: <br><pre> <code class="bash hljs">tar -xvzf /YOUR_PATH/kafka_2.12-2.2.0.tgz ln -s /YOUR_PATH/kafka_2.12-2.2.0 kafka</code> </pre><br>  Langkah selanjutnya adalah opsional.  Faktanya adalah bahwa pengaturan default tidak memungkinkan penggunaan penuh semua fitur dari Apache Kafka.  Misalnya, hapus topik, kategori, grup yang pesannya dapat dipublikasikan.  Untuk mengubah ini, edit file konfigurasi: <br><br><pre> <code class="bash hljs">vim ~/kafka/config/server.properties</code> </pre> <br>  Tambahkan berikut ini ke akhir file: <br><br><pre> <code class="bash hljs">delete.topic.enable = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br>  Sebelum memulai server Kafka, Anda perlu memulai server ZooKeeper, kami akan menggunakan skrip bantu yang datang dengan distribusi Kafka: <br><br><pre> <code class="bash hljs">Cd ~/kafka bin/zookeeper-server-start.sh config/zookeeper.properties</code> </pre><br>  Setelah ZooKeeper berhasil dimulai, di terminal terpisah kami meluncurkan server Kafka: <br><br><pre> <code class="bash hljs">bin/kafka-server-start.sh config/server.properties</code> </pre> <br>  Buat topik baru yang disebut Transaction: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic transaction</code> </pre> <br>  Pastikan topik dengan jumlah partisi dan replikasi yang tepat telah dibuat: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --describe --zookeeper localhost:2181</code> </pre> <br><img src="https://habrastorage.org/webt/s5/gh/bu/s5ghbuswhb0dcc0pmlvu_uloes4.png"><br><br>  Kami akan kehilangan momen menguji produsen dan konsumen untuk topik yang baru dibuat.  Untuk detail lebih lanjut tentang cara menguji pengiriman dan penerimaan pesan, lihat dokumentasi resmi - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kirim beberapa pesan</a> .  Baiklah, kami beralih ke menulis produser dengan Python menggunakan API KafkaProducer. <br><br><h2>  Penulisan Produser </h2><br>  Produser akan menghasilkan data acak - 100 pesan setiap detik.  Dengan data acak yang kami maksud adalah kamus yang terdiri dari tiga bidang: <br><br><ul><li>  <b>Cabang</b> - nama tempat penjualan lembaga kredit; </li><li>  <b>Mata uang</b> - mata uang transaksi; </li><li>  <b>Jumlah</b> - jumlah transaksi.  Jumlah tersebut akan menjadi angka positif jika itu adalah pembelian mata uang oleh Bank, dan negatif jika itu adalah penjualan. </li></ul><br>  Kode untuk produsen adalah sebagai berikut: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy.random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> choice, randint <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_random_value</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> new_dict = {} branch_list = [<span class="hljs-string"><span class="hljs-string">'Kazan'</span></span>, <span class="hljs-string"><span class="hljs-string">'SPB'</span></span>, <span class="hljs-string"><span class="hljs-string">'Novosibirsk'</span></span>, <span class="hljs-string"><span class="hljs-string">'Surgut'</span></span>] currency_list = [<span class="hljs-string"><span class="hljs-string">'RUB'</span></span>, <span class="hljs-string"><span class="hljs-string">'USD'</span></span>, <span class="hljs-string"><span class="hljs-string">'EUR'</span></span>, <span class="hljs-string"><span class="hljs-string">'GBP'</span></span>] new_dict[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>] = choice(branch_list) new_dict[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>] = choice(currency_list) new_dict[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>] = randint(<span class="hljs-number"><span class="hljs-number">-100</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> new_dict</code> </pre><br>  Selanjutnya, menggunakan metode kirim, kami mengirim pesan ke server, dalam topik yang kami butuhkan, dalam format JSON: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaProducer producer = KafkaProducer(bootstrap_servers=[<span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span>], value_serializer=<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:dumps(x).encode(<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>), compression_type=<span class="hljs-string"><span class="hljs-string">'gzip'</span></span>) my_topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> data = get_random_value() <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: future = producer.send(topic = my_topic, value = data) record_metadata = future.get(timeout=<span class="hljs-number"><span class="hljs-number">10</span></span>) print(<span class="hljs-string"><span class="hljs-string">'--&gt; The message has been sent to a topic: \ {}, partition: {}, offset: {}'</span></span> \ .format(record_metadata.topic, record_metadata.partition, record_metadata.offset )) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span> Exception <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> e: print(<span class="hljs-string"><span class="hljs-string">'--&gt; It seems an Error occurred: {}'</span></span>.format(e)) <span class="hljs-keyword"><span class="hljs-keyword">finally</span></span>: producer.flush()</code> </pre><br>  Saat menjalankan skrip, kami menerima pesan berikut di terminal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_e/3g/zj/_e3gzjrmsycjb8ntjmur6ztaspw.png"></div><br>  Ini berarti bahwa semuanya berfungsi seperti yang kita inginkan - produser menghasilkan dan mengirim pesan ke topik yang kita butuhkan. <br><br>  Langkah selanjutnya adalah menginstal Spark dan memproses aliran pesan ini. <br><br><h2>  Instal Apache Spark </h2><br>  <b>Apache Spark</b> adalah platform komputasi cluster serbaguna dan berkinerja tinggi. <br><br>  Dalam hal kinerja, Spark melampaui implementasi populer dari model MapReduce, secara bersamaan memberikan dukungan untuk jenis perhitungan yang lebih luas, termasuk kueri interaktif dan pemrosesan aliran.  Kecepatan memainkan peran penting dalam memproses data dalam jumlah besar, karena kecepatan itulah yang memungkinkan Anda untuk bekerja secara interaktif tanpa menghabiskan beberapa menit atau berjam-jam menunggu.  Salah satu kekuatan terbesar Spark pada kecepatan tinggi adalah kemampuannya untuk melakukan perhitungan dalam memori. <br><br>  Kerangka kerja ini ditulis dalam Scala, jadi Anda harus menginstalnya terlebih dahulu: <br><br><pre> <code class="bash hljs">sudo apt-get install scala</code> </pre> <br>  Unduh distribusi Spark dari situs web resmi: <br><br><pre> <code class="bash hljs">wget <span class="hljs-string"><span class="hljs-string">"http://mirror.linux-ia64.org/apache/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz"</span></span></code> </pre> <br>  Buka paket arsip: <br><br><pre> <code class="bash hljs">sudo tar xvf spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark</code> </pre> <br>  Tambahkan path ke Spark di file bash: <br><br><pre> <code class="bash hljs">vim ~/.bashrc</code> </pre> <br>  Tambahkan baris berikut melalui editor: <br><br><pre> <code class="bash hljs">SPARK_HOME=/usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> PATH=<span class="hljs-variable"><span class="hljs-variable">$SPARK_HOME</span></span>/bin:<span class="hljs-variable"><span class="hljs-variable">$PATH</span></span></code> </pre><br>  Jalankan perintah di bawah ini setelah membuat perubahan ke bashrc: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><h2>  Penerapan AWS PostgreSQL </h2><br>  Tetap menyebarkan database, tempat kami akan mengunggah informasi yang diproses dari stream.  Untuk ini, kami akan menggunakan layanan AWS RDS. <br><br>  Pergi ke konsol AWS -&gt; AWS RDS -&gt; Databases -&gt; Buat database: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dg/os/m7/dgosm7dwnh3fr-uksjdt_xpltsk.png"></div><br>  Pilih PostgreSQL dan klik tombol Berikutnya: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3y/d_/8r/3yd_8rsz2swfgaxaafpkyizthac.png"></div><br>  Karena  Contoh ini dipahami hanya untuk tujuan pendidikan, kami akan menggunakan server gratis "minimal" (Tier Gratis): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fn/6p/5b/fn6p5bjyitndy_ozs2cdcw_ssi0.png"></div><br>  Selanjutnya, beri tanda centang di blok Free Tier, dan setelah itu kami akan secara otomatis ditawari turunan dari kelas t2.micro - meskipun lemah, ini gratis dan cukup cocok untuk tugas kami: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mj/jh/wg/mjjhwg3cknoehrq8wyxk3uw5v74.png"></div><br>  Hal-hal yang sangat penting berikut: nama instance database, nama pengguna master dan kata sandinya.  Sebutkan contohnya: myHabrTest, pengguna master: <b>habr</b> , kata sandi: <b>habr12345</b> dan klik tombol Next: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lg/jt/mf/lgjtmfdfst0pvqthojb_bdpeohc.png"></div><br><br>  Halaman berikutnya berisi parameter yang bertanggung jawab atas ketersediaan server database kami dari luar (Aksesibilitas publik) dan ketersediaan port: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/40/z9/q7/40z9q7owar5kpnimyzrdj5laqgs.png"></div><br>  Mari kita buat konfigurasi baru untuk grup keamanan VPC, yang akan memungkinkan kita untuk mengakses server basis data kita dari luar melalui port 5432 (PostgreSQL). <br><br>  Di jendela browser terpisah, buka konsol AWS di Dasbor VPC -&gt; Grup Keamanan -&gt; Buat bagian grup keamanan: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fl/2i/ne/fl2inejlgnghwsh3itdrlcywdsu.png"></div><br>  Tetapkan nama untuk grup Keamanan - PostgreSQL, deskripsi, menunjukkan VPC mana yang harus dikaitkan dengan grup ini dan klik tombol Buat: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/js/8r/tv/js8rtvp8tudwjtpgso6xota5h-g.png"></div><br>  Isi grup aturan masuk yang baru dibuat untuk port 5432, seperti yang ditunjukkan pada gambar di bawah ini.  Anda tidak harus menentukan porta manual, tetapi pilih PostgreSQL dari daftar drop-down Type. <br><br>  Sebenarnya, nilai :: / 0 berarti ketersediaan lalu lintas masuk untuk server dari seluruh dunia, yang secara kanonik tidak sepenuhnya benar, tetapi untuk menguraikan contoh, mari kita gunakan pendekatan ini: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ge/8j/bn/ge8jbntssnooajc8so36h0tjo80.png"></div><br>  Kami kembali ke halaman browser, di mana kami memiliki "Konfigurasi pengaturan lanjutan" terbuka dan pilih di bagian Grup keamanan VPC -&gt; Pilih grup keamanan VPC yang ada -&gt; PostgreSQL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nk/ae/-s/nkae-ste1tp3wgvmyilicvwlk8e.png"></div><br>  Selanjutnya, di bagian Opsi basis data -&gt; Nama basis data -&gt; atur nama - <b>habrDB</b> . <br><br>  Kami dapat membiarkan parameter lainnya, dengan pengecualian menonaktifkan cadangan (periode penyimpanan cadangan - 0 hari), pemantauan, dan Wawasan Kinerja, secara default.  Klik pada tombol <b>Create database</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/1p/po/ex1ppogq_vdsk3nnvywm7l8vq8i.png"></div><br><h2>  Penangan arus </h2><br>  Langkah terakhir adalah pengembangan Spark-jobs, yang akan memproses setiap dua detik data baru yang berasal dari Kafka dan memasukkan hasilnya ke dalam basis data. <br><br>  Seperti disebutkan di atas, pos-pos pemeriksaan adalah mekanisme utama dalam SparkStreaming yang harus dikonfigurasi untuk memberikan toleransi kesalahan.  Kami akan menggunakan titik kontrol dan, jika terjadi penurunan prosedur, modul Spark Streaming hanya perlu kembali ke titik kontrol terakhir dan melanjutkan perhitungan darinya untuk mengembalikan data yang hilang. <br><br>  Anda dapat mengaktifkan pos pemeriksaan dengan mengatur direktori dalam sistem file yang toleran terhadap kesalahan, andal (misalnya, HDFS, S3, dll.), Di mana informasi pos pemeriksaan akan disimpan.  Ini dilakukan dengan menggunakan, misalnya: <br><br><pre> <code class="python hljs">streamingContext.checkpoint(checkpointDirectory)</code> </pre> <br>  Dalam contoh kita, kita akan menggunakan pendekatan berikut, yaitu, jika checkpointDirectory ada, maka konteksnya akan dibuat kembali dari data titik kontrol.  Jika direktori tidak ada (mis. Itu dijalankan untuk pertama kali), fungsi functionToCreateContext dipanggil untuk membuat konteks baru dan mengonfigurasi DStreams: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> StreamingContext context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)</code> </pre><br>  Buat objek DirectStream untuk terhubung ke topik "transaksi" menggunakan metode createDirectStream dari pustaka KafkaUtils: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming.kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaUtils sc = SparkContext(conf=conf) ssc = StreamingContext(sc, <span class="hljs-number"><span class="hljs-number">2</span></span>) broker_list = <span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span> topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {<span class="hljs-string"><span class="hljs-string">"metadata.broker.list"</span></span>: broker_list})</code> </pre><br>  Parsing data yang masuk dalam format JSON: <br><br><pre> <code class="python hljs">rowRdd = rdd.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> w: Row(branch=w[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>], currency=w[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>], amount=w[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>])) testDataFrame = spark.createDataFrame(rowRdd) testDataFrame.createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"treasury_stream"</span></span>)</code> </pre><br>  Menggunakan Spark SQL, kami membuat pengelompokan sederhana dan mencetak hasilnya ke konsol: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> from_unixtime(<span class="hljs-keyword"><span class="hljs-keyword">unix_timestamp</span></span>()) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> curr_time, t.branch <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> branch_name, t.currency <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> currency_code, <span class="hljs-keyword"><span class="hljs-keyword">sum</span></span>(amount) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> batch_value <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> treasury_stream t <span class="hljs-keyword"><span class="hljs-keyword">group</span></span> <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> t.branch, t.currency</code> </pre><br>  Mendapatkan teks kueri dan menjalankannya melalui Spark SQL: <br><br><pre> <code class="python hljs">sql_query = get_sql_query() testResultDataFrame = spark.sql(sql_query) testResultDataFrame.show(n=<span class="hljs-number"><span class="hljs-number">5</span></span>)</code> </pre><br>  Dan kemudian kami menyimpan data agregat yang diterima ke dalam tabel di AWS RDS.  Untuk menyimpan hasil agregasi ke tabel database, kami akan menggunakan metode tulis objek DataFrame: <br><br><pre> <code class="python hljs">testResultDataFrame.write \ .format(<span class="hljs-string"><span class="hljs-string">"jdbc"</span></span>) \ .mode(<span class="hljs-string"><span class="hljs-string">"append"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"driver"</span></span>, <span class="hljs-string"><span class="hljs-string">'org.postgresql.Driver'</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"url"</span></span>,<span class="hljs-string"><span class="hljs-string">"jdbc:postgresql://myhabrtest.ciny8bykwxeg.us-east-1.rds.amazonaws.com:5432/habrDB"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"dbtable"</span></span>, <span class="hljs-string"><span class="hljs-string">"transaction_flow"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"user"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr12345"</span></span>) \ .save()</code> </pre><br><blockquote>  Beberapa kata tentang pengaturan koneksi ke AWS RDS.  Kami menciptakan pengguna dan kata sandi untuk itu pada langkah "Menyebarkan AWS PostgreSQL".  Untuk url server database, gunakan Endpoint, yang ditampilkan di bagian Konektivitas &amp; keamanan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/sj/jd/9nsjjdun0hdy5qtwqub0xhvzunk.png"></div></blockquote><br>  Untuk menghubungkan Spark dan Kafka dengan benar, Anda harus menjalankan pekerjaan melalui smark-submit menggunakan <b>artefak spark-streaming-kafka-0-8_2.11</b> .  Selain itu, kami juga menerapkan artefak untuk berinteraksi dengan database PostgreSQL, kami akan mentransfernya melalui paket -. <br><br>  Untuk fleksibilitas skrip, kami juga mengambil nama server pesan dan topik dari mana kami ingin menerima data sebagai parameter input. <br><br>  Jadi, saatnya untuk memulai dan menguji sistem: <br><br><pre> <code class="bash hljs">spark-submit \ --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,\ org.postgresql:postgresql:9.4.1207 \ spark_job.py localhost:9092 transaction</code> </pre><br>  Semuanya berhasil!  Seperti yang dapat Anda lihat dalam gambar di bawah ini, selama pekerjaan aplikasi, hasil agregasi baru ditampilkan setiap 2 detik, karena kami menetapkan interval batching menjadi 2 detik ketika membuat objek StreamingContext: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cf/q1/25/cfq125zpzkyldktsuvdo175fazy.png"></div><br>  Selanjutnya, kami membuat kueri sederhana ke database untuk memeriksa catatan di tabel <b>transaction_flow</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7j/j9/qm/7jj9qmf4zpter3jkbblrmiqni2s.png"></div><br><h2>  Kesimpulan </h2><br>  Artikel ini membahas contoh pemrosesan informasi streaming menggunakan Spark Streaming bersamaan dengan Apache Kafka dan PostgreSQL.  Dengan pertumbuhan data dari berbagai sumber, sulit untuk melebih-lebihkan nilai praktis dari Spark Streaming untuk membuat aplikasi streaming dan aplikasi yang beroperasi secara real time. <br><br>  Anda dapat menemukan kode sumber lengkap di repositori saya di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">GitHub</a> . <br><br>  Saya siap untuk membahas artikel ini dengan senang hati, saya menantikan komentar Anda, dan saya juga berharap untuk kritik yang membangun dari semua pembaca yang peduli. <br><br>  Semoga sukses! <br><br>  <b>PS</b> Awalnya direncanakan menggunakan database PostgreSQL lokal, tetapi karena saya mencintai AWS, saya memutuskan untuk meletakkan database di cloud.  Pada artikel selanjutnya tentang topik ini, saya akan menunjukkan bagaimana menerapkan seluruh sistem yang dijelaskan di atas dalam AWS menggunakan AWS Kinesis dan AWS EMR.  Ikuti beritanya! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id451160/">https://habr.com/ru/post/id451160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id451148/index.html">Pemrograman Berorientasi Objek dalam Bahasa Grafis</a></li>
<li><a href="../id451150/index.html">Tangkap aku jika kau bisa. Versi manajer</a></li>
<li><a href="../id451152/index.html">Resistor di sirkuit gerbang atau cara melakukannya dengan benar</a></li>
<li><a href="../id451154/index.html">Sistem Akusisi Data Otonomi Daerah (lanjutan)</a></li>
<li><a href="../id451158/index.html">Sirkuit listrik. Jenis sirkuit</a></li>
<li><a href="../id451162/index.html">Koreksi Kesalahan - Konstanta Fisik di Masa Kini dan Versi Baru Sistem Satuan Internasional (SI)</a></li>
<li><a href="../id451164/index.html">Mencari tempat parkir gratis dengan Python</a></li>
<li><a href="../id451166/index.html">Apa yang akan ditawarkan repositori baru untuk sistem AI dan MO?</a></li>
<li><a href="../id451170/index.html">Jeff Bezos mengumumkan rencana untuk menaklukkan bulan</a></li>
<li><a href="../id451172/index.html">Julia: fungsi dan struktur sebagai fungsi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>