<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☂️ ⚔️ 🤜🏼 Creación de enrutamiento de clientes / búsqueda semántica en Profi.ru 💙 ❄️ ⛸️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Creación de enrutamiento de clientes / búsqueda semántica y agrupación de corpus externos arbitrarios en Profi.ru 
 TLDR 


 Este es un resumen ejecut...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Creación de enrutamiento de clientes / búsqueda semántica en Profi.ru</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428674/"><h1 id="building-client-routing--semantic-search-and-clustering-arbitrary-external-corpuses-at-profiru">  Creación de enrutamiento de clientes / búsqueda semántica y agrupación de corpus externos arbitrarios en Profi.ru </h1><br><h2 id="tldr">  <strong>TLDR</strong> </h2><br><p>  Este es un resumen ejecutivo muy breve (o un teaser) sobre lo que logramos hacer en aproximadamente 2 meses en el departamento de Profi.ru DS (estuve allí un poco más de tiempo, pero incorporarme a mí y a mi equipo fue algo aparte. hecho al principio). </p><a name="habracut"></a><br><h2 id="projected-goals">  Metas proyectadas </h2><br><ol><li> Comprenda la entrada / intención del cliente y enrute a los clientes en consecuencia (al final, optamos por un clasificador agnóstico de calidad de entrada, aunque también consideramos modelos de nivel de caracteres de escritura anticipada y modelos de lenguaje. Reglas de simplicidad); </li><li>  Encuentre servicios y sinónimos totalmente nuevos para los servicios existentes; </li><li>  Como un objetivo secundario de (2): aprender a construir grupos adecuados en corpus externos arbitrarios; </li></ol><br><h2 id="achieved-goals">  Objetivos alcanzados </h2><br><p>  Obviamente, algunos de estos resultados fueron logrados no solo por nuestro equipo, sino por varios equipos (es decir, obviamente no hicimos la parte de raspado para los corpus de dominio y la anotación manual, aunque creo que nuestro equipo también puede resolver el raspado, solo necesita suficientes proxies + probablemente algo de experiencia con selenio). </p><br><p>  <strong>Objetivos comerciales:</strong> </p><br><ol><li> ~ <code>88+%</code> (vs ~ <code>60%</code> con búsqueda elástica) precisión en la clasificación del enrutamiento / intento del cliente (~ <code>5k</code> clases); </li><li>  La búsqueda es independiente de la calidad de entrada (erratas / entrada parcial); </li><li>  El clasificador generaliza, la estructura morfológica del lenguaje es explotada; </li><li>  El clasificador supera severamente la búsqueda elástica en varios puntos de referencia (ver más abajo); </li><li>  Para estar seguro: se encontraron al menos <code>1,000</code> nuevos servicios + al menos <code>15,000</code> sinónimos (en comparación con el estado actual de <code>5,000</code> + ~ <code>30,000</code> ).  Espero que esta cifra se duplique o incluso triplique; </li></ol><br><p>  La última viñeta es una estimación aproximada, pero conservadora. <br>  También se realizarán pruebas AB.  Pero tengo confianza en estos resultados. </p><br><p>  <strong>Objetivos "científicos":</strong> </p><br><ol><li>  Comparamos a fondo muchas técnicas modernas de incrustación de oraciones utilizando una tarea de clasificación aguas abajo + KNN con una base de datos de sinónimos de servicio; </li><li>  Logramos superar la búsqueda elástica débilmente supervisada (esencialmente su clasificador es una bolsa de ngrams) en este punto de referencia (ver detalles a continuación) utilizando métodos <strong>NO SUPERVISADOS</strong> ; </li><li>  Desarrollamos una nueva forma de construir modelos de PNL aplicados (una simple bolsa de incrustaciones de vainilla bi-LSTM +, esencialmente texto rápido cumple con RNN): esto toma en consideración la morfología del idioma ruso y se generaliza bien; </li><li>  Demostramos que nuestra técnica de inclusión final (una capa de cuello de botella del mejor clasificador) combinada con algoritmos sin supervisión de última generación (UMAP + HDBSCAN) puede producir grupos estelares; </li><li>  Demostramos en la práctica la posibilidad, viabilidad y usabilidad de: <br><ul><li>  Destilación del conocimiento; </li><li>  Aumentos para datos de texto (sic!); </li></ul></li><li>  La capacitación de clasificadores basados ​​en texto con aumentos dinámicos redujo drásticamente el tiempo de convergencia (10x) en comparación con la generación de conjuntos de datos estáticos más grandes (es decir, la CNN aprende a generalizar el error que se muestra con oraciones drásticamente menos aumentadas); </li></ol><br><h2 id="overall-project-structure">  Estructura general del proyecto </h2><br><p>  Esto no incluye el clasificador final. <br>  También, al final, abandonamos los modelos falsos de RNN y de pérdida de tripletes a favor del cuello de botella del clasificador. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1c2/449/157/1c24491576ed703ebc571dfd4d7d8da3.png"></p><br><h2 id="what-works-in-nlp-now">  ¿Qué funciona en PNL ahora? </h2><br><p>  Una vista de pájaro: <br><img src="https://habrastorage.org/getpro/habr/post_images/5a1/8f5/df1/5a18f5df1e133bef082edf9315011da7.png"></p><br><p>  También puede saber que NLP puede estar experimentando el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">momento Imagenet ahora</a> . </p><br><h2 id="large-scale-umap-hack">  Hack de UMAP a gran escala </h2><br><p>  Cuando construimos clústeres, nos topamos con una forma / truco para aplicar esencialmente UMAP a conjuntos de datos de más de 100 millones de puntos (o tal vez incluso mil millones).  Esencialmente construya un gráfico KNN con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">FAISS</a> y luego simplemente reescriba el bucle principal de UMAP en PyTorch usando su GPU.  No necesitábamos eso y abandonamos el concepto (después de todo, teníamos solo 10-15 millones de puntos), pero sigan este <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hilo</a> para más detalles. </p><br><h2 id="what-works-best">  Lo que funciona mejor </h2><br><ul><li>  Para la clasificación supervisada, el texto rápido cumple con el conjunto de n-gramos RNN (bi-LSTM) + cuidadosamente elegido; </li><li>  Implementación: python simple para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">n-gramos</a> + capa de bolsa de inclusión PyTorch; </li><li>  Para el agrupamiento: la capa de cuello de botella de este modelo + UMAP + HDBSCAN; </li></ul><br><h2 id="best-classifier-benchmarks">  <strong>Los mejores puntos de referencia clasificadores</strong> </h2><br><p>  <strong>Conjunto de desarrollo anotado manualmente</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/04b/7cc/e7e/04b7cce7e8cee9cee4b066b6a353bed9.jpg"></p><br><p>  <strong>De izquierda a derecha</strong> <br>  (Precisión Top1) </p><br><ul><li>  Algoritmo actual (búsqueda elástica); </li><li>  Primer RNN; </li><li>  Nueva anotación; </li><li>  Tuning </li><li>  Capa de bolsa de incrustación de texto rápido; </li><li>  Agregar errores tipográficos y entrada parcial; </li><li>  Generación dinámica de errores y entrada parcial ( <strong>tiempo de entrenamiento reducido 10x</strong> ); </li><li>  Puntuación final </li></ul><br><p>  <strong>Conjunto de desarrollo anotado manualmente + 1-3 errores por consulta</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ae2/a31/040/ae2a31040dbd77402d6b6dfee9eeba28.jpg"></p><br><p>  <strong>De izquierda a derecha</strong> <br>  (Precisión Top1) </p><br><ul><li>  Algoritmo actual (búsqueda elástica); </li><li>  Capa de bolsa de inclusión de texto rápido; </li><li>  Agregar errores tipográficos y entrada parcial; </li><li>  Generación dinámica de errores y entrada parcial; </li><li>  Puntuación final </li></ul><br><p>  <strong>Conjunto de desarrollo anotado manualmente + entrada parcial</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/c3c/680/681/c3c680681dd3166b95246930f1f1b1a8.jpg"></p><br><p>  <strong>De izquierda a derecha</strong> <br>  (Precisión Top1) </p><br><ul><li>  Algoritmo actual (búsqueda elástica); </li><li>  Capa de bolsa de incrustación de texto rápido; </li><li>  Agregar errores tipográficos y entrada parcial; </li><li>  Generación dinámica de errores y entrada parcial; </li><li>  Puntuación final </li></ul><br><h2 id="large-scale-corpuses--n-gram-selection">  Corpus a gran escala / selección de n-gramas </h2><br><ul><li>  Recolectamos los corpus más grandes para el idioma ruso: <br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Areneum</a> - una versión procesada está disponible aquí - los autores del conjunto de datos no respondieron; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Taiga</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Common crawl</a> and <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">wiki</a> : siga estos artículos; </li></ul></li><li>  Recopilamos un diccionario de palabras de <code>100m</code> con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1 TB de rastreo</a> ; </li><li>  También use este <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">truco</a> para descargar dichos archivos más rápido (durante la noche); </li><li>  Seleccionamos un conjunto óptimo de <code>1m</code> n-gramos para que nuestro clasificador generalice mejor ( <code>500k</code> n-gramos más populares de texto rápido entrenado en Wikipedia en ruso + <code>500k</code> n-gramos más populares en nuestros datos de dominio); </li></ul><br><p>  <strong>Prueba de resistencia de nuestros 1M n-gramos en vocabulario 100M:</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/198/1fe/38b/1981fe38b03b4cdf76022f4ff6ef0074.png" alt="imagen"></p><br><h2 id="text-augmentations">  Aumentos de texto </h2><br><p>  En pocas palabras: </p><br><ul><li>  Tome un diccionario grande con errores (por ejemplo, 10-100m palabras únicas); </li><li>  Genere un error (suelte una letra, cambie una letra usando probabilidades calculadas, inserte una letra aleatoria, tal vez use la distribución del teclado, etc.); </li><li>  Verifique que la nueva palabra esté en el diccionario; </li></ul><br><p>  Bruto forzamos muchas consultas a servicios como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este</a> (en un intento de realizar ingeniería inversa en su conjunto de datos), y tienen un diccionario muy pequeño en su interior (también este servicio funciona con un clasificador de árbol con funciones de n-gramo).  Fue divertido ver que <strong>cubrían solo el 30-50% de las palabras que teníamos en algunos corpus</strong> . </p><br><p>  <strong>Nuestro enfoque es muy superior si tiene acceso a un amplio vocabulario de dominio</strong> . </p><br><h2 id="best-unsupervised--semi-supervised-results">  Los mejores resultados sin supervisión / semi-supervisados </h2><br><p>  KNN se utiliza como punto de referencia para comparar diferentes métodos de inclusión. </p><br><p>  (tamaño del vector) Lista de modelos probados: </p><br><ul><li>  (512) Detector de oraciones falsas a gran escala capacitado en 200 GB de datos de rastreo comunes; </li><li>  (300) Detector de frases falsas entrenado para distinguir una frase aleatoria de Wikipedia de un servicio; </li><li>  (300) Texto rápido obtenido de aquí, pre-entrenado en araneum corpus; </li><li>  (200) Texto rápido entrenado en nuestros datos de dominio; </li><li>  (300) Texto rápido entrenado en 200 GB de datos de rastreo común; </li><li>  (300) Una red siamesa con pérdida de triplete entrenada con servicios / sinónimos / oraciones aleatorias de Wikipedia; </li><li>  (200) Primera iteración de la capa de incrustación de la bolsa de inclusión RNN, una oración se codifica como una bolsa completa de incrustaciones; </li><li>  (200) Lo mismo, pero primero la oración se divide en palabras, luego cada palabra se incrusta, luego se toma el promedio; </li><li>  (300) Lo mismo que arriba pero para el modelo final; </li><li>  (300) Lo mismo que arriba pero para el modelo final; </li><li>  (250) Capa de cuello de botella del modelo final (250 neuronas); </li><li>  Línea de base de búsqueda elástica débilmente supervisada; </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca1/e0b/e9c/ca1e0be9c152d092d4149f9986b87289.png" alt="por defecto"></p><br><p>  Para evitar fugas, todas las oraciones aleatorias se muestrearon aleatoriamente.  Su extensión en palabras fue la misma que la duración de los servicios / sinónimos con los que se compararon.  También se tomaron medidas para asegurarse de que los modelos no solo aprendieran separando vocabularios (las incrustaciones se congelaron, Wikipedia se submuestreó para asegurarse de que hubiera al menos una palabra de dominio en cada oración de Wikipedia). </p><br><h2 id="cluster-visualization">  Visualización de clúster </h2><br><p>  <strong>3D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/4b7/f10/d19/4b7f10d19a785b5f690a28f2e2a039e6.gif"></p><br><p>  <strong>2D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ad7/0ad/441/ad70ad441ecae6f396c8bb76826484df.png"></p><br><h2 id="cluster-exploration-interface">  "Interfaz" de exploración de clústeres </h2><br><p>  Verde - nueva palabra / sinónimo. <br>  Fondo gris - probablemente nueva palabra. <br>  Texto gris: sinónimo existente. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cda/d17/00f/cdad1700ff0701ff6643a4aa14041d31.jpg"></p><br><h2 id="ablation-tests-and-what-works-what-we-tried-and-what-we-did-not">  Pruebas de ablación y qué funciona, qué probamos y qué no. </h2><br><ol><li>  Ver los cuadros anteriores; </li><li>  Promedio simple / tf-idf promedio de incrustaciones de texto rápido: una <strong>línea de base MUY formidable</strong> ; </li><li>  Texto rápido&gt; Word2Vec para ruso; </li><li>  La incrustación de oraciones mediante la detección de oraciones falsas funciona, pero palidece en comparación con otros métodos; </li><li>  BPE (oración) no mostró mejoras en nuestro dominio; </li><li>  Los modelos de nivel de char lucharon por generalizar, a pesar del papel retractado de google; </li><li>  Probamos un transformador de múltiples cabezales (con clasificador y cabezales de modelado de lenguaje), pero en la anotación disponible a mano, funcionó más o menos igual que los modelos basados ​​en LSTM.  Cuando migramos a incorporar malos enfoques, abandonamos esta línea de investigación debido a la menor practicidad del transformador y la impracticabilidad de tener un cabezal LM junto con una capa de bolsa de inclusión; </li><li>  <strong>BERT</strong> : parece ser excesivo, también algunas personas afirman que los transformadores entrenan literalmente durante semanas; </li><li>  <strong>ELMO</strong> : usar una biblioteca como AllenNLP parece contraproducente en mi opinión tanto en entornos de investigación / producción como en educación por razones que no proporcionaré aquí; </li></ol><br><h2 id="deploy">  Implementar </h2><br><p>  Hecho usando: </p><br><ul><li>  Contenedor Docker con un servicio web simple; </li><li>  CPU solo para inferencia es suficiente; </li><li>  ~ <code>2.5 ms</code> por consulta en la CPU, el procesamiento por lotes no es realmente necesario; </li><li>  ~ <code>1GB</code> memoria RAM; </li><li>  Casi no hay dependencias, aparte de <code>PyTorch</code> , <code>numpy</code> y <code>pandas</code> (y servidor web ofc). </li><li>  Imite la generación de n-gramas de texto rápido como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esta</a> ; </li><li>  Incrustar la capa de bolsa + índices como recién almacenados en un diccionario; </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es428674/">https://habr.com/ru/post/es428674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es428662/index.html">Tarea de programación minorista</a></li>
<li><a href="../es428664/index.html">Arranque del kernel de Linux. Parte 1</a></li>
<li><a href="../es428666/index.html">Cómo creé animaciones que cambian el estado de ánimo usando máscaras CSS</a></li>
<li><a href="../es428668/index.html">Blizzard anunció el lanzamiento del relanzamiento de WarCraft III en 2019. Pre-pedido abierto</a></li>
<li><a href="../es428672/index.html">Descripción general del silenciador activo QuietOn</a></li>
<li><a href="../es428676/index.html">Romper los fundamentos fundamentales de C #: asignar memoria para un tipo de referencia en la pila</a></li>
<li><a href="../es428680/index.html">Crear e integrar VK bot en un grupo a través de VkBotLongPoll [Python]</a></li>
<li><a href="../es428682/index.html">Beta-Fallout 76 de autodestrucción</a></li>
<li><a href="../es428688/index.html">Configurar el entorno de trabajo en Docker para la aplicación yii-framework</a></li>
<li><a href="../es428690/index.html">Cómo enseñarle a tu novia a programar si no eres maestra, pero ella cree en ti</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>