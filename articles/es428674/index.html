<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>锔 锔  Creaci贸n de enrutamiento de clientes / b煤squeda sem谩ntica en Profi.ru  锔 革</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Creaci贸n de enrutamiento de clientes / b煤squeda sem谩ntica y agrupaci贸n de corpus externos arbitrarios en Profi.ru 
 TLDR 


 Este es un resumen ejecut...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Creaci贸n de enrutamiento de clientes / b煤squeda sem谩ntica en Profi.ru</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428674/"><h1 id="building-client-routing--semantic-search-and-clustering-arbitrary-external-corpuses-at-profiru">  Creaci贸n de enrutamiento de clientes / b煤squeda sem谩ntica y agrupaci贸n de corpus externos arbitrarios en Profi.ru </h1><br><h2 id="tldr">  <strong>TLDR</strong> </h2><br><p>  Este es un resumen ejecutivo muy breve (o un teaser) sobre lo que logramos hacer en aproximadamente 2 meses en el departamento de Profi.ru DS (estuve all铆 un poco m谩s de tiempo, pero incorporarme a m铆 y a mi equipo fue algo aparte. hecho al principio). </p><a name="habracut"></a><br><h2 id="projected-goals">  Metas proyectadas </h2><br><ol><li> Comprenda la entrada / intenci贸n del cliente y enrute a los clientes en consecuencia (al final, optamos por un clasificador agn贸stico de calidad de entrada, aunque tambi茅n consideramos modelos de nivel de caracteres de escritura anticipada y modelos de lenguaje. Reglas de simplicidad); </li><li>  Encuentre servicios y sin贸nimos totalmente nuevos para los servicios existentes; </li><li>  Como un objetivo secundario de (2): aprender a construir grupos adecuados en corpus externos arbitrarios; </li></ol><br><h2 id="achieved-goals">  Objetivos alcanzados </h2><br><p>  Obviamente, algunos de estos resultados fueron logrados no solo por nuestro equipo, sino por varios equipos (es decir, obviamente no hicimos la parte de raspado para los corpus de dominio y la anotaci贸n manual, aunque creo que nuestro equipo tambi茅n puede resolver el raspado, solo necesita suficientes proxies + probablemente algo de experiencia con selenio). </p><br><p>  <strong>Objetivos comerciales:</strong> </p><br><ol><li> ~ <code>88+%</code> (vs ~ <code>60%</code> con b煤squeda el谩stica) precisi贸n en la clasificaci贸n del enrutamiento / intento del cliente (~ <code>5k</code> clases); </li><li>  La b煤squeda es independiente de la calidad de entrada (erratas / entrada parcial); </li><li>  El clasificador generaliza, la estructura morfol贸gica del lenguaje es explotada; </li><li>  El clasificador supera severamente la b煤squeda el谩stica en varios puntos de referencia (ver m谩s abajo); </li><li>  Para estar seguro: se encontraron al menos <code>1,000</code> nuevos servicios + al menos <code>15,000</code> sin贸nimos (en comparaci贸n con el estado actual de <code>5,000</code> + ~ <code>30,000</code> ).  Espero que esta cifra se duplique o incluso triplique; </li></ol><br><p>  La 煤ltima vi帽eta es una estimaci贸n aproximada, pero conservadora. <br>  Tambi茅n se realizar谩n pruebas AB.  Pero tengo confianza en estos resultados. </p><br><p>  <strong>Objetivos "cient铆ficos":</strong> </p><br><ol><li>  Comparamos a fondo muchas t茅cnicas modernas de incrustaci贸n de oraciones utilizando una tarea de clasificaci贸n aguas abajo + KNN con una base de datos de sin贸nimos de servicio; </li><li>  Logramos superar la b煤squeda el谩stica d茅bilmente supervisada (esencialmente su clasificador es una bolsa de ngrams) en este punto de referencia (ver detalles a continuaci贸n) utilizando m茅todos <strong>NO SUPERVISADOS</strong> ; </li><li>  Desarrollamos una nueva forma de construir modelos de PNL aplicados (una simple bolsa de incrustaciones de vainilla bi-LSTM +, esencialmente texto r谩pido cumple con RNN): esto toma en consideraci贸n la morfolog铆a del idioma ruso y se generaliza bien; </li><li>  Demostramos que nuestra t茅cnica de inclusi贸n final (una capa de cuello de botella del mejor clasificador) combinada con algoritmos sin supervisi贸n de 煤ltima generaci贸n (UMAP + HDBSCAN) puede producir grupos estelares; </li><li>  Demostramos en la pr谩ctica la posibilidad, viabilidad y usabilidad de: <br><ul><li>  Destilaci贸n del conocimiento; </li><li>  Aumentos para datos de texto (sic!); </li></ul></li><li>  La capacitaci贸n de clasificadores basados en texto con aumentos din谩micos redujo dr谩sticamente el tiempo de convergencia (10x) en comparaci贸n con la generaci贸n de conjuntos de datos est谩ticos m谩s grandes (es decir, la CNN aprende a generalizar el error que se muestra con oraciones dr谩sticamente menos aumentadas); </li></ol><br><h2 id="overall-project-structure">  Estructura general del proyecto </h2><br><p>  Esto no incluye el clasificador final. <br>  Tambi茅n, al final, abandonamos los modelos falsos de RNN y de p茅rdida de tripletes a favor del cuello de botella del clasificador. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1c2/449/157/1c24491576ed703ebc571dfd4d7d8da3.png"></p><br><h2 id="what-works-in-nlp-now">  驴Qu茅 funciona en PNL ahora? </h2><br><p>  Una vista de p谩jaro: <br><img src="https://habrastorage.org/getpro/habr/post_images/5a1/8f5/df1/5a18f5df1e133bef082edf9315011da7.png"></p><br><p>  Tambi茅n puede saber que NLP puede estar experimentando el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">momento Imagenet ahora</a> . </p><br><h2 id="large-scale-umap-hack">  Hack de UMAP a gran escala </h2><br><p>  Cuando construimos cl煤steres, nos topamos con una forma / truco para aplicar esencialmente UMAP a conjuntos de datos de m谩s de 100 millones de puntos (o tal vez incluso mil millones).  Esencialmente construya un gr谩fico KNN con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">FAISS</a> y luego simplemente reescriba el bucle principal de UMAP en PyTorch usando su GPU.  No necesit谩bamos eso y abandonamos el concepto (despu茅s de todo, ten铆amos solo 10-15 millones de puntos), pero sigan este <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hilo</a> para m谩s detalles. </p><br><h2 id="what-works-best">  Lo que funciona mejor </h2><br><ul><li>  Para la clasificaci贸n supervisada, el texto r谩pido cumple con el conjunto de n-gramos RNN (bi-LSTM) + cuidadosamente elegido; </li><li>  Implementaci贸n: python simple para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">n-gramos</a> + capa de bolsa de inclusi贸n PyTorch; </li><li>  Para el agrupamiento: la capa de cuello de botella de este modelo + UMAP + HDBSCAN; </li></ul><br><h2 id="best-classifier-benchmarks">  <strong>Los mejores puntos de referencia clasificadores</strong> </h2><br><p>  <strong>Conjunto de desarrollo anotado manualmente</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/04b/7cc/e7e/04b7cce7e8cee9cee4b066b6a353bed9.jpg"></p><br><p>  <strong>De izquierda a derecha</strong> <br>  (Precisi贸n Top1) </p><br><ul><li>  Algoritmo actual (b煤squeda el谩stica); </li><li>  Primer RNN; </li><li>  Nueva anotaci贸n; </li><li>  Tuning </li><li>  Capa de bolsa de incrustaci贸n de texto r谩pido; </li><li>  Agregar errores tipogr谩ficos y entrada parcial; </li><li>  Generaci贸n din谩mica de errores y entrada parcial ( <strong>tiempo de entrenamiento reducido 10x</strong> ); </li><li>  Puntuaci贸n final </li></ul><br><p>  <strong>Conjunto de desarrollo anotado manualmente + 1-3 errores por consulta</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ae2/a31/040/ae2a31040dbd77402d6b6dfee9eeba28.jpg"></p><br><p>  <strong>De izquierda a derecha</strong> <br>  (Precisi贸n Top1) </p><br><ul><li>  Algoritmo actual (b煤squeda el谩stica); </li><li>  Capa de bolsa de inclusi贸n de texto r谩pido; </li><li>  Agregar errores tipogr谩ficos y entrada parcial; </li><li>  Generaci贸n din谩mica de errores y entrada parcial; </li><li>  Puntuaci贸n final </li></ul><br><p>  <strong>Conjunto de desarrollo anotado manualmente + entrada parcial</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/c3c/680/681/c3c680681dd3166b95246930f1f1b1a8.jpg"></p><br><p>  <strong>De izquierda a derecha</strong> <br>  (Precisi贸n Top1) </p><br><ul><li>  Algoritmo actual (b煤squeda el谩stica); </li><li>  Capa de bolsa de incrustaci贸n de texto r谩pido; </li><li>  Agregar errores tipogr谩ficos y entrada parcial; </li><li>  Generaci贸n din谩mica de errores y entrada parcial; </li><li>  Puntuaci贸n final </li></ul><br><h2 id="large-scale-corpuses--n-gram-selection">  Corpus a gran escala / selecci贸n de n-gramas </h2><br><ul><li>  Recolectamos los corpus m谩s grandes para el idioma ruso: <br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Areneum</a> - una versi贸n procesada est谩 disponible aqu铆 - los autores del conjunto de datos no respondieron; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Taiga</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Common crawl</a> and <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">wiki</a> : siga estos art铆culos; </li></ul></li><li>  Recopilamos un diccionario de palabras de <code>100m</code> con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1 TB de rastreo</a> ; </li><li>  Tambi茅n use este <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">truco</a> para descargar dichos archivos m谩s r谩pido (durante la noche); </li><li>  Seleccionamos un conjunto 贸ptimo de <code>1m</code> n-gramos para que nuestro clasificador generalice mejor ( <code>500k</code> n-gramos m谩s populares de texto r谩pido entrenado en Wikipedia en ruso + <code>500k</code> n-gramos m谩s populares en nuestros datos de dominio); </li></ul><br><p>  <strong>Prueba de resistencia de nuestros 1M n-gramos en vocabulario 100M:</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/198/1fe/38b/1981fe38b03b4cdf76022f4ff6ef0074.png" alt="imagen"></p><br><h2 id="text-augmentations">  Aumentos de texto </h2><br><p>  En pocas palabras: </p><br><ul><li>  Tome un diccionario grande con errores (por ejemplo, 10-100m palabras 煤nicas); </li><li>  Genere un error (suelte una letra, cambie una letra usando probabilidades calculadas, inserte una letra aleatoria, tal vez use la distribuci贸n del teclado, etc.); </li><li>  Verifique que la nueva palabra est茅 en el diccionario; </li></ul><br><p>  Bruto forzamos muchas consultas a servicios como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este</a> (en un intento de realizar ingenier铆a inversa en su conjunto de datos), y tienen un diccionario muy peque帽o en su interior (tambi茅n este servicio funciona con un clasificador de 谩rbol con funciones de n-gramo).  Fue divertido ver que <strong>cubr铆an solo el 30-50% de las palabras que ten铆amos en algunos corpus</strong> . </p><br><p>  <strong>Nuestro enfoque es muy superior si tiene acceso a un amplio vocabulario de dominio</strong> . </p><br><h2 id="best-unsupervised--semi-supervised-results">  Los mejores resultados sin supervisi贸n / semi-supervisados </h2><br><p>  KNN se utiliza como punto de referencia para comparar diferentes m茅todos de inclusi贸n. </p><br><p>  (tama帽o del vector) Lista de modelos probados: </p><br><ul><li>  (512) Detector de oraciones falsas a gran escala capacitado en 200 GB de datos de rastreo comunes; </li><li>  (300) Detector de frases falsas entrenado para distinguir una frase aleatoria de Wikipedia de un servicio; </li><li>  (300) Texto r谩pido obtenido de aqu铆, pre-entrenado en araneum corpus; </li><li>  (200) Texto r谩pido entrenado en nuestros datos de dominio; </li><li>  (300) Texto r谩pido entrenado en 200 GB de datos de rastreo com煤n; </li><li>  (300) Una red siamesa con p茅rdida de triplete entrenada con servicios / sin贸nimos / oraciones aleatorias de Wikipedia; </li><li>  (200) Primera iteraci贸n de la capa de incrustaci贸n de la bolsa de inclusi贸n RNN, una oraci贸n se codifica como una bolsa completa de incrustaciones; </li><li>  (200) Lo mismo, pero primero la oraci贸n se divide en palabras, luego cada palabra se incrusta, luego se toma el promedio; </li><li>  (300) Lo mismo que arriba pero para el modelo final; </li><li>  (300) Lo mismo que arriba pero para el modelo final; </li><li>  (250) Capa de cuello de botella del modelo final (250 neuronas); </li><li>  L铆nea de base de b煤squeda el谩stica d茅bilmente supervisada; </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca1/e0b/e9c/ca1e0be9c152d092d4149f9986b87289.png" alt="por defecto"></p><br><p>  Para evitar fugas, todas las oraciones aleatorias se muestrearon aleatoriamente.  Su extensi贸n en palabras fue la misma que la duraci贸n de los servicios / sin贸nimos con los que se compararon.  Tambi茅n se tomaron medidas para asegurarse de que los modelos no solo aprendieran separando vocabularios (las incrustaciones se congelaron, Wikipedia se submuestre贸 para asegurarse de que hubiera al menos una palabra de dominio en cada oraci贸n de Wikipedia). </p><br><h2 id="cluster-visualization">  Visualizaci贸n de cl煤ster </h2><br><p>  <strong>3D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/4b7/f10/d19/4b7f10d19a785b5f690a28f2e2a039e6.gif"></p><br><p>  <strong>2D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ad7/0ad/441/ad70ad441ecae6f396c8bb76826484df.png"></p><br><h2 id="cluster-exploration-interface">  "Interfaz" de exploraci贸n de cl煤steres </h2><br><p>  Verde - nueva palabra / sin贸nimo. <br>  Fondo gris - probablemente nueva palabra. <br>  Texto gris: sin贸nimo existente. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cda/d17/00f/cdad1700ff0701ff6643a4aa14041d31.jpg"></p><br><h2 id="ablation-tests-and-what-works-what-we-tried-and-what-we-did-not">  Pruebas de ablaci贸n y qu茅 funciona, qu茅 probamos y qu茅 no. </h2><br><ol><li>  Ver los cuadros anteriores; </li><li>  Promedio simple / tf-idf promedio de incrustaciones de texto r谩pido: una <strong>l铆nea de base MUY formidable</strong> ; </li><li>  Texto r谩pido&gt; Word2Vec para ruso; </li><li>  La incrustaci贸n de oraciones mediante la detecci贸n de oraciones falsas funciona, pero palidece en comparaci贸n con otros m茅todos; </li><li>  BPE (oraci贸n) no mostr贸 mejoras en nuestro dominio; </li><li>  Los modelos de nivel de char lucharon por generalizar, a pesar del papel retractado de google; </li><li>  Probamos un transformador de m煤ltiples cabezales (con clasificador y cabezales de modelado de lenguaje), pero en la anotaci贸n disponible a mano, funcion贸 m谩s o menos igual que los modelos basados en LSTM.  Cuando migramos a incorporar malos enfoques, abandonamos esta l铆nea de investigaci贸n debido a la menor practicidad del transformador y la impracticabilidad de tener un cabezal LM junto con una capa de bolsa de inclusi贸n; </li><li>  <strong>BERT</strong> : parece ser excesivo, tambi茅n algunas personas afirman que los transformadores entrenan literalmente durante semanas; </li><li>  <strong>ELMO</strong> : usar una biblioteca como AllenNLP parece contraproducente en mi opini贸n tanto en entornos de investigaci贸n / producci贸n como en educaci贸n por razones que no proporcionar茅 aqu铆; </li></ol><br><h2 id="deploy">  Implementar </h2><br><p>  Hecho usando: </p><br><ul><li>  Contenedor Docker con un servicio web simple; </li><li>  CPU solo para inferencia es suficiente; </li><li>  ~ <code>2.5 ms</code> por consulta en la CPU, el procesamiento por lotes no es realmente necesario; </li><li>  ~ <code>1GB</code> memoria RAM; </li><li>  Casi no hay dependencias, aparte de <code>PyTorch</code> , <code>numpy</code> y <code>pandas</code> (y servidor web ofc). </li><li>  Imite la generaci贸n de n-gramas de texto r谩pido como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esta</a> ; </li><li>  Incrustar la capa de bolsa + 铆ndices como reci茅n almacenados en un diccionario; </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es428674/">https://habr.com/ru/post/es428674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es428662/index.html">Tarea de programaci贸n minorista</a></li>
<li><a href="../es428664/index.html">Arranque del kernel de Linux. Parte 1</a></li>
<li><a href="../es428666/index.html">C贸mo cre茅 animaciones que cambian el estado de 谩nimo usando m谩scaras CSS</a></li>
<li><a href="../es428668/index.html">Blizzard anunci贸 el lanzamiento del relanzamiento de WarCraft III en 2019. Pre-pedido abierto</a></li>
<li><a href="../es428672/index.html">Descripci贸n general del silenciador activo QuietOn</a></li>
<li><a href="../es428676/index.html">Romper los fundamentos fundamentales de C #: asignar memoria para un tipo de referencia en la pila</a></li>
<li><a href="../es428680/index.html">Crear e integrar VK bot en un grupo a trav茅s de VkBotLongPoll [Python]</a></li>
<li><a href="../es428682/index.html">Beta-Fallout 76 de autodestrucci贸n</a></li>
<li><a href="../es428688/index.html">Configurar el entorno de trabajo en Docker para la aplicaci贸n yii-framework</a></li>
<li><a href="../es428690/index.html">C贸mo ense帽arle a tu novia a programar si no eres maestra, pero ella cree en ti</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>