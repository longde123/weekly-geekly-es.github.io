<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úä ‚¨úÔ∏è ‚õ™Ô∏è Treinamento de refor√ßo: analisando jogos de v√≠deo üíπ üö¥üèª ü•Ö</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Na Confer√™ncia da AI, Vladimir Ivanov vivanov879 , Sr. falar√° sobre o uso da aprendizagem refor√ßada Engenheiro de aprendizado profundo na Nvidia . O e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Treinamento de refor√ßo: analisando jogos de v√≠deo</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/smileexpo/blog/428329/"><img align="left" src="https://habrastorage.org/webt/ls/va/9_/lsva9_rsrvkdmceogvse3sexdog.png"><br>  Na Confer√™ncia da AI, <b>Vladimir Ivanov <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">vivanov879</a> , Sr.</b> falar√° sobre o uso da aprendizagem refor√ßada  <b>Engenheiro de aprendizado profundo na Nvidia</b> .  O especialista est√° envolvido no aprendizado de m√°quina no departamento de testes: ‚ÄúAnaliso os dados que coletamos durante o teste de jogos e hardware de v√≠deo.  Para isso, uso aprendizado de m√°quina e vis√£o computacional.  A parte principal do trabalho √© an√°lise de imagens, limpeza de dados antes do treinamento, marca√ß√£o de dados e visualiza√ß√£o das solu√ß√µes obtidas. ‚Äù <br><br>  No artigo de hoje, Vladimir explica por que o aprendizado refor√ßado √© usado em carros aut√¥nomos e fala sobre como um agente √© treinado para agir em um ambiente em mudan√ßa - usando exemplos de videogame. <br><br>  Nos √∫ltimos anos, a humanidade acumulou uma enorme quantidade de dados.  Alguns conjuntos de dados s√£o compartilhados e dispostos manualmente.  Por exemplo, o conjunto de dados CIFAR, onde cada figura √© assinada, a qual classe pertence. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/ko/lp/16kolpeo1k3j1j9ppdh9vfjb6vc.jpeg"></div><br><br>  Existem conjuntos de dados em que voc√™ precisa atribuir uma classe n√£o apenas √† imagem como um todo, mas a todos os pixels da imagem.  Como, por exemplo, no CityScapes. <br><br><img src="https://habrastorage.org/webt/eq/hz/b0/eqhzb0gt9v2demntxriobd5ewji.png"><br><br>  O que une essas tarefas √© que uma rede neural de aprendizado precisa apenas lembrar os padr√µes nos dados.  Portanto, com quantidades de dados suficientemente grandes e, no caso do CIFAR, s√£o 80 milh√µes de imagens, a rede neural est√° aprendendo a generalizar.  Como resultado, ela lida bem com a classifica√ß√£o de imagens que nunca havia visto antes. <br><br>  Mas, atuando dentro da estrutura da t√©cnica de ensino com o professor, que trabalha para marcar imagens, √© imposs√≠vel resolver problemas onde queremos n√£o prever a marca, mas tomar decis√µes.  Como, por exemplo, no caso de dire√ß√£o aut√¥noma, em que a tarefa √© alcan√ßar com seguran√ßa e confiabilidade o ponto final da rota. <a name="habracut"></a><br><br>  Nos problemas de classifica√ß√£o, usamos a t√©cnica de ensino com o professor - quando cada figura recebe uma aula espec√≠fica.  Mas e se n√£o tivermos essa marca√ß√£o, mas houver um agente e um ambiente em que ele possa executar determinadas a√ß√µes?  Por exemplo, seja um videogame, e podemos clicar nas setas de controle. <br><br><img src="https://habrastorage.org/webt/bn/-c/ad/bn-cadnljvu7relew1mfes6p0xw.jpeg"><br><br>  Esse tipo de problema deve ser resolvido com o treinamento de refor√ßo.  Na declara√ß√£o geral do problema, queremos aprender como executar a sequ√™ncia correta de a√ß√µes.  √â de fundamental import√¢ncia que o agente tenha a capacidade de executar a√ß√µes repetidas vezes, explorando o ambiente em que est√°.  E, em vez da resposta correta, o que fazer em uma situa√ß√£o espec√≠fica, ele recebe uma recompensa por uma tarefa conclu√≠da corretamente.  Por exemplo, no caso de um t√°xi aut√¥nomo, o motorista receber√° um b√¥nus por cada viagem realizada. <br><br>  Vamos voltar a um exemplo simples - um videogame.  Pegue algo simples, como o jogo de t√™nis de mesa Atari. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hg/ev/0b/hgev0bl9kpvpzlllfzyyroentow.jpeg"></div><br><br>  Controlaremos o tablet √† esquerda.  Vamos jogar contra o jogador do computador programado de acordo com as regras √† direita.  Como trabalhamos com uma imagem e as redes neurais s√£o as que obt√™m mais sucesso na extra√ß√£o de informa√ß√µes, vamos aplicar uma imagem √† entrada de uma rede neural de tr√™s camadas com um tamanho de n√∫cleo 3x3.  Na sa√≠da, ela ter√° que escolher uma das duas a√ß√µes: mover o tabuleiro para cima ou para baixo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/o_/5z/mwo_5zeixn2llke6ethglty43ao.png"></div><br><br>  Treinamos a rede neural para executar a√ß√µes que levam √† vit√≥ria.  A t√©cnica de treinamento √© a seguinte.  Deixamos a rede neural jogar algumas rodadas de t√™nis de mesa.  Ent√£o come√ßamos a classificar os jogos disputados.  Nos jogos em que ela venceu, marcamos as imagens ‚ÄúUp‚Äù, onde ela levantou a raquete, e ‚ÄúDown‚Äù, onde ela a abaixou.  Nos jogos perdidos, fazemos o oposto.  Marcamos as fotos em que ela abaixou o quadro com o r√≥tulo "Para cima" e onde ela o levantou, "Para baixo".  Assim, reduzimos o problema √† abordagem que j√° conhecemos - treinando com um professor.  Temos um conjunto de fotos com tags. <br><br><img src="https://habrastorage.org/webt/al/16/48/al1648vo1sfp72qj_8n2fpif3va.png"><br><br>  Usando essa t√©cnica de treinamento, em algumas horas, nosso agente aprender√° a vencer um jogador de computador programado de acordo com as regras. <br><br>  O que fazer com a condu√ß√£o aut√¥noma?  O fato √© que o t√™nis de mesa √© um jogo muito simples.  E pode produzir milhares de quadros por segundo.  Na nossa rede agora existem apenas 3 camadas.  Portanto, o processo de aprendizado √© extremamente r√°pido.  O jogo gera uma enorme quantidade de dados e n√≥s os processamos instantaneamente.  No caso de dire√ß√£o aut√¥noma, a coleta de dados √© muito mais longa e mais cara.  Os carros s√£o caros e, com um carro, receberemos apenas 60 quadros por segundo.  Al√©m disso, o pre√ßo do erro aumenta.  Em um videogame, poder√≠amos jogar um jogo ap√≥s o outro no in√≠cio do treinamento.  Mas n√£o podemos dar ao luxo de estragar o carro. <br><br>  Nesse caso, vamos ajudar a rede neural no in√≠cio do treinamento.  Fixamos a c√¢mera no carro, colocamos um motorista experiente e gravaremos fotos da c√¢mera.  Para cada imagem, assinamos o √¢ngulo de dire√ß√£o do carro.  Treinaremos a rede neural para copiar o comportamento de um motorista experiente.  Assim, novamente reduzimos a tarefa ao ensino j√° conhecido com um professor. <br><br><img src="https://habrastorage.org/webt/pl/d0/oc/pld0oc75oafeojutvborl_upb8a.png"><br><br>  Com um conjunto de dados suficientemente amplo e diversificado, que incluir√° diferentes paisagens, esta√ß√µes do ano e condi√ß√µes clim√°ticas, a rede neural aprender√° como controlar com precis√£o o carro. <br><br>  No entanto, houve um problema com os dados.  Eles s√£o muito longos e caros para colecionar.  Vamos usar um simulador no qual toda a f√≠sica do movimento do carro ser√° implementada - por exemplo, o DeepDrive.  Podemos aprender sem medo de perder um carro. <br><br><img src="https://habrastorage.org/webt/fe/c0/4v/fec04vmzbamtd60xxv3ypctf2ua.jpeg"><br><br>  Neste simulador, temos acesso a todos os indicadores do carro e do mundo.  Al√©m disso, todas as pessoas, carros, suas velocidades e dist√¢ncias s√£o marcadas. <br><br><img src="https://habrastorage.org/webt/hq/37/1k/hq371k3xvab9kfcworznwlkelai.jpeg"><br><br>  Do ponto de vista do engenheiro, nesse simulador, voc√™ pode tentar com seguran√ßa novas t√©cnicas de treinamento.  O que um pesquisador deve fazer?  Por exemplo, estudando diferentes op√ß√µes para a descida do gradiente em problemas de aprendizagem com refor√ßo.  Para testar uma hip√≥tese simples, n√£o quero disparar pardais de um canh√£o e administrar um agente em um mundo virtual complexo e esperar dias seguidos pelos resultados da simula√ß√£o.  Nesse caso, vamos usar nosso poder de computa√ß√£o com mais efici√™ncia.  Deixe os agentes serem mais simples.  Tomemos, por exemplo, um modelo de aranha quadr√∫pede.  No simulador do Mujoco, √© assim: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yp/ad/dg/ypaddgwogfekvvjemaf3vseohdi.png"></div><br><br>  Estabelecemos a ele a tarefa de correr na velocidade mais alta poss√≠vel em uma determinada dire√ß√£o - por exemplo, para a direita.  O n√∫mero de par√¢metros observados para uma aranha √© um vetor de 39 dimens√µes, que registra a posi√ß√£o e a velocidade de todos os seus membros.  Ao contr√°rio da rede neural para t√™nis de mesa, onde havia apenas um neur√¥nio na sa√≠da, h√° oito na sa√≠da (j√° que a aranha neste modelo possui 8 articula√ß√µes). <br><br>  Em modelos t√£o simples, v√°rias hip√≥teses sobre a t√©cnica de ensino podem ser testadas.  Por exemplo, vamos comparar a velocidade de aprender a correr, dependendo do tipo de rede neural.  Seja uma rede neural de camada √∫nica, uma rede neural de tr√™s camadas, uma rede convolucional e uma rede recorrente: <br><br><img src="https://habrastorage.org/webt/sq/zl/xu/sqzlxuj-k_nts13x7x5veh-56ay.png"><br><br>  A conclus√£o pode ser tirada da seguinte forma: como o modelo de aranha e a tarefa s√£o bastante simples, os resultados do treinamento s√£o aproximadamente os mesmos para diferentes modelos.  Uma rede de tr√™s camadas √© muito complexa e, portanto, aprende pior. <br><br>  Apesar de o simulador funcionar com um modelo simples de aranha, dependendo da tarefa colocada na aranha, o treinamento pode durar dias.  Nesse caso, vamos animar v√°rias centenas de aranhas em uma superf√≠cie ao mesmo tempo, em vez de uma, e aprender com os dados que receberemos de todos.  Ent√£o, vamos acelerar o treinamento v√°rias centenas de vezes.  Aqui est√° um exemplo do mecanismo Flex. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/p4/6l/4j/p46l4jyulhhspwa9e5z_4oe40oq.png"></div><br><br>  A √∫nica coisa que mudou em termos de otimiza√ß√£o de rede neural √© a coleta de dados.  Quando executamos apenas uma aranha, recebemos dados sequencialmente.  Uma corrida atr√°s da outra. <br><br><img src="https://habrastorage.org/webt/8t/cd/hr/8tcdhrcxqbx4mk3w99lacw2tfbu.png"><br><br>  Agora pode acontecer que algumas aranhas estejam apenas come√ßando a corrida, enquanto outras j√° correm h√° muito tempo. <br><br><img src="https://habrastorage.org/webt/6u/i6/jv/6ui6jv9y3zmbdy3cwfweuv1drt4.png"><br><br>  Isso ser√° levado em considera√ß√£o durante a otimiza√ß√£o da rede neural.  Caso contr√°rio, tudo permanece o mesmo.  Como resultado, temos acelera√ß√£o no treinamento centenas de vezes, de acordo com o n√∫mero de aranhas que est√£o simultaneamente na tela. <br><br>  Como temos um simulador eficaz, vamos tentar resolver problemas mais complexos.  Por exemplo, correndo sobre terrenos acidentados. <br><br><img src="https://habrastorage.org/webt/zo/sq/dn/zosqdnbygiz_2p-cvxl4hvtuvqw.png"><br><br>  Como o ambiente nesse caso se tornou mais agressivo, vamos mudar e complicar tarefas durante o treinamento.  √â dif√≠cil de aprender, mas f√°cil na batalha.  Por exemplo, a cada poucos minutos para mudar o terreno.  Al√©m disso, vamos direcionar agentes externos para o agente.  Por exemplo, vamos jogar bolas nele e ligar e desligar o vento.  Ent√£o o agente aprende a correr mesmo em superf√≠cies que ele nunca conheceu.  Por exemplo, suba escadas. <br><br><img src="https://habrastorage.org/webt/zc/-7/bq/zc-7bqzt5kcltfgrwfc9piw3mli.png"><br><br>  Como aprendemos com tanta efic√°cia a executar simula√ß√µes, vamos verificar as t√©cnicas de treinamento por refor√ßo em disciplinas competitivas.  Por exemplo, em jogos de tiro.  A plataforma VizDoom oferece um mundo no qual voc√™ pode atirar, coletar armas e recuperar a sa√∫de.  Neste jogo tamb√©m usaremos uma rede neural.  S√≥ agora ela ter√° cinco sa√≠das: quatro para movimento e uma para fotografar. <br><br>  Para que o treinamento seja eficaz, vamos lev√°-lo gradualmente.  Do simples ao complexo.  Na entrada, a rede neural recebe uma imagem e, antes de come√ßar a fazer algo consciente, precisa aprender a entender em que consiste o mundo.  Estudando em cen√°rios simples, ela aprender√° a entender quais objetos habitam o mundo e como interagir com eles.  Vamos come√ßar com o tra√ßo: <br><br><img src="https://habrastorage.org/webt/99/cy/xm/99cyxm9xxkmd3wg9zryiguyqnoe.png"><br><br>  Tendo dominado esse cen√°rio, o agente entender√° que existem inimigos e eles devem ser atingidos, porque voc√™ ganha pontos por eles.  Em seguida, vamos trein√°-lo em um cen√°rio em que a sa√∫de est√° constantemente diminuindo e voc√™ precisa reabastecer. <br><br><img src="https://habrastorage.org/webt/06/gs/jr/06gsjr-tuvruizcpmy3da0gsyq4.jpeg"><br><br>  Aqui ele aprender√° que tem sa√∫de e precisa ser reabastecido, porque, em caso de morte, o agente recebe uma recompensa negativa.  Al√©m disso, ele aprender√° que, se voc√™ se mover em dire√ß√£o ao sujeito, poder√° colet√°-lo.  No primeiro cen√°rio, o agente n√£o p√¥de se mover. <br><br>  E no terceiro cen√°rio final, vamos deix√°-lo para atirar com os bots programados nas regras do jogo para que ele possa aprimorar suas habilidades. <br><br><img src="https://habrastorage.org/webt/dn/pz/iq/dnpziqqditttkptyp7zflcdh4ag.png"><br><br>  Durante o treinamento neste cen√°rio, a sele√ß√£o correta das recompensas que o agente recebe √© muito importante.  Por exemplo, se voc√™ der uma recompensa apenas aos rivais derrotados, o sinal ser√° muito raro: se houver poucos jogadores por perto, receberemos pontos a cada poucos minutos.  Portanto, vamos usar a combina√ß√£o de recompensas que eram antes.  O agente receber√° uma recompensa por cada a√ß√£o √∫til, seja para melhorar a sa√∫de, selecionar cartuchos ou atingir um oponente. <br><br><blockquote>  Como resultado, um agente treinado com recompensas bem escolhidas √© mais forte do que seus oponentes mais exigentes em termos computacionais.  Em 2016, esse sistema venceu a competi√ß√£o VizDoom com uma margem de mais da metade dos pontos conquistados no segundo lugar.  A equipe vice-campe√£ tamb√©m usou uma rede neural, apenas com um grande n√∫mero de camadas e informa√ß√µes adicionais do mecanismo de jogo durante o treinamento.  Por exemplo, informa√ß√µes sobre se h√° inimigos no campo de vis√£o do agente. </blockquote><br>  Examinamos abordagens para resolver problemas, onde √© importante tomar decis√µes.  Mas muitas tarefas com essa abordagem permanecer√£o sem solu√ß√£o.  Por exemplo, o jogo de busca Montezuma Revenge. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/s9/vp/_9/s9vp_9botnvsnfqcaxutbv-2pn0.jpeg"></div><br><br>  Aqui voc√™ precisa procurar chaves para abrir as portas dos quartos vizinhos.  Raramente recebemos chaves e abrimos salas com menos frequ√™ncia.  Tamb√©m √© importante n√£o se distrair com objetos estranhos.  Se voc√™ treinar o sistema, como fizemos nas tarefas anteriores, e recompensar os inimigos derrotados, ele simplesmente nocautear√° o cr√¢nio rolante repetidamente e n√£o examinar√° o mapa.  Se voc√™ estiver interessado, posso falar sobre a solu√ß√£o desses problemas em outro artigo. <br><br>  <b>Voc√™ pode ouvir o discurso de Vladimir Ivanov na Confer√™ncia da AI em 22 de novembro</b> .  Um programa detalhado e ingressos est√£o dispon√≠veis no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">site oficial do</a> evento. <br><br>  Leia a entrevista com Vladimir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt428329/">https://habr.com/ru/post/pt428329/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt428313/index.html">O telegrama no MacOS [presumivelmente] tamb√©m armazena correspond√™ncia localmente de forma acess√≠vel</a></li>
<li><a href="../pt428315/index.html">5 medos de desenvolvedores que superamos</a></li>
<li><a href="../pt428317/index.html">Reagir ganchos - ganhar ou perder?</a></li>
<li><a href="../pt428321/index.html">An√°lise preditiva de dados - modelagem e valida√ß√£o</a></li>
<li><a href="../pt428327/index.html">O que procurar: Regulamento Europeu de Identifica√ß√£o Eletr√¥nica eIDAS</a></li>
<li><a href="../pt428333/index.html">2018 RAIF Hackathon AI Hackathon Resultados</a></li>
<li><a href="../pt428335/index.html">Atualiza√ß√£o de atalho Siri</a></li>
<li><a href="../pt428337/index.html">JavaScript divertido: sem chaves</a></li>
<li><a href="../pt428339/index.html">N√£o automatize: m√°s dicas de neg√≥cios</a></li>
<li><a href="../pt428341/index.html">Tecnologia Qsan RAID EE</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>