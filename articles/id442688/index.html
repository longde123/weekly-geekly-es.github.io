<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📨 🦏 🖌️ Analisis data scala - kebutuhan mendesak atau peluang yang menyenangkan? 🕴🏿 👨 👩‍⚕️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Alat tradisional di bidang Ilmu Data adalah bahasa seperti sintaksis R dan Python -santai dan sejumlah besar pustaka untuk pembelajaran mesin dan pemr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analisis data scala - kebutuhan mendesak atau peluang yang menyenangkan?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/442688/"><p><img src="https://habrastorage.org/webt/5q/1e/fr/5q1efrbkxbuk_ndqoensinfl80a.jpeg"></p><br><p>  Alat tradisional di bidang Ilmu Data adalah bahasa seperti sintaksis <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">R</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Python</a> -santai dan sejumlah besar pustaka untuk pembelajaran mesin dan pemrosesan data memungkinkan Anda dengan cepat mendapatkan beberapa solusi yang berfungsi.  Namun, ada situasi di mana keterbatasan alat-alat ini menjadi penghambat yang signifikan - pertama-tama, jika Anda perlu mencapai kinerja tinggi dalam hal kecepatan pemrosesan dan / atau bekerja dengan kumpulan data yang sangat besar.  Dalam hal ini, spesialis harus dengan enggan beralih ke bantuan "sisi gelap" dan menghubungkan alat-alat dalam bahasa pemrograman "industri": <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Scala</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Java</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">C ++</a> . </p><br><p> Tapi apakah sisi ini sangat gelap?  Selama bertahun-tahun pengembangan, alat-alat dari Ilmu Data "industri" telah datang jauh dan hari ini mereka sangat berbeda dari versi mereka sendiri 2-3 tahun yang lalu.  Mari kita coba menggunakan contoh tugas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SNA Hackathon 2019</a> untuk mencari tahu seberapa besar ekosistem Scala + Spark dapat dikaitkan dengan Python Data Science. </p><a name="habracut"></a><br><p>  Dalam kerangka kerja <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SNA Hackathon 2019,</a> peserta memecahkan masalah pengurutan umpan berita dari pengguna jejaring sosial dalam salah satu dari tiga "disiplin": menggunakan data dari teks, gambar, atau fitur log.  Dalam publikasi ini, kita akan melihat bagaimana di Spark dimungkinkan untuk memecahkan masalah berdasarkan log tanda menggunakan alat pembelajaran mesin klasik. </p><br><p>  Dalam menyelesaikan masalah, kami akan menggunakan cara standar yang dilalui oleh setiap spesialis analisis data saat mengembangkan model: </p><br><ul><li>  Kami akan melakukan analisis data penelitian, membuat grafik. </li><li>  Kami menganalisis sifat statistik tanda-tanda dalam data, melihat perbedaannya antara set pelatihan dan tes. </li><li>  Kami akan melakukan pemilihan awal fitur berdasarkan properti statistik. </li><li>  Kami menghitung korelasi antara tanda-tanda dan variabel target, serta korelasi silang antara tanda-tanda. </li><li>  Kami akan membentuk set fitur terakhir, melatih model dan memeriksa kualitasnya. </li><li>  Mari kita menganalisis struktur internal model untuk mengidentifikasi titik pertumbuhan. </li></ul><br><p>  Selama "perjalanan" kita, kita akan berkenalan dengan alat-alat seperti notebook interaktif <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Zeppelin</a> , perpustakaan pembelajaran mesin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ML Spark</a> dan ekstensi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PravdaML</a> , paket <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">grafik</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">GraphX</a> , perpustakaan visualisasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Vegas</a> , dan, tentu saja, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apache Spark</a> dengan segala kemuliaan: )  Semua kode dan hasil percobaan tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">platform</a> notebook kolaborasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Zepl</a> . </p><br><h1 id="zagruzka-dannyh">  Pemuatan data </h1><br><p>  Fitur dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">data yang</a> diletakkan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SNA Hackathon 2019</a> adalah bahwa adalah mungkin untuk memprosesnya secara langsung menggunakan Python, tetapi sulit: sumber data dikemas secara efisien berkat kemampuan format kolom Apache Parket dan ketika membaca ke dalam memori "dengan dahi" itu didekompresi menjadi beberapa puluh gigabyte.  Ketika bekerja dengan Apache Spark, tidak perlu memuat data sepenuhnya ke dalam memori, arsitektur Spark dirancang untuk memproses data dalam bentuk potongan, memuat dari disk seperlunya. </p><br><p>  Oleh karena itu, langkah pertama - memeriksa distribusi data berdasarkan hari - mudah dilakukan oleh alat kotak: </p><br><pre><code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = sqlContext.read.parquet(<span class="hljs-string"><span class="hljs-string">"/events/hackatons/SNAHackathon/2019/collabTrain"</span></span>) z.show(train.groupBy($<span class="hljs-string"><span class="hljs-string">"date"</span></span>).agg( functions.count($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"count"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"users"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"objects"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"metadata_ownerId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"owners"</span></span>)) .orderBy(<span class="hljs-string"><span class="hljs-string">"date"</span></span>))</code> </pre> <br><p>  Apa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">grafik yang</a> sesuai <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">akan</a> ditampilkan di Zeppelin: </p><br><p><img src="https://habrastorage.org/webt/jp/lh/pw/jplhpwwz4tfgdtrs1u_u-tqsvzi.png"></p><br><p>  Saya harus mengatakan bahwa sintaks Scala cukup fleksibel, dan kode yang sama mungkin terlihat, misalnya, seperti ini: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = sqlContext.read.parquet(<span class="hljs-string"><span class="hljs-string">"/events/hackatons/SNAHackathon/2019/collabTrain"</span></span>) z.show( train groupBy $<span class="hljs-string"><span class="hljs-string">"date"</span></span> agg( count($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"count"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"users"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"objects"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"metadata_ownerId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"owners"</span></span>) orderBy <span class="hljs-string"><span class="hljs-string">"date"</span></span> )</code> </pre> <br><p>  Sebuah peringatan penting harus dibuat di sini: ketika bekerja dalam sebuah tim besar, di mana semua orang mendekati penulisan Scala-code secara eksklusif dari sudut pandang selera mereka sendiri, komunikasi jauh lebih sulit.  Jadi lebih baik untuk mengembangkan konsep gaya kode terpadu. </p><br><p>  Tapi kembali ke tugas kita.  Analisis sederhana pada siang hari menunjukkan adanya titik-titik abnormal pada 17 dan 18 Februari;  mungkin hari ini data yang tidak lengkap telah dikumpulkan, dan distribusi sifat mungkin bias.  Ini harus diperhitungkan dalam analisis lebih lanjut.  Selain itu, sangat mengejutkan bahwa jumlah pengguna unik sangat dekat dengan jumlah objek, sehingga masuk akal untuk mempelajari distribusi pengguna dengan jumlah objek yang berbeda: </p><br><pre> <code class="scala hljs">z.show(filteredTrain .groupBy($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).count .groupBy(<span class="hljs-string"><span class="hljs-string">"count"</span></span>).agg(functions.log(functions.count(<span class="hljs-string"><span class="hljs-string">"count"</span></span>)).as(<span class="hljs-string"><span class="hljs-string">"withCount"</span></span>)) .orderBy($<span class="hljs-string"><span class="hljs-string">"withCount"</span></span>.desc) .limit(<span class="hljs-number"><span class="hljs-number">100</span></span>) .orderBy($<span class="hljs-string"><span class="hljs-string">"count"</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/dh/i-/pp/dhi-ppdka19zurhimccpq7tij5w.png"></p><br><p>  Diharapkan melihat distribusi yang dekat dengan eksponensial, dengan ekor yang sangat panjang.  Dalam tugas-tugas seperti itu, sebagai suatu peraturan, adalah mungkin untuk mencapai peningkatan dalam kualitas pekerjaan dengan mensegmentasi model untuk pengguna dengan berbagai tingkat aktivitas.  Untuk memeriksa apakah perlu melakukan ini, bandingkan distribusi jumlah objek oleh pengguna di set tes: </p><br><p><img src="https://habrastorage.org/webt/du/sy/xt/dusyxten5shm24an736n7akolnm.png"></p><br><p>  Perbandingan dengan tes menunjukkan bahwa pengguna tes memiliki setidaknya dua objek dalam log (karena masalah peringkat diselesaikan pada hackathon, ini adalah kondisi yang diperlukan untuk menilai kualitas).  Di masa mendatang, saya sarankan untuk melihat lebih dekat pengguna di set pelatihan, yang kami nyatakan Fungsi Buatan Pengguna dengan filter: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//  ,     "",   , //     val testSimilar = sc.broadcast(filteredTrain.groupBy($"instanceId_userId") .agg( functions.count("feedback").as("count"), functions.sum(functions.expr("IF(array_contains(feedback, 'Liked'), 1.0, 0.0)")).as("sum") ) .where("count &gt; sum AND sum &gt; 0") .select("instanceId_userId").rdd.map(_.getInt(0)).collect.sorted) //           // User Defined Function val isTestSimilar = sqlContext.udf.register("isTestSimilar", (x: Int) =&gt; java.util.Arrays.binarySearch(testSimilar.value, x) &gt;= 0)</span></span></code> </pre> <br><p>  Sebuah komentar penting juga harus dibuat di sini: dari sudut pandang mendefinisikan UDF bahwa penggunaan Spark dari bawah Scala / Java dan dari bawah Python sangat berbeda.  Sementara kode PySpark menggunakan fungsionalitas dasar, semuanya bekerja hampir sama cepatnya, tetapi ketika fungsi yang ditimpa muncul, kinerja PySpark menurun berdasarkan urutan besarnya. </p><br><h1 id="pervyy-ml-konveyer">  Pipa ML pertama </h1><br><p>  Pada langkah berikutnya, kami akan mencoba menghitung statistik dasar tentang tindakan dan atribut.  Tetapi untuk ini kita membutuhkan kemampuan SparkML, jadi pertama-tama kita akan melihat arsitektur umumnya: </p><br><p><img src="https://habrastorage.org/webt/j7/ev/en/j7evenhyzzfvzouqfkbfq1j9hvu.png"></p><br><p>  SparkML dibangun berdasarkan konsep berikut: </p><br><ul><li>  Transformer - mengambil set data sebagai input dan mengembalikan set yang dimodifikasi (transform).  Sebagai aturan, ini digunakan untuk mengimplementasikan algoritma sebelum dan sesudah pemrosesan, ekstraksi fitur, dan juga dapat mewakili model-ML yang dihasilkan. </li><li>  Estimator - mengambil set data sebagai input, dan mengembalikan Transformer (cocok).  Secara alami, Pengukur dapat mewakili algoritma ML. </li><li>  Pipeline adalah kasus khusus Estimator, yang terdiri dari rantai transformer dan estimator.  Ketika metode ini disebut, fit melewati rantai, dan jika ia melihat sebuah transformator, ia menerapkannya pada data, dan jika ia melihat suatu estimator, ia melatih transformator dengannya, menerapkannya pada data dan melangkah lebih jauh. </li><li>  PipelineModel - hasil Pipeline juga mengandung rantai di dalamnya, tetapi hanya terdiri dari transformer.  Dengan demikian, PipelineModel sendiri juga merupakan transformator. </li></ul><br><p>  Pendekatan seperti pembentukan algoritma ML membantu untuk mencapai struktur modular yang jelas dan reproduktifitas yang baik - baik model dan saluran pipa dapat disimpan. </p><br><p>  Untuk memulainya, kami akan membangun jalur pipa sederhana yang dengannya kami menghitung statistik distribusi tindakan (bidang umpan balik) dari pengguna dalam rangkaian pelatihan: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> feedbackAggregator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-comment"><span class="hljs-comment">//         (feedback)  one-hot  new MultinominalExtractor().setInputCol("feedback").setOutputCol("feedback"), //       new VectorStatCollector() .setGroupByColumns("date").setInputCol("feedback") .setPercentiles(Array(0.1,0.5,0.9)), //        new VectorExplode().setValueCol("feedback") )).fit(train) z.show(feedbackAggregator .transform(filteredTrain) .orderBy($"date", $"feedback"))</span></span></code> </pre> <br><p>  Dalam pipa ini, fungsionalitas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PravdaML</a> digunakan secara aktif - perpustakaan dengan blok berguna yang diperluas untuk SparkML, yaitu: </p><br><ul><li>  MultinominalExtractor digunakan untuk menyandikan karakter tipe "array of strings" ke dalam vektor sesuai dengan prinsip one-hot.  Ini adalah satu-satunya penaksir dalam pipeline (untuk membangun encoding, Anda harus mengumpulkan garis unik dari dataset). </li><li>  VectorStatCollector digunakan untuk menghitung statistik vektor. </li><li>  VectorExplode digunakan untuk mengubah hasilnya menjadi format yang nyaman untuk visualisasi. </li></ul><br><p>  Hasil karya akan berupa grafik yang menunjukkan bahwa kelas dalam dataset tidak seimbang, namun, ketidakseimbangan untuk target Kelas yang disukai tidak ekstrem: </p><br><p><img src="https://habrastorage.org/webt/aa/db/x4/aadbx4el5s5lhuyput_cj4ns9zo.png"></p><br><p>  Analisis distribusi serupa di antara pengguna yang mirip dengan yang diuji (memiliki "positif" dan "negatif" dalam log) menunjukkan bahwa itu bias terhadap kelas positif: </p><br><p><img src="https://habrastorage.org/webt/x5/lu/px/x5lupxmayvvodo7lsb3meob1dou.png"></p><br><h1 id="statisticheskiy-analiz-priznakov">  Analisis statistik tanda-tanda </h1><br><p>  Pada tahap selanjutnya, kami akan melakukan analisis terperinci tentang sifat statistik atribut.  Kali ini kami membutuhkan conveyor yang lebih besar: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> statsAggregator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-comment"><span class="hljs-comment">//          new AutoAssembler() .setColumnsToExclude( (Seq("date", "feedback") ++ train.schema.fieldNames.filter(_.endsWith("Id")) : _*)) .setOutputCol("features"), new VectorStatCollector() .setGroupByColumns("date").setInputCol("features") .setPercentiles(Array(0.1,0.5,0.9)), new VectorExplode().setValueCol("features") ))</span></span></code> </pre> <br><p>  Karena sekarang kita perlu bekerja bukan dengan bidang terpisah, tetapi dengan semua atribut sekaligus, kita akan menggunakan dua utilitas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PravdaML yang</a> lebih berguna: </p><br><ul><li>  NullToDefaultReplacer memungkinkan Anda untuk mengganti elemen yang hilang dalam data dengan nilai defaultnya (0 untuk angka, false untuk variabel logis, dll.).  Jika Anda tidak melakukan konversi ini, maka nilai NaN akan muncul di vektor yang dihasilkan, yang fatal bagi banyak algoritma (meskipun, misalnya, XGBoost dapat bertahan hidup ini).  Alternatif untuk mengganti dengan nol dapat diganti dengan rata-rata, ini diterapkan di NaNToMeanReplacerEstimator. </li><li>  AutoAssembler adalah utilitas yang sangat kuat yang menganalisis tata letak tabel dan untuk setiap kolom memilih skema vektorisasi yang cocok dengan jenis kolom. </li></ul><br><p>  Dengan menggunakan pipa yang dihasilkan, kami menghitung statistik untuk tiga set (pelatihan, pelatihan dengan filter dan pengujian pengguna) dan menyimpan dalam file terpisah: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//   (   AutoAssembler  ) val trained = statsAggregator.fit(filteredTrain) //       - ,     . trained .transform(filteredTrain .withColumn("date", //  ,      ,     , //        All   functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(7).write.mode("overwrite").parquet("sna2019/featuresStat") trained .transform(filteredTrain .where(isTestSimilar($"instanceId_userId")) .withColumn("date", functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(7).write.mode("overwrite").parquet("sna2019/filteredFeaturesStat") trained .transform(filteredTest.withColumn("date", functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(3).write.mode("overwrite").parquet("sna2019/testFeaturesStat")</span></span></code> </pre> <br><p>  Setelah menerima tiga dataset dengan statistik atribut, kami menganalisis hal-hal berikut: </p><br><ul><li>  Apakah kita memiliki tanda-tanda yang memiliki emisi besar. <br>  - Tanda-tanda tersebut harus dibatasi, atau catatan pencilan harus disaring. </li><li>  Apakah kita memiliki tanda-tanda dengan bias rata-rata relatif terhadap median. <br>  - Pergeseran demikian sering terjadi di hadapan distribusi daya, masuk akal untuk logaritma tanda-tanda ini. </li><li>  Apakah ada pergeseran dalam distribusi rata-rata antara pelatihan dan set tes. </li><li>  Seberapa rapatnya matriks fitur kami. </li></ul><br><p>  Untuk memperjelas aspek-aspek ini, permintaan semacam itu akan membantu kami: </p><br><pre> <code class="scala hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compareWithTest</span></span></span></span>(data: <span class="hljs-type"><span class="hljs-type">DataFrame</span></span>) : <span class="hljs-type"><span class="hljs-type">DataFrame</span></span> = { data.where(<span class="hljs-string"><span class="hljs-string">"date = 'All'"</span></span>) .select( $<span class="hljs-string"><span class="hljs-string">"features"</span></span>, <span class="hljs-comment"><span class="hljs-comment">//         // ( ) functions.log($"features_mean" / $"features_p50").as("skewenes"), //    90-      //    90-  —    functions.log( ($"features_max" - $"features_p90") / ($"features_p90" - $"features_p50")).as("outlieres"), //       ,  //    ($"features_nonZeros" / $"features_count").as("train_fill"), $"features_mean".as("train_mean")) .join(testStat.where("date = 'All'") .select($"features", $"features_mean".as("test_mean"), ($"features_nonZeros" / $"features_count").as("test_fill")), Seq("features")) //          .withColumn("meanDrift", (($"train_mean" - $"test_mean" ) / ($"train_mean" + $"test_mean"))) //      .withColumn("fillDrift", ($"train_fill" - $"test_fill") / ($"train_fill" + $"test_fill")) } //         val comparison = compareWithTest(trainStat).withColumn("mode", functions.lit("raw")) .unionByName(compareWithTest(filteredStat).withColumn("mode", functions.lit("filtered")))</span></span></code> </pre> <br><p>  Pada tahap ini, muncul pertanyaan visualisasi: sulit untuk segera menampilkan semua aspek menggunakan alat Zeppelin biasa, dan notebook dengan sejumlah besar grafik mulai terasa melambat karena DOM yang membengkak.  Perpustakaan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Vegas</a> - DSL di Scala untuk membangun spesifikasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">vega-lite</a> dapat mengatasi masalah ini.  Vegas tidak hanya menyediakan kemampuan visualisasi yang lebih kaya (sebanding dengan matplotlib), tetapi juga menggambarnya di atas Kanvas tanpa menggembungkan DOM :). </p><br><p>  Spesifikasi bagan yang kami minati akan terlihat seperti ini: </p><br><pre> <code class="scala hljs">vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>(width = <span class="hljs-number"><span class="hljs-number">1024</span></span>, height = <span class="hljs-number"><span class="hljs-number">648</span></span>) <span class="hljs-comment"><span class="hljs-comment">//   .withDataFrame(comparison.na.fill(0.0)) //           .encodeX("meanDrift", Quant, scale = Scale(domainValues = List(-1.0, 1.0), clamp = true)) //   -       .encodeY("train_fill", Quant) //       .encodeColor("outlieres", Quant, scale=Scale( rangeNominals=List("#00FF00", "#FF0000"), domainValues = List(0.0, 5), clamp = true)) //       .encodeSize("skewenes", Quant) //   -   (   ) .encodeShape("mode", Nom) .mark(vegas.Point) .show</span></span></code> </pre> <br><p>  Grafik di bawah ini seharusnya berbunyi seperti ini: </p><br><ul><li>  Sumbu X menunjukkan pergeseran pusat distribusi antara set tes dan pelatihan (semakin dekat ke 0, semakin stabil tandanya). </li><li>  Persentase elemen bukan nol diplot sepanjang sumbu Y (semakin tinggi, semakin banyak data yang ada untuk semakin banyak poin berdasarkan atribut). </li><li>  Ukurannya menunjukkan pergeseran rata-rata relatif terhadap median (semakin besar titik, semakin besar kemungkinan distribusi hukum kekuasaan untuk itu). </li><li>  Warna menunjukkan emisi (semakin merah, semakin banyak emisi). </li><li>  Nah, formulir dibedakan oleh mode perbandingan: dengan filter pengguna di set pelatihan atau tanpa filter. </li></ul><br><p><img src="https://habrastorage.org/webt/op/hb/6g/ophb6gi3wa_uvsld9rre6ujzquu.png"></p><br><p>  Jadi, kita bisa menarik kesimpulan berikut: </p><br><ul><li>  Beberapa tanda memerlukan filter emisi - kami akan membatasi nilai maksimum untuk persentil ke-90. </li><li>  Beberapa tanda menunjukkan distribusi yang dekat dengan eksponensial - kami akan mengambil logaritma. </li><li>  Beberapa fitur tidak disajikan dalam tes - kami akan mengecualikan mereka dari pelatihan. </li></ul><br><h1 id="korrelyacionnyy-analiz">  Analisis korelasi </h1><br><p>  Setelah mendapatkan gagasan umum tentang bagaimana atribut didistribusikan dan bagaimana mereka menghubungkan antara pelatihan dan set tes, mari kita coba menganalisis korelasi.  Untuk melakukan ini, konfigurasikan ekstraktor fitur berdasarkan pengamatan sebelumnya: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//             val expressions = filteredTrain.schema.fieldNames //          .filterNot(x =&gt; x == "date" || x == "audit_experiment" || idsColumns(x) || x.contains("vd_")) .map(x =&gt; if(skewedFeautres(x)) { //      s"log($x) AS $x" } else { //     cappedFeatures.get(x).map(capping =&gt; s"IF($x &lt; $capping, $x, $capping) AS $x").getOrElse(x) }) val rawFeaturesExtractor = new Pipeline().setStages(Array( new SQLTransformer().setStatement(s"SELECT ${expressions.mkString(", ")} FROM __THIS__"), new NullToDefaultReplacer(), new AutoAssembler().setOutputCol("features") )) //       val raw = rawFeaturesExtractor.fit(filteredTrain).transform( filteredTrain.where(isTestSimilar($"instanceId_userId")))</span></span></code> </pre> <br><p>  Dari mesin-mesin baru dalam pipa ini, utilitas SQLTransformer menarik perhatian, yang memungkinkan transformasi SQL sewenang-wenang dari tabel input. </p><br><p>  Ketika menganalisis korelasi, penting untuk menyaring kebisingan yang diciptakan oleh korelasi alami fitur satu-panas.  Untuk melakukan ini, saya ingin memahami elemen vektor apa yang sesuai dengan kolom sumber mana.  Tugas ini di Spark diselesaikan menggunakan metadata kolom (disimpan dengan data) dan grup atribut.  Blok kode berikut digunakan untuk memfilter pasangan nama atribut yang berasal dari kolom yang sama dari tipe String: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> attributes = <span class="hljs-type"><span class="hljs-type">AttributeGroup</span></span>.fromStructField(raw.schema(<span class="hljs-string"><span class="hljs-string">"features"</span></span>)).attributes.get <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> originMap = filteredTrain .schema.filter(_.dataType == <span class="hljs-type"><span class="hljs-type">StringType</span></span>) .flatMap(x =&gt; attributes.map(_.name.get).filter(_.startsWith(x.name + <span class="hljs-string"><span class="hljs-string">"_"</span></span>)).map(_ -&gt; x.name)) .toMap <span class="hljs-comment"><span class="hljs-comment">//   ,          val isNonTrivialCorrelation = sqlContext.udf.register("isNonTrivialCorrelation", (x: String, y : String) =&gt; //    Scala-quiz   Option originMap.get(x).map(_ != originMap.getOrElse(y, "")).getOrElse(true))</span></span></code> </pre> <br><p>  Memiliki set data dengan kolom vektor, menghitung korelasi silang menggunakan Spark cukup sederhana, tetapi hasilnya adalah sebuah matriks, untuk penyebaran yang Anda harus bermain sedikit ke dalam satu set pasangan: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pearsonCorrelation = <span class="hljs-comment"><span class="hljs-comment">//    Pearson  Spearman Correlation.corr(raw, "features", "pearson").rdd.flatMap( //           _.getAs[Matrix](0).rowIter.zipWithIndex.flatMap(x =&gt; { //   ,   (  , //  ) val name = attributes(x._2).name.get //    ,     x._1.toArray.zip(attributes).map(y =&gt; (name, y._2.name.get, y._1)) } //     DataFrame )).toDF("feature1", "feature2", "corr") .na.drop //   .where(isNonTrivialCorrelation($"feature1", $"feature2")) //    . pearsonCorrelation.coalesce(1).write.mode("overwrite") .parquet("sna2019/pearsonCorrelation")</span></span></code> </pre> <br><p>  Dan, tentu saja, visualisasi: kita akan membutuhkan lagi bantuan Vegas untuk menggambar peta panas: </p><br><pre> <code class="scala hljs">vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>(<span class="hljs-string"><span class="hljs-string">"Pearson correlation heatmap"</span></span>) .withDataFrame(pearsonCorrelation .withColumn(<span class="hljs-string"><span class="hljs-string">"isPositive"</span></span>, $<span class="hljs-string"><span class="hljs-string">"corr"</span></span> &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) .withColumn(<span class="hljs-string"><span class="hljs-string">"abs_corr"</span></span>, functions.abs($<span class="hljs-string"><span class="hljs-string">"corr"</span></span>)) .where(<span class="hljs-string"><span class="hljs-string">"feature1 &lt; feature2 AND abs_corr &gt; 0.05"</span></span>) .orderBy(<span class="hljs-string"><span class="hljs-string">"feature1"</span></span>, <span class="hljs-string"><span class="hljs-string">"feature2"</span></span>)) .encodeX(<span class="hljs-string"><span class="hljs-string">"feature1"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .encodeY(<span class="hljs-string"><span class="hljs-string">"feature2"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .encodeColor(<span class="hljs-string"><span class="hljs-string">"abs_corr"</span></span>, <span class="hljs-type"><span class="hljs-type">Quant</span></span>, scale=<span class="hljs-type"><span class="hljs-type">Scale</span></span>(rangeNominals=<span class="hljs-type"><span class="hljs-type">List</span></span>(<span class="hljs-string"><span class="hljs-string">"#FFFFFF"</span></span>, <span class="hljs-string"><span class="hljs-string">"#FF0000"</span></span>))) .encodeShape(<span class="hljs-string"><span class="hljs-string">"isPositive"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .mark(vegas.<span class="hljs-type"><span class="hljs-type">Point</span></span>) .show</code> </pre> <br><p>  Hasilnya lebih baik untuk melihat di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Zepl-e</a> .  Untuk pemahaman umum: </p><br><p><img src="https://habrastorage.org/webt/nm/d9/bm/nmd9bmrion4goaov9_vgx5vbjoy.png"></p><br><p>  Peta panas menunjukkan bahwa beberapa korelasi jelas ada di sana.  Mari kita coba untuk memilih blok-blok fitur yang paling berkorelasi kuat, untuk ini kita menggunakan perpustakaan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">GraphX</a> : kita mengubah matriks korelasi menjadi grafik, menyaring tepi berdasarkan berat, setelah itu kita menemukan komponen yang terhubung dan hanya menyisakan komponen yang tidak mengalami degenerasi (dari lebih dari satu elemen).  Prosedur semacam itu pada dasarnya mirip dengan penerapan algoritma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DBSCAN</a> dan adalah sebagai berikut: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//   (GrpahX   ID) val featureIndexMap = spearmanCorrelation.select("feature1").distinct.rdd.map( _.getString(0)).collect.zipWithIndex.toMap val featureIndex = sqlContext.udf.register("featureIndex", (x: String) =&gt; featureIndexMap(x)) //    val vertices = sc.parallelize(featureIndexMap.map(x =&gt; x._2.toLong -&gt; x._1).toSeq, 1) //    val edges = spearmanCorrelation.select(featureIndex($"feature1"), featureIndex($"feature2"), $"corr") //     .where("ABS(corr) &gt; 0.7") .rdd.map(r =&gt; Edge(r.getInt(0), r.getInt(1), r.getDouble(2))) //       val components = Graph(vertices, edges).connectedComponents() val reversedMap = featureIndexMap.map(_.swap) //    ,    ,   //   val clusters = components .vertices.map(x =&gt; reversedMap(x._2.toInt) -&gt; reversedMap(x._1.toInt)) .groupByKey().map(x =&gt; x._2.toSeq) .filter(_.size &gt; 1) .sortBy(-_.size) .collect</span></span></code> </pre> <br><p>  Hasilnya disajikan dalam bentuk tabel: </p><br><p><img src="https://habrastorage.org/webt/4b/t2/bl/4bt2blcjzmxbzucnm7zynpvpj-q.png"></p><br><p>  Berdasarkan hasil pengelompokan, kita dapat menyimpulkan bahwa kelompok yang paling berkorelasi terbentuk di sekitar tanda-tanda yang terkait dengan keanggotaan pengguna dalam grup (membership_status_A), serta di sekitar jenis objek (instanceId_objectType).  Untuk pemodelan terbaik dari interaksi tanda, masuk akal untuk menerapkan segmentasi model - untuk melatih model yang berbeda untuk berbagai jenis objek, secara terpisah untuk kelompok di mana pengguna berada dan tidak. </p><br><h1 id="mashinnoe-obuchenie">  Pembelajaran mesin </h1><br><p>  Kami mendekati hal yang paling menarik - pembelajaran mesin.  Pipa untuk melatih model paling sederhana (regresi logistik) menggunakan ekstensi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SparkML</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PravdaML</a> adalah sebagai berikut: </p><br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement( <span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-type"><span class="hljs-type">Scaler</span></span>.scale(<span class="hljs-type"><span class="hljs-type">Interceptor</span></span>.intercept(<span class="hljs-type"><span class="hljs-type">UnwrappedStage</span></span>.repartition( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LogisticRegressionLBFSG</span></span>(), numPartitions = <span class="hljs-number"><span class="hljs-number">127</span></span>)))</code> </pre> <br><p>  Di sini kita melihat tidak hanya banyak elemen yang akrab, tetapi juga beberapa yang baru: </p><br><ul><li>  LogisticRegressionLBFSG adalah penduga dengan pelatihan regresi logistik terdistribusi. </li><li>  Untuk mencapai kinerja maksimum dari algoritma ML terdistribusi.  data harus didistribusikan secara optimal di seluruh partisi.  Utilitas UnwrappedStage.repartition akan membantu dalam hal ini, menambahkan operasi partisi ulang ke pipa sehingga hanya digunakan pada tahap pelatihan (setelah semua, ketika membangun prakiraan, tidak diperlukan lagi). </li><li>  Sehingga model linier dapat memberikan hasil yang baik.  data harus diskalakan, di mana utilitas Scaler.scale bertanggung jawab.  Namun, kehadiran dua transformasi linear berturut-turut (penskalaan dan perkalian dengan bobot regresi) menyebabkan biaya yang tidak perlu, dan diharapkan untuk menutup operasi ini.  Saat menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PravdaML,</a> hasilnya akan menjadi model bersih dengan satu transformasi :). </li><li>  Nah, tentu saja, untuk model seperti itu, kita memerlukan anggota gratis, yang kita tambahkan menggunakan operasi Interceptor.intercept. </li></ul><br><p>  Pipa yang dihasilkan, diterapkan pada semua data, memberikan AUC 0,6889 per pengguna (kode validasi tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Zepl</a> ).  Sekarang tetap menerapkan semua penelitian kami: memfilter data, mengubah fitur, dan model segmen.  Pipa akhir akan terlihat seperti ini: </p><br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">s"SELECT instanceId_userId, instanceId_objectId, </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${expressions.mkString(", ")}</span></span></span><span class="hljs-string"> FROM __THIS__"</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label, concat(IF(membership_status = 'A', 'OwnGroup_', 'NonUser_'), instanceId_objectType) AS type FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>, <span class="hljs-string"><span class="hljs-string">"type"</span></span>,<span class="hljs-string"><span class="hljs-string">"instanceId_objectType"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-type"><span class="hljs-type">CombinedModel</span></span>.perType( <span class="hljs-type"><span class="hljs-type">Scaler</span></span>.scale(<span class="hljs-type"><span class="hljs-type">Interceptor</span></span>.intercept(<span class="hljs-type"><span class="hljs-type">UnwrappedStage</span></span>.repartition( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LogisticRegressionLBFSG</span></span>(), numPartitions = <span class="hljs-number"><span class="hljs-number">127</span></span>))), numThreads = <span class="hljs-number"><span class="hljs-number">6</span></span>) ))</code> </pre> <br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PravdaML</a> —    CombinedModel.perType.       ,     numThreads = 6.             . </p><br><p> ,   ,  per-user AUC 0.7004.    ?  ,   " "    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">XGBoost</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">XGBoostRegressor</span></span>() .setNumRounds(<span class="hljs-number"><span class="hljs-number">100</span></span>) .setMaxDepth(<span class="hljs-number"><span class="hljs-number">15</span></span>) .setObjective(<span class="hljs-string"><span class="hljs-string">"reg:logistic"</span></span>) .setNumWorkers(<span class="hljs-number"><span class="hljs-number">17</span></span>) .setNthread(<span class="hljs-number"><span class="hljs-number">4</span></span>) .setTrackerConf(<span class="hljs-number"><span class="hljs-number">600000</span></span>L, <span class="hljs-string"><span class="hljs-string">"scala"</span></span>) ))</code> </pre> <br><p> ,     — XGBoost  Spark !         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DLMC</a> ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PravdaML</a> ,       (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> ).  XGboost " "   10     per-user AUC 0.6981. </p><br><h1 id="analiz-rezultatov">   </h1><br><p> ,     ,  ,       .    SparkML     ,      .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PravdaML</a>  :      Parquet            Spark: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//     val perTypeWeights = sqlContext.read.parquet("sna2019/perType/stages/*/weights") //     20    ( //  ) val topFeatures = new TopKTransformer[Double]() .setGroupByColumns("type") .setColumnToOrderGroupsBy("abs_weight") .setTopK(20) .transform(perTypeWeights.withColumn("abs_weight", functions.abs($"unscaled_weight"))) .orderBy("type", "unscaled_weight")</span></span></code> </pre> <br><p>     Parquet,        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PravdaML</a> — TopKTransformer,           . </p><br><p>      Vegas (   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Zepl</a> ): </p><br><p><img src="https://habrastorage.org/webt/q7/_n/fn/q7_nfnto-hw-9nbdgjf3chhm0oc.png"></p><br><p> ,    -   .      XGBoost? </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> significance = sqlContext.read.parquet( <span class="hljs-string"><span class="hljs-string">"sna2019/xgBoost15_100_raw/stages/*/featuresSignificance"</span></span> vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>() .withDataFrame(significance.na.drop.orderBy($<span class="hljs-string"><span class="hljs-string">"significance"</span></span>.desc).limit(<span class="hljs-number"><span class="hljs-number">40</span></span>)) .encodeX(<span class="hljs-string"><span class="hljs-string">"name"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>, sortField = <span class="hljs-type"><span class="hljs-type">Sort</span></span>(<span class="hljs-string"><span class="hljs-string">"significance"</span></span>, <span class="hljs-type"><span class="hljs-type">AggOps</span></span>.<span class="hljs-type"><span class="hljs-type">Mean</span></span>)) .encodeY(<span class="hljs-string"><span class="hljs-string">"significance"</span></span>, <span class="hljs-type"><span class="hljs-type">Quant</span></span>) .mark(vegas.<span class="hljs-type"><span class="hljs-type">Bar</span></span>) .show</code> </pre> <br><p><img src="https://habrastorage.org/webt/i2/oy/zb/i2oyzbrjlo15x7u1uwadmeswocq.png"></p><br><p>  ,   ,   XGBoost,         ,    .           <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> .   ,  XGBoost     ,    ,   . </p><br><h1 id="vyvody">  Kesimpulan </h1><br><p>  ,       :).     : </p><br><ol><li>    ,     Scala  Spark    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">  </a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> . </li><li>    Scala  Spark        Python:    ETL  ML,    ,      ,     . </li><li>  ,   ,   ,    (,  )    ,     ,      . </li><li> ,     ,       .          ,      ,     , -, . </li></ol><br><p> ,       ,    ,        ,    -.        , ,  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">   Scala</a> "  Newprolab. </p><br><p> ,  ,      —   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SNA Hackathon 2019</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id442688/">https://habr.com/ru/post/id442688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id442678/index.html">Proyek terbesar dalam litografi stereo: kerangka raksasa dicetak pada printer 3D</a></li>
<li><a href="../id442680/index.html">Teknologi penggantian sensorik akan memungkinkan Anda untuk melihat dunia dengan bantuan suara: bagaimana neuroplastisitas otak manusia bekerja</a></li>
<li><a href="../id442682/index.html">Apa yang diketik dan cara merakit proyek C ++</a></li>
<li><a href="../id442684/index.html">Performa situs seimbang. Bagian 3: Konten</a></li>
<li><a href="../id442686/index.html">Tutorial DataPower</a></li>
<li><a href="../id442690/index.html">Misi Lunar "Bereshit" - selfie di latar belakang Bumi</a></li>
<li><a href="../id442692/index.html">Blockchain tanpa perantara: cara kami mengirim sekuritas ke registri terdistribusi</a></li>
<li><a href="../id442694/index.html">Salah satu raksasa streaming diluncurkan di India dan menarik sejuta pengguna dalam seminggu</a></li>
<li><a href="../id442696/index.html">S for Security: Internet Security of Things dan laporan di InoThings ++ 2019</a></li>
<li><a href="../id442698/index.html">Aplikasi metro Moskow untuk Windows Store</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>