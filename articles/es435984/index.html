<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>„ÄΩÔ∏è ü§¥üèæ üßìüèΩ Redes neuronales y la filosof√≠a del lenguaje. üß£ üëë ü§ûüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="¬øPor qu√© las teor√≠as de Wittgenstein siguen siendo la base de toda la PNL moderna? 

 La representaci√≥n vectorial de las palabras es quiz√°s una de las...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neuronales y la filosof√≠a del lenguaje.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435984/"> <font color="gray">¬øPor qu√© las teor√≠as de Wittgenstein siguen siendo la base de toda la PNL moderna?</font> <br><br>  La representaci√≥n vectorial de las palabras es quiz√°s una de las ideas m√°s bellas y rom√°nticas en la historia de la inteligencia artificial.  La filosof√≠a del lenguaje es una rama de la filosof√≠a que explora la relaci√≥n entre el lenguaje y la realidad y c√≥mo hacer que el discurso sea significativo y comprensible.  Una representaci√≥n vectorial de palabras es un m√©todo muy espec√≠fico en el procesamiento moderno del lenguaje natural (PNL).  En cierto sentido, es una evidencia emp√≠rica de las teor√≠as de Ludwig Wittgenstein, uno de los fil√≥sofos m√°s relevantes del siglo pasado.  Para Wittgenstein, el uso de palabras es un movimiento en un <i>juego de</i> lenguaje social jugado por miembros de la comunidad que se entienden entre s√≠.  El significado de una palabra depende solo de su utilidad en un contexto; no corresponde uno a uno con un objeto del mundo real. <br><br><blockquote>  Para una gran clase de casos en los que usamos la palabra "significado", se puede definir como el <b>significado de la palabra es su uso en el lenguaje</b> . </blockquote><a name="habracut"></a><br>  Por supuesto, entender el significado exacto de una palabra es muy dif√≠cil.  Hay muchos aspectos a considerar: <br><br><ul><li>  a qu√© objeto puede referirse la palabra; </li><li>  qu√© parte del discurso es; </li><li>  si es una expresi√≥n idiom√°tica; </li><li>  todos los matices de significados; </li><li>  Y as√≠ sucesivamente. </li></ul><br>  Todos estos aspectos, al final, se reducen a una cosa: saber c√≥mo usar la palabra. <br><br>  El concepto de <i>significado</i> y por qu√© un conjunto ordenado de caracteres tiene una connotaci√≥n definitiva en el lenguaje no es solo una cuesti√≥n filos√≥fica, sino tambi√©n probablemente el mayor problema que tienen que enfrentar los especialistas en IA que trabajan con PNL.  Es bastante obvio para una persona de habla rusa que un "perro" es un "animal", y se parece m√°s a un "gato" que a un "delf√≠n", pero esta tarea est√° lejos de ser simple para una soluci√≥n sistem√°tica. <br><br>  Habiendo corregido ligeramente las teor√≠as de Wittgenstein, podemos decir que los perros se parecen a los gatos porque a menudo aparecen en los mismos contextos: probablemente puedas encontrar perros y gatos asociados con las palabras "hogar" y "jard√≠n" que con las palabras "mar" y el "oc√©ano".  Es esta intuici√≥n la que subyace en <b>Word2Vec</b> , una de las implementaciones m√°s famosas y exitosas de la representaci√≥n vectorial de palabras.  Hoy en d√≠a, las m√°quinas distan mucho de <i>comprender</i> realmente textos largos y pasajes, pero la representaci√≥n vectorial de las palabras es, sin duda, el √∫nico m√©todo que nos ha permitido dar el mayor paso en esta direcci√≥n durante la √∫ltima d√©cada. <br><br><h1>  De BoW a Word2Vec </h1><br>  En muchos problemas inform√°ticos, el primer problema es presentar los datos en forma num√©rica;  Las palabras y oraciones son probablemente las m√°s dif√≠ciles de imaginar de esta forma.  En nuestra configuraci√≥n, las palabras <b>D</b> se seleccionan del diccionario, y a cada palabra se le puede asignar un √≠ndice num√©rico <b>i</b> . <br><br>  Durante muchas d√©cadas, se ha adoptado un enfoque cl√°sico para representar cada palabra como un vector num√©rico D-dimensional de todos los ceros, excepto uno en la posici√≥n i.  Como ejemplo, considere un diccionario de tres palabras: "perro", "gato" y "delf√≠n" (D = 3).  Cada palabra puede representarse como un vector tridimensional: "perro" corresponde a [1,0,0], "gato" a [0,1,0] y "delf√≠n", obviamente, [0,0,1].  El documento se puede representar como un vector D-dimensional, donde cada elemento cuenta las apariciones de la i-√©sima palabra en el documento.  Este modelo se llama Bag-of-words (BoW), y se ha utilizado durante d√©cadas. <br><br>  A pesar de su √©xito en los a√±os 90, BoW carec√≠a de la √∫nica funci√≥n interesante de las palabras: su significado.  Sabemos que dos palabras muy diferentes pueden tener significados similares, incluso si son completamente diferentes desde el punto de vista de la ortograf√≠a.  "Gato" y "perro" son animales dom√©sticos, "rey" y "reina" est√°n cerca uno del otro, "manzana" y "cigarrillo" no tienen ninguna relaci√≥n.  <i>Sabemos</i> esto, pero en el modelo BoW, todas estas palabras est√°n a la misma distancia en el espacio vectorial: 1. <br><br>  El mismo problema se aplica a los documentos: al usar BoW, podemos concluir que los documentos son similares solo si contienen la misma palabra varias veces.  Y aqu√≠ viene Word2Vec, que introduce en los t√©rminos del aprendizaje autom√°tico muchas preguntas filos√≥ficas que Wittgenstein discuti√≥ en sus <i>Estudios filos√≥ficos</i> hace 60 a√±os. <br><br>  En un diccionario de tama√±o D, donde la palabra se identifica por su √≠ndice, el objetivo es calcular la representaci√≥n vectorial N-dimensional de cada palabra para N &lt;&lt; D.  Idealmente, queremos que sea un vector denso que represente algunos aspectos sem√°nticamente espec√≠ficos del significado.  Por ejemplo, idealmente queremos que "perro" y "gato" tengan representaciones similares, y "manzana" y "cigarrillo" est√°n muy distantes en el espacio vectorial. <br><br>  Queremos realizar algunas operaciones algebraicas b√°sicas en vectores, como <code>+‚àí=</code> .  Quiero que la distancia entre los vectores "actor" y "actriz" coincida sustancialmente con la distancia entre el "pr√≠ncipe" y la "princesa".  Aunque estos resultados son bastante ut√≥picos, los experimentos muestran que los vectores Word2Vec exhiben propiedades muy cercanas a estos. <br><br>  Word2Vec no aprende puntos de vista directamente de esto, pero los recibe como un subproducto de la clasificaci√≥n sin un maestro.  El conjunto de datos de corpus de palabras de NLP promedio consiste en un conjunto de oraciones;  cada palabra en una oraci√≥n aparece en el contexto de las palabras circundantes.  El prop√≥sito del clasificador es predecir la palabra objetivo, considerando las palabras contextuales como entrada.  Para la oraci√≥n "el perro marr√≥n juega en el jard√≠n", las palabras [marr√≥n, juegos, en el jard√≠n] se proporcionan al modelo como entrada, y ella debe predecir la palabra "perro".  Esta tarea se considera como aprender sin un maestro, ya que el corpus no necesita ser marcado con una fuente externa de verdad: si tiene un conjunto de oraciones, siempre puede crear autom√°ticamente ejemplos positivos y negativos.  Mirando al "perro marr√≥n jugando en el jard√≠n" como un ejemplo positivo, podemos crear muchos patrones negativos, como "avi√≥n marr√≥n jugando en el jard√≠n" o "uva marr√≥n jugando en el jard√≠n", reemplazando la palabra objetivo "perro" con palabras aleatorias del conjunto de datos. <br><br>  Y ahora la aplicaci√≥n de las teor√≠as de Wittgenstein es perfectamente clara: el contexto es crucial para la representaci√≥n vectorial de las palabras, ya que es importante atribuir significado a la palabra en sus teor√≠as.  Si dos palabras tienen significados similares, tendr√°n representaciones similares (una peque√±a distancia en el espacio N-dimensional) solo porque a menudo aparecen en contextos similares.  Por lo tanto, el "gato" y el "perro" eventualmente tendr√°n vectores cercanos porque a menudo aparecen en los mismos contextos: es √∫til que el modelo use representaciones vectoriales similares para ellos, porque esto es lo m√°s conveniente que puede hacer, para obtener mejores resultados en la predicci√≥n de dos palabras en funci√≥n de sus contextos. <br><br>  El art√≠culo original ofrece dos arquitecturas diferentes: CBOW y Skip-gram.  En ambos casos, las representaciones verbales se ense√±an junto con una tarea de clasificaci√≥n espec√≠fica, proporcionando las mejores representaciones vectoriales posibles de palabras que maximizan el rendimiento del modelo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2b9/e32/ee7/2b9e32ee767ad0c402e214a566d848a0.png"><br>  <i><font color="gray">Figura 1. Comparaci√≥n de arquitecturas CBOW y Skip-gram</font></i> <br><br>  <b>CBOW</b> significa Bolsa Continua de Palabras, y su tarea es adivinar una palabra con el contexto en mente como entrada.  Las entradas y salidas se representan como vectores D-dimensionales que se proyectan en un espacio N-dimensional con pesos comunes.  Solo buscamos pesos de proyecci√≥n.  De hecho, la representaci√≥n vectorial de las palabras es D √ó N matrices, donde cada fila representa una palabra del diccionario.  Todas las palabras de contexto se proyectan en una posici√≥n, y sus representaciones vectoriales se promedian;  por lo tanto, el orden de las palabras no afecta el resultado. <br><br>  <b>Skip-gram</b> hace lo mismo, pero viceversa: trata de predecir las palabras de contexto <b>C</b> , tomando la palabra objetivo como entrada.  La tarea de predecir varias palabras contextuales puede reformularse en un conjunto de problemas de clasificaci√≥n binaria independientes, y ahora el objetivo es predecir la presencia (o ausencia) de palabras contextuales. <br><br>  Como regla general, Skip-gram requiere m√°s tiempo para el entrenamiento y, a menudo, ofrece resultados ligeramente mejores, pero, como de costumbre, las diferentes aplicaciones tienen diferentes requisitos, y es dif√≠cil predecir de antemano cu√°les mostrar√°n el mejor resultado.  A pesar de la simplicidad del concepto, aprender este tipo de arquitectura es una verdadera pesadilla debido a la cantidad de datos y la potencia de procesamiento necesaria para optimizar los pesos.  Afortunadamente, en Internet puede encontrar algunas representaciones vectoriales de palabras previamente entrenadas, y puede estudiar el espacio vectorial, el m√°s interesante, con solo unas pocas l√≠neas de c√≥digo Python. <br><br><h1>  Posibles mejoras: GloVe y fastText </h1><br>  Sobre el cl√°sico Word2Vec en los √∫ltimos a√±os, se han propuesto muchas posibles mejoras.  Los dos m√°s interesantes y com√∫nmente utilizados son GloVe (Universidad de Stanford) y fastText (desarrollado por Facebook).  Est√°n tratando de identificar y superar las limitaciones del algoritmo original. <br><br>  En un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo cient√≠fico original</a> , los autores de GloVe enfatizan que el entrenamiento modelo en un contexto local separado hace un uso deficiente de las estad√≠sticas globales del corpus.  El primer paso para superar esta limitaci√≥n es crear una matriz global <b>X</b> , donde cada elemento <b>i, j</b> cuenta el n√∫mero de referencias a la palabra <b>j</b> en el contexto de la palabra <b>i</b> .  La segunda idea importante de este documento es la comprensi√≥n de que solo las probabilidades por s√≠ solas no son suficientes para una predicci√≥n confiable de los valores, y tambi√©n se necesita una matriz de coincidencia, de la que se pueden extraer directamente ciertos aspectos de los valores. <br><br><blockquote>  Considere dos palabras i y j que son de particular inter√©s.  Para concreci√≥n, supongamos que estamos interesados ‚Äã‚Äãen el concepto de un estado termodin√°mico, para el cual podemos tomar <code>i = </code> y <code>j = </code> .  La relaci√≥n de estas palabras puede investigarse estudiando la raz√≥n de sus probabilidades de ocurrencia conjunta usando diferentes palabras sonoras, k.  Para las palabras k relacionadas con hielo pero no con vapor, digamos <code>k = </code> [s√≥lido, estado de la materia], esperamos que la relaci√≥n Pik / Pjk sea mayor.  De manera similar, para las palabras k asociadas con vapor, pero no con hielo, digamos <code>k = </code> , la proporci√≥n debe ser peque√±a.  Para palabras como "agua" o "moda", que est√°n igualmente relacionadas con el hielo y el vapor, o que no tienen relaci√≥n con ellas, esta relaci√≥n debe estar cerca de la unidad. </blockquote><br>  Esta raz√≥n de probabilidades se convierte en el punto de partida para estudiar la representaci√≥n vectorial de las palabras.  Queremos poder calcular vectores que, en combinaci√≥n con una funci√≥n espec√≠fica <b>F,</b> mantengan esta relaci√≥n constante en el espacio de representaci√≥n vectorial. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4dc/d5d/137/4dcd5d13763ca26ad997565ec2e6e513.jpg"></div><br>  <i><font color="gray">Figura 2. La f√≥rmula m√°s com√∫n para la representaci√≥n vectorial de palabras en el modelo GloVe</font></i> <br><br>  La funci√≥n F y la dependencia de la palabra k pueden simplificarse reemplazando los exponenciales y las compensaciones fijas, lo que da la funci√≥n de minimizar errores por el m√©todo <b>J de</b> m√≠nimos cuadrados: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e48/049/738/e48049738a71657b998f5630dac792c4.jpg"></div><br>  <i><font color="gray">Figura 3. La funci√≥n final de calcular la representaci√≥n vectorial de palabras en el modelo GloVe</font></i> <br><br>  La funci√≥n <b>f</b> es una funci√≥n de conteo que intenta no cargar coincidencias muy frecuentes y raras, mientras que <b>bi</b> y <b>bj</b> son compensaciones para restaurar la simetr√≠a de la funci√≥n.  En los √∫ltimos p√°rrafos del art√≠culo se muestra que el entrenamiento de este modelo al final no es muy diferente del entrenamiento del modelo cl√°sico Skip-gram, aunque en pruebas emp√≠ricas GloVe es superior a ambas implementaciones de Word2Vec. <br><br>  Por otro lado, <b>fastText</b> corrige un inconveniente completamente diferente de Word2Vec: si el entrenamiento del modelo comienza con la codificaci√≥n directa de un vector D-dimensional, entonces se ignora la estructura interna de las palabras.  En lugar de codificar directamente la codificaci√≥n de palabras que aprenden representaciones verbales, fastText ofrece estudiar N-gramos de caracteres y representar palabras como la suma de vectores N-gram.  Por ejemplo, con N = 3, la palabra "flor" se codifica como 6 diferentes 3 gramos [&lt;fl, flo, low, Debt, wer, er&gt;] m√°s una secuencia especial &lt;flower&gt;.  Observe c√≥mo se usan los corchetes angulares para indicar el principio y el final de una palabra.  Por lo tanto, la palabra est√° representada por su √≠ndice en el diccionario de palabras y el conjunto de N-gramos que contiene, asignados a enteros utilizando la funci√≥n hash.  Esta mejora simple le permite dividir representaciones de N-gram entre palabras y calcular representaciones vectoriales de palabras que no estaban en el caso de aprendizaje. <br><br><h1>  Experimentos y posibles aplicaciones. </h1><br>  Como ya dijimos, para <b>usar</b> estas representaciones vectoriales solo necesita unas pocas l√≠neas de c√≥digo Python.  <a href="">Realic√©</a> varios experimentos con el <a href="">modelo GloVe de 50 dimensiones</a> , entrenado en 6 mil millones de palabras de oraciones de Wikipedia, as√≠ como con el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">modelo de texto r√°pido de 300 dimensiones, entrenado en Common Crawl</a> (que dio 600 mil millones de tokens).  Esta secci√≥n proporciona enlaces a los resultados de ambos experimentos solo para probar los conceptos y dar una comprensi√≥n general del tema. <br><br>  En primer lugar, quer√≠a verificar algunas similitudes b√°sicas de palabras, la caracter√≠stica m√°s simple pero importante de su representaci√≥n vectorial.  Como era de esperar, las palabras m√°s similares con la palabra "perro" fueron "gato" (0.92), "perros" (0.85), "caballo" (0.79), "cachorro" (0.78) y "mascota" (0.77).  Tenga en cuenta que la forma plural tiene casi el mismo significado que el singular.  Nuevamente, es bastante trivial para nosotros decir eso, pero para un autom√≥vil no es un hecho.  Ahora comida: las palabras m√°s similares para "pizza" son "sandwich" (0.87), "sandwiches" (0.86), "snack" (0.81), "productos horneados" (0.79), "papas fritas" (0.79) y "hamburguesas" ( 0,78).  Tiene sentido, los resultados son satisfactorios y el modelo se comporta bastante bien. <br><br>  El siguiente paso es realizar algunos c√°lculos b√°sicos en el espacio vectorial y verificar qu√© tan correctamente el modelo ha adquirido algunas propiedades importantes.  De hecho, como resultado del c√°lculo de los vectores <code>+-</code> , el resultado es "actriz" (0,94), y como resultado del c√°lculo del <code>+-</code> , se obtiene la palabra "rey" (0,86).  En t√©rminos generales, si el valor es <code>a:b=c:d</code> , entonces la palabra <b>d</b> debe obtenerse como <code>d=b-a+c</code> .  Pasando al siguiente nivel, es imposible imaginar c√≥mo estas operaciones vectoriales incluso describen aspectos geogr√°ficos: sabemos que Roma es la capital de Italia, ya que Berl√≠n es la capital de Alemania, de hecho <code>+-= (0.88)</code> y <code>+-= (0.83)</code> . <br><br>  Y ahora para la parte divertida.  Siguiendo la misma idea, intentaremos sumar y restar conceptos.  Por ejemplo, ¬øCu√°l es el equivalente estadounidense de pizza para los italianos?  <code>+-= (0.60)</code> , luego <code> (0.59)</code> .  Desde que me mud√© a Holanda, siempre digo que este pa√≠s es una mezcla de tres cosas: un poco de capitalismo estadounidense, fr√≠o sueco y calidad de vida, y, finalmente, una pizca de <i>abundancia</i> napolitana.  Al cambiar ligeramente el teorema original, eliminando un poco de precisi√≥n suiza, obtenemos Holanda (0,68) como resultado de <code>++-</code> : bastante impresionante, para ser sincero. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fd8/f76/e41/fd8f76e41d78598394d1613652f44f26.png"><br>  <i><font color="gray">Figura 4. Para todos los lectores holandeses: tome esto como un cumplido, ¬øde acuerdo?</font></i> <br><br>  Se pueden encontrar buenos recursos pr√°cticos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> para usar estas representaciones vectoriales pre-entrenadas.  <b>Gensim</b> es una biblioteca Python simple y completa con algunas funciones algebraicas y de similitud listas para usar.  Estas representaciones vectoriales pre-entrenadas se pueden usar de varias maneras (y √∫tiles), por ejemplo, para mejorar el rendimiento de los analizadores del estado de √°nimo o los modelos de lenguaje.  Cualquiera sea la tarea, el uso de vectores N-dimensionales mejorar√° significativamente la eficiencia del modelo en comparaci√≥n con la codificaci√≥n directa.  Por supuesto, la capacitaci√≥n en representaciones vectoriales en un √°rea espec√≠fica mejorar√° a√∫n m√°s el resultado, pero esto puede requerir, quiz√°s, un esfuerzo y tiempo excesivos. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es435984/">https://habr.com/ru/post/es435984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es435970/index.html">Gu√≠a para principiantes sobre desarrollo de servidores web con Node.js</a></li>
<li><a href="../es435972/index.html">Introduciendo programaci√≥n reactiva en Spring</a></li>
<li><a href="../es435974/index.html">Three.js - hacer controles para espacio o planetario</a></li>
<li><a href="../es435976/index.html">WebAssembly en producci√≥n y el "campo minado" de Smart TV: una entrevista con Andrei Nagih</a></li>
<li><a href="../es435978/index.html">Soluciones de protecci√≥n biom√©trica</a></li>
<li><a href="../es435986/index.html">Windows reservar√° 7 GB para actualizaciones del sistema para evitar quedarse sin espacio en el disco duro</a></li>
<li><a href="../es435988/index.html">Una introducci√≥n a las anotaciones de tipo Python. Continuaci√≥n</a></li>
<li><a href="../es435990/index.html">¬øC√≥mo hacer un cambio?</a></li>
<li><a href="../es435992/index.html">Los jugadores Fallout 76, que ser√°n atrapados en una ubicaci√≥n secreta de desarrolladores, ser√°n prohibidos</a></li>
<li><a href="../es435994/index.html">¬øEs Karma, beb√©, o por qu√© el ataque a las redes inal√°mbricas que se supon√≠a que se hundir√≠a en el olvido todav√≠a est√° vivo?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>