<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚èπÔ∏è ü§öüèª üßúüèª Modifications r√©centes de la pile Linux IO du point de vue DBA ü§™ üßì üë©üèæ‚Äçüåæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les principaux probl√®mes li√©s √† l'utilisation de la base de donn√©es sont li√©s aux fonctionnalit√©s de l'appareil du syst√®me d'exploitation sur lequel l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Modifications r√©centes de la pile Linux IO du point de vue DBA</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/459444/">  Les principaux probl√®mes li√©s √† l'utilisation de la base de donn√©es sont li√©s aux fonctionnalit√©s de l'appareil du syst√®me d'exploitation sur lequel la base de donn√©es fonctionne.  Linux est d√©sormais le principal syst√®me d'exploitation des bases de donn√©es.  Solaris, Microsoft et m√™me HPUX sont toujours utilis√©s dans l'entreprise, mais ils ne prendront jamais la premi√®re place, m√™me lorsqu'ils sont combin√©s.  Linux gagne du terrain en toute confiance car il existe de plus en plus de bases de donn√©es open source.  Par cons√©quent, la question de l'interaction de la base de donn√©es avec le syst√®me d'exploitation concerne √©videmment les bases de donn√©es Linux.  Ceci est superpos√© au probl√®me √©ternel de la base de donn√©es - les performances d'E / S.  Il est bon que ces derni√®res ann√©es, Linux ait subi une refonte majeure de la pile d'E / S et il y a de l'espoir pour l'illumination. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0o7uNUOS-Ho" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Ilya Kosmodemyansky ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">hydrobiont</a> ) travaille pour Data Egret, une entreprise qui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">consulte</a> et prend en charge PostgreSQL, et en sait beaucoup sur l'interaction entre le syst√®me d'exploitation et les bases de donn√©es.  Dans un rapport sur HighLoad ++, Ilya a parl√© de l'interaction des E / S et des bases de donn√©es en utilisant l'exemple de PostgreSQL, mais a √©galement montr√© comment d'autres bases de donn√©es fonctionnent avec les E / S.  J'ai regard√© la pile Linux IO, quelles nouvelles et bonnes choses y sont apparues et pourquoi tout n'est pas comme il y a quelques ann√©es.  Pour rappel utile - une liste de contr√¥le des param√®tres PostgreSQL et Linux pour des performances maximales du sous-syst√®me d'E / S dans les nouveaux noyaux. <br><a name="habracut"></a><br>  <i>La vid√©o du rapport contient beaucoup d'anglais, dont la plupart ont √©t√© traduits dans l'article.</i> <br><br><h2>  Pourquoi parler d'IO? </h2><br>  <strong>Les E / S rapides sont la chose la plus critique pour les administrateurs de bases de donn√©es</strong> .  Tout le monde sait ce qui peut √™tre chang√© en travaillant avec le CPU, cette m√©moire peut √™tre √©tendue, mais les E / S peuvent tout g√¢cher.  En cas de probl√®me avec les disques et trop d'E / S, la base de donn√©es g√©mira.  IO deviendra un goulot d'√©tranglement. <br><br><blockquote>  Pour que tout fonctionne bien, vous devez tout configurer. </blockquote><br>  Pas seulement la base de donn√©es ou seulement le mat√©riel - c'est tout.  M√™me Oracle de haut niveau, qui est lui-m√™me un syst√®me d'exploitation √† certains endroits, n√©cessite une configuration.  Nous lisons les instructions dans le "Guide d'installation" d'Oracle: changez ces param√®tres du noyau, changez les autres - il y a beaucoup de r√©glages.  En plus du fait que dans le noyau incassable, une grande partie est d√©j√† par d√©faut c√¢bl√©e √† Oracle Linux. <br><br>  Pour PostgreSQL et MySQL, encore plus de modifications sont n√©cessaires.  En effet, ces technologies reposent sur des m√©canismes de syst√®me d'exploitation.  Un DBA qui fonctionne avec PostgreSQL, MySQL ou NoSQL moderne doit √™tre un ing√©nieur d'exploitation Linux et tordre diff√©rents √©crous du syst√®me d'exploitation. <br><br>  Tous ceux qui veulent g√©rer les param√®tres du noyau se tournent vers <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LWN</a> .  La ressource est ing√©nieuse, minimaliste, contient beaucoup d'informations utiles, mais a √©t√© <strong>√©crite par les d√©veloppeurs du noyau pour les d√©veloppeurs du noyau</strong> .  Qu'est-ce que les d√©veloppeurs du noyau √©crivent bien?  Le noyau, pas l'article, comment l'utiliser.  Par cons√©quent, je vais essayer de tout vous expliquer pour les d√©veloppeurs et de les laisser √©crire le noyau. <br><br>  Tout est compliqu√© plusieurs fois par le fait qu'au d√©part, le d√©veloppement du noyau Linux et le traitement de sa pile √©taient en retard, et ces derni√®res ann√©es, ils sont all√©s tr√®s vite.  Ni le fer ni les d√©veloppeurs avec des articles derri√®re lui ne suivent. <br><br><h2>  Base de donn√©es typique </h2><br>  Commen√ßons par les exemples pour PostgreSQL - voici les E / S tamponn√©es.  Il a une m√©moire partag√©e, qui est allou√©e dans <strong>l'espace utilisateur</strong> du point de vue du syst√®me d'exploitation, et a le m√™me cache dans le cache du <strong>noyau</strong> dans l' <strong>espace du noyau</strong> . <br><br><img src="https://habrastorage.org/webt/jd/dh/d2/jddhd25l97pocaqtqqf6ccygcxi.jpeg"><br><br>  <strong>La t√¢che principale d'une base de donn√©es moderne</strong> : <br><br><ul><li>  r√©cup√©rer des pages du disque en m√©moire; </li><li>  lorsqu'un changement se produit, marquez les pages comme sales; </li><li>  √©crire dans le journal d'√©criture anticip√©e; </li><li>  synchronisez ensuite la m√©moire afin qu'elle soit coh√©rente avec le disque. </li></ul><br>  Dans une situation PostgreSQL, il s'agit d'un aller-retour constant: depuis la m√©moire partag√©e que PostgreSQL contr√¥le dans le noyau Page Cache, puis vers le disque √† travers la pile Linux enti√®re.  Si vous utilisez une base de donn√©es sur un syst√®me de fichiers, il fonctionnera sur cet algorithme avec n'importe quel syst√®me de type UNIX et avec n'importe quelle base de donn√©es.  Les diff√©rences sont, mais insignifiantes. <br><br>  L'utilisation d'Oracle ASM sera diff√©rente - Oracle lui-m√™me interagit avec le disque.  Mais le principe est le m√™me: avec Direct IO ou avec Page Cache, mais la t√¢che est <strong>de dessiner des pages dans toute la pile d'E / S le plus rapidement possible</strong> , quelle qu'elle soit.  Et des probl√®mes peuvent survenir √† chaque √©tape. <br><br><h3>  Deux probl√®mes d'IO </h3><br>  Bien que tout soit en <strong>lecture seule</strong> , il n'y a aucun probl√®me.  Ils lisent et, s'il y a suffisamment de m√©moire, toutes les donn√©es qui doivent √™tre lues sont plac√©es dans la RAM.  Le fait que dans le cas de PostgreSQL dans <strong>Buffer Cache</strong> soit le m√™me, nous ne sommes pas tr√®s inquiets. <br><br><img src="https://habrastorage.org/webt/mh/0j/sm/mh0jsmmdrh5bosmamuaiko9l-jw.jpeg"><br><br>  <strong>Le premier probl√®me avec IO est la synchronisation du cache.</strong>  Se produit lorsque l'enregistrement est requis.  Dans ce cas, vous devrez parcourir beaucoup plus de m√©moire. <br><br><img src="https://habrastorage.org/webt/tq/mw/u7/tqmwu7fy-wlwqjx6nxec6jfcyrk.jpeg"><br><br>  En cons√©quence, vous devez configurer PostgreSQL ou MySQL de sorte que tout cela arrive sur le disque √† partir de la m√©moire partag√©e.  Dans le cas de PostgreSQL - vous devez toujours affiner la triche en arri√®re-plan des pages sales sous Linux pour tout envoyer sur le disque. <br><br>  <strong>Le deuxi√®me probl√®me courant est l'√©chec d'√©criture du journal d'√©criture anticip√©e</strong> .  Il appara√Æt lorsque la charge est si puissante que m√™me un journal enregistr√© s√©quentiellement repose sur le disque.  Dans cette situation, il doit √©galement √™tre enregistr√© rapidement. <br><br>  La situation n'est pas tr√®s diff√©rente de <strong>la synchronisation</strong> du <strong>cache</strong> .  Dans PostgreSQL, nous travaillons avec un grand nombre de tampons partag√©s, la base de donn√©es dispose de m√©canismes pour un enregistrement efficace du journal d'√©criture anticip√©e, elle est optimis√©e √† la limite.  La seule chose qui peut √™tre faite pour rendre le journal lui-m√™me plus efficace est de modifier les param√®tres Linux. <br><br><h2>  Les principaux probl√®mes de travail avec la base de donn√©es </h2><br>  <strong>Le segment de m√©moire partag√©e peut √™tre tr√®s volumineux</strong> .  J'ai commenc√© √† en parler lors de conf√©rences en 2012.  Puis j'ai dit que la m√©moire avait baiss√© de prix, m√™me il y a des serveurs avec 32 Go de RAM.  En 2019, il y en a peut-√™tre d√©j√† plus sur les ordinateurs portables, de plus en plus souvent sur les serveurs 128, 256, etc. <br><br>  <strong>Vraiment beaucoup de m√©moire</strong> .  L'enregistrement banal prend du temps et des ressources, et les <strong>technologies que nous utilisons pour cela sont conservatrices</strong> .  Les bases de donn√©es sont anciennes, elles sont d√©velopp√©es depuis longtemps, elles √©voluent lentement.  Les m√©canismes des bases de donn√©es ne correspondent pas exactement aux derni√®res technologies. <br><br>  <strong>La synchronisation des pages en m√©moire avec le disque entra√Æne d'√©normes op√©rations d'E / S.</strong>  Lorsque nous synchronisons des caches, un grand flux d'E / S survient et un autre probl√®me survient - <strong>nous ne pouvons pas tordre quelque chose et regarder l'effet.</strong>  Dans une exp√©rience scientifique, les chercheurs modifient un param√®tre - obtenir l'effet, le second - obtenir l'effet, le troisi√®me.  Nous ne r√©ussirons pas.  Nous tordons certains param√®tres dans PostgreSQL, configurons les points de contr√¥le - nous n'avons pas vu l'effet.  Ensuite, configurez √† nouveau toute la pile afin d'obtenir au moins un r√©sultat.  Twist un param√®tre ne fonctionne pas - nous sommes oblig√©s de tout configurer √† la fois. <br><br>  La plupart des IO PostgreSQL g√©n√®rent la synchronisation des pages: points de contr√¥le et autres m√©canismes de synchronisation.  Si vous avez travaill√© avec PostgreSQL, vous pourriez avoir vu des pointes de contr√¥le quand une ¬´scie¬ª appara√Æt p√©riodiquement sur les graphiques.  Auparavant, beaucoup √©taient confront√©s √† ce probl√®me, mais maintenant il existe des manuels sur la fa√ßon de le r√©soudre, il est devenu plus facile. <br><br>  Aujourd'hui, les SSD sauvent consid√©rablement la situation.  Chez PostgreSQL, quelque chose repose rarement directement sur l'enregistrement de valeur.  Tout d√©pend de la synchronisation: quand un point de contr√¥le se produit, fsync est appel√© et il y a une sorte de ¬´frapper¬ª un point de contr√¥le sur un autre.  Trop d'E / S.  Un point de contr√¥le n'est pas encore termin√©, n'a pas termin√© tous ses fsyncs, mais a d√©j√† gagn√© un autre point de contr√¥le, et il a commenc√©! <br><br>  PostgreSQL a une fonctionnalit√© unique - <strong>autovacuum</strong> .  Il s'agit d'une longue histoire de b√©quilles pour l'architecture de base de donn√©es.  Si l'autovacuum ne r√©siste pas, il est g√©n√©ralement configur√© pour fonctionner de mani√®re agressive et ne pas interf√©rer avec le reste: il y a beaucoup de travailleurs de l'autovacuum, se d√©clenchant fr√©quemment un peu, traitant rapidement les tables.  Sinon, il y aura des probl√®mes avec DDL et avec les verrous. <br><br><blockquote>  Mais lorsque l'Autovacuum est agressif, il commence √† m√¢cher les E / S. </blockquote><br>  Si l'auto-vide est superpos√© aux points de contr√¥le, la plupart du temps, les disques sont recycl√©s √† pr√®s de 100%, et c'est la source des probl√®mes. <br><br>  Curieusement, il y a un probl√®me de <strong>remplissage du cache</strong> .  Elle est g√©n√©ralement moins connue pour DBA.  Un exemple typique: la base de donn√©es a d√©marr√© et pendant un certain temps, tout ralentit tristement.  Par cons√©quent, m√™me si vous avez beaucoup de RAM, achetez de bons disques pour que la pile r√©chauffe le cache. <br><br>  Tout cela affecte s√©rieusement les performances.  Les probl√®mes ne commencent pas imm√©diatement apr√®s le red√©marrage de la base de donn√©es, mais ult√©rieurement.  Par exemple, le point de contr√¥le a r√©ussi et de nombreuses pages sont sales dans la base de donn√©es.  Ils sont copi√©s sur le disque car vous devez les synchroniser.  Ensuite, les demandes demandent une nouvelle version des pages du disque et la base de donn√©es s'affaisse.  Les graphiques montreront comment la recharge du cache apr√®s chaque point de contr√¥le contribue √† un certain pourcentage de la charge. <br><br>  La chose la plus d√©sagr√©able dans les entr√©es / sorties de la base de donn√©es est <strong>Worker IO.</strong>  Lorsque chaque travailleur que vous demandez, commence √† g√©n√©rer son E / S.  Dans Oracle, c'est plus facile avec, mais dans PostgreSQL c'est un probl√®me. <br><br>  Il existe de nombreuses raisons aux probl√®mes rencontr√©s avec <strong>Worker IO</strong> : il n'y a pas suffisamment de cache pour ¬´publier¬ª de nouvelles pages √† partir du disque.  Par exemple, il arrive que tous les tampons soient partag√©s, ils sont tous sales, les points de contr√¥le ne l'ont pas encore √©t√©.  Pour que le travailleur effectue la s√©lection la plus simple, vous devez prendre le cache quelque part.  Pour ce faire, vous devez d'abord tout enregistrer sur le disque.  Vous n'avez pas de processus de pointeur de contr√¥le sp√©cialis√©, et le travailleur d√©marre fsync pour le lib√©rer et le remplir avec quelque chose de nouveau. <br><br>  Cela pose un probl√®me encore plus important: le travailleur est une chose non sp√©cialis√©e, et l'ensemble du processus n'est pas du tout optimis√©.  Il est possible d'optimiser quelque part au niveau Linux, mais dans PostgreSQL, c'est une mesure d'urgence. <br><br><h2>  Probl√®me d'E / S principal pour DB </h2><br>  <strong>Quel probl√®me r√©solvons-nous lorsque nous mettons en place quelque chose?</strong>  Nous voulons maximiser le trajet des pages sales entre le disque et la m√©moire. <br><br>  Mais il arrive souvent que ces choses ne touchent pas directement le disque.  Un cas typique - vous voyez une tr√®s grande moyenne de charge.  Pourquoi  Parce que quelqu'un attend le disque et tous les autres processus attendent √©galement.  Il semble qu'il n'y ait pas d'utilisation explicite des disques, juste quelque chose qui bloque le disque l√†-bas, et le probl√®me vient de toute fa√ßon de l'entr√©e / sortie. <br><br><blockquote>  Les probl√®mes d'E / S de base de donn√©es ne concernent pas toujours uniquement les disques. </blockquote><br>  Tout est impliqu√© dans ce probl√®me: disques, m√©moire, CPU, IO Schedulers, syst√®mes de fichiers et param√®tres de base de donn√©es.  Passons maintenant en revue la pile, voyons quoi en faire et quelles bonnes choses ont √©t√© invent√©es sous Linux pour que tout fonctionne mieux. <br><br><h3>  Disques </h3><br>  Pendant de nombreuses ann√©es, les disques √©taient terriblement lents et personne n'√©tait impliqu√© dans la latence ou l'optimisation des √©tapes de transition.  L'optimisation de fsyncs n'avait aucun sens.  Le disque tournait, les t√™tes se d√©pla√ßaient comme un disque phonographique et fsyncs √©tait si long que les probl√®mes ne se posaient pas. <br><br><h3>  La m√©moire </h3><br>  Il est inutile de consulter les principales requ√™tes sans r√©gler la base de donn√©es.  Vous allez configurer une quantit√© suffisante de m√©moire partag√©e, etc., et vous aurez une nouvelle requ√™te sup√©rieure - vous devrez la reconfigurer.  Voici la m√™me histoire.  La pile Linux enti√®re a √©t√© cr√©√©e √† partir de ce calcul. <br><br><h3>  Bande passante et latence </h3><br>  <strong>Maximiser les performances d'E / S en maximisant le d√©bit est facile jusqu'√† un certain point.</strong>  Un processus PageWriter auxiliaire a √©t√© invent√© dans PostgreSQL qui d√©chargeait le point de contr√¥le.  Le travail est devenu parall√®le, mais il reste encore des bases pour l'ajout du parall√©lisme.  Et minimiser la latence est la t√¢che du dernier kilom√®tre, pour laquelle des super technologies sont n√©cessaires. <br><br>  Ces super technologies sont des SSD.  Lorsqu'elles sont apparues, la latence a fortement chut√©.  Mais √† toutes les autres √©tapes de la pile, des probl√®mes sont apparus: tant du c√¥t√© des fabricants de bases de donn√©es que des fabricants Linux.  Les probl√®mes doivent √™tre r√©solus. <br><br>  Le d√©veloppement de la base de donn√©es s'est concentr√© sur la maximisation du d√©bit, tout comme le d√©veloppement du noyau Linux.  De nombreuses m√©thodes pour optimiser l'√®re d'E / S des disques en rotation ne sont pas aussi bonnes pour les SSD. <br><br>  Entre les deux, nous avons √©t√© contraints de sauvegarder pour l'infrastructure Linux actuelle, mais avec de nouveaux disques.  Nous avons regard√© les tests de performances du fabricant avec un grand nombre d'IOPS diff√©rents, et la base de donn√©es ne s'est pas am√©lior√©e, car la base de donn√©es n'est pas seulement et pas tellement sur IOPS.  Il arrive souvent que nous puissions sauter 50 000 IOPS par seconde, ce qui est bien.  Mais si nous ne connaissons pas la latence, ne connaissons pas sa distribution, alors nous ne pouvons rien dire sur les performances.  √Ä un moment donn√©, la base de donn√©es commencera √† v√©rifier et la latence augmentera consid√©rablement. <br><br>  Pendant longtemps, comme maintenant, cela a √©t√© un gros probl√®me de performances sur les bases de donn√©es virtuala.  Les E / S virtuelles se caract√©risent par une latence in√©gale, ce qui, bien s√ªr, entra√Æne √©galement des probl√®mes. <br><br><h2>  Pile d'E / S.  Comme avant </h2><br><img src="https://habrastorage.org/webt/yl/3v/oz/yl3vozgbt2ltrkqo8lbey-wzdfo.jpeg"><br><br>  Il y a de l'espace utilisateur - cette m√©moire, qui est g√©r√©e par la base de donn√©es elle-m√™me.  Dans une base de donn√©es configur√©e pour que tout fonctionne comme il se doit.  Cela peut √™tre fait dans un rapport s√©par√©, et m√™me pas un.  Ensuite, tout passe in√©vitablement par Page Cache ou par l'interface Direct IO, il entre dans la <strong>couche Block Input / Output</strong> . <br><br>  Imaginez une interface de syst√®me de fichiers.  Les pages qui se trouvaient dans le Buffer Cache, comme elles √©taient √† l'origine dans la base de donn√©es, c'est-√†-dire les blocs, disparaissent √† travers elle.  La couche Block IO traite des √©l√©ments suivants.  Il existe une structure C qui d√©crit un bloc dans le noyau.  La structure prend ces blocs et en recueille des vecteurs (tableaux) de requ√™tes d'entr√©e ou de sortie.  Sous la couche BIO se trouve la couche demandeur.  Les vecteurs sont collect√©s sur cette couche et iront plus loin. <br><br>  Pendant longtemps, ces deux couches sous Linux ont √©t√© affin√©es pour un enregistrement efficace sur disques magn√©tiques.  Il √©tait impossible de se passer d'une transition.  Il existe des blocs faciles √† g√©rer √† partir de la base de donn√©es.  Il est n√©cessaire d'assembler ces blocs en vecteurs qui sont commod√©ment √©crits sur le disque afin qu'ils se trouvent quelque part √† proximit√©.  Pour que cela fonctionne efficacement, ils ont mis au point des ascenseurs ou des planificateurs IO. <br><br><h2>  Ascenseurs </h2><br>  Les ascenseurs √©taient principalement impliqu√©s dans la combinaison et le tri des vecteurs.  Tout cela pour que le pilote de bloc SD - le pilote de quasi-disque - les blocs d'enregistrement arrivent dans l'ordre qui lui convient.  Le pilote a traduit des blocs dans ses secteurs et √©crit sur le disque. <br><br>  Le probl√®me √©tait qu'il fallait faire plusieurs transitions, et √† chaque impl√©menter leur propre logique du processus optimal. <br><br><h3>  Ascenseurs: jusqu'au noyau 2.6 </h3><br>  <strong>Avant le noyau 2.6, il y avait Linus Elevator</strong> - le planificateur d'E / S le plus primitif, √©crit par vous qui devinez qui.  Pendant longtemps, il a √©t√© consid√©r√© comme absolument in√©branlable et bon, jusqu'√† ce qu'ils d√©veloppent quelque chose de nouveau. <br><br>  Linus Elevator a eu beaucoup de probl√®mes.  <strong>Il a combin√© et tri√©</strong> <strong>selon la fa√ßon d'enregistrer plus efficacement</strong> .  Dans le cas des disques m√©caniques rotatifs, cela a conduit √† l'√©mergence de la " <strong>famine"</strong> : une situation o√π l'efficacit√© d'enregistrement d√©pend de la rotation du disque.  Si vous avez soudainement besoin de lire efficacement en m√™me temps, mais qu'il est d√©j√† mal tourn√©, il est mal lu √† partir d'un tel disque. <br><br>  Peu √† peu, il est devenu clair que c'√©tait une mani√®re inefficace.  Par cons√©quent, √† partir du noyau 2.6, tout un zoo d'ordonnanceurs a commenc√© √† appara√Ætre, destin√© √† diff√©rentes t√¢ches. <br><br><h3>  Ascenseurs: entre 2,6 et 3 </h3><br>  Beaucoup de gens confondent ces planificateurs avec les planificateurs du syst√®me d'exploitation car ils ont des noms similaires.  <strong>CFQ - Completely Fair Queuing</strong> n'est pas la m√™me chose que les planificateurs de syst√®me d'exploitation.  Seuls les noms sont similaires.  Il a √©t√© invent√© comme un planificateur universel. <br><br>  <strong>Qu'est-ce qu'un planificateur universel?</strong>  Pensez-vous que vous avez une charge moyenne ou, au contraire, unique?  Les bases de donn√©es ont une tr√®s faible polyvalence.  La charge universelle peut √™tre imagin√©e comme un ordinateur portable ordinaire.  Tout s'y passe: on √©coute de la musique, on joue, on tape du texte.  Pour cela, seuls des ordonnanceurs universels ont √©t√© √©crits. <br><br>  <strong>La t√¢che principale de l'ordonnanceur universel:</strong> dans le cas de Linux, pour chaque terminal virtuel et processus, cr√©er une file d'attente de requ√™tes.  Lorsque nous voulons √©couter de la musique dans un lecteur audio, IO pour le lecteur prend une file d'attente.  Si nous voulons sauvegarder quelque chose en utilisant la commande cp, quelque chose d'autre est impliqu√©. <br><br>  Dans le cas des bases de donn√©es, un probl√®me se produit.  En r√®gle g√©n√©rale, une base de donn√©es est un processus qui a d√©marr√© et, pendant son fonctionnement, des processus parall√®les sont apparus qui se terminent toujours dans la m√™me file d'attente d'E / S.  La raison en est qu'il s'agit de la m√™me application, du m√™me processus parent.  Pour de tr√®s petites charges, une telle programmation √©tait appropri√©e, pour le reste cela n'avait pas de sens.  Il √©tait plus facile de l'√©teindre et de ne pas l'utiliser si possible. <br><br>  Progressivement, le <strong>planificateur de d√©lais</strong> est apparu - il fonctionne de mani√®re plus astucieuse, mais il s'agit essentiellement de fusion et de tri pour les disques en rotation.  √âtant donn√© la conception d'un sous-syst√®me de disque sp√©cifique, nous collectons des vecteurs de bloc pour les √©crire de la mani√®re optimale.  Il avait moins de probl√®mes de <strong>famine</strong> , mais ils √©taient l√†. <br><br>  Par cons√©quent, plus pr√®s du troisi√®me noyau Linux est apparu <strong>noop</strong> ou <strong>none</strong> , ce qui fonctionnait beaucoup mieux avec la propagation des SSD.  En incluant l'ordonnanceur noop, nous d√©sactivons en fait l'ordonnancement: il n'y a pas de tri, de fusion et d'autres choses similaires √† CFQ et √† la date limite. <br><br>  Cela fonctionne mieux avec les SSD, car les SSD sont intrins√®quement parall√®les: il a des cellules de m√©moire.  Plus il y a de ces √©l√©ments √† entasser sur une carte PCIe, plus il fonctionnera efficacement. <br><br>  Scheduler de certains de ses autres mondes, du point de vue du SSD, consid√©rations, recueille certains vecteurs et les envoie quelque part.  Tout se termine par un entonnoir.  Donc, nous tuons la concurrence des SSD, ne les utilisez pas au maximum.  Par cons√©quent, un simple arr√™t, lorsque les vecteurs vont au hasard sans aucun tri, a mieux fonctionn√© en termes de performances.  Pour cette raison, on pense que les lectures al√©atoires, l'√©criture al√©atoire sont meilleures sur les SSD. <br><br><h3>  Ascenseurs: √† partir de 3,13 </h3><br>  √Ä partir du noyau 3.13, <strong>blk-mq est apparu</strong> .  Un peu plus t√¥t, il y avait un prototype, mais en 3.13 une version fonctionnelle est apparue pour la premi√®re fois. <br><br>  <strong>Blk-mq a commenc√©</strong> comme un planificateur, mais il est difficile de l'appeler un planificateur - il est autonome sur le plan architectural.  Il s'agit d'un remplacement pour la couche de demande dans le noyau.  Lentement, le d√©veloppement de blk-mq a conduit √† une refonte majeure de l'ensemble de la pile d'E / S Linux. <br><br>  L'id√©e est la suivante: utilisons la capacit√© native des SSD pour effectuer une concurrence efficace pour les E / S.  Selon le nombre de flux d'E / S parall√®les que vous pouvez utiliser, il existe des files d'attente honn√™tes √† travers lesquelles nous √©crivons simplement telles quelles sur le SSD.  Chaque CPU a sa propre file d'attente pour l'enregistrement. <br><br>  Actuellement, <strong>blk-mq se</strong> d√©veloppe et fonctionne activement.  Il n'y a aucune raison de ne pas l'utiliser.  Dans les c≈ìurs modernes, √† partir de 4 et plus, √† partir de <strong>blk-mq, le</strong> gain est perceptible - pas 5-10%, mais beaucoup plus. <br><br><blockquote>  blk-mq est probablement la meilleure option pour travailler avec des SSD. </blockquote><br>  Dans sa forme actuelle, <strong>blk-mq est</strong> directement li√© au pilote <strong>NVMe</strong> Linux.  Il n'y a pas seulement un pilote pour Linux, mais aussi un pilote pour Microsoft.  Mais l'id√©e de faire <strong>blk-mq</strong> et le pilote NVMe est le traitement m√™me de la pile Linux, dont les bases de donn√©es ont grandement b√©n√©fici√©. <br><br>  Un consortium de plusieurs soci√©t√©s a d√©cid√© de faire une sp√©cification, ce protocole m√™me.  Maintenant, il fonctionne d√©j√† bien en version de production pour les SSD PCIe locaux.  Solution presque pr√™te pour les baies de disques connect√©es via des optiques. <br><br><blockquote>  Le pilote blk-mq et NVMe sont plus qu'un planificateur.  Le syst√®me vise √† remplacer l'ensemble du niveau des demandes. </blockquote><br>  Plongeons-nous pour comprendre ce que c'est.  La sp√©cification NVMe est grande, donc nous ne prendrons pas en compte tous les d√©tails, mais passons simplement en revue. <br><br><h3>  Ancienne approche des ascenseurs </h3><br><img src="https://habrastorage.org/webt/ya/82/ou/ya82oun8cusxylg0bsuz5y0e1la.jpeg"><br><br>  Le cas le plus simple: il y a un CPU, il y a son tour, et en quelque sorte on va sur disque. <br><br>  Les ascenseurs plus avanc√©s fonctionnaient diff√©remment.  Il y a plusieurs CPU et plusieurs files d'attente.  D'une mani√®re ou d'une autre, par exemple, en fonction du processus parent auquel les travailleurs de la base de donn√©es ont fil√©, IO est mis en file d'attente sur les disques. <br><br><h3>  Une nouvelle approche des ascenseurs </h3><br>  blk-mq est une approche compl√®tement nouvelle.  Chaque CPU, chaque zone NUMA ajoute √† son tour sa propre entr√©e / sortie.  De plus, les donn√©es tombent sur les disques, quelle que soit leur connexion, car le pilote est nouveau.  Il n'y a pas de pilote SD qui fonctionne avec les concepts de cylindres, de blocs. <br><br><img src="https://habrastorage.org/webt/zf/12/7n/zf127n1eqhli2qioregzrsiftp8.jpeg"><br><br>  Il y a eu une p√©riode de transition.  √Ä un moment donn√©, tous les fournisseurs de baies RAID ont commenc√© √† vendre des modules compl√©mentaires qui leur permettaient de contourner le cache RAID.  Si les SSD sont connect√©s, √©crivez directement √† cet endroit.  Ils ont d√©sactiv√© l'utilisation du pilote SD pour leurs produits, comme blq-mq. <br><br><h2>  Nouvelle pile avec blk-mq </h2><br>  Voici √† quoi ressemble la pile sous une nouvelle forme. <br><br><img src="https://habrastorage.org/webt/me/ma/3e/mema3e595wllaoub6thn61q8kvu.jpeg"><br><br>  D'en haut, tout reste aussi.  Par exemple, les bases de donn√©es sont loin derri√®re.  Les E / S de la base de donn√©es, comme pr√©c√©demment, tombent dans la couche Block IO.  Il y a le tr√®s <strong>blk-mq</strong> qui remplace la couche de requ√™te, pas le planificateur. <br><br>  Dans le noyau 3.13, toute l'optimisation s'est termin√©e √† peu pr√®s √† ce sujet, mais de nouvelles technologies sont utilis√©es dans les noyaux modernes.  Des ordonnanceurs sp√©ciaux pour <strong>blk-mq ont</strong> commenc√© √† appara√Ætre, qui sont con√ßus pour un parall√©lisme plus fort.     Linux    schedulers IO ‚Äî  Kyber  BFQ.         <strong>blk-mq</strong> . <br><br> <strong>BFQ</strong> <strong>‚Äî Budget Fair Queueing ‚Äî </strong> <strong></strong> <strong></strong> <strong>FQ</strong> .   ,     . BFQ ‚Äî  scheduler   .         IO.     IO,  / .       ,   .     ‚Äî  .   BFQ,   ,    . <br><br> <strong>Kyber ‚Äî  </strong> .   BFQ,   .  Kyber  scheduler   .    ‚Äî   CPU  . Kyber    . <br><br>      ‚Äî <strong>blk-mq    SD-</strong> .      ,    ,  ,    IO-.  blk-mq  NVMe driver      .     . <br><br>        ‚Äî   latency,      .   SSD,    ‚Äî     .      -, ,    NVMe-,   blk-mq    ,    .    . <br><br><h2>   Linux IO </h2><br>         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> / Linux. <br><br><img src="https://habrastorage.org/webt/ak/05/qb/ak05qbritsgbiaeybq3dmz2mpyi.png"><br><br>   ,    ,   ,  Elevators,   . <br><br>   ,           ,    . <br><br><h2>  NVM Express </h2><br> <strong>NVM Express  NVMe ‚Äî  ,  ,     SSD.</strong>     Linux. Linux ‚Äî     . <br><br>     .          20 /   SSD ,  NVMe  ,   , ‚Äî <strong> 32 /</strong> .  SD    ,   ,     . <br><br><blockquote>    ,  ,   . </blockquote><br>  Une fois que les bases de donn√©es ont √©t√© √©crites pour les disques rotatifs et orient√©es vers eux, elles ont des index sous la forme d'un arbre B, par exemple.  La question se pose: <strong>les bases de donn√©es sont-elles pr√™tes pour NVMe</strong> ?  Les bases de donn√©es sont-elles capables de m√¢cher une telle charge? <br><br>  Pas encore, mais ils s'adaptent.  La liste de diffusion PostgreSQL a r√©cemment eu quelques <code>pwrite()</code> et des choses similaires.  Les d√©veloppeurs PostgreSQL et MySQL interagissent avec les d√©veloppeurs du noyau.  Bien s√ªr, j'aimerais plus d'interaction. <br><br><h2>  D√©veloppements r√©cents </h2><br>  Au cours de la derni√®re ann√©e et demie, NVMe a ajout√© le <strong>sondage IO</strong> . <br><br>  Au d√©but, il y avait des disques qui tournaient avec une latence √©lev√©e.  Viennent ensuite les SSD, qui sont beaucoup plus rapides.  Mais il y avait un jambage: fsync continue, l'enregistrement d√©marre et √† un niveau tr√®s bas - profond√©ment dans le pilote, une demande est envoy√©e directement au mat√©riel - notez-le. <br><br>  Le m√©canisme √©tait simple - ils l'ont envoy√© et nous attendons que l'interruption soit trait√©e.  L'attente du traitement d'interruption n'est pas un probl√®me par rapport √† l'√©criture sur un disque en rotation.  Il a fallu si longtemps pour attendre que d√®s la fin de l'enregistrement, l'interruption a fonctionn√©. <br><br>  Depuis que le SSD √©crit tr√®s rapidement, un m√©canisme pour interroger le mat√©riel sur l'enregistrement est apparu de force.  Dans les premi√®res versions, l'augmentation de la vitesse d'E / S a atteint 50% du fait que nous n'attendons pas d'interruption, mais nous demandons activement le morceau de fer √† propos du record.  <strong>Ce m√©canisme est appel√© interrogation IO</strong> . <br><br>  Il a √©t√© introduit dans les versions r√©centes.  Dans la version 4.12, des <strong>ordonnanceurs IO</strong> sont apparus, sp√©cialement affin√©s pour travailler avec <strong>blk-mq</strong> et NVMe, √† propos desquels j'ai dit <strong>Kyber et BFQ</strong> .  Ils sont d√©j√† officiellement dans le noyau, ils peuvent √™tre utilis√©s. <br><br>  Maintenant, sous une forme utilisable, il y a ce qu'on appelle le <strong>marquage IO</strong> .  La plupart des fabricants de clouds et de machines virtuelles contribueront √† ce d√©veloppement.  En gros, l'apport d'une application sp√©cifique peut √™tre clou√© et lui donner la priorit√©.  Les bases de donn√©es ne sont pas encore pr√™tes pour cela, mais restez √† l'√©coute.  Je pense que ce sera bient√¥t le courant dominant. <br><br><h2>  Notes d'E / S directes </h2><br>  <strong>PostgreSQL ne prend pas en charge Direct IO, et il existe un certain nombre de probl√®mes qui rendent difficile l'activation de la prise en charge</strong> .  Maintenant, cela n'est pris en charge que pour la valeur, et uniquement si la r√©plication n'est pas activ√©e.  Il est n√©cessaire <strong>d'√©crire beaucoup de code sp√©cifique au syst√®me d'exploitation</strong> , et pour l'instant tout le monde s'en abstient. <br><br>  Malgr√© le fait que Linux jure fortement sur l'id√©e de Direct IO et sur la fa√ßon dont il est impl√©ment√©, toutes les bases de donn√©es y vont.  Dans Oracle et MySQL, Direct IO est largement utilis√©.  PostgreSQL est la seule base de donn√©es que Direct IO ne tol√®re pas. <br><br><h2>  Liste de contr√¥le </h2><br>  Comment vous prot√©ger des surprises fsync dans PostgreSQL: <br><br><ul><li>  Configurez des points de contr√¥le pour qu'ils soient moins fr√©quents et plus grands. </li><li>  Configurez un r√©dacteur en arri√®re-plan pour aider √† v√©rifier </li><li>  Tirez Autovacuum pour qu'il n'y ait pas d'E / S parasites inutiles. </li></ul><br><blockquote>  Selon la tradition, en novembre, nous attendons des d√©veloppeurs professionnels de services hautement charg√©s √† Skolkovo sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HighLoad ++</a> .  Il reste encore un mois pour <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">demander</a> un rapport, mais nous avons d√©j√† accept√© les premiers rapports au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">programme</a> .  Inscrivez-vous √† notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">newsletter</a> et d√©couvrez de nouveaux sujets de premi√®re main. </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr459444/">https://habr.com/ru/post/fr459444/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr459430/index.html">Application mobile avec g√©n√©ration automatique de formulaires: notre cas</a></li>
<li><a href="../fr459432/index.html">RD-180: les √âtats-Unis peuvent-ils fabriquer des moteurs-fus√©es?</a></li>
<li><a href="../fr459434/index.html">React Hook Router Une alternative moderne aux routeurs React</a></li>
<li><a href="../fr459438/index.html">Les donn√©es sont encore plus importantes</a></li>
<li><a href="../fr459442/index.html">5 syst√®mes de gestion d'√©v√©nements de s√©curit√© open source</a></li>
<li><a href="../fr459446/index.html">Cinq tendances effrayantes du design moderne</a></li>
<li><a href="../fr459450/index.html">La vuln√©rabilit√© du logiciel de t√©l√©conf√©rence Zoom permet √† n'importe quel site Web d'espionner les utilisateurs via webcam</a></li>
<li><a href="../fr459452/index.html">Un agro-robot avec l'IA a appris √† ramasser soigneusement uniquement la salade m√ªrie du jardin</a></li>
<li><a href="../fr459454/index.html">Comment s'est pass√© le premier hackathon √† The Standoff</a></li>
<li><a href="../fr459456/index.html">Dagaz: √©pisodes (partie 1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>