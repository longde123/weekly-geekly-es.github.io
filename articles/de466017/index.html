<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🖕 🚵🏼 👨🏼‍🎤 Livy - das fehlende Glied in der Hadoop Spark Airflow Python-Kette 🎅🏾 💃🏼 🍈</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits, einige Informationen "unter der Haube" sind das Datum der technischen Werkstatt von Alfastrakhovaniya - was unsere technischen Köpfe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Livy - das fehlende Glied in der Hadoop Spark Airflow Python-Kette</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/alfastrah/blog/466017/"><p>  Hallo allerseits, einige Informationen "unter der Haube" sind das Datum der technischen Werkstatt von Alfastrakhovaniya - was unsere technischen Köpfe begeistert. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/956/f04/a6a/956f04a6ae545bee58a8c34f2938a850.png" alt="Bild"></p><br><p>  Apache Spark ist ein wunderbares Tool, mit dem Sie große Datenmengen schnell und einfach auf relativ bescheidenen Computerressourcen verarbeiten können (ich meine Cluster-Verarbeitung). </p><br><p>  Traditionell wird das Jupyter-Notebook in der Ad-hoc-Datenverarbeitung verwendet.  In Kombination mit Spark können wir so langlebige Datenrahmen manipulieren (Spark befasst sich mit der Zuweisung von Ressourcen, die Datenrahmen befinden sich irgendwo im Cluster, ihre Lebensdauer ist durch die Lebensdauer des Spark-Kontexts begrenzt). </p><br><p>  Nach der Übertragung der Datenverarbeitung an Apache Airflow wird die Lebensdauer der Frames erheblich verkürzt - der Spark-Kontext "lebt" innerhalb derselben Airflow-Anweisung.  Wie man das umgeht, warum man herumkommt und was Livy damit zu tun hat - lesen Sie unter dem Schnitt. </p><a name="habracut"></a><br><p>  Schauen wir uns ein sehr, sehr einfaches Beispiel an: Angenommen, wir müssen Daten in einer großen Tabelle denormalisieren und das Ergebnis zur weiteren Verarbeitung in einer anderen Tabelle speichern (ein typisches Element der Datenverarbeitungspipeline). </p><br><p>  Wie würden wir das machen: </p><br><ul><li>  geladene Daten in Datenrahmen (Auswahl aus einer großen Tabelle und Verzeichnissen) </li><li>  schaute mit "Augen" auf das Ergebnis (hat es richtig geklappt) </li><li>  gespeicherter Datenrahmen in der Hive-Tabelle (zum Beispiel) </li></ul><br><p>  Basierend auf den Ergebnissen der Analyse müssen wir möglicherweise im zweiten Schritt eine bestimmte Verarbeitung einfügen (Wörterbuchersatz oder etwas anderes).  In Bezug auf die Logik haben wir drei Schritte </p><br><ul><li>  Schritt 1: herunterladen </li><li>  Schritt 2: Verarbeitung </li><li>  Schritt 3: Speichern </li></ul><br><p>  In jupyter notebook machen wir das so - wir können die heruntergeladenen Daten für eine beliebig lange Zeit verarbeiten und so die Spark-Ressourcen kontrollieren. </p><br><p> Es ist logisch zu erwarten, dass eine solche Partition an Airflow übertragen werden kann.  Das heißt, ein Diagramm dieser Art zu haben </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/312/30b/d7e/31230bd7e4c3beb62f92aebb709e2010.png" alt="Bild"></p><br><p>  Leider ist dies bei Verwendung der Airflow + Spark-Kombination nicht möglich: Jede Airflow-Anweisung wird in einem eigenen Python-Interpreter ausgeführt. Daher muss jede Anweisung unter anderem die Ergebnisse ihrer Aktivitäten irgendwie "beibehalten".  Somit wird unsere Verarbeitung in einem Schritt "komprimiert" - "Daten denormalisieren". </p><br><p>  Wie kann die Flexibilität des Jupyter-Notebooks wieder auf Airflow übertragen werden?  Es ist klar, dass das obige Beispiel „es nicht wert“ ist (vielleicht stellt sich im Gegenteil ein gut verständlicher Verarbeitungsschritt heraus).  Aber dennoch - wie können Airflow-Anweisungen im selben Spark-Kontext über den gemeinsamen Datenrahmenbereich ausgeführt werden? </p><br><h2 id="privetstvuem-livy">  Willkommen Livy </h2><br><p>  Ein weiteres Hadoop-Ökosystemprodukt kommt zur Rettung - Apache Livy. </p><br><p>  Ich werde hier nicht versuchen zu beschreiben, was für ein "Biest" es ist.  Wenn es sehr kurz und schwarzweiß ist - Mit Livy können Sie Python-Code in ein Programm "einfügen", das der Treiber ausführt: </p><br><ul><li>  Zuerst erstellen wir eine Livy-Sitzung </li><li>  Danach haben wir die Möglichkeit, in dieser Sitzung beliebigen Python-Code auszuführen (sehr ähnlich der Jupyter / Ipython-Ideologie). </li></ul><br><p>  Und zu all dem gibt es eine REST-API. </p><br><p>  Zurück zu unserer einfachen Aufgabe: Mit Livy können wir die ursprüngliche Logik unserer Denormalisierung speichern </p><br><ul><li>  Im ersten Schritt (der ersten Anweisung unseres Diagramms) werden wir den Datenladecode in den Datenrahmen laden und ausführen </li><li>  im zweiten Schritt (zweite Anweisung) - führen Sie den Code für die notwendige zusätzliche Verarbeitung dieses Datenrahmens aus </li><li>  im dritten Schritt - der Code zum Speichern des Datenrahmens in der Tabelle </li></ul><br><p>  Was in Bezug auf den Luftstrom so aussehen könnte: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b9b/255/bcd/b9b255bcd00525201a000ef1a3fbafa3.png" alt="Bild"></p><br><p>  (Da es sich bei dem Bild um einen sehr realen Screenshot handelt, wurden zusätzliche „Realitäten“ hinzugefügt. Das Erstellen des Spark-Kontexts wurde zu einem separaten Vorgang mit einem seltsamen Namen. Die „Verarbeitung“ der Daten verschwand, weil sie nicht benötigt wurden usw.) </p><br><p>  Zusammenfassend erhalten wir </p><br><ul><li>  Universelle Luftstromanweisung, die Python-Code in einer Livy-Sitzung ausführt </li><li>  die Fähigkeit, Python-Code in ziemlich komplexen Graphen zu "organisieren" (Airflow dafür) </li><li>  Die Fähigkeit, Optimierungen auf höherer Ebene in Angriff zu nehmen, z. B. in welcher Reihenfolge wir unsere Transformationen durchführen müssen, damit Spark die allgemeinen Daten so lange wie möglich im Cluster-Speicher behalten kann </li></ul><br><p>  Eine typische Pipeline zum Vorbereiten von Daten für die Modellierung enthält ungefähr 25 Abfragen über 10 Tabellen. Es ist offensichtlich, dass einige Tabellen häufiger verwendet werden als andere (dieselben "allgemeinen Daten"), und es gibt etwas zu optimieren. </p><br><h2 id="chto-dalshe">  Was weiter </h2><br><p>  Die technischen Fähigkeiten wurden getestet, wir überlegen weiter - wie wir unsere Transformationen technologischer in dieses Paradigma umsetzen können.  Und wie man sich der oben erwähnten Optimierung nähert.  Wir stehen noch am Anfang dieses Teils unserer Reise - wenn es etwas Interessantes gibt, werden wir es definitiv teilen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de466017/">https://habr.com/ru/post/de466017/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de465991/index.html">Arbeiter sauberer schneller Architektur</a></li>
<li><a href="../de465993/index.html">Sie müssen keine digitale Sicherheit sparen</a></li>
<li><a href="../de465995/index.html">LDC - Ausflug</a></li>
<li><a href="../de466001/index.html">"Mobile" Feng Shui, oder wir schlafen richtig (Kaffee, Kakerlaken und Intoleranz bei Habré)</a></li>
<li><a href="../de466015/index.html">Ein bisschen mehr über Trigonometrie beim Rechnen</a></li>
<li><a href="../de466019/index.html">ABBYY Mobile Web Capture: Hochwertige Fotos von Dokumenten direkt im Browser Ihres Smartphones</a></li>
<li><a href="../de466021/index.html">Wie ich Yandex.Alice beigebracht habe, über Sexspielzeug zu sprechen</a></li>
<li><a href="../de466027/index.html">Das Buch "Der Weg von Python. Schwarzer Gürtel für Entwicklung, Skalierung, Test und Bereitstellung “</a></li>
<li><a href="../de466029/index.html">Wie man einen Quantencomputer in einen perfekten Zufallszahlengenerator verwandelt</a></li>
<li><a href="../de466031/index.html">DeepMinds epische Mission, das komplexeste wissenschaftliche Problem zu lösen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>