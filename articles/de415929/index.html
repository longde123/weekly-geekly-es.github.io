<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üóØÔ∏è üéÑ üôèüèº Optimierung der Architektur der k√ºnstlichen Intelligenz: Das Rennen beginnt üöµ üòî üì¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Da sich die KI-Architektur verbessert und die Kosten sinken, sagen Experten, dass immer mehr Unternehmen diese Technologien beherrschen werden, was In...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Optimierung der Architektur der k√ºnstlichen Intelligenz: Das Rennen beginnt</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/icl_services/blog/415929/">  Da sich die KI-Architektur verbessert und die Kosten sinken, sagen Experten, dass immer mehr Unternehmen diese Technologien beherrschen werden, was Innovationen Impulse geben und sowohl f√ºr Unternehmen als auch f√ºr KI-Entwickler gro√üe Dividenden bringen wird. <br><br>  KI-Anwendungen arbeiten h√§ufig auf der Grundlage v√∂llig anderer Architekturen als herk√∂mmliche Unternehmensanwendungen.  Die Lieferanten sind wiederum bereit, viel zu tun, um neue Komponenten bereitzustellen, deren Nachfrage w√§chst. <br><br><img src="https://habrastorage.org/webt/kx/tl/dk/kxtldk4vqyzwqzmeaxzo5u8pulo.jpeg" align="left"><blockquote> "Die Computerbranche befindet sich in einem gro√üen Wandel. Das Interesse der Unternehmen an KI gibt Impulse f√ºr Innovationen, die dazu beitragen, KI in jeder Gr√∂√üenordnung zu beherrschen und einzusetzen", sagte Keith Strier, KI-Experte und Berater bei EY.  Investoren investieren viel Geld in Startups, die die KI optimieren, und gro√üe Hersteller bieten nicht nur Chips und Speicher an, sondern auch die f√ºr die Bereitstellung erforderlichen Netzwerk- und Cloud-Services. ‚Äú </blockquote>  . <br>  Ihm zufolge besteht die Hauptaufgabe der IT-Direktoren nun darin, die geeignete Architektur f√ºr k√ºnstliche Intelligenz f√ºr die Anforderungen des Unternehmens auszuw√§hlen. <br><br>  Streer sagt, dass die Implementierung dieser Technologie v√∂llig andere technische Bedingungen und Sicherheitstools erfordert als bei bekannten Unternehmens-Workloads, da KI Mathematik in einem beispiellosen Ausma√ü ist.  Um AI optimal nutzen zu k√∂nnen, m√ºssen Lieferanten die f√ºr AI erforderliche technische Infrastruktur, Cloud und andere Dienste bereitstellen, ohne die solche komplexen Berechnungen nicht m√∂glich w√§ren. <br><a name="habracut"></a><br>  Aber wir sind bereits auf dem Weg dorthin, und in Zukunft wird es noch fortschrittlichere Architekturen f√ºr k√ºnstliche Intelligenz geben.  Streer glaubt, dass die Bereitstellung von Flexibilit√§t, Leistung und Geschwindigkeit von Computerarchitekturen nicht nur kleine Unternehmen f√ºr die Entwicklung von Hochleistungsrechnern sein werden, sondern auch andere Vertreter der Hochleistungsrechnerbranche, einschlie√ülich Startups zur Erstellung von Mikrochips und Cloud-Diensten, die einen hohen Standard f√ºr AI- setzen m√∂chten. Computing. <br><br>  Je mehr Spezialisten und Entwickler auf dem Gebiet der KI auftauchen, desto zug√§nglicher wird diese Technologie, die Innovationen einen guten Impuls verleiht und sp√ºrbare Dividenden bringt - f√ºr Unternehmen und Lieferanten. <br><br>  In der Zwischenzeit sollten sich IT-Direktoren mit den Schwierigkeiten vertraut machen, die mit der Erstellung einer Architektur f√ºr k√ºnstliche Intelligenz f√ºr den Unternehmensgebrauch verbunden sind, um bereit zu sein, diese zu l√∂sen. <br><br><h3>  Chip-Entwicklung <br></h3><br>  Die wichtigste Voraussetzung f√ºr den √úbergang von traditionellen Computerarchitekturen zu KI war die Entwicklung von Grafikprozessoren, integrierten Schaltkreisen (FPGAs) und speziellen KI-Chips.  Die Verbreitung von Architekturen auf Basis von GPUs und FPGAs wird dazu beitragen, die Produktivit√§t und Flexibilit√§t von Computer- und Speichersystemen zu steigern, wodurch L√∂sungsanbieter eine Reihe fortschrittlicher Dienste f√ºr KI- und maschinelle Lernanwendungen anbieten k√∂nnen. <br><br><img src="https://habrastorage.org/webt/ik/yt/d4/ikytd4x8p5ajbjv_m4tjqq-lgzi.jpeg" align="left"><blockquote>  "Dies sind Chip-Architekturen, die viele erweiterte Funktionen von der Last befreien [wie z. B. KI-Training] und dabei helfen, einen verbesserten Stack f√ºr Computer und Speicher zu implementieren, der un√ºbertroffene Leistung und Effizienz bietet", sagte Surya Varanasi, Gr√ºnderin und CTO von Vexata Inc., Anbieter von Datenverwaltungsl√∂sungen. </blockquote><br>  Aber w√§hrend die neuen Mikroschaltungen nicht zu etwas Komplexerem f√§hig sind.  Um die optimale Architektur f√ºr AI-Workloads auszuw√§hlen, m√ºssen umfangreiche Berechnungen durchgef√ºhrt werden, die einen hohen Durchsatz erfordern und nicht ohne Verz√∂gerungen auskommen k√∂nnen.  Der Schl√ºssel zum Erfolg sind hier Hochgeschwindigkeitsnetzwerke.  Viele KI-Algorithmen m√ºssen jedoch warten, bis der n√§chste Datensatz eingegeben wurde, damit Sie die Verz√∂gerung nicht aus den Augen verlieren. <br><br>  Dar√ºber hinaus durchlaufen Daten beim √úberschreiten von Servergrenzen oder beim √úbertragen von Servern zum Speicher mehrere Protokolle.  Um diese Prozesse zu vereinfachen, versuchen Datenexperten m√∂glicherweise, die Daten lokal zu lokalisieren, sodass ein Server gro√üe Datenmengen verarbeiten kann, ohne auf andere zu warten.  Durch die verbesserte Integration zwischen GPUs und Speicher k√∂nnen Sie auch Geld sparen.  Andere Anbieter suchen nach M√∂glichkeiten, das Design von AI-Servern zu vereinfachen, um die Kompatibilit√§t sicherzustellen, sodass dieselben Server f√ºr unterschiedliche Workloads verwendet werden k√∂nnen. <br><br><h3>  Nichtfl√ºchtiger Speicher zur Verarbeitung von AI-Workloads <br></h3><br>  Das Herzst√ºck vieler auf der GPU basierender L√∂sungen ist ein direkt angeschlossenes Laufwerk (DAS), das das verteilte Lernen und die Bildung logischer Schlussfolgerungen f√ºr die KI erheblich erschwert.  Infolgedessen wird die Installation und Verwaltung dieser Datenleitungen f√ºr Deep Learning zu einer komplexen und zeitaufw√§ndigen Aufgabe. <br><br>  Um dieses Problem zu l√∂sen, eignet sich nichtfl√ºchtiger Speicher (NVM), der urspr√ºnglich f√ºr eine qualitativ hochwertige Konnektivit√§t zwischen Solid-State-Laufwerken (SSDs) und herk√∂mmlichen Unternehmensservern entwickelt wurde.  Jetzt ist diese Art von Speicher h√§ufig in E / A-Matrizen enthalten, um die AI-Workloads zu optimieren. <br><br>  Das Fazit ist, dass NVMe over Fabrics (NVMeF) - die sogenannten diese Schnittstellen - dazu beitragen wird, die Kosten f√ºr die Konvertierung zwischen Netzwerkprotokollen zu senken und die Eigenschaften jedes SSD-Typs zu steuern.  Auf diese Weise k√∂nnen CIOs die Kosten f√ºr KI-Anwendungen rechtfertigen, die gro√üe Datenmengen verwenden. <br><br>  Schnittstellen NVMeF birgt Risiken, einschlie√ülich der Notwendigkeit hoher Kosten f√ºr fortschrittliche Technologien.  Dar√ºber hinaus besteht in dieser Branche nach wie vor eine Abh√§ngigkeit von NVMeF-Anbietern. Daher sollten IT-Direktoren versuchen, bei der Auswahl eines Produkts herstellerspezifische Beziehungen zu vermeiden. <br>  Die Implementierung von NVMeF erm√∂glicht es Ihnen jedoch, einen weiteren Schritt zur Optimierung der Unternehmensarchitektur der k√ºnstlichen Intelligenz zu unternehmen, glaubt Varanasi. <br><br><img src="https://habrastorage.org/webt/ik/yt/d4/ikytd4x8p5ajbjv_m4tjqq-lgzi.jpeg" align="left"><blockquote>  ‚ÄûTrotz der Tatsache, dass die Erweiterung der NVMe √ºber Fabrics-Architektur im industriellen Ma√üstab weitere ein bis anderthalb Jahre dauern kann, haben wir bereits die Hauptkomponenten und die Pioniere berichten bereits √ºber vielversprechende Ergebnisse‚Äú, sagt Varanasi. <br><br></blockquote><br>  CIOs, die AI-Anwendungen entwickeln m√∂chten, k√∂nnen versuchen, einen f√ºr AI f√ºr NVMeF optimierten gemeinsam genutzten Speicherpool zu erstellen, wenn er vorhandene Speichernetzwerke kurzfristig erfolgreich ersetzen kann.  Wenn Sie jedoch warten, bis NVMeF abw√§rtskompatibel ist, k√∂nnen Sie viel verlieren. <br><br><h3>  Datenbewegung reduzieren <br></h3><br>  Bei der Planung der verschiedenen Phasen der KI-Bereitstellung m√ºssen Sie besonders auf die Kosten f√ºr das Verschieben von Daten achten.  KI-Projekte, einschlie√ülich solcher zur Verarbeitung und Transformation von Daten sowie f√ºr Trainingsalgorithmen, erfordern gro√üe Datenmengen. <br><br>  Die Hardware und die Humanressourcen, die zum Ausf√ºhren dieser Aufgaben erforderlich sind, sowie die Zeit, die zum Verschieben der Daten selbst ben√∂tigt wird, k√∂nnen KI-Projekte zu kostspielig machen.  Wenn CIOs das Verschieben von Daten zwischen Phasen vermeiden k√∂nnen, ist es wahrscheinlich, dass sie eine funktionsf√§hige KI-Infrastruktur entwickeln k√∂nnen, die diese Anforderungen erf√ºllt, sagte Dr. Haris Pozidis, Manager, Spezialist f√ºr Speicherbeschleunigungstechnologie bei IBM Research.  Die Hersteller arbeiten bereits an diesem Problem. <br><br>  Beispielsweise experimentiert IBM mit verschiedenen Optionen zur Hardware- und Softwareoptimierung, um die Datenbewegung f√ºr gro√üe KI-Anwendungen in Labors in Z√ºrich zu reduzieren.  Solche Optimierungen haben dazu beigetragen, die Leistung des Testskripts des beliebten Klickanalyse-Tools 46-mal zu steigern.  Laut Pozidis stehen verteiltes Lernen und GPU-Beschleunigung im Mittelpunkt dieser Arbeit, die die Unterst√ºtzung f√ºr sp√§rliche Datenstrukturen verbessert. <br><br>  Parallelit√§t ist eine weitere wichtige Komponente bei der Beschleunigung von KI-Workloads.  F√ºr verteiltes Training m√ºssen √Ñnderungen auf Hardware- und Softwareebene vorgenommen werden, um die Verarbeitungseffizienz paralleler Grafikprozessoralgorithmen zu verbessern.  IBM Forscher haben eine Prototypplattform mit Datenparallelit√§t erstellt, mit der Sie gro√üe Datenmengen skalieren und lernen k√∂nnen, die die Speichermenge auf einem Computer √ºberschreiten.  Dies ist sehr wichtig f√ºr Gro√üanwendungen.  Eine neue Plattform, die f√ºr das Lernen in der Kommunikation und die Bereitstellung von Datenlokalit√§t optimiert ist, hat dazu beigetragen, die Datenbewegung zu reduzieren. <br><br>  Auf Hardwareebene verwendeten IBM Forscher NVMeF, um die Interkonnektivit√§t der GPU-, CPU- und Speicherkomponenten auf den Servern sowie zwischen Servern und Speicher zu verbessern. <br><br><img src="https://habrastorage.org/webt/es/li/m1/eslim1mvwlg3vqgesw1lfuzlyqa.jpeg" align="left"><blockquote>  ‚ÄûDie Leistung verschiedener KI-Workloads kann durch Netzwerkengp√§sse, Speicherbandbreite und Bandbreite zwischen CPU und GPU eingeschr√§nkt sein.  Wenn Sie jedoch effizientere Verbindungsalgorithmen und -protokolle in allen Teilen des Systems implementieren, k√∂nnen Sie einen gro√üen Schritt in Richtung der Entwicklung schnellerer KI-Anwendungen tun ‚Äú, sagt Pozidis. </blockquote><br><br><h3>  Compound Computing <br></h3>  Heutzutage verwenden die meisten Workloads eine vorkonfigurierte Datenbank, die f√ºr eine bestimmte Hardwarearchitektur optimiert ist. <br><br><img src="https://habrastorage.org/webt/iy/qd/xq/iyqdxqbenrorpibc--ylt9o0zfs.jpeg" align="left"><br><blockquote>  Laut Chad Miley, Vice President f√ºr analytische Produkte und L√∂sungen bei Teradata, bewegt sich der Markt in Richtung softwaregesteuerter Hardware, mit der Unternehmen die Verarbeitung je nach aktueller Aufgabe intelligent auf GPUs und CPUs verteilen k√∂nnen. </blockquote><br><br>  Die Schwierigkeit liegt in der Tatsache, dass Unternehmen unterschiedliche Computer-Engines verwenden, um auf unterschiedliche Speicheroptionen zuzugreifen.  Gro√üe Unternehmen bevorzugen es, wertvolle Daten zu speichern, auf die regelm√§√üig zugegriffen werden muss, z. B. Informationen √ºber Kunden, Finanzen, Lieferkette, Produkte und andere Komponenten, und zwar in leistungsstarken Input-Output-Umgebungen.  Selten verwendete Datens√§tze wie Sensorwerte, Webinhalte und Multimedia werden wiederum in einem kosteng√ºnstigen Cloud-Speicher gespeichert. <br><br>  Eines der Ziele von Composite Computing ist die Verwendung von Containern, um die Leistung von Instanzen wie SQL-Engines, Graph-Engines, maschinellem Lernen und Deep-Learning-Engines zu optimieren, die auf Daten zugreifen, die √ºber verschiedene Repositorys verteilt sind.  Die Bereitstellung mehrerer analytischer Computer-Engines erm√∂glicht die Verwendung von Multiprozessormodellen, die Daten von verschiedenen Engines verwenden und in der Regel bessere Ergebnisse erzielen. <br><br>  IT-Anbieter wie Dell Technologies, Hewlett Packard Enterprise und Liquid entfernen sich allm√§hlich von herk√∂mmlichen Architekturen, die Workloads auf Computerebene zuweisen.  Stattdessen versuchen sie, einem gesamten System, das aus Zentraleinheiten, GPUs, Speicher und Speicherger√§ten besteht, KI-Workloads zuzuweisen.  F√ºr einen solchen √úbergang ist es erforderlich, neue Netzwerkkomponenten zu beherrschen, die die Geschwindigkeit erh√∂hen und die Verz√∂gerung beim Verbinden verschiedener Komponenten des Systems verringern. <br><br>  Beispielsweise verwenden viele Cloud-Rechenzentren Ethernet, um Computerkomponenten und Speicher zu verbinden, wobei die Verz√∂gerung etwa 15 Mikrosekunden betr√§gt.  Das geschaltete Hochgeschwindigkeits-Computernetzwerk von InfiniBand, das in vielen konvergierten Infrastrukturen verwendet wird, kann die Latenz um bis zu 1,5 Mikrosekunden reduzieren.  Liquid hat eine Reihe von Tools zum Verbinden verschiedener Knoten mit PCI Express (PCIE) entwickelt, mit denen die Verz√∂gerung auf 150 Nanosekunden reduziert wird. <br><br>  Dar√ºber hinaus schlagen einige Experten vor, den Arbeitsspeicher f√ºr GPUs zu erh√∂hen, die f√ºr gro√üe Lasten mit schnellen Verbindungen verwendet werden.  Beispielsweise wird DDR4 h√§ufig zusammen mit RAM verwendet, wodurch die Verz√∂gerung auf 14 Nanosekunden reduziert wird.  Dies funktioniert jedoch nur f√ºr kleine Segmente von wenigen Zentimetern. <br><br>  Little Marrek, der Gr√ºnder und Entwickler des ClusterOne AI-Verwaltungsdienstes, ist der Ansicht, dass mehr Arbeit erforderlich ist, um die Kompatibilit√§t von AI-Workloads in einer Softwareumgebung sicherzustellen.  Trotz der Tatsache, dass einige Unternehmen bereits versuchen, die Kompatibilit√§t mit Docker und Kubernetes sicherzustellen, ist es noch zu fr√ºh, denselben Ansatz auf GPUs anzuwenden. <br><br><img src="https://habrastorage.org/webt/gy/kf/vo/gykfvonm7odfn4osmkaya-lwfas.jpeg" align="left"><blockquote>  ‚ÄûIm Allgemeinen ist es nicht einfach, GPU-Workloads auszuf√ºhren und zu √ºberwachen‚Äú, sagt Marrek.  "Es gibt keine universelle L√∂sung, mit der alle Systeme √ºberwacht werden k√∂nnen." <br><br></blockquote><br><br><h3>  Speicher und GPU <br></h3><br>  Ein anderer Ansatz besteht darin, einen Grafikprozessor zu verwenden, um die Daten vorzuverarbeiten, um die f√ºr eine bestimmte Art von Analyse erforderliche Menge zu reduzieren und die Daten zu organisieren und ihnen Beschriftungen zuzuweisen.  Auf diese Weise k√∂nnen Sie einen geeigneten Datensatz f√ºr mehrere an der Verarbeitung beteiligte GPUs vorbereiten, sodass der Algorithmus aus dem Inneren des Speichers heraus arbeiten kann, anstatt Daten aus den Speichern √ºber langsame Netzwerke zu √ºbertragen. <br><br><img src="https://habrastorage.org/webt/ys/qp/cb/ysqpcbtqxhszcgd3hpgn9kutmss.jpeg" align="left"><blockquote>  "Wir betrachten Speicher, Computer und Speicher als separate Komponenten der L√∂sung, die sich in der Vergangenheit entwickelt hat, und versuchen daher, das Verarbeitungsvolumen zu erh√∂hen", sagte Alex St. John, CTO und Gr√ºnder von Nyriad Ltd., einem Unternehmen f√ºr Speichersoftware, das in Das Ergebnis der Forschung f√ºr das weltweit gr√∂√üte Radioteleskop - ein Teleskop mit einer Antennenanordnung von Quadratkilometern (SKA). </blockquote>  Je gr√∂√üer die Datenmengen sind, desto schwieriger ist es, sie zur Verarbeitung an einen anderen Ort zu verschieben. <br><br>  Das SKA-Teleskop ben√∂tigte viel Strom, um 160 TB Echtzeit-Funksignaldaten zu verarbeiten, was das Haupthindernis f√ºr die Forscher war.  Infolgedessen beschlossen sie, die in Rechenzentren am h√§ufigsten verwendeten RAID-Speicher aufzugeben und ein paralleles Cluster-Dateisystem wie BeeGFS bereitzustellen, das die Vorbereitung von Daten f√ºr AI-Workloads vereinfacht. <br><br>  IT-Direktoren, die an der optimalen Strategie f√ºr die Architektur der k√ºnstlichen Intelligenz arbeiten, sollten besonders auf die Benutzerfreundlichkeit achten.  Wenn Entwickler, Datenspezialisten sowie Entwicklungs- und Betriebsintegrationsteams die neue Technologie schnell beherrschen, k√∂nnen sie ihre Zeit und Energie in die Erstellung einer erfolgreichen Gesch√§ftslogik investieren, anstatt Bereitstellungsprobleme und Datenleitungen zu l√∂sen. <br><br>  Dar√ºber hinaus m√ºssen Unternehmen sorgf√§ltig √ºberlegen, wie viel Aufwand und Zeit erforderlich sind, um eine neue KI-Architektur in ein vorhandenes √ñkosystem zu integrieren. <br><br>  ‚ÄûVor der Implementierung neuer Infrastrukturen und der Planung gro√üer Workloads m√ºssen CIOs bewerten, wie viele ersch√∂pfbare Ressourcen erforderlich sind‚Äú, sagt Asaf Someh, Gr√ºnder und CEO von Iguazio. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de415929/">https://habr.com/ru/post/de415929/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de415917/index.html">"Fr√ºhlingsgesetz" trat in Kraft: Was kommt als n√§chstes?</a></li>
<li><a href="../de415919/index.html">Refactoring eines Programms auf Go: 23-fache Beschleunigung</a></li>
<li><a href="../de415923/index.html">Ist die Einheit langsam? Achtung LINQ</a></li>
<li><a href="../de415925/index.html">MasterCard patentierte anonyme Blockchain-Technologie</a></li>
<li><a href="../de415927/index.html">Industrielampe Breeze 50</a></li>
<li><a href="../de415933/index.html">So erstellen Sie eine IIoT-Architektur zum Selbermachen</a></li>
<li><a href="../de415935/index.html">Insertion Sorts</a></li>
<li><a href="../de415937/index.html">Die japanische Privatrakete MOMO-2 explodierte auf der Startrampe</a></li>
<li><a href="../de415939/index.html">Verteilte Grafikverarbeitung mit Spark GraphX</a></li>
<li><a href="../de415941/index.html">Wie wir versucht haben, Barcodes herauszufinden und nichts verstanden haben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>