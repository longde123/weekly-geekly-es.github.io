<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üóûÔ∏è üêÑ üë∂üèª Redes de Kubernetes: entrada üôåüèø üïé ‚ôªÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hoy publicamos una traducci√≥n de la tercera parte de la Gu√≠a de redes de Kubernetes. La primera parte fue sobre pods, la segunda sobre servicios, y ho...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes de Kubernetes: entrada</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/442646/">  Hoy publicamos una traducci√≥n de la tercera parte de la Gu√≠a de redes de Kubernetes.  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primera</a> parte fue sobre pods, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">segunda</a> sobre servicios, y hoy hablaremos sobre el equilibrio de carga y los recursos de Kubernetes del tipo Ingress. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/te/wp/ce/tewpcee5cggzqu97irog_mj_qgo.png"></div><a name="habracut"></a><h2>  <font color="#3AC1EF">El enrutamiento no es equilibrio de carga</font> </h2><br>  En el art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">anterior</a> de esta serie, consideramos una configuraci√≥n que consta de un par de hogares y un servicio al que se le asign√≥ una direcci√≥n IP llamada "IP de cl√∫ster".  Las consultas destinadas a hogares se enviaron a esta direcci√≥n.  Aqu√≠ continuaremos trabajando en nuestro sistema de capacitaci√≥n, comenzando donde nos graduamos la √∫ltima vez.  Recuerde que la direcci√≥n IP del cl√∫ster del servicio, <code>10.3.241.152</code> , pertenece a un rango de direcciones IP que es diferente de la utilizada en la red de hogar y de la utilizada en la red en la que se encuentran los nodos.  Llam√© a la red definida por este espacio de direcciones "red de servicio", aunque apenas merece un nombre especial, ya que no hay dispositivos conectados a esta red, y su espacio de direcciones, de hecho, consiste completamente en reglas de enrutamiento.  Anteriormente se demostr√≥ c√≥mo esta red se implementa sobre la base del componente Kubernetes llamado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">kube-proxy</a> e interact√∫a con el m√≥dulo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">netfilter del</a> kernel de Linux para interceptar y redirigir el tr√°fico enviado al cl√∫ster IP para trabajar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/35f/1ec/61535f1ec0169dbd13732aba4c9a5621.png"></div><br>  <i><font color="#999999">Diagrama de red</font></i> <br><br>  Hasta ahora, hablamos de "conexiones" y "solicitudes" e incluso utilizamos el concepto dif√≠cil de interpretar de "tr√°fico", pero para comprender las caracter√≠sticas del mecanismo de Kubernetes Ingress, necesitamos usar t√©rminos m√°s precisos.  Por lo tanto, las conexiones y solicitudes funcionan en el 4 ¬∞ nivel del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">modelo OSI</a> (tcp) o en el 7 ¬∞ nivel (http, rpc, etc.).  Las reglas de Netfilter son reglas de enrutamiento, funcionan con paquetes IP en el tercer nivel.  Todos los enrutadores, incluido netfilter, toman decisiones m√°s o menos basadas solo en la informaci√≥n contenida en el paquete.  En general, est√°n interesados ‚Äã‚Äãen saber de d√≥nde viene el paquete y hacia d√≥nde va.  Por lo tanto, para describir este comportamiento en t√©rminos del tercer nivel del modelo OSI, se debe decir que cada paquete destinado al servicio ubicado en <code>10.3.241.152:80</code> , que llega a la interfaz del nodo <code>eth0</code> , es procesado por netfilter y, de acuerdo con Las reglas establecidas para nuestro servicio se redirigen a la direcci√≥n IP de un hogar viable. <br><br>  Parece bastante obvio que cualquier mecanismo que usemos para permitir que clientes externos accedan a pods deber√≠a usar la misma infraestructura de enrutamiento.  Como resultado, estos clientes externos acceder√°n a la direcci√≥n IP y al puerto del cl√∫ster, ya que son el "punto de acceso" a todos los mecanismos de los que hemos hablado hasta ahora.  Nos permiten no preocuparnos sobre d√≥nde exactamente se ejecuta en un determinado momento.  Sin embargo, no es del todo obvio c√≥mo hacer que todo funcione. <br><br>  El servicio IP de cl√∫ster solo es accesible con la interfaz Ethernet del nodo.  Nada fuera del cl√∫ster sabe qu√© hacer con las direcciones del rango al que pertenece esta direcci√≥n.  ¬øC√≥mo puedo redirigir el tr√°fico desde una direcci√≥n IP p√∫blica a una direcci√≥n a la que solo se puede acceder si el paquete ya lleg√≥ al host? <br><br>  Si tratamos de encontrar una soluci√≥n a este problema, una de las cosas que se pueden hacer en el proceso de encontrar una soluci√≥n ser√° el estudio de las reglas de netfilter utilizando la utilidad <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">iptables</a> .  Si hace esto, puede descubrir algo que, a primera vista, puede parecer inusual: las reglas para el servicio no se limitan a una red de origen espec√≠fica.  Esto significa que cualquier paquete generado en cualquier lugar que llegue a la interfaz Ethernet del nodo y tenga una direcci√≥n de destino de <code>10.3.241.152:80</code> ser√° reconocido como conforme a la regla y ser√° redirigido a la sub.  ¬øPodemos darles a los clientes un cl√∫ster de IP, quiz√°s vincul√°ndolo a un nombre de dominio adecuado y luego configurar una ruta que nos permita organizar la entrega de estos paquetes a uno de los nodos? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f3/6ba/ea7/5f36baea7589e3559632de385f3f2bf6.png"></div><br>  <i><font color="#999999">Cliente externo y cl√∫ster</font></i> <br><br>  Si todo est√° configurado de esta manera, tal dise√±o resultar√° estar funcionando.  Los clientes acceden a la IP del cl√∫ster, los paquetes siguen la ruta que conduce al host y luego se redirigen a la parte inferior.  En este momento, puede parecerle que dicha soluci√≥n puede ser limitada, pero tiene algunos problemas serios.  El primero es que los nodos, de hecho, el concepto de ef√≠mero, no son particularmente diferentes a este respecto de los hogares.  Ellos, por supuesto, est√°n un poco m√°s cerca del mundo material que los pods, pero pueden migrar a nuevas m√°quinas virtuales, los cl√∫steres pueden escalar hacia arriba o hacia abajo, y as√≠ sucesivamente.  Los enrutadores funcionan en el tercer nivel del modelo OSI y los paquetes no pueden distinguir entre los servicios que funcionan normalmente y los que no funcionan correctamente.  Esperan que la pr√≥xima transici√≥n en la ruta sea accesible y estable.  Si no se puede alcanzar el nodo, la ruta dejar√° de funcionar y permanecer√° as√≠, en la mayor√≠a de los casos, mucho tiempo.  Incluso si la ruta es resistente a fallas, tal esquema conducir√° al hecho de que todo el tr√°fico externo pasa a trav√©s de un solo nodo, lo que probablemente no sea √≥ptimo. <br><br>  No importa c√≥mo llevemos el tr√°fico de clientes al sistema, debemos hacerlo para que no dependa del estado de ning√∫n nodo del cl√∫ster.  Y, de hecho, no hay una manera confiable de hacerlo utilizando solo el enrutamiento, sin alg√∫n medio de administrar activamente el enrutador.  De hecho, es precisamente este papel, el papel del sistema de control, el que juega kube-proxy en relaci√≥n con netfilter.  Extender la responsabilidad de Kubernetes a la administraci√≥n de un enrutador externo probablemente no ten√≠a mucho sentido para los arquitectos de sistemas, especialmente porque ya tenemos herramientas comprobadas para distribuir el tr√°fico de clientes a trav√©s de m√∫ltiples servidores.  Se llaman equilibradores de carga, y no es sorprendente que sean la soluci√≥n verdaderamente confiable para Kubernetes Ingress.  Para entender exactamente c√≥mo sucede esto, necesitamos levantarnos del tercer nivel de OSI y volver a hablar sobre las conexiones. <br><br>  Para utilizar el equilibrador de carga para distribuir el tr√°fico del cliente entre los nodos del cl√∫ster, necesitamos una direcci√≥n IP p√∫blica a la que los clientes puedan conectarse, y tambi√©n necesitamos las direcciones de los propios nodos a los que el equilibrador de carga puede redirigir las solicitudes.  Por las razones anteriores, no podemos simplemente crear una ruta est√°tica estable entre el enrutador de la puerta de enlace y los nodos utilizando una red basada en servicios (cl√∫ster IP). <br><br>  Entre las otras direcciones con las que puede trabajar, solo se pueden anotar las direcciones de la red a la que est√°n conectadas las interfaces Ethernet de los nodos, es decir, en este ejemplo, <code>10.100.0.0/24</code> .  El enrutador ya sabe c√≥mo reenviar paquetes a estas interfaces, y las conexiones enviadas desde el equilibrador de carga al enrutador ir√°n a donde deber√≠an ir.  Pero si el cliente quiere conectarse a nuestro servicio en el puerto 80, entonces no podemos enviar paquetes a este puerto en las interfaces de red de los nodos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4cd/c77/40b/4cdc7740be5ecfc3069a8b3fb157c605.png"></div><br>  <i><font color="#999999">Balanceador de carga, intento fallido de acceder al puerto 80 de la interfaz de red del host</font></i> <br><br>  La raz√≥n por la que esto no se puede hacer es completamente obvia.  Es decir, estamos hablando del hecho de que no hay ning√∫n proceso esperando conexiones a las <code>10.100.0.3:80</code> (y si lo hay, definitivamente este no es el mismo proceso), y las reglas de netfilter, que, como esper√°bamos, interceptar√≠an la solicitud y se lo enviar√°n, no trabajar√°n en esa direcci√≥n de destino.  Responden solo a una red IP de cl√∫ster basada en servicios, es decir, a la direcci√≥n <code>10.3.241.152:80</code> .  Como resultado, estos paquetes, a su llegada, no se pueden entregar a la direcci√≥n de destino, y el n√∫cleo emitir√° una respuesta <code>ECONNREFUSED</code> .  Esto nos pone en una posici√≥n confusa: no es f√°cil trabajar con una red para la redirecci√≥n de paquetes a la que se configura netfilter al redirigir datos desde la puerta de enlace a los nodos, y una red para la que el enrutamiento es f√°cil de configurar no es la red a la que netfilter redirige los paquetes.  Para resolver este problema, puede crear un puente entre estas redes.  Esto es exactamente lo que hace Kubernetes usando un servicio como NodePort. <br><br><h2>  <font color="#3AC1EF">Servicios como NodePort</font> </h2><br>  El servicio que, por ejemplo, creamos en el art√≠culo anterior, no tiene asignado un tipo, por lo que adopt√≥ el tipo predeterminado: <code>ClusterIP</code> .  Hay dos tipos m√°s de servicios que difieren en caracter√≠sticas adicionales, y el que nos interesa en este momento es <code>NodePort</code> .  Aqu√≠ hay un ejemplo de una descripci√≥n de servicio de este tipo: <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: NodePort selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Los servicios de tipo <code>NodePort</code> son servicios de tipo <code>ClusterIP</code> que tienen una oportunidad adicional: el acceso a ellos se puede obtener tanto por la direcci√≥n IP asignada al host como por la direcci√≥n asignada al cl√∫ster en la red de servicios.  Esto se logra de una manera bastante simple: cuando Kubernetes crea un servicio NodePort, kube-proxy asigna un puerto en el rango de 30000-32767 y abre este puerto en la interfaz <code>eth0</code> de cada nodo (de ah√≠ el nombre del tipo de servicio - <code>NodePort</code> ).  Las conexiones realizadas a este puerto (llamaremos a dichos puertos <code>NodePort</code> ) se redirigen a la IP del cl√∫ster del servicio.  Si creamos el servicio descrito anteriormente y ejecutamos el <code>kubectl get svc service-test</code> , podemos ver el puerto asignado a √©l. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME           CLUSTER-IP EXTERNAL-IP   PORT(S) AGE service-test   10.3.241.152 &lt;none&gt;        80:32213/TCP 1m</code> </pre> <br>  En este caso, el servicio tiene asignado NodePort <code>32213</code> .  Esto significa que ahora podemos conectarnos al servicio a trav√©s de cualquier nodo en nuestro cl√∫ster experimental en <code>10.100.0.2:32213</code> o en <code>10.100.0.3:32213</code> .  En este caso, el tr√°fico se redirigir√° al servicio. <br><br>  Una vez que esta parte del sistema ha ocupado su lugar, tenemos todos los fragmentos de la tuber√≠a para equilibrar la carga creada por las solicitudes del cliente en todos los nodos del cl√∫ster. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/da5/78e/735/da578e735e468dce8077f8a1d27d8490.png"></div><br>  <i><font color="#999999">Servicio NodePort</font></i> <br><br>  En la figura anterior, el cliente se conecta al equilibrador de carga a trav√©s de una direcci√≥n IP p√∫blica, el equilibrador de carga selecciona el nodo y se conecta a √©l en <code>10.100.0.3:32213</code> , kube-proxy acepta esta conexi√≥n y la redirige al servicio accesible a trav√©s del cl√∫ster IP <code>10.3.241.152:80</code> .  Aqu√≠, la solicitud se procesa con √©xito de acuerdo con las reglas establecidas por netfilter, y se redirige al pod del servidor en la direcci√≥n <code>10.0.2.2:8080</code> .  Quiz√°s todo esto parezca un poco complicado, y hasta cierto punto lo es, pero no es f√°cil encontrar una soluci√≥n m√°s sencilla que admita todas las excelentes funciones que nos brindan pods y redes basadas en servicios. <br><br>  Sin embargo, este mecanismo no est√° exento de problemas.  El uso de servicios como <code>NodePort</code> brinda a los clientes acceso a servicios utilizando un puerto no est√°ndar.  A menudo, esto no es un problema, ya que el equilibrador de carga puede proporcionarles un puerto normal y ocultar <code>NodePort</code> a los usuarios finales.  Pero en algunos escenarios, por ejemplo, cuando se usa un equilibrador de carga externo de la plataforma Google Cloud, puede ser necesario implementar <code>NodePort</code> clientes.  Cabe se√±alar que dichos puertos, adem√°s, representan recursos limitados, aunque 2768 puertos son probablemente suficientes incluso para los grupos m√°s grandes.  En la mayor√≠a de los casos, puede dejar que Kubernetes seleccione n√∫meros de puerto al azar, pero puede configurarlos usted mismo si es necesario.  Otro problema son algunas limitaciones con respecto al almacenamiento de direcciones IP de origen en las solicitudes.  Para saber c√≥mo resolver estos problemas, puede consultar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este</a> material en la documentaci√≥n de Kubernetes. <br><br>  Ports <code>NodePorts</code> es el mecanismo fundamental por el cual todo el tr√°fico externo ingresa al cl√∫ster de Kubernetes.  Sin embargo, ellos mismos no nos presentan una soluci√≥n preparada.  Por las razones anteriores, antes del cl√∫ster, si los clientes son entidades internas o externas ubicadas en una red p√∫blica, siempre se requiere tener alg√∫n tipo de equilibrador de carga. <br><br>  Los arquitectos de la plataforma, al darse cuenta de esto, proporcionaron dos formas de configurar el equilibrador de carga desde la plataforma Kubernetes.  Discutamos esto. <br><br><h2>  <font color="#3AC1EF">Servicios como LoadBalancer y recursos del tipo Ingress</font> </h2><br>  Servicios como <code>LoadBalancer</code> y recursos del tipo <code>Ingress</code> son algunos de los mecanismos de Kubernetes m√°s complejos.  Sin embargo, no pasaremos demasiado tiempo con ellos, ya que su uso no conduce a cambios fundamentales en todo lo que hemos hablado hasta ahora.  Todo el tr√°fico externo, como antes, ingresa al cl√∫ster a trav√©s de <code>NodePort</code> . <br><br>  Los arquitectos podr√≠an detenerse aqu√≠, permitiendo que aquellos que crean cl√∫steres se preocupen solo por las direcciones IP p√∫blicas y los equilibradores de carga.  De hecho, en ciertas situaciones, como iniciar un cl√∫ster en servidores regulares o en casa, esto es exactamente lo que hacen.  Pero en entornos que admiten configuraciones de recursos de red controlados por API, Kubernetes le permite configurar todo lo que necesita en un solo lugar. <br><br>  El primer enfoque para resolver este problema, el m√°s simple, es usar los servicios de <code>LoadBalancer</code> como <code>LoadBalancer</code> .  Dichos servicios tienen todas las capacidades de servicios como <code>NodePort</code> y, adem√°s, tienen la capacidad de crear rutas completas para el tr√°fico entrante, bas√°ndose en el supuesto de que el cl√∫ster se ejecuta en entornos como GCP o AWS que admiten la configuraci√≥n de recursos de red a trav√©s de la API. <br><br><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: service-test spec: type: LoadBalancer selector:   app: service_test_pod ports: - port: 80   targetPort: http</code> </pre> <br>  Si eliminamos y volvemos a crear el servicio de nuestro ejemplo en Google Kubernetes Engine, poco despu√©s, usando el <code>kubectl get svc service-test</code> , podemos verificar que la IP externa est√© asignada. <br><br><pre> <code class="plaintext hljs">$ kubectl get svc service-test NAME      CLUSTER-IP      EXTERNAL-IP PORT(S)          AGE openvpn   10.3.241.52     35.184.97.156 80:32213/TCP     5m</code> </pre> <br>  Se dice anteriormente que podremos verificar el hecho de asignar una direcci√≥n IP externa "pronto", a pesar del hecho de que la asignaci√≥n de una IP externa puede tomar varios minutos, lo cual no es sorprendente dada la cantidad de recursos que deben llevarse a un estado saludable.  En la plataforma GCP, por ejemplo, esto requiere que el sistema cree una direcci√≥n IP externa, reglas de redirecci√≥n de tr√°fico, un servidor proxy de destino, un servicio de back-end y, posiblemente, una instancia de un grupo.  Despu√©s de asignar una direcci√≥n IP externa, puede conectarse al servicio a trav√©s de esta direcci√≥n, asignarle un nombre de dominio e informar a los clientes.  Hasta que el servicio se destruya y se vuelva a crear (para hacer esto, raramente cuando haya una buena raz√≥n), la direcci√≥n IP no cambiar√°. <br><br>  Servicios como <code>LoadBalancer</code> tienen algunas limitaciones.  Dicho servicio no se puede configurar para descifrar el tr√°fico HTTPS.  No puede crear hosts virtuales o configurar el enrutamiento basado en rutas, por lo que no puede, utilizando configuraciones pr√°cticas, usar un solo equilibrador de carga con muchos servicios.  Estas limitaciones llevaron a la introducci√≥n de Kubernetes 1.1.  Un recurso especial para configurar equilibradores de carga.  Este es un recurso del tipo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ingress</a> .  Servicios como <code>LoadBalancer</code> tienen como objetivo ampliar las capacidades de un solo servicio para admitir clientes externos.  En contraste, los recursos de <code>Ingress</code> son recursos especiales que le permiten configurar de manera flexible los equilibradores de carga.  La API Ingress admite el descifrado del tr√°fico TLS, los hosts virtuales y el enrutamiento basado en rutas.  Con esta API, el equilibrador de carga se puede configurar f√°cilmente para que funcione con m√∫ltiples servicios de back-end. <br><br>  La API de recursos del tipo <code>Ingress</code> es demasiado grande para analizar sus caracter√≠sticas aqu√≠; adem√°s, no afecta particularmente c√≥mo funcionan los recursos Ingress a nivel de red.  La implementaci√≥n de este recurso sigue el patr√≥n habitual de Kubernetes: hay un tipo de recurso y un controlador para controlar este tipo.  El recurso en este caso es el recurso <code>Ingress</code> , que describe las solicitudes a los recursos de la red.  As√≠ es como se ver√≠a la descripci√≥n de un recurso <code>Ingress</code> . <br><br><pre> <code class="plaintext hljs">apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress annotations:   kubernetes.io/ingress.class: "gce" spec: tls:   - secretName: my-ssl-secret rules: - host: testhost.com   http:     paths:     - path: /*       backend:         serviceName: service-test         servicePort: 80</code> </pre> <br>  El controlador de Ingress es responsable de ejecutar estas solicitudes llevando otros recursos al estado deseado.  Cuando se usa Ingress, se crean servicios como <code>NodePort</code> , despu√©s de lo cual el controlador Ingress puede tomar decisiones sobre c√≥mo dirigir el tr√°fico a los nodos.  Existe una implementaci√≥n del controlador Ingress para equilibradores de carga GCE, para equilibradores AWS, para servidores proxy populares como nginx y haproxy.  Tenga en cuenta que mezclar recursos y servicios de Ingress como <code>LoadBalancer</code> puede causar problemas menores en algunos entornos.  Son f√°ciles de manejar, pero, en general, es mejor usar Ingress incluso para servicios simples. <br><br><h2>  <font color="#3AC1EF">HostPort y HostNetwork</font> </h2><br>  Lo que vamos a hablar ahora, a saber, <code>HostPort</code> y <code>HostNetwork</code> , puede atribuirse m√°s bien a la categor√≠a de rarezas interesantes, y no a herramientas √∫tiles.  De hecho, me comprometo a afirmar que en el 99,99% de los casos su uso puede considerarse un antipatr√≥n, y cualquier sistema en el que se utilicen debe someterse a una verificaci√≥n obligatoria de su arquitectura. <br><br>  Pens√© que no val√≠a la pena hablar de ellos en absoluto, pero son algo as√≠ como las herramientas utilizadas por los recursos de Ingress para procesar el tr√°fico entrante, as√≠ que decid√≠ que val√≠a la pena mencionarlos, al menos brevemente. <br><br>  Primero, <code>HostPort</code> .  Esta es una propiedad de contenedor (declarada en la estructura <code>ContainerPort</code> ).  Cuando se escribe un cierto n√∫mero de puerto, esto conduce a la apertura de este puerto en el nodo y a su redirecci√≥n directamente al contenedor.  No hay mecanismos de representaci√≥n, y el puerto se abre solo en los nodos en los que se ejecuta el contenedor.  En los primeros d√≠as de la plataforma, antes de que aparecieran los mecanismos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DaemonSet</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">StatefulSet</a> , <code>HostPort</code> era un truco que hac√≠a posible lanzar solo un contenedor de cierto tipo en cualquier nodo.  Por ejemplo, una vez us√© esto para crear un cl√∫ster <code>HostPort</code> configurando <code>HostPort</code> en <code>9200</code> y especificando tantas r√©plicas como nodos.       ,          Kubernetes,    -     <code>HostPort</code> . <br><br>   <code>NostNetwork</code> , ,   Kubernetes    ,  <code>HostPort</code> .       <code>true</code> ,       - <code>network=host</code>  <code>docker run</code> .    ,           .            <code>eth0</code>    .  ,             .      ,  ,  ,    Kubernetes,     - . <br><br><h2>  <font color="#3AC1EF">Resumen</font> </h2><br>        Kubernetes,   ,          Ingress. ,  ,    ,       Kubernetes. <br><br>  <b>Estimados lectores!</b>     Ingress? <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/442646/">https://habr.com/ru/post/442646/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../442636/index.html">¬øTraes malas noticias a la gerencia?</a></li>
<li><a href="../442638/index.html">Escalado de aplicaciones Kubernetes basado en m√©tricas de Prometheus</a></li>
<li><a href="../442640/index.html">Error perfecto: uso de Type Confusion en Flash. Parte 1</a></li>
<li><a href="../442642/index.html">Qu√© leer en marzo: 22 libros nuevos para vendedores, gerentes, desarrolladores y dise√±adores</a></li>
<li><a href="../442644/index.html">La mayor√≠a de las habilidades que no son de programaci√≥n aumentan el valor del desarrollador</a></li>
<li><a href="../442648/index.html">Ir mecanismos de asignaci√≥n</a></li>
<li><a href="../442650/index.html">An√°lisis y optimizaci√≥n de aplicaciones React.</a></li>
<li><a href="../442652/index.html">Uso de Fastify y Preact para prototipar r√°pidamente aplicaciones web</a></li>
<li><a href="../442654/index.html">Cambiar a Next.js y acelerar la carga de la p√°gina de inicio de manifold.co 7.5 veces</a></li>
<li><a href="../442658/index.html">8 trucos para trabajar con CSS: paralaje, pie de p√°gina adhesivo y otros</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>