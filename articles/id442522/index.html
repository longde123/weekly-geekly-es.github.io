<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👕 👶🏾 🏂 Intuitive RL (Reinforcement Learning): Pengantar Advantage-Actor-Critic (A2C) 🏂🏼 👨🏾‍⚖️ 🧜🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! Saya membawa kepada Anda terjemahan dari artikel oleh Rudy Gilman dan Katherine Wang Intuitive RL: Intro to Advantage-Actor-Critic (A2C) ....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Intuitive RL (Reinforcement Learning): Pengantar Advantage-Actor-Critic (A2C)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442522/"><p>  Halo, Habr!  Saya membawa kepada Anda terjemahan dari artikel oleh Rudy Gilman dan Katherine Wang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Intuitive RL: Intro to Advantage-Actor-Critic (A2C)</a> . </p><img vspace="10" src="https://habrastorage.org/webt/_g/tr/gc/_gtrgckvsc0pemxqwe4iv6sxzsu.png"><p>  Reinforced Learning Specialists (RL) telah menghasilkan banyak tutorial yang luar biasa.  Namun, sebagian besar menggambarkan RL dalam hal persamaan matematika dan diagram abstrak.  Kami suka berpikir tentang subjek dari perspektif yang berbeda.  RL sendiri terinspirasi oleh bagaimana hewan belajar, jadi mengapa tidak menerjemahkan mekanisme RL yang mendasarinya kembali ke fenomena alam yang dimaksudkan untuk disimulasikan?  Orang belajar dengan baik melalui cerita. </p><br><p>  Ini adalah kisah model Actor Advantage Critic (A2C).  Model subjek-kritik adalah bentuk populer dari model Gradient Kebijakan, yang dengan sendirinya adalah algoritma RL tradisional.  Jika Anda memahami A2C, Anda memahami RL dalam. </p><br><a name="habracut"></a><p>  Setelah Anda mendapatkan pemahaman A2C yang intuitif, periksa: </p><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Implementasi</a> kode A2C <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kami yang sederhana</a> (untuk pelatihan) atau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">versi</a> industri kami <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dari PyTorch</a> berdasarkan model <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">OpenAI TensorFlow Baselines</a> ; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pengantar RL oleh Barto &amp; Sutton</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kursus kanonik David Silver</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ulasan oleh Yusi Lee,</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">repositori Denny Brits di GitHub</a> untuk menyelam lebih dalam ke RL; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kursus fast.ai yang luar biasa</a> untuk liputan intuitif dan praktis pembelajaran mendalam secara umum, diimplementasikan di PyTorch; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Arthur Giuliani</a> RL Tutorial diimplementasikan di TensorFlow. </li></ul><p>  Ilustrasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">@embermarke</a> </p><br><p>  Dalam RL, agen, rubah Klyukovka, bergerak melalui negara-negara yang dikelilingi oleh tindakan, mencoba untuk memaksimalkan hadiah selama perjalanan. </p><img vspace="10" src="https://habrastorage.org/webt/8y/wv/jb/8ywvjb4hyom44rjn5c-nynakjua.png"><p>  A2C menerima input status - input sensor dalam kasus Klukovka - dan menghasilkan dua output: <br>  1) Penilaian terhadap berapa banyak remunerasi yang akan diterima, mulai dari saat keadaan saat ini, dengan pengecualian dari remunerasi saat ini (yang ada). <br>  2) Rekomendasi tentang tindakan apa yang harus diambil (kebijakan). <br><br>  Critic: wow, lembah yang luar biasa!  Ini akan menjadi hari yang bermanfaat untuk mencari makan!  Saya bertaruh hari ini saya akan mengumpulkan 20 poin sebelum matahari terbenam. <br>  "Subjek": bunga-bunga ini terlihat indah, saya merasakan keinginan untuk "A". </p><img vspace="10" src="https://habrastorage.org/webt/gf/_x/wf/gf_xwfrsu-z-64k9ijhxyakmoks.png"><p>  Model Deep RL adalah mesin pemetaan input-output, seperti model klasifikasi atau regresi lainnya.  Alih-alih mengkategorikan gambar atau teks, model RL yang mendalam membawa status ke tindakan dan / atau menyatakan ke nilai-nilai status.  A2C melakukan keduanya. </p><img vspace="10" src="https://habrastorage.org/webt/cb/vv/st/cbvvstverqhmym9qtsnbpptrrwc.png"><img vspace="10" src="https://habrastorage.org/webt/cq/0a/hj/cq0ahjxh9khnmukf2fnivdfsify.png"><p>  Himpunan tindakan negara ini merupakan satu pengamatan.  Dia akan menulis baris data ini ke jurnalnya, tetapi dia belum akan memikirkannya.  Dia akan mengisinya ketika dia berhenti berpikir. <br><br>  Beberapa penulis mengasosiasikan hadiah 1 dengan langkah waktu 1, yang lain mengaitkannya dengan langkah 2, tetapi semua memiliki konsep yang sama: hadiah dikaitkan dengan negara, dan tindakan segera mendahuluinya. </p><img vspace="10" src="https://habrastorage.org/webt/ht/gj/vw/htgjvw00nace9trmp4ggdse9j_g.png"><p>  Hooking mengulangi proses itu lagi.  Pertama, dia melihat sekelilingnya dan mengembangkan fungsi V (S) dan rekomendasi untuk tindakan. <br><br>  Critic: Lembah ini terlihat sangat standar.  V (S) = 19. <br>  Subjek: Opsi untuk tindakan terlihat sangat mirip.  Saya pikir saya hanya akan pergi ke jalur "C". </p><img vspace="10" src="https://habrastorage.org/webt/al/eo/oy/aleooy4igiqksso14znacoxewjq.png"><p>  Maka itu bertindak. </p><img vspace="10" src="https://habrastorage.org/webt/qd/kz/1y/qdkz1y1xfopu075_7yhhqtzmdkm.png"><p>  Menerima hadiah +20!  Dan mencatat pengamatannya. </p><img vspace="10" src="https://habrastorage.org/webt/kg/gs/-v/kggs-vsqtnvt1pos-jf9yurnl_w.png"><p>  Dia mengulangi proses itu lagi. </p><img vspace="10" src="https://habrastorage.org/webt/wy/sp/d8/wyspd8yva6igzeeg29bw5x4lfdo.png"><p>  Setelah mengumpulkan tiga pengamatan, Klyukovka berhenti untuk berpikir. <br><br>  Keluarga model lain menunggu sampai akhir hari (Monte Carlo), sementara yang lain berpikir setelah setiap langkah (satu langkah). <br>  Sebelum dapat mengatur kritik internalnya, Klukovka perlu menghitung berapa banyak poin yang akan ia terima di setiap negara bagian. <br><br>  Tapi pertama! <br>  Mari kita lihat bagaimana sepupu Klukovka, Lis Monte Carlo, menghitung arti sebenarnya dari masing-masing negara. <br><br>  Model Monte Carlo tidak mencerminkan pengalaman mereka sampai akhir permainan, dan karena nilai kondisi terakhir adalah nol, sangat mudah untuk menemukan nilai sebenarnya dari kondisi sebelumnya sebagai jumlah hadiah yang diterima setelah momen ini. </p><img vspace="10" src="https://habrastorage.org/webt/xb/4m/g9/xb4mg9-occctbf6y-ixw-yhlohu.png"><p>  Bahkan, ini hanya sampel dispersi tinggi V (S).  Agen dapat dengan mudah mengikuti lintasan yang berbeda dari negara yang sama, sehingga menerima hadiah agregat yang berbeda. </p><br><p>  Tapi Klyukovka pergi, berhenti dan berefleksi berkali-kali sampai hari berakhir.  Dia ingin tahu berapa banyak poin yang akan dia dapatkan dari setiap negara bagian hingga akhir pertandingan, karena ada beberapa jam tersisa hingga akhir pertandingan. <br><br>  Di situlah dia melakukan sesuatu yang sangat cerdas - rubah Klyukovka memperkirakan berapa banyak poin yang akan dia terima untuk keadaan terakhir dalam set ini.  Untungnya, ia memiliki penilaian yang benar tentang kondisinya - kritiknya. <br>  Dengan penilaian ini, Klyukovka dapat menghitung nilai "benar" dari negara-negara sebelumnya persis seperti yang dilakukan rubah Monte Carlo. <br><br>  Lis Monte Carlo mengevaluasi tanda target, membuat penyebaran lintasan dan menambahkan hadiah ke depan dari masing-masing negara.  A2C memotong lintasan ini dan menggantinya dengan penilaian kritiknya.  Beban awal ini mengurangi varian skor dan memungkinkan A2C berjalan terus menerus, meskipun dengan memperkenalkan bias kecil. </p><img vspace="10" src="https://habrastorage.org/webt/cw/7f/jo/cw7fjo1fqmzudzpagzrp5yqsuye.png"><p>  Hadiah sering dikurangi untuk mencerminkan fakta bahwa remunerasi sekarang lebih baik daripada di masa depan.  Untuk mempermudah, Klukovka tidak mengurangi imbalannya. </p><img vspace="10" src="https://habrastorage.org/webt/xn/2b/49/xn2b49qlafvhtjjjxd-buohiafs.png"><p>  Klukovka sekarang dapat melalui setiap baris data dan membandingkan perkiraan nilai-nilai negara dengan nilai-nilai aktualnya.  Dia menggunakan perbedaan antara angka-angka ini untuk menyempurnakan keterampilan prediksinya.  Setiap tiga langkah sepanjang hari, Klyukovka mengumpulkan pengalaman berharga yang patut dipertimbangkan. <br><br>  “Saya memberi nilai buruk pada peringkat 1 dan 2. Apa yang saya lakukan salah?  Ya!  Lain kali saya melihat bulu seperti ini, saya akan meningkatkan V (S). <br><br>  Tampaknya gila bahwa Klukovka dapat menggunakan peringkat V (S) sebagai dasar untuk membandingkannya dengan perkiraan lainnya.  Tetapi hewan (termasuk kita) melakukan ini setiap saat!  Jika Anda merasa segalanya berjalan baik, Anda tidak perlu melatih kembali tindakan yang membawa Anda ke keadaan ini. </p><img vspace="10" src="https://habrastorage.org/webt/lg/qz/to/lgqztow-bkbik5suijbm_ix4bte.png"><p>  Dengan memangkas output yang dihitung dan menggantinya dengan estimasi beban awal, kami mengganti varians Monte Carlo yang besar dengan bias yang kecil.  Model RL biasanya menderita dispersi tinggi (mewakili semua jalur yang mungkin), dan penggantian seperti itu biasanya sepadan. </p><br><p>  Klukovka mengulangi proses ini sepanjang hari, mengumpulkan tiga pengamatan tentang tindakan-tindakan-negara dan merefleksikannya. </p><img vspace="10" src="https://habrastorage.org/webt/ia/d8/r2/iad8r2v24aklliudj27dnsggaek.png"><p>  Setiap rangkaian dari tiga pengamatan adalah serangkaian kecil data pelatihan berlabel yang terkait secara otokorelasi.  Untuk mengurangi autokorelasi ini, banyak A2C melatih banyak agen secara paralel, menambahkan pengalaman mereka bersama sebelum mengirimnya ke jaringan saraf umum. </p><img vspace="10" src="https://habrastorage.org/webt/4w/qx/rd/4wqxrd2ilmgr1cjvzpymeou4dvk.png"><p>  Hari akhirnya akan berakhir.  Tinggal dua langkah lagi. <br>  Seperti yang kami katakan sebelumnya, rekomendasi tindakan Klukovka dinyatakan dalam persentase keyakinan tentang kemampuannya.  Alih-alih hanya memilih pilihan yang paling dapat diandalkan, Klukovka memilih dari distribusi tindakan ini.  Ini memastikan bahwa dia tidak selalu setuju dengan tindakan yang aman, tetapi berpotensi tindakan biasa-biasa saja. </p><br><p>  Saya bisa menyesalinya, tapi ... Kadang-kadang, menjelajahi hal-hal yang tidak diketahui, Anda bisa datang ke penemuan baru yang menarik ... </p><img vspace="10" src="https://habrastorage.org/webt/bd/w1/mq/bdw1mqtm96hfbvp6suy7ftdliec.png"><p>  Untuk lebih mendorong penelitian, nilai yang disebut entropi dikurangi dari fungsi kerugian.  Entropi berarti "ruang lingkup" dari distribusi tindakan. <br>  - Tampaknya permainan telah terbayar! <br></p><img vspace="10" src="https://habrastorage.org/webt/nu/u1/tr/nuu1tr5-gopjruyry7z3qph7wfk.png"><p>  Atau tidak? <br><br>  Terkadang agen berada dalam kondisi di mana semua tindakan mengarah pada hasil negatif.  Namun, A2C dapat mengatasi situasi buruk dengan baik. </p><img vspace="10" src="https://habrastorage.org/webt/3w/kq/tn/3wkqtngcomlmylol0jdj79u6wfc.png"><img vspace="10" src="https://habrastorage.org/webt/va/9x/x6/va9xx61axwvy7lio5lgtyvcftzw.png"><p>  Ketika matahari terbenam, Klyukovka merenungkan serangkaian solusi terakhir. </p><img vspace="10" src="https://habrastorage.org/webt/ql/k0/sw/qlk0sw5tngzheqw6t5obdw_p_mw.png"><p>  Kami berbicara tentang bagaimana Klyukovka mengatur kritik batinnya.  Tetapi bagaimana ia menyempurnakan “subjek” batinnya?  Bagaimana dia belajar membuat pilihan yang begitu indah? </p><br><p>  Kebijakan Gradien rubah yang berpikiran sederhana akan melihat pendapatan yang sebenarnya setelah tindakan dan menyesuaikan kebijakannya untuk membuat pendapatan yang baik lebih mungkin: - Tampaknya kebijakan saya di negara ini menyebabkan hilangnya 20 poin, saya pikir di masa depan lebih baik melakukan "C" kecil kemungkinannya. <br><br>  - Tapi tunggu!  Tidak adil untuk menyalahkan tindakan "C".  Status ini memiliki nilai estimasi -100, jadi memilih "C" dan diakhiri dengan -20 sebenarnya merupakan peningkatan relatif 80!  Saya harus membuat "C" lebih mungkin di masa depan. <br><br>  Alih-alih menyesuaikan kebijakannya dalam menanggapi total pendapatan yang diterima dengan memilih tindakan C, ia menyesuaikan tindakannya dengan pendapatan relatif dari tindakan C. Ini disebut “keuntungan”. </p><img vspace="10" src="https://habrastorage.org/webt/tl/l6/zq/tll6zqru_0k4izo07o3gyutrsdi.png"><p>  Apa yang kami sebut keuntungan hanyalah kesalahan.  Sebagai keuntungan, Klukovka menggunakannya untuk membuat kegiatan yang ternyata bagus, lebih mungkin.  Sebagai kesalahan, dia menggunakan jumlah yang sama untuk mendorong kritik internalnya untuk meningkatkan penilaiannya terhadap nilai status. <br><br>  Subjek mengambil keuntungan dari: <br>  - "Wow, itu bekerja lebih baik daripada yang saya kira, tindakan C pasti ide yang bagus." <br>  Pengkritik menggunakan kesalahan: <br>  “Tapi mengapa aku terkejut?  Saya mungkin seharusnya tidak mengevaluasi kondisi ini secara negatif. " <br><br>  Sekarang kami dapat menunjukkan bagaimana total kerugian dihitung - kami meminimalkan fungsi ini untuk meningkatkan model kami. <br>  “Total kerugian = kehilangan tindakan + kehilangan nilai - entropi” <br><br>  Harap perhatikan bahwa untuk menghitung gradien dari tiga jenis yang berbeda secara kualitatif, kami mengambil nilai “hingga satu”.  Ini efektif, tetapi dapat membuat konvergensi lebih sulit. </p><img vspace="10" src="https://habrastorage.org/webt/qa/2x/oa/qa2xoa3zbrhfvjiuvej7awoas9e.png"><p>  Seperti semua hewan, seiring bertambahnya usia Klyukovka, ia akan mengasah kemampuannya untuk memprediksi nilai-nilai negara, mendapatkan lebih banyak kepercayaan pada tindakannya, dan lebih jarang terkejut dengan penghargaan. </p><br><p>  Agen RL, seperti Klukovka, tidak hanya menghasilkan semua data yang diperlukan, hanya berinteraksi dengan lingkungan, tetapi juga mengevaluasi label target sendiri.  Itu benar, model RL memperbarui nilai sebelumnya untuk lebih cocok dengan nilai baru dan lebih baik. <br><br>  David Silver, kepala kelompok RL di Google Deepmind mengatakan: AI = DL + RL.  Ketika seorang agen seperti Klyukovka dapat mengatur kecerdasannya sendiri, kemungkinannya tidak terbatas ... </p><img vspace="10" src="https://habrastorage.org/webt/ea/5k/fm/ea5kfmjm_1hzvkjupaxfpiaucju.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id442522/">https://habr.com/ru/post/id442522/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id442512/index.html">Kustomisasi Django ORM pada contoh ZomboDB</a></li>
<li><a href="../id442514/index.html">Sistem terdistribusi. Pola desain. Ulasan Buku</a></li>
<li><a href="../id442516/index.html">Panduan Pandas untuk Analisis Data Besar</a></li>
<li><a href="../id442518/index.html">10 Teknik Peretasan Web Terbaik 2018</a></li>
<li><a href="../id442520/index.html">Kasing. Hemat 300 000 p. per bulan pada iklan kontekstual</a></li>
<li><a href="../id442524/index.html">Cara meningkatkan keamanan dalam identifikasi pribadi dan sistem kontrol akses</a></li>
<li><a href="../id442526/index.html">Sejarah pemutar kaset Soviet (bagian dua): booming Walkmen, gadget untuk KGB dan tape recorder</a></li>
<li><a href="../id442528/index.html">Cara membuat game bekerja pada 60fps</a></li>
<li><a href="../id442530/index.html">Wireshark 3.0.0: ulasan inovasi</a></li>
<li><a href="../id442532/index.html">Perekam video untuk pengawasan video - gratis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>