<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ§œğŸ¼ ğŸ™†ğŸ» ğŸ†š Glusterfs + LÃ¶schcodierung: Wenn Sie viel brauchen, billig und zuverlÃ¤ssig ğŸ¤¸ğŸ¿ ğŸ‡ğŸ¿ ğŸ˜’</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nur wenige Menschen haben einen Glaster in Russland, und jede Erfahrung ist interessant. Wir haben es groÃŸ und industriell und nach der Diskussion im ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Glusterfs + LÃ¶schcodierung: Wenn Sie viel brauchen, billig und zuverlÃ¤ssig</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/417475/">  Nur wenige Menschen haben einen Glaster in Russland, und jede Erfahrung ist interessant.  Wir haben es groÃŸ und industriell und nach der Diskussion im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">letzten Beitrag</a> nachgefragt.  Ich habe Ã¼ber den Anfang der Erfahrung mit der Migration von Backups vom Enterprise-Speicher auf Glusterfs gesprochen. <br><br>  Das ist nicht hardcore genug.  Wir hÃ¶rten nicht auf und beschlossen, etwas Ernsthafteres zu sammeln.  Daher werden wir hier Ã¼ber Dinge wie LÃ¶schcodierung, Sharding, Neuausgleich und dessen Drosselung, Stresstests usw. sprechen. <br><br><img src="https://habrastorage.org/webt/8t/ol/2d/8tol2dsnki7fr_jfcvhxwldsxdk.jpeg"><br><br><ul><li>  Mehr Volum / Subwolum-Theorie </li><li>  heiÃŸes Ersatzteil </li><li>  heilen / heilen / wieder ins Gleichgewicht bringen </li><li>  Schlussfolgerungen nach dem Neustart von 3 Knoten (niemals tun) </li><li>  Wie wirkt sich das Aufzeichnen mit unterschiedlichen Geschwindigkeiten von verschiedenen VMs und das Ein- und Ausschalten des Shards auf die Subvolumenlast aus? </li><li>  Neuausgleich nach dem Verlassen der Scheibe </li><li>  schnelles Gleichgewicht </li></ul><br><a name="habracut"></a><h3>  Was wolltest du? </h3><br>  <b>Die Aufgabe ist einfach:</b> ein billiges, aber zuverlÃ¤ssiges GeschÃ¤ft zu sammeln.  GÃ¼nstig wie mÃ¶glich, zuverlÃ¤ssig - damit es nicht unheimlich wird, eigene Dateien zum Verkauf darauf zu speichern.  TschÃ¼ss.  Dann nach langen Tests und Backups auf einem anderen Speichersystem - auch auf Client-Systemen. <br><br>  <b>Anwendung (sequentielle E / A)</b> : <br><br>  - Backups <br>  - Testinfrastrukturen <br>  - Testen Sie den Speicher fÃ¼r schwere Mediendateien. <br>  Wir sind hier <br>  - Battle File und ernsthafte Testinfrastruktur <br>  - Speicherung wichtiger Daten. <br><br>  Wie beim letzten Mal ist die Hauptanforderung die Netzwerkgeschwindigkeit zwischen Glaster-Instanzen.  10G ist zunÃ¤chst in Ordnung. <br><br><h3>  Theorie: Was ist dispergiertes Volumen? </h3><br>  Das verteilte Volume basiert auf der Erasure Coding (EC) -Technologie, die einen ziemlich wirksamen Schutz gegen Festplatten- oder ServerausfÃ¤lle bietet.  Es ist wie RAID 5 oder 6, aber nicht wirklich.  Es speichert das codierte Fragment der Datei fÃ¼r jeden Baustein so, dass nur eine Teilmenge der in den verbleibenden Briks gespeicherten Fragmente erforderlich ist, um die Datei wiederherzustellen.  Die Anzahl der Bausteine, die ohne Verlust des Zugriffs auf Daten mÃ¶glicherweise nicht verfÃ¼gbar sind, wird vom Administrator wÃ¤hrend der Erstellung des Volumes konfiguriert. <br><br><img src="https://habrastorage.org/webt/rt/br/t1/rtbrt12s-0oyc9avxlsp32qzsus.png"><br><br><h3>  Was ist ein Subvolumen? </h3><br>  Die Essenz des Subvolumens in der GlusterFS-Terminologie manifestiert sich zusammen mit verteilten Volumes.  Bei der verteilten LÃ¶schung funktioniert die Codierung nur im Rahmen des Subwoofers.  Und im Fall von beispielsweise verteilten replizierten Daten werden Daten im Rahmen des Subwoofers repliziert. <br>  Jeder von ihnen ist auf verschiedenen Servern verteilt, wodurch sie frei verlieren oder zur Synchronisierung ausgeben kÃ¶nnen.  In der Abbildung sind die Server (physisch) grÃ¼n markiert, die SubwÃ¶lfe sind gepunktet.  Jeder von ihnen wird dem Anwendungsserver als DatentrÃ¤ger (Volume) angezeigt: <br><br><img src="https://habrastorage.org/webt/w-/fp/dj/w-fpdjqguwigusvhtprq_6su3z8.png"><br><br>  Es wurde entschieden, dass die verteilte 4 + 2-Konfiguration auf 6 Knoten ziemlich zuverlÃ¤ssig aussieht. Wir kÃ¶nnen 2 Server oder 2 Festplatten in jedem Subwoofer verlieren, wÃ¤hrend wir weiterhin Zugriff auf Daten haben. <br><br>  Wir verfÃ¼gten Ã¼ber 6 alte DELL PowerEdge R510 mit 12 FestplattensteckplÃ¤tzen und 3,5 x SATA-Laufwerken mit 48 x 2 TB.  Wenn es einen Server mit 12 FestplattensteckplÃ¤tzen gibt und bis zu 12 TB Laufwerke auf dem Markt sind, kÃ¶nnen wir im Prinzip Speicher mit bis zu 576 TB nutzbarem Speicherplatz sammeln.  Vergessen Sie jedoch nicht, dass die maximale FestplattengrÃ¶ÃŸe von Jahr zu Jahr weiter zunimmt, die Leistung jedoch stillsteht und ein Neuaufbau einer 10-12-TB-Festplatte eine Woche dauern kann. <br><br><img src="https://habrastorage.org/webt/xo/ud/6f/xoud6fl7sdtknj7vrbn_5fgslpu.png"><br><br>  <b>Volumenerstellung:</b> <br>  Eine ausfÃ¼hrliche Beschreibung zur Herstellung von Ziegeln finden Sie in meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Beitrag</a> <br><br><pre><code class="bash hljs">gluster volume create freezer disperse-data 4 redundancy 2 transport tcp \ $(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {0..7} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> {sl051s,sl052s,sl053s,sl064s,sl075s,sl078s}:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/freezer ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>)</code> </pre> <br>  Wir erstellen, aber wir haben es nicht eilig zu starten und zu mounten, da wir noch einige wichtige Parameter anwenden mÃ¼ssen. <br><br>  <b>Was wir haben:</b> <br><br><img src="https://habrastorage.org/webt/8j/il/e9/8jile9swj-qws3hgdjo3gtht3oo.png"><br><br>  Alles sieht ganz normal aus, aber es gibt eine EinschrÃ¤nkung. <br><br>  <b>Es besteht darin, ein solches Volumen auf den Steinen aufzunehmen:</b> <br>  Dateien werden einzeln in den SubwÃ¶lfen abgelegt und nicht gleichmÃ¤ÃŸig Ã¼ber sie verteilt. Daher werden wir frÃ¼her oder spÃ¤ter auf ihre GrÃ¶ÃŸe und nicht auf die GrÃ¶ÃŸe des gesamten Volumes stoÃŸen.  Die maximale DateigrÃ¶ÃŸe, die wir in dieses Repository einfÃ¼gen kÃ¶nnen, ist die verwendbare GrÃ¶ÃŸe des Subwoofers abzÃ¼glich des bereits belegten Speicherplatzes.  In meinem Fall ist es &lt;8 Tb. <br><br>  <b>Was tun?</b>  <b>Wie soll ich sein?</b> <br>  Dieses Problem wird durch Sharding oder Streifenvolumen gelÃ¶st, aber wie die Praxis gezeigt hat, funktioniert der Streifen sehr schlecht. <br><br>  Deshalb werden wir versuchen zu scherben. <br><br>  <b>Was ist Splitter, im Detail</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br>  <b>Was ist Scherben, kurz gesagt</b> : <br>  Jede Datei, die Sie in ein Volume einfÃ¼gen, wird in Teile (Shards) unterteilt, die in SubwÃ¶lfen relativ gleichmÃ¤ÃŸig angeordnet sind.  Die GrÃ¶ÃŸe des Shards wird vom Administrator festgelegt, der Standardwert betrÃ¤gt 4 MB. <br><br>  <b>Aktivieren Sie das Sharding, nachdem Sie ein Volume erstellt haben, aber bevor es beginnt</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard on</code> </pre> <br>  <b>Wir stellen die GrÃ¶ÃŸe der Scherbe ein (was ist optimal? Jungs von oVirt empfehlen 512 MB)</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard-block-size 512MB</code> </pre> <br>  Empirisch stellt sich heraus, dass die tatsÃ¤chliche GrÃ¶ÃŸe des Splitters in den Ziegeln unter Verwendung des dispergierten Volumens 4 + 2 gleich der SplitterblockgrÃ¶ÃŸe / 4 ist, in unserem Fall 512 M / 4 = 128 M. <br><br>  Jeder Shard gemÃ¤ÃŸ der LÃ¶schcodierungslogik wird gemÃ¤ÃŸ den Bausteinen im Rahmen der Unterwelt mit diesen Teilen zerlegt: 4 * 128M + 2 * 128M <br><br>  <b>Zeichnen Sie die FehlerfÃ¤lle, die Gluster mit dieser Konfiguration Ã¼berlebt:</b> <br>  In dieser Konfiguration kÃ¶nnen wir den Fall von 2 Knoten oder 2 von DatentrÃ¤gern innerhalb desselben Subvolumens Ã¼berleben. <br><br>  FÃ¼r Tests haben wir uns entschlossen, den resultierenden Speicher in unsere Cloud zu verschieben und fio von virtuellen Maschinen aus auszufÃ¼hren. <br><br>  Wir aktivieren die sequentielle Aufzeichnung von 15 VMs und gehen wie folgt vor. <br><br>  <b>Neustart des 1. Knotens:</b> <br>  17:09 <br>  Es sieht unkritisch aus (~ 5 Sekunden NichtverfÃ¼gbarkeit durch den Parameter ping.timeout). <br><br>  17:19 <br>  Startete Heilung voll. <br>  Die Anzahl der HeilungseintrÃ¤ge nimmt nur zu, wahrscheinlich aufgrund des hohen Schreibaufwands fÃ¼r den Cluster. <br><br>  17:32 <br>  Es wurde beschlossen, die Aufzeichnung von der VM aus zu deaktivieren. <br>  Die Anzahl der HeilungseintrÃ¤ge begann zu sinken. <br><br>  17:50 <br>  heilen getan. <br><br>  <b>Starten Sie 2 Knoten neu:</b> <br><br>  <i>Es werden die gleichen Ergebnisse wie beim 1. Knoten beobachtet.</i> <br><br>  <b>Starten Sie 3 Knoten neu:</b> <br>  <i>Mountpunkt ausgegeben Transportendpunkt ist nicht verbunden, VMs haben Fehler erhalten.</i> <i><br></i>  <i>Nach dem Einschalten der Knoten stellte sich der Glaster ohne StÃ¶rung von unserer Seite wieder her und der Behandlungsprozess begann.</i> <br><br>  4 von 15 VMs konnten jedoch nicht steigen.  Ich habe Fehler auf dem Hypervisor gesehen: <br><br><pre> <code class="bash hljs">2018.04.27 13:21:32.719 ( volumes.py:0029): I: Attaching volume vol-BA3A1BE1 (/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1) with attach <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> generic... 2018.04.27 13:21:32.721 ( qmp.py:0166): D: Querying QEMU: __com.redhat_drive_add({<span class="hljs-string"><span class="hljs-string">'file'</span></span>: u<span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_rd'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'media'</span></span>: <span class="hljs-string"><span class="hljs-string">'disk'</span></span>, <span class="hljs-string"><span class="hljs-string">'format'</span></span>: <span class="hljs-string"><span class="hljs-string">'qcow2'</span></span>, <span class="hljs-string"><span class="hljs-string">'cache'</span></span>: <span class="hljs-string"><span class="hljs-string">'none'</span></span>, <span class="hljs-string"><span class="hljs-string">'detect-zeroes'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>, <span class="hljs-string"><span class="hljs-string">'id'</span></span>: <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_wr'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'discard'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>})... 2018.04.27 13:21:32.784 ( instance.py:0298): E: Failed to attach volume vol-BA3A1BE1 to the instance: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized Traceback (most recent call last): File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/ic/instance.py"</span></span>, line 292, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> emulation_started c2.qemu.volumes.attach(controller.qemu(), device) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/volumes.py"</span></span>, line 36, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> attach c2.qemu.query(qemu, drive_meth, drive_args) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/_init_.py"</span></span>, line 247, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> c2.qemu.qmp.query(qemu.pending_messages, qemu.qmp_socket, <span class="hljs-built_in"><span class="hljs-built_in">command</span></span>, args, suppress_logging) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/qmp.py"</span></span>, line 194, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query message[<span class="hljs-string"><span class="hljs-string">"error"</span></span>].get(<span class="hljs-string"><span class="hljs-string">"desc"</span></span>, <span class="hljs-string"><span class="hljs-string">"Unknown error"</span></span>) QmpError: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized qemu-img: Could not open <span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>: Could not <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> determining its format: Input/output error</code> </pre><br>  <b>Harte Auszahlung 3 Knoten mit deaktiviertem Sharding</b> <br><br><pre> <code class="bash hljs">Transport endpoint is not connected (107) /GLU/volumes/e0/e0bf9a42-8915-48f7-b509-2f6dd3f17549: ERROR: cannot <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> (Input/output error)</code> </pre> <br>  Wir verlieren auch Daten, eine Wiederherstellung ist nicht mÃ¶glich. <br><br>  <b>Zahle 3 Knoten vorsichtig mit Sharding zurÃ¼ck. Wird es zu DatenbeschÃ¤digungen kommen?</b> <br>  Es gibt, aber viel weniger (Zufall?), Ich habe 3 von 30 Laufwerken verloren. <br><br>  <b>Schlussfolgerungen:</b> <br><br><ol><li>  Die Heilung dieser Dateien hÃ¤ngt endlos, ein Ausgleich hilft nicht.  Wir schlieÃŸen daraus, dass die Dateien, fÃ¼r die beim Ausschalten des 3. Knotens eine aktive Aufzeichnung durchgefÃ¼hrt wurde, fÃ¼r immer verloren gehen. </li><li>  Laden Sie niemals mehr als 2 Knoten in einer 4 + 2-Konfiguration in der Produktion neu! </li><li>  Wie kÃ¶nnen Sie keine Daten verlieren, wenn Sie wirklich mehr als 3 Knoten neu starten mÃ¶chten?  P Beenden Sie die Aufnahme am EinhÃ¤ngepunkt und / oder stoppen Sie die LautstÃ¤rke. </li><li>  Knoten oder Steine â€‹â€‹sollten so schnell wie mÃ¶glich ersetzt werden.  Zu diesem Zweck ist es sehr wÃ¼nschenswert, zum Beispiel 1-2 a la Hot-Spare-Steine â€‹â€‹in jedem Knoten fÃ¼r einen schnellen Austausch zu haben.  Und noch ein Ersatzknoten mit Bausteinen fÃ¼r den Fall eines Knotendumps. </li></ol><br><img src="https://habrastorage.org/webt/-m/rj/ti/-mrjtikde2imxdydeka4w-hvjjk.png"><br><br>  Es ist auch sehr wichtig, AntriebsersatzfÃ¤lle zu testen <br><br>  <b>Abfahrten von Briks (Scheiben):</b> <b><br></b>  <b>17:20</b> <br>  Wir schlagen einen Ziegelstein aus: <br><br><pre> <code class="bash hljs">/dev/sdh 1.9T 598G 1.3T 33% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6</code> </pre> <br>  <b>17:22</b> <br><pre> <code class="bash hljs">gluster volume replace-brick freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick_spare_1/freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/freezer commit force</code> </pre> <br>  Sie kÃ¶nnen einen solchen Drawdown zum Zeitpunkt des Austauschs des Bausteins sehen (Aufzeichnung aus 1 Quelle): <br><br><img src="https://habrastorage.org/webt/jk/96/ns/jk96ns2kzhy0radczfpxlfpodso.png"><br><br>  Der Ersetzungsprozess ist ziemlich lang, mit einer geringen Aufzeichnungsstufe pro Cluster und Standardeinstellungen von 1 TB dauert die Wiederherstellung etwa einen Tag. <br><br>  <b>Einstellbare Parameter fÃ¼r die Behandlung:</b> <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> cluster.background-self-heal-count 20 <span class="hljs-comment"><span class="hljs-comment"># Default Value: 8 # Description: This specifies the number of per client self-heal jobs that can perform parallel heals in the background. gluster volume set cluster.heal-timeout 500 # Default Value: 600 # Description: time interval for checking the need to self-heal in self-heal-daemon gluster volume set cluster.self-heal-window-size 2 # Default Value: 1 # Description: Maximum number blocks per file for which self-heal process would be applied simultaneously. gluster volume set cluster.data-self-heal-algorithm diff # Default Value: (null) # Description: Select between "full", "diff". The "full" algorithm copies the entire file from source to # sink. The "diff" algorithm copies to sink only those blocks whose checksums don't match with those of # source. If no option is configured the option is chosen dynamically as follows: If the file does not exist # on one of the sinks or empty file exists or if the source file size is about the same as page size the # entire file will be read and written ie "full" algo, otherwise "diff" algo is chosen. gluster volume set cluster.self-heal-readdir-size 2KB # Default Value: 1KB # Description: readdirp size for performing entry self-heal</span></span></code> </pre> <br>  <i>Option: zerstreuen. Hintergrundheilungen</i> <i><br></i>  <i>Standardwert: 8</i> <i><br></i>  <i>Beschreibung: Mit dieser Option kÃ¶nnen Sie die Anzahl der parallelen Heilungen steuern</i> <i><br><br></i>  <i>Option: disperse.heal-wait-qlength</i> <i><br></i>  <i>Standardwert: 128</i> <i><br></i>  <i>Beschreibung: Mit dieser Option kÃ¶nnen Sie die Anzahl der Heilungen steuern, die warten kÃ¶nnen</i> <i><br><br></i>  <i>Option: disperse.shd-max-threads</i> <i><br></i>  <i>Standardwert: 1</i> <i><br></i>  <i>Beschreibung: Maximale Anzahl paralleler Heilungen, die SHD pro lokalem Stein ausfÃ¼hren kann.</i>  <i>Dies kann die Heilungszeiten erheblich verkÃ¼rzen, aber auch Ihre Steine â€‹â€‹zerdrÃ¼cken, wenn Sie nicht Ã¼ber die dafÃ¼r erforderliche Speicherhardware verfÃ¼gen.</i> <i><br><br></i>  <i>Option: disperse.shd-wait-qlength</i> <i><br></i>  <i>Standardwert: 1024</i> <i><br></i>  <i>Beschreibung: Mit dieser Option kÃ¶nnen Sie die Anzahl der Heilungen steuern, die in SHD pro Subvolumen warten kÃ¶nnen</i> <i><br><br></i>  <i>Option: disperse.cpu-Erweiterungen</i> <i><br></i>  <i>Standardwert: auto</i> <i><br></i>  <i>Beschreibung: Erzwingen Sie, dass die CPU-Erweiterungen verwendet werden, um die Galois-Feldberechnungen zu beschleunigen.</i> <i><br><br></i>  <i>Option: disperse.self-heal-window-size</i> <i><br></i>  <i>Standardwert: 1</i> <i><br></i>  <i>Beschreibung: Maximale Anzahl von BlÃ¶cken (128 KB) pro Datei, fÃ¼r die gleichzeitig ein Selbstheilungsprozess angewendet wird.</i> <br><br>  Stand: <br><br><pre> <code class="bash hljs">disperse.shd-max-threads: 6 disperse.self-heal-window-size: 4 cluster.self-heal-readdir-size: 2KB cluster.data-self-heal-algorithm: diff cluster.self-heal-window-size: 2 cluster.heal-timeout: 500 cluster.background-self-heal-count: 20 cluster.disperse-self-heal-daemon: <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> disperse.background-heals: 18</code> </pre> <br>  Mit neuen Parametern wurde 1 TB Daten in 8 Stunden fertiggestellt (dreimal schneller!) <br><br>  <b>Der unangenehme Moment ist, dass das Ergebnis ein grÃ¶ÃŸerer Brik ist als es war</b> <br><br>  <b>war:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdd 1.9T 645G 1.2T 35% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2</code> </pre> <br>  <b>wurde:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdj 1.9T 1019G 843G 55% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/hot_spare_brick_0</code> </pre> <br>  Es ist notwendig zu verstehen.  Wahrscheinlich blÃ¤st das Ding dÃ¼nne Scheiben auf.  Mit dem anschlieÃŸenden Austausch des erhÃ¶hten Ziegels blieb die GrÃ¶ÃŸe gleich. <br><br>  <b>Neuausrichtung:</b> <br>  <i>Nach dem Erweitern oder Verkleinern (ohne Migrieren von Daten) eines Volumes (mit den Befehlen add-paint bzw. remove-paint) mÃ¼ssen Sie die Daten zwischen den Servern neu verteilen.</i>  <i>In einem nicht replizierten Volume sollten alle Bausteine â€‹â€‹aktiv sein, um den Ersetzungsvorgang auszufÃ¼hren (Startoption).</i>  <i>In einem replizierten Volume sollte mindestens einer der Bausteine â€‹â€‹im Replikat aktiv sein.</i> <br><br>  <b>Ausgleich gestalten:</b> <br><br>  <i>Option: cluster.rebal-throttle</i> <i><br></i>  <i>Standardwert: normal</i> <i><br></i>  <i>Beschreibung: Legt die maximale Anzahl paralleler Dateimigrationen fest, die auf einem Knoten wÃ¤hrend des Neuausgleichsvorgangs zulÃ¤ssig sind.</i>  <i>Der Standardwert ist normal und erlaubt maximal [($ (Verarbeitungseinheiten) - 4) / 2), 2] Dateien zu b</i> <i><br></i>  <i>Wir sind gleichzeitig migriert.</i>  <i>Lazy erlaubt jeweils nur die Migration einer Datei und aggressiv erlaubt maximal [($ (Verarbeitungseinheiten) - 4) / 2), 4]</i> <br><br>  <i>Option: cluster.lock-Migration</i> <i><br></i>  <i>Standardwert: aus</i> <i><br></i>  <i>Beschreibung: Wenn diese Funktion aktiviert ist, werden die mit einer Datei verknÃ¼pften Posix-Sperren wÃ¤hrend des Neuausgleichs migriert</i> <br><br>  <i>Option: cluster.weighted-rebalance</i> <i><br></i>  <i>Standardwert: ein</i> <i><br></i>  <i>Beschreibung: Wenn diese Option aktiviert ist, werden Dateien mit einer Wahrscheinlichkeit proportional zu ihrer GrÃ¶ÃŸe den Bausteinen zugewiesen.</i>  <i>Andernfalls haben alle Steine â€‹â€‹die gleiche Wahrscheinlichkeit (Legacy-Verhalten).</i> <br><br>  <b>Vergleich des Schreibens und anschlieÃŸenden Lesens derselben fio-Parameter (detailliertere Ergebnisse von Leistungstests - in PM):</b> <br><br><pre> <code class="bash hljs">fio --fallocate=keep --ioengine=libaio --direct=1 --buffered=0 --iodepth=1 --bs=64k --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --rw=write/<span class="hljs-built_in"><span class="hljs-built_in">read</span></span> --filename=/dev/vdb --runtime=6000</code> </pre><br><img src="https://habrastorage.org/webt/fw/up/j0/fwupj0gj9m6vn25bepslaaox01e.png"><br><br><img src="https://habrastorage.org/webt/xi/yf/pd/xiyfpdsecqbfc52fudoz4nwklty.png"><br><br><img src="https://habrastorage.org/webt/nh/xp/es/nhxpestbf-gfjkcwogumbbqabc4.jpeg"><br><br>  <b>Wenn es interessant ist, vergleichen Sie die Rsync-Geschwindigkeit mit dem Verkehr zu den Glaster-Knoten:</b> <br><br><img src="https://habrastorage.org/webt/6i/cz/ox/6iczoxword1qaauuhkwm3vfkk-q.png"><br><br><img src="https://habrastorage.org/webt/42/ka/eq/42kaeqkdbcuzhq8rwdrqybgkc5u.png"><br><br>  <i>Es ist ersichtlich, dass ungefÃ¤hr 170 MB / s / Verkehr bis 110 MB / s / Nutzlast.</i>  <i>Es stellt sich heraus, dass dies 33% des zusÃ¤tzlichen Datenverkehrs sowie 1/3 der Redundanz der LÃ¶schcodierung sind.</i> <br><br>  <b>Der Speicherverbrauch auf der Serverseite mit und ohne Last Ã¤ndert sich nicht:</b> <br><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><br>  <b>Die Last auf den Cluster-Hosts mit der maximalen Last auf dem Volume:</b> <br><br><img src="https://habrastorage.org/webt/vb/u3/3c/vbu33cgi-rmgjn7c2w1guz5ps3c.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417475/">https://habr.com/ru/post/de417475/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar486156/index.html">ÙƒÙ…Ø§ Ø¹Ù„Ù…Øª ØŒ Ø«Ù… ÙƒØªØ¨ Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø¨ÙŠØ«ÙˆÙ†</a></li>
<li><a href="../ar486158/index.html">ØªØµÙˆØ± Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Ù†Ù…Ø§Ø°Ø¬ seq2seq Ù…Ø¹ Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…)</a></li>
<li><a href="../ar486164/index.html">ÙÙŠØ±ÙˆØ³ ÙƒÙˆØ±ÙˆÙ†Ø§ 2019-nCoV. Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø© Ø¹Ù† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„ØªÙ†ÙØ³ÙŠ ÙˆØ§Ù„ØªØ·Ù‡ÙŠØ±</a></li>
<li><a href="../ar486174/index.html">Ù„Ø¯ÙŠ ØµÙØ± Ø¯ÙˆØ±Ø§Ù†</a></li>
<li><a href="../de417473/index.html">VertrauenswÃ¼rdiger Speicher mit DRBD9 und Proxmox (Teil 1: NFS)</a></li>
<li><a href="../de417477/index.html">Hot Desking</a></li>
<li><a href="../de417479/index.html">Schnellere Verkettung von Zeichenfolgen zum Selbermachen in Go</a></li>
<li><a href="../de417481/index.html">Informationen zu Generatoren in JavaScript ES6 und warum es optional ist, sie zu studieren</a></li>
<li><a href="../de417483/index.html">Vergleich von JS-Frameworks: React, Vue und Hyperapp</a></li>
<li><a href="../de417485/index.html">[Lesezeichen] Systemadministrator-Spickzettel fÃ¼r Linux-Netzwerktools</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>