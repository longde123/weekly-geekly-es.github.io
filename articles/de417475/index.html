<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧜🏼 🙆🏻 🆚 Glusterfs + Löschcodierung: Wenn Sie viel brauchen, billig und zuverlässig 🤸🏿 🏇🏿 😒</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nur wenige Menschen haben einen Glaster in Russland, und jede Erfahrung ist interessant. Wir haben es groß und industriell und nach der Diskussion im ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Glusterfs + Löschcodierung: Wenn Sie viel brauchen, billig und zuverlässig</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/417475/">  Nur wenige Menschen haben einen Glaster in Russland, und jede Erfahrung ist interessant.  Wir haben es groß und industriell und nach der Diskussion im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">letzten Beitrag</a> nachgefragt.  Ich habe über den Anfang der Erfahrung mit der Migration von Backups vom Enterprise-Speicher auf Glusterfs gesprochen. <br><br>  Das ist nicht hardcore genug.  Wir hörten nicht auf und beschlossen, etwas Ernsthafteres zu sammeln.  Daher werden wir hier über Dinge wie Löschcodierung, Sharding, Neuausgleich und dessen Drosselung, Stresstests usw. sprechen. <br><br><img src="https://habrastorage.org/webt/8t/ol/2d/8tol2dsnki7fr_jfcvhxwldsxdk.jpeg"><br><br><ul><li>  Mehr Volum / Subwolum-Theorie </li><li>  heißes Ersatzteil </li><li>  heilen / heilen / wieder ins Gleichgewicht bringen </li><li>  Schlussfolgerungen nach dem Neustart von 3 Knoten (niemals tun) </li><li>  Wie wirkt sich das Aufzeichnen mit unterschiedlichen Geschwindigkeiten von verschiedenen VMs und das Ein- und Ausschalten des Shards auf die Subvolumenlast aus? </li><li>  Neuausgleich nach dem Verlassen der Scheibe </li><li>  schnelles Gleichgewicht </li></ul><br><a name="habracut"></a><h3>  Was wolltest du? </h3><br>  <b>Die Aufgabe ist einfach:</b> ein billiges, aber zuverlässiges Geschäft zu sammeln.  Günstig wie möglich, zuverlässig - damit es nicht unheimlich wird, eigene Dateien zum Verkauf darauf zu speichern.  Tschüss.  Dann nach langen Tests und Backups auf einem anderen Speichersystem - auch auf Client-Systemen. <br><br>  <b>Anwendung (sequentielle E / A)</b> : <br><br>  - Backups <br>  - Testinfrastrukturen <br>  - Testen Sie den Speicher für schwere Mediendateien. <br>  Wir sind hier <br>  - Battle File und ernsthafte Testinfrastruktur <br>  - Speicherung wichtiger Daten. <br><br>  Wie beim letzten Mal ist die Hauptanforderung die Netzwerkgeschwindigkeit zwischen Glaster-Instanzen.  10G ist zunächst in Ordnung. <br><br><h3>  Theorie: Was ist dispergiertes Volumen? </h3><br>  Das verteilte Volume basiert auf der Erasure Coding (EC) -Technologie, die einen ziemlich wirksamen Schutz gegen Festplatten- oder Serverausfälle bietet.  Es ist wie RAID 5 oder 6, aber nicht wirklich.  Es speichert das codierte Fragment der Datei für jeden Baustein so, dass nur eine Teilmenge der in den verbleibenden Briks gespeicherten Fragmente erforderlich ist, um die Datei wiederherzustellen.  Die Anzahl der Bausteine, die ohne Verlust des Zugriffs auf Daten möglicherweise nicht verfügbar sind, wird vom Administrator während der Erstellung des Volumes konfiguriert. <br><br><img src="https://habrastorage.org/webt/rt/br/t1/rtbrt12s-0oyc9avxlsp32qzsus.png"><br><br><h3>  Was ist ein Subvolumen? </h3><br>  Die Essenz des Subvolumens in der GlusterFS-Terminologie manifestiert sich zusammen mit verteilten Volumes.  Bei der verteilten Löschung funktioniert die Codierung nur im Rahmen des Subwoofers.  Und im Fall von beispielsweise verteilten replizierten Daten werden Daten im Rahmen des Subwoofers repliziert. <br>  Jeder von ihnen ist auf verschiedenen Servern verteilt, wodurch sie frei verlieren oder zur Synchronisierung ausgeben können.  In der Abbildung sind die Server (physisch) grün markiert, die Subwölfe sind gepunktet.  Jeder von ihnen wird dem Anwendungsserver als Datenträger (Volume) angezeigt: <br><br><img src="https://habrastorage.org/webt/w-/fp/dj/w-fpdjqguwigusvhtprq_6su3z8.png"><br><br>  Es wurde entschieden, dass die verteilte 4 + 2-Konfiguration auf 6 Knoten ziemlich zuverlässig aussieht. Wir können 2 Server oder 2 Festplatten in jedem Subwoofer verlieren, während wir weiterhin Zugriff auf Daten haben. <br><br>  Wir verfügten über 6 alte DELL PowerEdge R510 mit 12 Festplattensteckplätzen und 3,5 x SATA-Laufwerken mit 48 x 2 TB.  Wenn es einen Server mit 12 Festplattensteckplätzen gibt und bis zu 12 TB Laufwerke auf dem Markt sind, können wir im Prinzip Speicher mit bis zu 576 TB nutzbarem Speicherplatz sammeln.  Vergessen Sie jedoch nicht, dass die maximale Festplattengröße von Jahr zu Jahr weiter zunimmt, die Leistung jedoch stillsteht und ein Neuaufbau einer 10-12-TB-Festplatte eine Woche dauern kann. <br><br><img src="https://habrastorage.org/webt/xo/ud/6f/xoud6fl7sdtknj7vrbn_5fgslpu.png"><br><br>  <b>Volumenerstellung:</b> <br>  Eine ausführliche Beschreibung zur Herstellung von Ziegeln finden Sie in meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Beitrag</a> <br><br><pre><code class="bash hljs">gluster volume create freezer disperse-data 4 redundancy 2 transport tcp \ $(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {0..7} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> {sl051s,sl052s,sl053s,sl064s,sl075s,sl078s}:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/freezer ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>)</code> </pre> <br>  Wir erstellen, aber wir haben es nicht eilig zu starten und zu mounten, da wir noch einige wichtige Parameter anwenden müssen. <br><br>  <b>Was wir haben:</b> <br><br><img src="https://habrastorage.org/webt/8j/il/e9/8jile9swj-qws3hgdjo3gtht3oo.png"><br><br>  Alles sieht ganz normal aus, aber es gibt eine Einschränkung. <br><br>  <b>Es besteht darin, ein solches Volumen auf den Steinen aufzunehmen:</b> <br>  Dateien werden einzeln in den Subwölfen abgelegt und nicht gleichmäßig über sie verteilt. Daher werden wir früher oder später auf ihre Größe und nicht auf die Größe des gesamten Volumes stoßen.  Die maximale Dateigröße, die wir in dieses Repository einfügen können, ist die verwendbare Größe des Subwoofers abzüglich des bereits belegten Speicherplatzes.  In meinem Fall ist es &lt;8 Tb. <br><br>  <b>Was tun?</b>  <b>Wie soll ich sein?</b> <br>  Dieses Problem wird durch Sharding oder Streifenvolumen gelöst, aber wie die Praxis gezeigt hat, funktioniert der Streifen sehr schlecht. <br><br>  Deshalb werden wir versuchen zu scherben. <br><br>  <b>Was ist Splitter, im Detail</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br>  <b>Was ist Scherben, kurz gesagt</b> : <br>  Jede Datei, die Sie in ein Volume einfügen, wird in Teile (Shards) unterteilt, die in Subwölfen relativ gleichmäßig angeordnet sind.  Die Größe des Shards wird vom Administrator festgelegt, der Standardwert beträgt 4 MB. <br><br>  <b>Aktivieren Sie das Sharding, nachdem Sie ein Volume erstellt haben, aber bevor es beginnt</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard on</code> </pre> <br>  <b>Wir stellen die Größe der Scherbe ein (was ist optimal? Jungs von oVirt empfehlen 512 MB)</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard-block-size 512MB</code> </pre> <br>  Empirisch stellt sich heraus, dass die tatsächliche Größe des Splitters in den Ziegeln unter Verwendung des dispergierten Volumens 4 + 2 gleich der Splitterblockgröße / 4 ist, in unserem Fall 512 M / 4 = 128 M. <br><br>  Jeder Shard gemäß der Löschcodierungslogik wird gemäß den Bausteinen im Rahmen der Unterwelt mit diesen Teilen zerlegt: 4 * 128M + 2 * 128M <br><br>  <b>Zeichnen Sie die Fehlerfälle, die Gluster mit dieser Konfiguration überlebt:</b> <br>  In dieser Konfiguration können wir den Fall von 2 Knoten oder 2 von Datenträgern innerhalb desselben Subvolumens überleben. <br><br>  Für Tests haben wir uns entschlossen, den resultierenden Speicher in unsere Cloud zu verschieben und fio von virtuellen Maschinen aus auszuführen. <br><br>  Wir aktivieren die sequentielle Aufzeichnung von 15 VMs und gehen wie folgt vor. <br><br>  <b>Neustart des 1. Knotens:</b> <br>  17:09 <br>  Es sieht unkritisch aus (~ 5 Sekunden Nichtverfügbarkeit durch den Parameter ping.timeout). <br><br>  17:19 <br>  Startete Heilung voll. <br>  Die Anzahl der Heilungseinträge nimmt nur zu, wahrscheinlich aufgrund des hohen Schreibaufwands für den Cluster. <br><br>  17:32 <br>  Es wurde beschlossen, die Aufzeichnung von der VM aus zu deaktivieren. <br>  Die Anzahl der Heilungseinträge begann zu sinken. <br><br>  17:50 <br>  heilen getan. <br><br>  <b>Starten Sie 2 Knoten neu:</b> <br><br>  <i>Es werden die gleichen Ergebnisse wie beim 1. Knoten beobachtet.</i> <br><br>  <b>Starten Sie 3 Knoten neu:</b> <br>  <i>Mountpunkt ausgegeben Transportendpunkt ist nicht verbunden, VMs haben Fehler erhalten.</i> <i><br></i>  <i>Nach dem Einschalten der Knoten stellte sich der Glaster ohne Störung von unserer Seite wieder her und der Behandlungsprozess begann.</i> <br><br>  4 von 15 VMs konnten jedoch nicht steigen.  Ich habe Fehler auf dem Hypervisor gesehen: <br><br><pre> <code class="bash hljs">2018.04.27 13:21:32.719 ( volumes.py:0029): I: Attaching volume vol-BA3A1BE1 (/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1) with attach <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> generic... 2018.04.27 13:21:32.721 ( qmp.py:0166): D: Querying QEMU: __com.redhat_drive_add({<span class="hljs-string"><span class="hljs-string">'file'</span></span>: u<span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_rd'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'media'</span></span>: <span class="hljs-string"><span class="hljs-string">'disk'</span></span>, <span class="hljs-string"><span class="hljs-string">'format'</span></span>: <span class="hljs-string"><span class="hljs-string">'qcow2'</span></span>, <span class="hljs-string"><span class="hljs-string">'cache'</span></span>: <span class="hljs-string"><span class="hljs-string">'none'</span></span>, <span class="hljs-string"><span class="hljs-string">'detect-zeroes'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>, <span class="hljs-string"><span class="hljs-string">'id'</span></span>: <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_wr'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'discard'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>})... 2018.04.27 13:21:32.784 ( instance.py:0298): E: Failed to attach volume vol-BA3A1BE1 to the instance: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized Traceback (most recent call last): File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/ic/instance.py"</span></span>, line 292, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> emulation_started c2.qemu.volumes.attach(controller.qemu(), device) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/volumes.py"</span></span>, line 36, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> attach c2.qemu.query(qemu, drive_meth, drive_args) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/_init_.py"</span></span>, line 247, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> c2.qemu.qmp.query(qemu.pending_messages, qemu.qmp_socket, <span class="hljs-built_in"><span class="hljs-built_in">command</span></span>, args, suppress_logging) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/qmp.py"</span></span>, line 194, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query message[<span class="hljs-string"><span class="hljs-string">"error"</span></span>].get(<span class="hljs-string"><span class="hljs-string">"desc"</span></span>, <span class="hljs-string"><span class="hljs-string">"Unknown error"</span></span>) QmpError: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized qemu-img: Could not open <span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>: Could not <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> determining its format: Input/output error</code> </pre><br>  <b>Harte Auszahlung 3 Knoten mit deaktiviertem Sharding</b> <br><br><pre> <code class="bash hljs">Transport endpoint is not connected (107) /GLU/volumes/e0/e0bf9a42-8915-48f7-b509-2f6dd3f17549: ERROR: cannot <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> (Input/output error)</code> </pre> <br>  Wir verlieren auch Daten, eine Wiederherstellung ist nicht möglich. <br><br>  <b>Zahle 3 Knoten vorsichtig mit Sharding zurück. Wird es zu Datenbeschädigungen kommen?</b> <br>  Es gibt, aber viel weniger (Zufall?), Ich habe 3 von 30 Laufwerken verloren. <br><br>  <b>Schlussfolgerungen:</b> <br><br><ol><li>  Die Heilung dieser Dateien hängt endlos, ein Ausgleich hilft nicht.  Wir schließen daraus, dass die Dateien, für die beim Ausschalten des 3. Knotens eine aktive Aufzeichnung durchgeführt wurde, für immer verloren gehen. </li><li>  Laden Sie niemals mehr als 2 Knoten in einer 4 + 2-Konfiguration in der Produktion neu! </li><li>  Wie können Sie keine Daten verlieren, wenn Sie wirklich mehr als 3 Knoten neu starten möchten?  P Beenden Sie die Aufnahme am Einhängepunkt und / oder stoppen Sie die Lautstärke. </li><li>  Knoten oder Steine ​​sollten so schnell wie möglich ersetzt werden.  Zu diesem Zweck ist es sehr wünschenswert, zum Beispiel 1-2 a la Hot-Spare-Steine ​​in jedem Knoten für einen schnellen Austausch zu haben.  Und noch ein Ersatzknoten mit Bausteinen für den Fall eines Knotendumps. </li></ol><br><img src="https://habrastorage.org/webt/-m/rj/ti/-mrjtikde2imxdydeka4w-hvjjk.png"><br><br>  Es ist auch sehr wichtig, Antriebsersatzfälle zu testen <br><br>  <b>Abfahrten von Briks (Scheiben):</b> <b><br></b>  <b>17:20</b> <br>  Wir schlagen einen Ziegelstein aus: <br><br><pre> <code class="bash hljs">/dev/sdh 1.9T 598G 1.3T 33% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6</code> </pre> <br>  <b>17:22</b> <br><pre> <code class="bash hljs">gluster volume replace-brick freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick_spare_1/freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/freezer commit force</code> </pre> <br>  Sie können einen solchen Drawdown zum Zeitpunkt des Austauschs des Bausteins sehen (Aufzeichnung aus 1 Quelle): <br><br><img src="https://habrastorage.org/webt/jk/96/ns/jk96ns2kzhy0radczfpxlfpodso.png"><br><br>  Der Ersetzungsprozess ist ziemlich lang, mit einer geringen Aufzeichnungsstufe pro Cluster und Standardeinstellungen von 1 TB dauert die Wiederherstellung etwa einen Tag. <br><br>  <b>Einstellbare Parameter für die Behandlung:</b> <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> cluster.background-self-heal-count 20 <span class="hljs-comment"><span class="hljs-comment"># Default Value: 8 # Description: This specifies the number of per client self-heal jobs that can perform parallel heals in the background. gluster volume set cluster.heal-timeout 500 # Default Value: 600 # Description: time interval for checking the need to self-heal in self-heal-daemon gluster volume set cluster.self-heal-window-size 2 # Default Value: 1 # Description: Maximum number blocks per file for which self-heal process would be applied simultaneously. gluster volume set cluster.data-self-heal-algorithm diff # Default Value: (null) # Description: Select between "full", "diff". The "full" algorithm copies the entire file from source to # sink. The "diff" algorithm copies to sink only those blocks whose checksums don't match with those of # source. If no option is configured the option is chosen dynamically as follows: If the file does not exist # on one of the sinks or empty file exists or if the source file size is about the same as page size the # entire file will be read and written ie "full" algo, otherwise "diff" algo is chosen. gluster volume set cluster.self-heal-readdir-size 2KB # Default Value: 1KB # Description: readdirp size for performing entry self-heal</span></span></code> </pre> <br>  <i>Option: zerstreuen. Hintergrundheilungen</i> <i><br></i>  <i>Standardwert: 8</i> <i><br></i>  <i>Beschreibung: Mit dieser Option können Sie die Anzahl der parallelen Heilungen steuern</i> <i><br><br></i>  <i>Option: disperse.heal-wait-qlength</i> <i><br></i>  <i>Standardwert: 128</i> <i><br></i>  <i>Beschreibung: Mit dieser Option können Sie die Anzahl der Heilungen steuern, die warten können</i> <i><br><br></i>  <i>Option: disperse.shd-max-threads</i> <i><br></i>  <i>Standardwert: 1</i> <i><br></i>  <i>Beschreibung: Maximale Anzahl paralleler Heilungen, die SHD pro lokalem Stein ausführen kann.</i>  <i>Dies kann die Heilungszeiten erheblich verkürzen, aber auch Ihre Steine ​​zerdrücken, wenn Sie nicht über die dafür erforderliche Speicherhardware verfügen.</i> <i><br><br></i>  <i>Option: disperse.shd-wait-qlength</i> <i><br></i>  <i>Standardwert: 1024</i> <i><br></i>  <i>Beschreibung: Mit dieser Option können Sie die Anzahl der Heilungen steuern, die in SHD pro Subvolumen warten können</i> <i><br><br></i>  <i>Option: disperse.cpu-Erweiterungen</i> <i><br></i>  <i>Standardwert: auto</i> <i><br></i>  <i>Beschreibung: Erzwingen Sie, dass die CPU-Erweiterungen verwendet werden, um die Galois-Feldberechnungen zu beschleunigen.</i> <i><br><br></i>  <i>Option: disperse.self-heal-window-size</i> <i><br></i>  <i>Standardwert: 1</i> <i><br></i>  <i>Beschreibung: Maximale Anzahl von Blöcken (128 KB) pro Datei, für die gleichzeitig ein Selbstheilungsprozess angewendet wird.</i> <br><br>  Stand: <br><br><pre> <code class="bash hljs">disperse.shd-max-threads: 6 disperse.self-heal-window-size: 4 cluster.self-heal-readdir-size: 2KB cluster.data-self-heal-algorithm: diff cluster.self-heal-window-size: 2 cluster.heal-timeout: 500 cluster.background-self-heal-count: 20 cluster.disperse-self-heal-daemon: <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> disperse.background-heals: 18</code> </pre> <br>  Mit neuen Parametern wurde 1 TB Daten in 8 Stunden fertiggestellt (dreimal schneller!) <br><br>  <b>Der unangenehme Moment ist, dass das Ergebnis ein größerer Brik ist als es war</b> <br><br>  <b>war:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdd 1.9T 645G 1.2T 35% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2</code> </pre> <br>  <b>wurde:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdj 1.9T 1019G 843G 55% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/hot_spare_brick_0</code> </pre> <br>  Es ist notwendig zu verstehen.  Wahrscheinlich bläst das Ding dünne Scheiben auf.  Mit dem anschließenden Austausch des erhöhten Ziegels blieb die Größe gleich. <br><br>  <b>Neuausrichtung:</b> <br>  <i>Nach dem Erweitern oder Verkleinern (ohne Migrieren von Daten) eines Volumes (mit den Befehlen add-paint bzw. remove-paint) müssen Sie die Daten zwischen den Servern neu verteilen.</i>  <i>In einem nicht replizierten Volume sollten alle Bausteine ​​aktiv sein, um den Ersetzungsvorgang auszuführen (Startoption).</i>  <i>In einem replizierten Volume sollte mindestens einer der Bausteine ​​im Replikat aktiv sein.</i> <br><br>  <b>Ausgleich gestalten:</b> <br><br>  <i>Option: cluster.rebal-throttle</i> <i><br></i>  <i>Standardwert: normal</i> <i><br></i>  <i>Beschreibung: Legt die maximale Anzahl paralleler Dateimigrationen fest, die auf einem Knoten während des Neuausgleichsvorgangs zulässig sind.</i>  <i>Der Standardwert ist normal und erlaubt maximal [($ (Verarbeitungseinheiten) - 4) / 2), 2] Dateien zu b</i> <i><br></i>  <i>Wir sind gleichzeitig migriert.</i>  <i>Lazy erlaubt jeweils nur die Migration einer Datei und aggressiv erlaubt maximal [($ (Verarbeitungseinheiten) - 4) / 2), 4]</i> <br><br>  <i>Option: cluster.lock-Migration</i> <i><br></i>  <i>Standardwert: aus</i> <i><br></i>  <i>Beschreibung: Wenn diese Funktion aktiviert ist, werden die mit einer Datei verknüpften Posix-Sperren während des Neuausgleichs migriert</i> <br><br>  <i>Option: cluster.weighted-rebalance</i> <i><br></i>  <i>Standardwert: ein</i> <i><br></i>  <i>Beschreibung: Wenn diese Option aktiviert ist, werden Dateien mit einer Wahrscheinlichkeit proportional zu ihrer Größe den Bausteinen zugewiesen.</i>  <i>Andernfalls haben alle Steine ​​die gleiche Wahrscheinlichkeit (Legacy-Verhalten).</i> <br><br>  <b>Vergleich des Schreibens und anschließenden Lesens derselben fio-Parameter (detailliertere Ergebnisse von Leistungstests - in PM):</b> <br><br><pre> <code class="bash hljs">fio --fallocate=keep --ioengine=libaio --direct=1 --buffered=0 --iodepth=1 --bs=64k --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --rw=write/<span class="hljs-built_in"><span class="hljs-built_in">read</span></span> --filename=/dev/vdb --runtime=6000</code> </pre><br><img src="https://habrastorage.org/webt/fw/up/j0/fwupj0gj9m6vn25bepslaaox01e.png"><br><br><img src="https://habrastorage.org/webt/xi/yf/pd/xiyfpdsecqbfc52fudoz4nwklty.png"><br><br><img src="https://habrastorage.org/webt/nh/xp/es/nhxpestbf-gfjkcwogumbbqabc4.jpeg"><br><br>  <b>Wenn es interessant ist, vergleichen Sie die Rsync-Geschwindigkeit mit dem Verkehr zu den Glaster-Knoten:</b> <br><br><img src="https://habrastorage.org/webt/6i/cz/ox/6iczoxword1qaauuhkwm3vfkk-q.png"><br><br><img src="https://habrastorage.org/webt/42/ka/eq/42kaeqkdbcuzhq8rwdrqybgkc5u.png"><br><br>  <i>Es ist ersichtlich, dass ungefähr 170 MB / s / Verkehr bis 110 MB / s / Nutzlast.</i>  <i>Es stellt sich heraus, dass dies 33% des zusätzlichen Datenverkehrs sowie 1/3 der Redundanz der Löschcodierung sind.</i> <br><br>  <b>Der Speicherverbrauch auf der Serverseite mit und ohne Last ändert sich nicht:</b> <br><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><br>  <b>Die Last auf den Cluster-Hosts mit der maximalen Last auf dem Volume:</b> <br><br><img src="https://habrastorage.org/webt/vb/u3/3c/vbu33cgi-rmgjn7c2w1guz5ps3c.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417475/">https://habr.com/ru/post/de417475/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar486156/index.html">كما علمت ، ثم كتب دليل التدريب في بيثون</a></li>
<li><a href="../ar486158/index.html">تصور الترجمة الآلية العصبية (نماذج seq2seq مع آلية الاهتمام)</a></li>
<li><a href="../ar486164/index.html">فيروس كورونا 2019-nCoV. أسئلة وأجوبة عن حماية الجهاز التنفسي والتطهير</a></li>
<li><a href="../ar486174/index.html">لدي صفر دوران</a></li>
<li><a href="../de417473/index.html">Vertrauenswürdiger Speicher mit DRBD9 und Proxmox (Teil 1: NFS)</a></li>
<li><a href="../de417477/index.html">Hot Desking</a></li>
<li><a href="../de417479/index.html">Schnellere Verkettung von Zeichenfolgen zum Selbermachen in Go</a></li>
<li><a href="../de417481/index.html">Informationen zu Generatoren in JavaScript ES6 und warum es optional ist, sie zu studieren</a></li>
<li><a href="../de417483/index.html">Vergleich von JS-Frameworks: React, Vue und Hyperapp</a></li>
<li><a href="../de417485/index.html">[Lesezeichen] Systemadministrator-Spickzettel für Linux-Netzwerktools</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>