<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍚 🍩 👩🏾‍🎤 Tendances en vision par ordinateur. Faits saillants ICCV 2019 🙎🏼 🧑🏿‍🤝‍🧑🏾 🎅🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les réseaux de neurones en vision par ordinateur se développent activement, de nombreuses tâches sont encore loin d'être résolues. Pour être tendance ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tendances en vision par ordinateur. Faits saillants ICCV 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/474902/"><img src="https://habrastorage.org/webt/in/lj/qf/inljqfmjnklszyujlyua8n_w0bo.jpeg"><br><br>  Les réseaux de neurones en vision par ordinateur se développent activement, de nombreuses tâches sont encore loin d'être résolues.  Pour être tendance dans votre domaine, suivez simplement les influenceurs sur Twitter et lisez les articles pertinents sur arXiv.org.  Mais nous avons eu l'occasion d'aller à la Conférence internationale sur la vision par ordinateur (ICCV) 2019. Cette année, elle se tient en Corée du Sud.  Maintenant, nous voulons partager avec les lecteurs de Habr ce que nous avons vu et appris. <br><a name="habracut"></a><br>  Nous étions nombreux à Yandex: des développeurs de véhicules sans pilote, des chercheurs et des personnes impliquées dans les tâches de CV dans les services sont arrivés.  Mais maintenant, nous voulons présenter un point de vue légèrement subjectif de notre équipe - le laboratoire d'intelligence artificielle (Yandex MILAB).  D'autres gars ont probablement regardé la conférence sous leur angle. <br><br><div class="spoiler">  <b class="spoiler_title">Que fait le laboratoire</b> <div class="spoiler_text">  Nous réalisons des projets expérimentaux liés à la génération d'images et de musique à des fins de divertissement.  Nous sommes particulièrement intéressés par les réseaux de neurones qui vous permettent de changer le contenu de l'utilisateur (pour une photo, cette tâche est appelée manipulation d'image).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un exemple du</a> résultat de notre travail de la conférence YaC 2019. </div></div><br>  Il y a beaucoup de conférences scientifiques, mais les meilleures conférences dites A * se démarquent d'elles, où des articles sur les technologies les plus intéressantes et importantes sont généralement publiés.  Il n'y a pas de liste exacte des conférences A *, voici un exemple et incomplet: NeurIPS (anciennement NIPS), ICML, SIGIR, WWW, WSDM, KDD, ACL, CVPR, ICCV, ECCV.  Les trois derniers se spécialisent dans le sujet du CV. <br><br><h2>  ICCV en bref: affiches, tutoriels, ateliers, stands </h2><br>  1075 communications ont été acceptées lors de la conférence, les participants étaient 7 500. 103 personnes venaient de Russie, il y avait des articles d'employés de Yandex, Skoltech, Samsung AI Center Moscow et Samara University.  Cette année, peu de grands chercheurs ont visité l'ICCV, mais ici, par exemple, Alexey (Alyosha) Efros, qui rassemble toujours beaucoup de monde: <br><br><img src="https://habrastorage.org/webt/4g/ie/3w/4gie3wyqaablh0wmnxbq4ucdwbs.jpeg"><br><br><div class="spoiler">  <b class="spoiler_title">Statistiques</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/1e/lz/am/1elzamxsr2xf9k_gqrwvvvrwey0.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/vb/yr/1i/vbyr1im56rz6ib-6sj_0fjiokjc.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/l6/wc/iy/l6wciy-qakwq9wwe65hz9-pqjl8.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/jb/dr/rq/jbdrrqkeo6mw26wx3zi5taa_zog.jpeg" width="500"><br><br><img src="https://habrastorage.org/webt/2g/rb/rt/2grbrtsd1xwbzwzxfstgdck6jis.jpeg" width="500"><br></div></div><br>  Dans toutes ces conférences, les articles sont présentés sous forme d'affiches ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plus</a> sur le format), et les meilleurs sont également présentés sous forme de courts rapports. <br><br><div class="spoiler">  <b class="spoiler_title">Voici une partie du travail de la Russie</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/iu/sc/po/iuscpof__g3ikvdguuy94javuvo.jpeg"><br><br><img src="https://habrastorage.org/webt/ae/e0/xj/aee0xjbluiby-b_xxoednb7yfws.jpeg"><br><br><img src="https://habrastorage.org/webt/1s/qt/la/1sqtla2h9xgccuhu2dr1iyvxuek.jpeg"><br></div></div><br>  Sur les tutoriels, vous pouvez vous immerger dans un domaine, cela ressemble à une conférence dans une université.  Il est lu par une seule personne, généralement sans parler d'œuvres spécifiques.  Exemple de tutoriel sympa ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Michael Brown, Understanding Color and the In-Camera Image Processing Pipeline for Computer Vision</a> ): <br><br><img src="https://habrastorage.org/webt/2z/sq/e6/2zsqe6wgv1rtrl0tpvwl-eu5enw.jpeg"><br><br>  Dans les ateliers, au contraire, ils parlent d'articles.  Habituellement, il s'agit de travaux sur un sujet étroit, d'histoires de chefs de laboratoire sur tous les derniers travaux d'étudiants ou d'articles qui n'ont pas été acceptés lors de la conférence principale. <br><br>  Les entreprises sponsors viennent à l'ICCV avec des stands.  Cette année, Google, Facebook, Amazon et de nombreuses autres sociétés internationales sont arrivées, ainsi qu'un grand nombre de startups - coréennes et chinoises.  Il y avait surtout de nombreuses startups spécialisées dans le balisage de données.  Il y a des représentations sur les stands, vous pouvez prendre des marchandises et poser des questions.  Les sociétés de parrainage organisent des fêtes de chasse.  Ils réussissent à convaincre les recruteurs que vous êtes intéressé et que vous pouvez potentiellement être interviewé.  Si vous avez publié un article (ou, en outre, fait une présentation avec lui), commencé ou terminé votre doctorat - c'est un plus, mais parfois vous pouvez vous mettre d'accord sur un stand, poser des questions intéressantes aux ingénieurs de l'entreprise. <br><br><h2>  Les tendances </h2><br>  La conférence vous permet de parcourir toute la zone CV.  Par le nombre d'affiches d'un sujet particulier, vous pouvez évaluer à quel point le sujet est chaud.  Certaines conclusions demandent des mots-clés: <br><br><img src="https://habrastorage.org/webt/7u/td/1v/7utd1vf3hcbbhtrgl3xvldtjnjc.jpeg"><br><br><h4>  Zero-shot, one-shot, few-shot, auto-supervisé et semi-supervisé: de nouvelles approches pour des problèmes longtemps étudiés </h4><br>  Les gens apprennent à utiliser les données plus efficacement.  Par exemple, dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">FUNIT,</a> vous pouvez générer des expressions faciales d'animaux qui n'étaient pas dans le jeu d'entraînement (en appliquant plusieurs images de référence dans l'application).  Les idées de Deep Image Prior ont été développées, et maintenant les réseaux <abbr title="Réseaux accusatoires génératifs, réseaux accusatoires génératifs.">GAN</abbr> peuvent être formés sur une seule image - nous en parlerons plus tard <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans les faits saillants</a> .  Vous pouvez utiliser l'autosurveillance pour la pré-formation (résoudre un problème pour lequel il est possible de synthétiser des données alignées, par exemple, pour prédire l'angle de rotation d'une image) ou pour apprendre en même temps à partir de données marquées et non étiquetées.  En ce sens, la couronne de création peut être considérée comme un article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">S4L: Apprentissage semi-supervisé auto-supervisé</a> .  Mais la pré-formation sur ImageNet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">n'aide pas toujours</a> . <br><br><img src="https://habrastorage.org/webt/gj/yy/n4/gjyyn40ktbwpjaslfp0v7c-avhi.jpeg"><br><br><img src="https://habrastorage.org/webt/x5/6-/l8/x56-l8y26lyq9unxyt0soa8koay.jpeg"><br><br><h4>  3D et 360 ° </h4><br>  Les tâches, principalement résolues pour les photos (segmentation, détection), nécessitent des recherches supplémentaires pour les modèles 3D et les vidéos panoramiques.  Nous avons vu de nombreux articles sur la conversion de RVB et <abbr title="Image à profondeur RVB. Pour chaque point, non seulement sa couleur est connue, mais aussi sa «profondeur» - la distance du point de vue / prise de vue.">RVB-D</abbr> en 3D.  Certaines tâches, telles que la détermination de la pose d'une personne (estimation de la pose), sont résolues plus naturellement si nous passons à des modèles tridimensionnels.  Mais jusqu'à présent, il n'y a pas de consensus sur la façon exacte de représenter les modèles 3D - sous la forme d'une grille, d'un nuage de points, de <abbr title="Analogues de pixels en 3D.">voxels</abbr> ou de <abbr title="Champs de distance signés - champs de distance signés.">SDF</abbr> .  Voici une autre option: <br><br><img src="https://habrastorage.org/webt/n7/i-/1x/n7i-1xtwmc5xxvs4cfsf4vt4srw.jpeg"><br><br>  Dans les panoramas, les convolutions sur la sphère se développent activement (voir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Segmentation sémantique orientée sur les sphères d'icosaèdres</a> ) et la recherche d'objets clés dans le cadre. <br><br><img src="https://habrastorage.org/webt/bk/4l/gw/bk4lgwc3dzrh_uliw83x21hskyy.png"><br><br><h4>  Définition de la posture et prédiction des mouvements humains </h4><br>  Afin de déterminer la pose en 2D, il y a déjà du succès - maintenant le focus s'est déplacé vers le travail avec plusieurs caméras et en 3D.  Par exemple, vous pouvez déterminer le squelette à travers le mur, en suivant les modifications du signal Wi-Fi lors de son passage dans le corps humain. <br><br>  Beaucoup de travail a été fait dans le domaine de la détection manuelle des points clés.  De nouveaux ensembles de données sont apparus, y compris ceux basés sur la vidéo avec des dialogues de deux personnes - vous pouvez désormais prédire les gestes des mains par audio ou texte d'une conversation!  Les mêmes progrès ont été réalisés dans les tâches d'évaluation du regard. <br><br><img src="https://habrastorage.org/webt/j0/-j/kv/j0-jkvftadbawqem0ccmm7qmdpa.jpeg"><br><br><img src="https://habrastorage.org/webt/u_/gj/j6/u_gjj6f2d-icebbun428-1e0ztc.jpeg"><br><br>  Vous pouvez également mettre en évidence un large éventail d'œuvres liées à la prédiction du mouvement humain (par exemple, la prévision du mouvement humain <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">via la peinture spatio-temporelle</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la prévision structurée aide à la modélisation 3D du mouvement humain</a> ).  La tâche est importante et, basée sur des conversations avec les auteurs, elle est le plus souvent utilisée pour analyser le comportement des piétons en conduite autonome. <br><br><h4>  Manipulation de personnes dans des photos et des vidéos, cabines d'essayage virtuelles </h4><br>  La tendance principale est de changer les images faciales en termes de paramètres interprétés.  Idées: <abbr title="Substitution d'étrangers dans la vidéo.">deepfake</abbr> sur une image, changement d'expression par rendu de visage ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PuppetGAN</a> ), changement anticipé des paramètres (par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">âge</a> ).  Les transferts de style sont passés du titre du sujet à l'application du travail.  Autre histoire - les cabines d'essayage virtuelles, elles fonctionnent presque toujours mal, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">voici un exemple de</a> démo. <br><br><img src="https://habrastorage.org/webt/u-/q9/rw/u-q9rwdkgxxor2snnsrkstmykbe.jpeg"><br><br><img src="https://habrastorage.org/webt/qw/f4/ys/qwf4ys-wamesjxss4gs30v9wbpq.jpeg"><br><br><h4>  Génération d'esquisse / graphique </h4><br>  Le développement de l'idée «Laissons la grille générer quelque chose en fonction de l'expérience précédente» est devenu différent: «Montrons à la grille quelle option nous intéresse». <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SC-FEGAN</a> vous permet de faire de la peinture guidée: l'utilisateur peut dessiner une partie du visage dans la zone effacée de l'image et obtenir l'image restaurée en fonction du rendu. <br><br><img src="https://habrastorage.org/webt/kn/pv/0d/knpv0dzfajvu2hhcbqdgiw-sbek.gif"><br><br>  Dans l'un des 25 articles Adobe pour ICCV, deux GAN sont combinés: l'un dessine une esquisse pour l'utilisateur, l'autre génère une image photo-réaliste à partir de l'esquisse ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">page du projet</a> ). <br><br><img src="https://habrastorage.org/webt/ua/ba/ap/uabaap4mv5jwm9tdc0qgsxty3ho.gif"><br><br>  Plus tôt dans la génération d'images, les graphiques n'étaient pas nécessaires, mais maintenant ils sont devenus un conteneur de connaissances sur la scène.  Le prix ICCV Best Paper Honorable Mentions a également été décerné à l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Spécification des attributs et des relations des objets dans la génération de scènes interactives</a> .  En général, vous pouvez les utiliser de différentes manières: générer des graphiques à partir d'images ou des images et des textes à partir de graphiques. <br><br><img src="https://habrastorage.org/webt/5h/qf/zw/5hqfzwxfjjt-1bnyqopp7ozoqi4.png"><br><br><h4>  Ré-identification des personnes et des machines, en comptant le nombre de foules (!) </h4><br>  De nombreux articles sont consacrés au suivi des personnes et à la <abbr title="Ré-identification - peut être librement traduit par «désanonymisation».">réidentification des</abbr> personnes et des machines.  Mais ce qui nous a surpris, c'est un tas d'articles sur le comptage des gens dans une foule, et tous en provenance de Chine. <br><br><div class="spoiler">  <b class="spoiler_title">Affiches</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/fh/cn/ma/fhcnma3kitjamuo8yty7hjp1loa.jpeg"><br><br><img src="https://habrastorage.org/webt/ej/_p/66/ej_p66wxpnx52yzlv97osbkys1u.jpeg"><br><br><img src="https://habrastorage.org/webt/j7/7z/bv/j77zbvjegwrfp6emzv-ddcylgmm.jpeg"><br><br><img src="https://habrastorage.org/webt/q9/lw/kr/q9lwkrpkzozcvfa609k6t5krmnw.jpeg"><br><br><img src="https://habrastorage.org/webt/3x/rv/1i/3xrv1ibiocsa5cgbmdvecwxbkyu.jpeg"></div></div><br>  Mais Facebook, au contraire, anonymise la photo.  De plus, il le fait de manière intéressante: il apprend au réseau neuronal à générer un visage sans détails uniques - similaire, mais pas tellement qu'il est correctement détecté par les systèmes de reconnaissance faciale. <br><br><img src="https://habrastorage.org/webt/jg/az/oe/jgazoe4ptlfckqpaxdgri2kdlwc.jpeg"><br><br><h4>  Protection contre les attaques adverses </h4><br>  Avec le développement d'applications de vision par ordinateur dans le monde réel (dans les véhicules sans pilote, en reconnaissance faciale), la question de la fiabilité de tels systèmes se pose plus souvent.  Pour utiliser pleinement le CV, vous devez vous assurer que le système résiste aux attaques contradictoires - il n'y avait donc pas moins d'articles sur la protection contre eux que sur les attaques elles-mêmes.  Beaucoup de travail consistait à expliquer les prévisions du réseau (carte de saillance) et à mesurer la confiance dans le résultat. <br><br><h4>  Tâches combinées </h4><br>  Dans la plupart des tâches avec un seul objectif, les possibilités d'améliorer la qualité sont presque épuisées; l'un des nouveaux domaines de la croissance de la qualité consiste à enseigner aux réseaux de neurones à résoudre plusieurs problèmes similaires en même temps.  Exemples: <br>  - prédiction d'actions + prédiction de flux optique, <br>  - présentation vidéo + représentation linguistique ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VideoBERT</a> ), <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">super-résolution + HDR</a> . <br><br>  Et il y avait des articles sur la segmentation, la détermination de la posture et la réidentification des animaux! <br><br><img src="https://habrastorage.org/webt/qe/gk/fi/qegkfif0smvsnit1kjrpdbkajco.jpeg"><br><br><img src="https://habrastorage.org/webt/tz/hk/fo/tzhkfogbi5sxyvzbhu5bjaoii54.jpeg"><br><br><a name="highlights"></a><h2>  Faits saillants </h2><br>  Presque tous les articles étaient connus à l'avance, le texte était disponible sur arXiv.org.  Par conséquent, la présentation d'œuvres telles que Everybody Dance Now, FUNIT, Image2StyleGAN semble plutôt étrange - ce sont des œuvres très utiles, mais pas nouvelles du tout.  Il semble que le processus classique de publication scientifique échoue ici - la science se développe trop rapidement. <br><br>  Il est très difficile de déterminer les meilleures œuvres - il y en a beaucoup, les sujets sont différents.  Plusieurs articles ont reçu des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prix et des références</a> . <br><br>  Nous voulons mettre en évidence des travaux intéressants en termes de manipulation d'images, car c'est notre sujet.  Ils se sont avérés assez frais et intéressants pour nous (nous ne prétendons pas être objectifs). <br><br><h4>  SinGAN (prix du meilleur article) et InGAN </h4>  SinGAN: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">page du projet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arXiv</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">code</a> . <br>  INGAN: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">page du projet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arXiv</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">code</a> . <br><br>  Le développement de l'idée de Deep Image Prior par Dmitry Ulyanov, Andrea Vedaldi et Victor Lempitsky.  Au lieu de former le GAN sur un ensemble de données, les réseaux apprennent à partir de fragments de la même image afin de se souvenir des statistiques qu'il contient.  Le réseau formé vous permet d'éditer et d'animer des photos (SinGAN) ou de générer de nouvelles images de toute taille à partir des textures de l'image d'origine, tout en conservant la structure locale (InGAN). <br><br>  SinGAN: <br><br><img src="https://habrastorage.org/webt/oc/ba/pt/ocbaptuxkshaswnhnfhloplqmrm.png"><br><br>  INGAN: <br><br><img src="https://habrastorage.org/webt/xn/cc/i0/xncci0dgonpmajjiv0fisak0twa.gif"><br><br><h4>  Voir ce qu'un GAN ne peut pas générer </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Page du projet</a> . <br><br>  Les réseaux de neurones générateurs d'images reçoivent souvent un vecteur de bruit aléatoire en entrée.  Dans un réseau formé, de nombreux vecteurs d'entrée forment un espace, de petits mouvements le long desquels entraînent de petits changements dans l'image.  Grâce à l'optimisation, vous pouvez résoudre le problème inverse: trouver un vecteur d'entrée approprié pour une image du monde réel.  L'auteur montre qu'il n'est presque jamais possible de trouver une image complètement correspondante dans un réseau neuronal presque jamais.  Certains objets de l'image ne sont pas générés (apparemment, en raison de la grande variabilité de ces objets). <br><br><img src="https://habrastorage.org/webt/pv/pa/f2/pvpaf2havdxksu-mhmbus-naina.png"><br><br>  L'auteur émet l'hypothèse que le GAN ne couvre pas tout l'espace des images, mais seulement certains sous-ensembles bourrés de trous, comme du fromage.  Lorsque nous essayons d'y trouver des photos du monde réel, nous échouerons toujours, car le GAN ne génère toujours pas de vraies photos.  Vous pouvez surmonter les différences entre les images réelles et générées uniquement en modifiant le poids du réseau, c'est-à-dire en le recyclant pour une photo spécifique. <br><br><img src="https://habrastorage.org/webt/ci/vg/bp/civgbpxixfgs_76svkwewjksn3a.jpeg"><br><br>  Lorsque le réseau est recyclé pour une photo spécifique, vous pouvez essayer d'effectuer diverses manipulations avec cette image.  Dans l'exemple ci-dessous, une fenêtre a été ajoutée à la photo et le réseau a en outre généré des réflexions sur l'ensemble de cuisine.  Cela signifie que le réseau après le recyclage pour la photographie n'a pas perdu la possibilité de voir la connexion entre les objets de la scène. <br><br><img src="https://habrastorage.org/webt/ov/gp/qd/ovgpqdyldwsdptpav699a_vl2qo.jpeg"><br><br><h4>  GANalyze: vers des définitions visuelles des propriétés des images cognitives </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Page de projet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arXiv</a> . <br><br>  En utilisant l'approche de ce travail, vous pouvez visualiser et analyser ce que le réseau neuronal a appris.  Les auteurs proposent de former le GAN à la création d'images pour lesquelles le réseau générera des prédictions données.  Plusieurs réseaux ont été utilisés comme exemples dans l'article, y compris MemNet, qui prédit la mémorisation des photos.  Il s'est avéré que pour une meilleure mémorisation, l'objet sur la photo devrait: <br><br><ul><li>  être plus proche du centre </li><li>  avoir une forme ronde ou carrée et une structure simple, </li><li>  être sur un fond uniforme, </li><li>  contenir des yeux expressifs (au moins pour les photos de chiens), </li><li>  être plus lumineux, plus riche, dans certains cas - plus rouge. </li></ul><br><img src="https://habrastorage.org/webt/9y/b2/wd/9yb2wdgndc2qvmugfdk5yctxfbg.png"><br><br><h4>  Liquid Warping GAN: Un cadre unifié pour l'imitation du mouvement humain, le transfert d'apparence et la synthèse de vues innovantes </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Page du projet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arXiv</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">code</a> . <br><br>  Pipeline pour générer des photos de personnes à partir d'une seule photo.  Les auteurs montrent des exemples réussis de transfert du mouvement d'une personne à une autre, de transfert de vêtements entre les personnes et de génération de nouvelles perspectives d'une personne - le tout à partir d'une seule photographie.  Contrairement aux travaux précédents, ici, pour créer des conditions, ce ne sont pas des points clés en 2D (pose) qui sont utilisés, mais un maillage 3D du corps (pose + forme).  Les auteurs ont également compris comment transférer des informations de l'image d'origine vers l'image générée (Liquid Warping Block).  Les résultats semblent corrects, mais la résolution de l'image résultante n'est que de 256x256.  À titre de comparaison, vid2vid, qui est apparu il y a un an, est capable de générer à une résolution de 2048x1024, mais il nécessite jusqu'à 10 minutes de tournage vidéo comme un ensemble de données. <br><br><img src="https://habrastorage.org/webt/el/m1/eh/elm1ehlgjasetl9leqejn9elvbu.png"><br><br><h4>  FSGAN: Sujet Agnostic Face Swapping and Recenactment </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Page de projet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arXiv</a> . <br><br>  Au premier abord il ne semble rien d'inhabituel: du deepfake de qualité plus ou moins normale.  Mais la principale réalisation de l'œuvre est la substitution de visages sur une seule image.  Contrairement aux travaux antérieurs, une formation était requise sur une variété de photographies d'une personne en particulier.  Le pipeline s'est avéré lourd (reconstitution et segmentation, interpolation de vue, peinture, mélange) et avec beaucoup de hacks techniques, mais le résultat en vaut la peine. <br><br><img src="https://habrastorage.org/webt/43/33/zn/4333zncbuoqflhf2srkk-05e6m0.png"><br><br><h4>  Détecter l'inattendu via la resynthèse d'images </h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arXiv</a> . <br><br>  Comment un drone peut-il comprendre qu'un objet est soudainement apparu devant lui et ne tombe dans aucune classe de segmentation sémantique?  Il existe plusieurs méthodes, mais les auteurs proposent un nouvel algorithme intuitif qui fonctionne mieux que ses prédécesseurs.  La segmentation sémantique est prédite à partir de l'image d'entrée de la route.  Il est introduit dans le GAN (pix2pixHD), qui essaie de restaurer l'image d'origine uniquement à partir de la carte sémantique.  Les anomalies qui ne tombent dans aucun des segments différeront considérablement dans la source et l'image générée.  Ensuite, trois images (initiale, segmentation et reconstruite) sont soumises à un autre réseau, qui prédit des anomalies.  L'ensemble de données pour cela a été généré à partir de l'ensemble de données bien connu Cityscapes, changeant accidentellement des classes sur la segmentation sémantique.  Fait intéressant, dans ce cadre, un chien debout au milieu de la route, mais correctement segmenté (ce qui signifie qu'il y a une classe pour lui), n'est pas une anomalie, car le système a pu le reconnaître. <br><br><img src="https://habrastorage.org/webt/fc/1_/ug/fc1_ugp2xbh5qxttjgskyxprji4.png"><br><br><h2>  Conclusion </h2><br>  Avant la conférence, il est important de savoir quels sont vos intérêts scientifiques, quels sont les discours que j'aimerais aborder, avec qui parler.  Ensuite, tout sera beaucoup plus productif. <br><br>  ICCV est principalement un réseau.  Vous comprenez qu'il y a des institutions et des scientifiques de haut niveau, vous commencez à comprendre cela, à connaître des gens.  Et vous pouvez lire des articles sur arXiv - et en passant, c'est très cool que vous ne puissiez aller nulle part pour des connaissances. <br><br>  De plus, lors de la conférence, vous pouvez plonger profondément dans des sujets qui ne sont pas proches de vous, voir les tendances.  Eh bien, écrivez une liste d'articles à lire.  Si vous êtes étudiant, c'est l'occasion pour vous de vous familiariser avec un scientifique potentiel, si vous êtes de l'industrie, puis avec un nouvel employeur, et si l'entreprise, alors montrez-vous. <br><br>  Abonnez-vous à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">@loss_function_porn</a> !  C'est un projet personnel: nous sommes avec <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">karfly</a> .  Tout le travail que nous avons aimé pendant la conférence, nous avons posté ici: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">@loss_function_live</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr474902/">https://habr.com/ru/post/fr474902/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr474890/index.html">Test comparatif des caméras de vieux téléphones portables et un peu d'histoire</a></li>
<li><a href="../fr474892/index.html">Programmation pour les enfants. Cinq des meilleurs jeux HTML et JavaScript</a></li>
<li><a href="../fr474894/index.html">Résumé à travers les yeux d'un intervieweur</a></li>
<li><a href="../fr474896/index.html">Les scientifiques ont découvert un nouveau facteur dans l'administration efficace de médicaments dans la tumeur</a></li>
<li><a href="../fr474900/index.html">La puce à puce ouverte OpenTitan remplace les racines de confiance propriétaires Intel et ARM</a></li>
<li><a href="../fr474906/index.html">Xamarin.Forms - Cartographie QRCode décorative avec SkiaSharp</a></li>
<li><a href="../fr474910/index.html">Que jouer avec les enfants avant l'école</a></li>
<li><a href="../fr474912/index.html">Messages et alertes sur Android via JSON</a></li>
<li><a href="../fr474916/index.html">Appliquer l'environnement Nix-Shell dans Visual Studio Code</a></li>
<li><a href="../fr474918/index.html">Amélioration de la conception conjointe des composants électromécaniques</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>