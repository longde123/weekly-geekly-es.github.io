<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏨 👩‍🚒 🛸 Seberapa dalam jaringan saraf terlihat dan mengapa mereka membutuhkan begitu banyak memori 💻 🧑🏾‍🤝‍🧑🏼 🍨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Saat ini, grafik adalah salah satu cara yang paling dapat diterima untuk menggambarkan model yang dibuat dalam sistem pembelajaran mesin. Grafik kompu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Seberapa dalam jaringan saraf terlihat dan mengapa mereka membutuhkan begitu banyak memori</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402641/"><img src="https://habrastorage.org/getpro/geektimes/post_images/802/6f4/eab/8026f4eab4ee028d6894159b00421c56.jpg" alt="gambar"><br><br>  Saat ini, grafik adalah salah satu cara yang paling dapat diterima untuk menggambarkan model yang dibuat dalam sistem pembelajaran mesin.  Grafik komputasi ini terdiri dari simpul neuron yang dihubungkan oleh tepi sinaps yang menggambarkan hubungan antar simpul. <br><br>  Tidak seperti prosesor grafis skalar pusat atau vektor, IPU - jenis prosesor baru yang dirancang untuk pembelajaran mesin, memungkinkan Anda membuat grafik seperti itu.  Komputer yang dirancang untuk manajemen grafik adalah mesin yang ideal untuk model grafik komputasi yang dibuat sebagai bagian dari pembelajaran mesin. <br><br>  Salah satu cara termudah untuk menggambarkan cara kerja kecerdasan mesin adalah memvisualisasikannya.  Tim pengembang Graphcore telah membuat koleksi gambar-gambar ini yang ditampilkan di IPU.  Dasarnya adalah perangkat lunak Poplar, yang memvisualisasikan karya kecerdasan buatan.  Para peneliti dari perusahaan ini juga menemukan mengapa jaringan yang dalam membutuhkan begitu banyak memori, dan solusi apa yang ada. <a name="habracut"></a><br><br>  Poplar menyertakan kompiler grafis yang dibuat dari awal untuk menerjemahkan operasi standar yang digunakan sebagai bagian dari pembelajaran mesin ke dalam kode aplikasi yang sangat dioptimalkan untuk IPU.  Ini memungkinkan Anda untuk mengumpulkan grafik-grafik ini bersama-sama dengan prinsip yang sama dengan POPNN dirakit.  Perpustakaan berisi satu set berbagai jenis simpul untuk primitif umum. <br><br>  Grafik adalah paradigma yang menjadi dasar semua perangkat lunak.  Di Poplar, grafik memungkinkan Anda untuk menentukan proses perhitungan, di mana simpul melakukan operasi dan tepi menggambarkan hubungan di antara mereka.  Misalnya, jika Anda ingin menambahkan dua angka bersama-sama, Anda dapat menentukan titik dengan dua input (angka yang ingin Anda tambahkan), beberapa perhitungan (fungsi menambahkan dua angka) dan output (hasil). <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/708/107/2ad/7081072ad3e33d401cabae1f3914bccb.jpg" alt="gambar"><br><br>  Biasanya, operasi vertex jauh lebih rumit daripada dalam contoh yang dijelaskan di atas.  Seringkali mereka didefinisikan oleh program kecil yang disebut codelets (nama kode).  Abstraksi grafis menarik karena tidak membuat asumsi tentang struktur komputasi dan memecah komputasi menjadi komponen yang dapat digunakan prosesor IPU untuk bekerja. <br><br>  Poplar menggunakan abstraksi sederhana ini untuk membuat grafik yang sangat besar yang direpresentasikan sebagai gambar.  Pembuatan grafik secara terprogram berarti bahwa kita dapat menyesuaikannya dengan perhitungan spesifik yang diperlukan untuk memastikan penggunaan sumber daya IPU yang paling efisien. <br><br>  Kompiler menerjemahkan operasi standar yang digunakan dalam sistem pembelajaran mesin ke dalam kode aplikasi yang sangat dioptimalkan untuk IPU.  Kompiler grafik membuat gambar perantara dari grafik komputasi yang digunakan pada satu atau lebih perangkat IPU.  Compiler dapat menampilkan grafik komputasi ini, sehingga aplikasi yang ditulis pada tingkat struktur jaringan saraf menampilkan gambar dari grafik komputasi yang berjalan pada IPU. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/45a/eb2/4b8/45aeb24b80e244c6c16c6fc584d99678.jpg" alt="gambar"><br>  <i>Grafik pembelajaran siklus penuh AlexNet maju dan mundur</i> <br><br>  Kompiler grafis Poplar mengubah deskripsi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">AlexNet</a> menjadi grafik komputasi 18,7 juta simpul dan 115,8 juta tepi.  Pengelompokan yang terlihat jelas adalah hasil dari koneksi yang kuat antara proses di setiap lapisan jaringan dengan koneksi yang lebih mudah antar level. <br><br>  Contoh lain adalah jaringan sederhana dengan konektivitas penuh, dilatih di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">MNIST</a> - dataset sederhana untuk visi komputer, semacam "Halo, dunia" dalam pembelajaran mesin.  Jaringan sederhana untuk mengeksplorasi dataset ini membantu untuk memahami grafik yang dikendalikan oleh aplikasi Poplar.  Dengan mengintegrasikan pustaka grafik dengan lingkungan seperti TensorFlow, perusahaan menyediakan salah satu cara termudah untuk menggunakan IPU dalam aplikasi pembelajaran mesin. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/f07/3bd/4e3/f073bd4e334225380ae51b16fb2e94b6.jpg" alt="gambar"><br><br>  Setelah grafik dibangun menggunakan kompiler, itu perlu dieksekusi.  Ini dimungkinkan menggunakan Graph Engine.  Menggunakan ResNet-50 sebagai contoh, operasinya diperagakan. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/5d7/cb1/ce5/5d7cb1ce55a80f2b8e4ebefe38681710.jpg" alt="gambar"><br>  <i>Hitung ResNet-50</i> <br><br>  Arsitektur ResNet-50 memungkinkan Anda membuat jaringan yang dalam dari pengulangan partisi.  Prosesor hanya perlu menentukan partisi ini sekali dan memanggilnya kembali.  Sebagai contoh, klaster level conv4 dijalankan enam kali, tetapi hanya sekali diterapkan pada grafik.  Gambar juga menunjukkan berbagai bentuk lapisan konvolusional, karena masing-masing memiliki grafik yang dibangun sesuai dengan bentuk alami perhitungan. <br><br>  Mesin membuat dan mengontrol pelaksanaan model pembelajaran mesin menggunakan grafik yang dibuat oleh kompiler.  Setelah digunakan, Graph Engine memonitor dan merespons IPU atau perangkat yang digunakan oleh aplikasi. <br><br>  Gambar ResNet-50 menunjukkan keseluruhan model.  Pada tingkat ini, sulit untuk membedakan antara masing-masing simpul, jadi Anda harus melihat gambar yang diperbesar.  Berikut ini adalah beberapa contoh bagian dalam lapisan jaringan saraf. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/dbf/166/408/dbf166408a0b1626a4bd720e1be1dbe9.jpg" alt="gambar"><br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/d69/9c1/fb6/d699c1fb6f2e43b9f4d03b405c543186.jpg" alt="gambar"><br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/802/6f4/eab/8026f4eab4ee028d6894159b00421c56.jpg" alt="gambar"><br><br><h3>  Mengapa jaringan yang dalam membutuhkan begitu banyak memori? </h3><br>  Sejumlah besar memori yang ditempati adalah salah satu masalah terbesar dari jaringan saraf yang dalam.  Para peneliti sedang mencoba untuk berurusan dengan bandwidth terbatas perangkat DRAM, yang harus digunakan oleh sistem modern untuk menyimpan sejumlah besar bobot dan aktivasi dalam jaringan saraf yang dalam. <br><br>  Arsitektur dikembangkan menggunakan chip prosesor yang dirancang untuk pemrosesan sekuensial dan optimalisasi DRAM untuk memori kepadatan tinggi.  Antarmuka antara kedua perangkat adalah hambatan yang memperkenalkan batasan bandwidth dan menambah biaya konsumsi energi yang signifikan. <br><br>  Meskipun kita masih belum memiliki gambaran lengkap tentang otak manusia dan cara kerjanya, secara umum jelas bahwa tidak ada fasilitas penyimpanan terpisah yang besar untuk memori.  Dipercayai bahwa fungsi memori jangka panjang dan jangka pendek di otak manusia tertanam dalam struktur neuron + sinapsis.  Bahkan organisme sederhana seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">cacing</a> dengan struktur saraf otak, yang terdiri dari lebih dari 300 neuron, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">memiliki</a> beberapa tingkat fungsi memori. <br><br>  Membangun memori dalam prosesor konvensional adalah salah satu cara untuk mengatasi kemacetan memori dengan membuka bandwidth besar dengan konsumsi daya yang jauh lebih sedikit.  Namun demikian, memori pada sebuah chip adalah hal yang mahal yang tidak dirancang untuk jumlah memori yang sangat besar, yang terhubung ke prosesor pusat dan grafis yang saat ini digunakan untuk persiapan dan penyebaran jaringan saraf yang dalam. <br><br>  Oleh karena itu, penting untuk melihat bagaimana memori digunakan saat ini di unit pemrosesan terpusat dan sistem pembelajaran mendalam pada akselerator grafis, dan tanyakan pada diri sendiri: mengapa mereka membutuhkan perangkat penyimpanan memori yang begitu besar ketika otak manusia bekerja dengan baik tanpa mereka? <br><br>  Jaringan saraf memerlukan memori untuk menyimpan data input, parameter bobot dan fungsi aktivasi, karena input didistribusikan melalui jaringan.  Dalam pelatihan, aktivasi pada input harus dipertahankan hingga dapat digunakan untuk menghitung kesalahan gradien pada output. <br><br>  Sebagai contoh, jaringan ResNet 50-lapis memiliki sekitar 26 juta parameter bobot dan menghitung 16 juta aktivasi ke depan.  Jika Anda menggunakan angka floating-point 32-bit untuk menyimpan setiap bobot dan aktivasi, maka ini membutuhkan ruang sekitar 168 MB.  Menggunakan nilai akurasi yang lebih rendah untuk menyimpan skala dan aktivasi ini, kami dapat membagi dua atau bahkan empat kali lipat persyaratan penyimpanan ini. <br><br>  Masalah memori serius muncul dari kenyataan bahwa GPU bergantung pada data yang direpresentasikan sebagai vektor padat.  Oleh karena itu, mereka dapat menggunakan aliran instruksi tunggal (SIMD) untuk mencapai komputasi kepadatan tinggi.  Prosesor sentral menggunakan blok vektor serupa untuk komputasi kinerja tinggi. <br><br>  Dalam GPU, sinapsinya memiliki lebar 1024 bit, sehingga mereka menggunakan data floating point 32-bit, sehingga mereka sering memecahnya menjadi batch mini paralel dari 32 sampel untuk membuat vektor data 1024-bit.  Pendekatan untuk mengatur paralelisme vektor ini meningkatkan jumlah aktivasi sebanyak 32 kali dan kebutuhan penyimpanan lokal dengan kapasitas lebih dari 2 GB. <br><br>  GPU dan mesin lain yang dirancang untuk aljabar matriks juga dikenakan beban memori dari bobot atau aktivasi jaringan saraf.  GPU tidak dapat secara efisien melakukan konvolusi kecil yang digunakan dalam jaringan saraf yang dalam.  Oleh karena itu, transformasi yang disebut "downgrade" digunakan untuk mengubah konvolusi ini menjadi multiplikasi matriks-matriks (GEMM), yang dapat ditangani oleh akselerator grafis secara efektif. <br><br>  Memori tambahan juga diperlukan untuk menyimpan data input, nilai waktu, dan instruksi program.  Mengukur penggunaan memori saat melatih ResNet-50 pada GPU berkinerja tinggi telah menunjukkan bahwa membutuhkan lebih dari 7,5 GB DRAM lokal. <br><br>  Mungkin seseorang akan memutuskan bahwa akurasi yang lebih rendah dapat mengurangi jumlah memori yang dibutuhkan, tetapi bukan itu masalahnya.  Ketika Anda mengubah nilai data ke setengah akurasi untuk bobot dan aktivasi, Anda mengisi hanya setengah lebar vektor SIMD, menghabiskan setengah dari sumber daya komputasi yang tersedia.  Untuk mengimbangi ini, ketika Anda beralih dari akurasi penuh ke akurasi setengah pada GPU, maka Anda harus menggandakan ukuran mini-batch untuk menyebabkan paralelisme data yang cukup untuk menggunakan semua perhitungan yang tersedia.  Dengan demikian, transisi ke skala akurasi yang lebih rendah dan aktivasi pada GPU masih membutuhkan lebih dari 7.5GB memori dinamis dengan akses gratis. <br><br>  Dengan begitu banyak data yang akan disimpan, tidak mungkin memasukkan semua ini ke dalam GPU.  Pada setiap lapisan jaringan saraf convolutional, perlu untuk menyimpan keadaan DRAM eksternal, memuat lapisan jaringan berikutnya dan kemudian memuat data ke dalam sistem.  Akibatnya, antarmuka memori eksternal, yang sudah dibatasi oleh bandwidth memori, menderita beban tambahan untuk terus-menerus memuat ulang keseimbangan, serta menyimpan dan mengambil fungsi aktivasi.  Ini secara signifikan memperlambat waktu pelatihan dan secara signifikan meningkatkan konsumsi energi. <br><br>  Ada beberapa solusi untuk masalah ini.  Pertama, operasi seperti fungsi aktivasi dapat dilakukan "di tempat", memungkinkan Anda untuk menimpa input langsung ke output.  Dengan demikian, memori yang ada dapat digunakan kembali.  Kedua, peluang untuk menggunakan kembali memori dapat diperoleh dengan menganalisis ketergantungan data antara operasi pada jaringan dan distribusi memori yang sama untuk operasi yang tidak menggunakannya pada saat ini. <br><br>  Pendekatan kedua sangat efektif ketika seluruh jaringan saraf dapat dianalisis pada tahap kompilasi untuk membuat memori yang dialokasikan tetap, karena biaya manajemen memori berkurang menjadi hampir nol.  Ternyata kombinasi dari metode-metode ini mengurangi penggunaan memori jaringan saraf dua hingga tiga kali lipat. <br>  Pendekatan signifikan ketiga baru-baru ini ditemukan oleh tim Baidu Deep Speech.  Mereka menerapkan berbagai metode menghemat memori untuk mendapatkan pengurangan 16 kali lipat dalam konsumsi memori dengan fungsi aktivasi, yang memungkinkan mereka untuk melatih jaringan dengan 100 lapisan.  Sebelumnya, dengan jumlah memori yang sama, mereka bisa melatih jaringan dengan sembilan lapisan. <br><br>  Menggabungkan sumber daya memori dan pemrosesan dalam satu perangkat memiliki potensi signifikan untuk meningkatkan produktivitas dan efisiensi jaringan saraf convolutional, serta bentuk pembelajaran mesin lainnya.  Anda dapat membuat kompromi antara memori dan sumber daya komputasi untuk menyeimbangkan kemampuan dan kinerja sistem. <br><br>  Jaringan saraf dan model pengetahuan dalam metode pembelajaran mesin lainnya dapat dianggap sebagai grafik matematika.  Dalam grafik ini, sejumlah besar paralelisme terkonsentrasi.  Prosesor paralel yang dirancang untuk menggunakan konkurensi dalam grafik tidak bergantung pada mini-batch dan dapat secara signifikan mengurangi jumlah penyimpanan lokal yang diperlukan. <br><br>  Hasil penelitian modern telah menunjukkan bahwa semua metode ini dapat secara signifikan meningkatkan kinerja jaringan saraf.  Grafik modern dan unit pemrosesan pusat memiliki memori internal yang sangat terbatas, hanya beberapa megabyte total.  Arsitektur prosesor baru yang dirancang khusus untuk pembelajaran mesin memberikan keseimbangan antara memori dan komputasi on-chip, memberikan peningkatan kinerja dan efisiensi yang signifikan dibandingkan dengan unit pemrosesan sentral modern dan akselerator grafis. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id402641/">https://habr.com/ru/post/id402641/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id402629/index.html">Relay kecilku: Komputer Brainfuck itu sihir</a></li>
<li><a href="../id402631/index.html">Apa monitor denyut jantung untuk memilih di musim baru: solusi kompromi dalam tiga hingga empat ribu rubel</a></li>
<li><a href="../id402633/index.html">The Tale of Battlefield 1 dalam Full HD pada grafik terintegrasi dalam prosesor dan perakitan konsol untuk "tidak dapat rusak"</a></li>
<li><a href="../id402637/index.html">Siswa 17 tahun mengoreksi kesalahan NASA</a></li>
<li><a href="../id402639/index.html">Peter Watts tentang SOMA</a></li>
<li><a href="../id402643/index.html">"Dunia kurus." Bab 10</a></li>
<li><a href="../id402645/index.html">Dengan menggunakan program ServoStudio 12 dan papan Arduino, Anda dapat membuat robot sendiri tanpa menulis satu baris kode</a></li>
<li><a href="../id402649/index.html">Yang paling akurat di dunia: Monitor detak jantung Valencell untuk Jabra, Suunto, Atlas, Sony dan lainnya</a></li>
<li><a href="../id402651/index.html">Implan polietilen dengan berat molekul sangat tinggi menggantikan jaringan tulang atau Polimer Besi</a></li>
<li><a href="../id402653/index.html">Robomobiles memiliki masalah dengan pengendara sepeda</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>