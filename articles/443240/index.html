<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßõüèº ‚úåÔ∏è üë∏üèº Comprender el Q-learning, el problema "Caminar sobre una roca" üõ∞Ô∏è üçò üè∫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Les traigo a su atenci√≥n una traducci√≥n del art√≠culo "Entendiendo Q-Learning, el problema de Cliff Walking" de Lucas V√°zquez . 




 En la ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comprender el Q-learning, el problema "Caminar sobre una roca"</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/443240/">  Hola Habr!  Les traigo a su atenci√≥n una traducci√≥n del art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Entendiendo Q-Learning, el problema de Cliff Walking"</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Lucas V√°zquez</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ez/dl/ol/ezdlol2oj19smcejxtdlt4aon68.jpeg"></div><br><p> En la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">√∫ltima publicaci√≥n,</a> presentamos el problema "Caminar sobre una roca" y nos decidimos por un algoritmo terrible que no ten√≠a sentido.  Esta vez revelaremos los secretos de esta caja gris y veremos que no da tanto miedo. </p><br><h3>  Resumen </h3><br><p>  Llegamos a la conclusi√≥n de que al maximizar la cantidad de recompensas futuras, tambi√©n encontramos el camino m√°s r√°pido hacia la meta, por lo que nuestro objetivo ahora es encontrar una manera de hacerlo. </p><br><a name="habracut"></a><br><h3>  Introducci√≥n a Q-Learning </h3><br><ul><li>  Comencemos construyendo una tabla que mida qu√© tan bien se desempe√±ar√° una determinada acci√≥n en cualquier estado (podemos medirla con un valor escalar simple, por lo que cuanto mayor sea el valor, mejor ser√° la acci√≥n) </li><li>  Esta tabla tendr√° una fila para cada estado y una columna para cada acci√≥n.  En nuestro mundo, la cuadr√≠cula tiene 48 estados (4 por Y por 12 por X) y se permiten 4 acciones (arriba, abajo, izquierda, derecha), por lo que la tabla ser√° 48 x 4. </li><li>  Los valores almacenados en esta tabla se denominan "valores Q". </li><li>  Estas son estimaciones de la cantidad de recompensas futuras.  En otras palabras, estiman cu√°ntas recompensas m√°s podemos obtener antes del final del juego, estar en el estado S y realizar la acci√≥n A. </li><li>  Inicializamos la tabla con valores aleatorios (o alguna constante, por ejemplo, todos ceros). </li></ul><br><p>  La "tabla Q" √≥ptima tiene valores que nos permiten tomar las mejores acciones en cada estado, d√°ndonos la mejor manera de ganar al final.  El problema est√° resuelto, saludos, Robot Lords :). </p><br><img src="https://habrastorage.org/webt/5a/ii/wl/5aiiwljmx4igtrsrhrc3qoymoge.png"><br>  <i>Valores Q de los primeros cinco estados.</i> <br><br><h4>  Q-learning </h4><br><ul><li>  Q-learning es un algoritmo que "aprende" estos valores. </li><li>  A cada paso obtenemos m√°s informaci√≥n sobre el mundo. </li><li>  Esta informaci√≥n se utiliza para actualizar los valores en la tabla. </li></ul><br><p>  Por ejemplo, supongamos que estamos a un paso del objetivo (casilla [2, 11]), y si decidimos bajar, obtenemos una recompensa de 0 en lugar de -1. <br>  Podemos usar esta informaci√≥n para actualizar el valor del par estado-acci√≥n en nuestra tabla, y la pr√≥xima vez que lo visitemos, ya sabremos que movernos hacia abajo nos da una recompensa de 0. </p><br><img src="https://habrastorage.org/webt/7a/iq/u3/7aiqu3ttrz1qnypctrqvlgyd93e.png"><br><p>  ¬°Ahora podemos difundir esta informaci√≥n a√∫n m√°s!  Dado que ahora conocemos el camino hacia la meta desde el cuadrado [2, 11], cualquier acci√≥n que nos lleve al cuadrado [2, 11] tambi√©n ser√° buena, por lo tanto, actualizamos el valor Q del cuadrado, lo que nos lleva a [2, 11] estar m√°s cerca de 0. </p><br><p>  <b>¬°Y eso, damas y caballeros, es la esencia del Q-learning!</b> </p><br><p>  Tenga en cuenta que cada vez que alcanzamos la meta, aumentamos nuestro "mapa" de c√≥mo lograr la meta en un cuadrado, por lo que despu√©s de un n√∫mero suficiente de iteraciones tendremos un mapa completo que nos mostrar√° c√≥mo llegar a la meta desde cada estado. </p><br><img src="https://habrastorage.org/webt/mj/q0/sh/mjq0shtkn3u37zlhdpptbh2oppe.gif"><br>  <i>Se genera una ruta al tomar la mejor acci√≥n en cada estado.</i>  <i>La tecla verde representa el significado de una mejor acci√≥n, las teclas m√°s saturadas representan un valor m√°s alto.</i>  <i>El texto representa un valor para cada acci√≥n (arriba, abajo, derecha, izquierda).</i> <br><br><h3>  Ecuaci√≥n de Bellman </h3><br><p>  Antes de hablar de c√≥digo, hablemos de matem√°ticas: el concepto b√°sico de Q-learning, la ecuaci√≥n de Bellman. </p><br><img src="https://habrastorage.org/webt/i2/_u/gx/i2_ugxlinshqmsyzkawlbmirxri.png"><br><br><ul><li>  Primero olvidemos Œ≥ en esta ecuaci√≥n </li><li>  La ecuaci√≥n establece que el valor Q para un par de estado-acci√≥n en particular deber√≠a ser la recompensa recibida en la transici√≥n a un nuevo estado (al realizar esta acci√≥n), sumado al valor de la mejor acci√≥n en el siguiente estado. </li></ul><br><p>  <b>En otras palabras, ¬°difundimos informaci√≥n sobre los valores de las acciones paso a paso!</b> </p><br><p>  Pero podemos decidir que recibir una recompensa en este momento es m√°s valioso que recibir una recompensa en el futuro, y por lo tanto tenemos Œ≥, un n√∫mero de 0 a 1 (generalmente de 0.9 a 0.99) que se multiplica por una recompensa en el futuro, descontando futuras recompensas. </p><br><p>  Entonces, dado Œ≥ = 0.9 y aplicando esto a algunos estados de nuestro mundo (grilla), tenemos: </p><br><img src="https://habrastorage.org/webt/e7/yp/gi/e7ypginhzc-cetmrcbqa6ar3byg.png"><br><br><p>  Podemos comparar estos valores con los anteriores en GIF y ver que son iguales. </p><br><br><h3>  Implementaci√≥n </h3><br><p>  Ahora que tenemos una comprensi√≥n intuitiva de c√≥mo funciona el Q-learning, podemos comenzar a pensar en implementar todo esto, y utilizaremos el pseudoc√≥digo de Q-learning <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">del libro</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Sutton</a> como gu√≠a. </p><br><img src="https://habrastorage.org/webt/wf/6x/fi/wf6xfiyazgu0echvfsw8d9-oly4.png"><br>  <i>Seudoc√≥digo del libro de Sutton.</i> <br><br><p>  C√≥digo: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Initialize Q arbitrarily, in this case a table full of zeros q_values = np.zeros((num_states, num_actions)) # Iterate over 500 episodes for _ in range(500): state = env.reset() done = False # While episode is not over while not done: # Choose action action = egreedy_policy(q_values, state, epsilon=0.1) # Do the action next_state, reward, done = env.step(action) # Update q_values td_target = reward + gamma * np.max(q_values[next_state]) td_error = td_target - q_values[state][action] q_values[state][action] += learning_rate * td_error # Update state state = next_state</span></span></code> </pre> <br><ul><li>  Primero, decimos: "Para todos los estados y acciones, inicializamos Q (s, a) arbitrariamente", esto significa que podemos crear nuestra tabla de valores Q con cualquier valor que nos guste, pueden ser aleatorios, pueden ser cualquier tipo de permanente no importa.  Vemos que en la l√≠nea 2 creamos una tabla llena de ceros. </li></ul><br><p>  <b>Tambi√©n decimos: "El valor de Q para los estados finales es cero", no podemos realizar ninguna acci√≥n en los estados finales, por lo tanto, consideramos que el valor de todas las acciones en este estado es cero.</b> </p><br><ul><li>  Para cada episodio, tenemos que "inicializar S", esta es solo una forma elegante de decir "reiniciar el juego", en nuestro caso significa poner al jugador en la posici√≥n inicial;  En nuestro mundo hay un m√©todo que hace exactamente eso, y lo llamamos en la l√≠nea 6. </li><li>  Para cada paso de tiempo (cada vez que necesitamos actuar), debemos elegir la acci√≥n obtenida de Q. </li></ul><br><p>  Recuerde, dije, ‚Äú¬øestamos tomando las acciones que son m√°s valiosas en todas las condiciones? </p><br><p>  Cuando hacemos esto, usamos nuestros valores Q para crear la pol√≠tica;  en este caso ser√° una pol√≠tica codiciosa, porque siempre tomamos acciones que, en nuestra opini√≥n, son las mejores en todos los estados, por lo tanto, se dice que actuamos con avidez. </p><br><br><h3>  Basura </h3><br><p>  Pero hay un problema con este enfoque: imagina que estamos en un laberinto que tiene dos recompensas, una de las cuales es +1 y la otra es +100 (y cada vez que encontramos una de ellas, el juego termina).  Dado que siempre tomamos medidas que consideramos las mejores, nos quedaremos atrapados con el primer premio encontrado, siempre volviendo a √©l, por lo tanto, si primero reconocemos el premio +1, entonces perderemos el gran premio +100. </p><br><br><h3>  Soluci√≥n </h3><br><p>  Necesitamos asegurarnos de que hemos estudiado nuestro mundo lo suficiente (esta es una tarea incre√≠blemente dif√≠cil).  Aqu√≠ es donde entra en juego Œµ.  Œµ en el algoritmo codicioso significa que debemos actuar con entusiasmo, PERO hacer acciones aleatorias como un porcentaje de Œµ a lo largo del tiempo, por lo que con un n√∫mero infinito de intentos, debemos examinar todos los estados. </p><br><p>  La acci√≥n se selecciona de acuerdo con esta estrategia en la l√≠nea 12, con epsilon = 0.1, lo que significa que estamos investigando el mundo el 10% del tiempo.  La implementaci√≥n de la pol√≠tica es la siguiente: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">egreedy_policy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(q_values, state, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Get a random number from a uniform distribution between 0 and 1, # if the number is lower than epsilon choose a random action if np.random.random() &lt; epsilon: return np.random.choice(4) # Else choose the action with the highest value else: return np.argmax(q_values[state])</span></span></code> </pre><br><p>  En la l√≠nea 14 de la primera lista, llamamos al m√©todo de pasos para realizar la acci√≥n, el mundo nos devuelve el siguiente estado, recompensa e informaci√≥n sobre el final del juego. </p><br><p>  De vuelta a las matem√°ticas: </p><br><p>  Tenemos una ecuaci√≥n larga, pensemos en ello: </p><br><img src="https://habrastorage.org/webt/9v/bn/ws/9vbnws8g-1gclwefuvtpjv_yqpm.png"><br><br><p>  Si tomamos Œ± = 1: </p><br><img src="https://habrastorage.org/webt/7r/aw/er/7rawertkpcilxbfzrorhpygtrok.png"><br><br><p>  ¬°Que coincide exactamente con la ecuaci√≥n de Bellman, que vimos hace unos p√°rrafos!  Entonces ya sabemos que esta es la l√≠nea responsable de diseminar informaci√≥n sobre los valores estatales. </p><br><p>  Pero, por lo general, Œ± (principalmente conocido como velocidad de aprendizaje) es mucho menor que 1, su objetivo principal es evitar grandes cambios en una actualizaci√≥n, por lo que en lugar de volar hacia la meta, nos acercamos lentamente.  En nuestro enfoque tabular, establecer Œ± = 1 no causa ning√∫n problema, pero cuando se trabaja con redes neuronales (m√°s sobre esto en los siguientes art√≠culos), todo puede salirse de control f√°cilmente. </p><br><p>  Mirando el c√≥digo, vemos que en la l√≠nea 16 en la primera lista definimos td_target, este es el valor al que debemos acercarnos, pero en lugar de ir directamente a este valor en la l√≠nea 17, calculamos td_error, usaremos este valor en combinaci√≥n con la velocidad aprendiendo a avanzar lentamente hacia la meta. </p><br><p>  <b>Recuerde que esta ecuaci√≥n es una entidad de Q-Learning.</b> </p><br><p>  Ahora solo necesitamos actualizar nuestro estado, y todo est√° listo, esta es la l√≠nea 20. Repetimos este proceso hasta llegar al final del episodio, morir en las rocas o alcanzar la meta. </p><br><br><h3>  Conclusi√≥n </h3><br><p>  Ahora intuitivamente entendemos y sabemos c√≥mo codificar Q-Learning (al menos la versi√≥n tabular), aseg√∫rese de verificar todo el c√≥digo utilizado para esta publicaci√≥n, disponible en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GitHub</a> . </p><br><p>  Visualizaci√≥n de la prueba del proceso de aprendizaje: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Vto8n9C7DSQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Tenga en cuenta que todas las acciones comienzan con un valor que excede su valor final, este es un truco para estimular la exploraci√≥n del mundo. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/443240/">https://habr.com/ru/post/443240/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443224/index.html">Estera con un elefante y un caballo. M√©todo TWIX</a></li>
<li><a href="../443232/index.html">Cat√°lisis de muones en t√©rminos de qu√≠mica cu√°ntica. Parte I: hidr√≥geno ordinario vs. muon hidr√≥geno</a></li>
<li><a href="../443234/index.html">Lo que hacen los ingenieros de Apple e Intel en la oficina: un curso en l√≠nea orientado a una carrera en microelectr√≥nica moderna para estudiantes</a></li>
<li><a href="../443236/index.html">Desmitificar redes neuronales convolucionales</a></li>
<li><a href="../443238/index.html">C√≥mo no convertirse en una lib√©lula si tiene muchas bases de datos diferentes</a></li>
<li><a href="../443242/index.html">Quarkus es un Java subat√≥mico supers√≥nico. Una breve descripci√≥n del marco</a></li>
<li><a href="../443244/index.html">Debriefing tareas. Beanpoisk_1</a></li>
<li><a href="../443246/index.html">C√≥mo reinventamos Askozia IP PBX despu√©s de que el desarrollador vendi√≥ y cerr√≥ el proyecto</a></li>
<li><a href="../443248/index.html">Protocolos de reserva perfecta de PRP y HSR</a></li>
<li><a href="../443250/index.html">Recolector de basura casero para OpenJDK</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>