<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíÜüèæ ü¶Ü üëãüèø Neuronale Netzwerkarchitektur ‚ûó ü§∂üèø üåµ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√úbersetzung neuronaler Netzwerkarchitekturen 

 Algorithmen tiefer neuronaler Netze haben heute gro√üe Popularit√§t erlangt, was weitgehend durch die du...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netzwerkarchitektur</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/430524/">  <i>√úbersetzung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neuronaler Netzwerkarchitekturen</a></i> <br><br>  Algorithmen tiefer neuronaler Netze haben heute gro√üe Popularit√§t erlangt, was weitgehend durch die durchdachte Architektur sichergestellt wird.  Schauen wir uns die Geschichte ihrer Entwicklung in den letzten Jahren an.  Wenn Sie an einer tieferen Analyse interessiert sind, lesen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diese Arbeit</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/29b/f51/960/29bf5196085373528be31e27f2489bdd.jpg"><br>  <i>Vergleich g√§ngiger Architekturen f√ºr die Top-1-Genauigkeit bei einer Ernte und der Anzahl der f√ºr einen direkten Durchgang erforderlichen Vorg√§nge.</i>  <i>Weitere Details <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> .</i> <br><a name="habracut"></a><br><h3>  Lenet5 </h3><br>  1994 wurde eines der ersten Faltungsnetzwerke entwickelt, das den Grundstein f√ºr tiefes Lernen legte.  Diese Pionierarbeit von Yann LeCun hie√ü nach vielen erfolgreichen Iterationen seit 1988 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LeNet5</a> ! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b19/9c3/8f2/b199c38f21a72c44d7cd3afbca1c94eb.jpg"><br><br>  Die LeNet5-Architektur ist f√ºr das Deep Learning von grundlegender Bedeutung, insbesondere im Hinblick auf die Verteilung der Bildeigenschaften im gesamten Bild.  Faltungen mit Lernparametern erm√∂glichten die Verwendung mehrerer Parameter, um dieselben Eigenschaften effizient von verschiedenen Stellen zu extrahieren.  In jenen Jahren gab es keine Grafikkarten, die den Lernprozess beschleunigen k√∂nnten, und selbst die zentralen Prozessoren waren langsam.  Daher war der Hauptvorteil der Architektur die M√∂glichkeit, Parameter und Berechnungsergebnisse zu speichern, im Gegensatz zur Verwendung jedes Pixels als separate Eingabedaten f√ºr ein gro√ües mehrschichtiges neuronales Netzwerk.  In LeNet5 werden Pixel in der ersten Ebene nicht verwendet, da Bilder stark r√§umlich korreliert sind. Wenn Sie also einzelne Pixel als Eingabeeigenschaften verwenden, k√∂nnen Sie diese Korrelationen nicht nutzen. <br><br>  Funktionen von LeNet5: <br><br><ul><li>  Ein Faltungsnetzwerk, das eine Folge von drei Schichten verwendet: Faltungsschichten, Poolschichten und Nichtlinearit√§tsschichten -&gt; seit der Ver√∂ffentlichung von Lekuns Arbeit ist dies m√∂glicherweise eines der Hauptmerkmale des tiefen Lernens in Bezug auf Bilder. </li><li>  Verwendet die Faltung, um r√§umliche Eigenschaften abzurufen. </li><li>  Unterabtastung unter Verwendung der r√§umlichen Kartenmittelung. </li><li>  Nichtlinearit√§t in Form von hyperbolischer Tangente oder Sigmoid. </li><li>  Der endg√ºltige Klassifikator in Form eines mehrschichtigen neuronalen Netzwerks (MLP). </li><li>  Die sp√§rliche Konnektivit√§tsmatrix zwischen den Schichten reduziert den Rechenaufwand. </li></ul><br>  Dieses neuronale Netzwerk bildete die Grundlage vieler nachfolgender Architekturen und inspirierte viele Forscher. <br><br><h3>  Entwicklung </h3><br>  Von 1998 bis 2010 befanden sich die neuronalen Netze in einem Inkubationszustand.  Die meisten Leute bemerkten ihre wachsenden F√§higkeiten nicht, obwohl viele Entwickler ihre Algorithmen schrittweise verfeinerten.  Dank der Bl√ºtezeit der Handykameras und der Verbilligung der Digitalkameras stehen uns immer mehr Trainingsdaten zur Verf√ºgung.  Gleichzeitig wuchsen die Rechenkapazit√§ten, die Prozessoren wurden leistungsf√§higer und Grafikkarten wurden zum Hauptrechner.  Alle diese Prozesse erm√∂glichten die Entwicklung neuronaler Netze, wenn auch eher langsam.  Das Interesse an Aufgaben, die mit Hilfe neuronaler Netze gel√∂st werden konnten, wuchs und schlie√ülich wurde die Situation offensichtlich ... <br><br><h3>  Dan Ciresan Netz </h3><br>  Im Jahr 2010 ver√∂ffentlichten Dan Claudiu Ciresan und J√ºrgen Schmidhuber eine der ersten Beschreibungen der Implementierung <a href="">neuronaler GPU-Netze</a> .  Ihre Arbeit beinhaltete die direkte und umgekehrte Implementierung eines 9-lagigen neuronalen Netzwerks auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NVIDIA GTX 280</a> . <br><br><h3>  Alexnet </h3><br>  Im Jahr 2012 ver√∂ffentlichte Alexei Krizhevsky <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AlexNet</a> , eine ausf√ºhrliche und erweiterte Version von LeNet, die im ImageNet-Wettbewerb mit gro√üem Vorsprung gewann. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aad/4ad/3ca/aad4ad3ca7345f8d7e198c2b131298d1.png"><br><br>  Bei AlexNet werden die Ergebnisse von LeNet-Berechnungen in ein viel gr√∂√üeres neuronales Netzwerk skaliert, das viel komplexere Objekte und ihre Hierarchien untersuchen kann.  Merkmale dieser L√∂sung: <br><br><ul><li>  Verwendung von linearen Gleichrichtungseinheiten (ReLU) als Nichtlinearit√§ten. </li><li>  Die Verwendung von Verwerfungstechniken zum selektiven Ignorieren einzelner Neuronen w√§hrend des Trainings, wodurch ein √úbertraining des Modells vermieden wird. </li><li>  √úberlappen Sie das maximale Pooling, wodurch die Auswirkungen einer durchschnittlichen durchschnittlichen Poolbildung vermieden werden. </li><li>  Verwenden von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NVIDIA GTX 580</a> , um das Lernen zu beschleunigen. </li></ul><br>  Zu diesem Zeitpunkt war die Anzahl der Kerne in Grafikkarten erheblich gestiegen, wodurch die Trainingszeit um das Zehnfache verk√ºrzt werden konnte. Infolgedessen konnten viel gr√∂√üere Datens√§tze und Bilder verwendet werden. <br><br>  Der Erfolg von AlexNet l√∂ste eine kleine Revolution aus. Faltungs-Neuronale Netze wurden zu einem Arbeitspferd des tiefen Lernens - dieser Begriff bedeutet jetzt "gro√üe Neuronale Netze, die n√ºtzliche Probleme l√∂sen k√∂nnen". <br><br><h3>  √úberfeature </h3><br>  Im Dezember 2013 ver√∂ffentlichte das NYU-Labor von Jan Lekun eine Beschreibung von <a href="">Overfeat</a> , einer Variante von AlexNet.  Der Artikel beschrieb auch die trainierten Begrenzungsrahmen, und anschlie√üend wurden viele andere Arbeiten zu diesem Thema ver√∂ffentlicht.  Wir glauben, dass es besser ist, das Segmentieren von Objekten zu lernen, als k√ºnstliche Begrenzungsrahmen zu verwenden. <br><br><h3>  Vgg </h3><br>  In <a href="">VGG-</a> Netzwerken, die in Oxford entwickelt wurden, wurden in jeder Faltungsschicht zum ersten Mal 3 √ó 3-Filter verwendet, und sogar diese Schichten wurden in einer Folge von Faltungen kombiniert. <br><br>  Dies widerspricht den in LeNet festgelegten Prinzipien, nach denen gro√üe Windungen verwendet wurden, um dieselben Bildeigenschaften zu extrahieren.  Anstelle der in AlexNet verwendeten 9x9- und 11x11-Filter wurden viel kleinere Filter verwendet, die gef√§hrlich nahe an 1x1-Windungen lagen und die LeNet-Autoren zumindest in den ersten Schichten des Netzwerks zu vermeiden versuchten.  Der gro√üe Vorteil von VGG war jedoch die Feststellung, dass mehrere 3x3-Faltungen, die in einer Sequenz kombiniert werden, gr√∂√üere Empfangsfelder emulieren k√∂nnen, beispielsweise 5x5 oder 7x7.  Diese Ideen werden sp√§ter in den Inception- und ResNet-Architekturen verwendet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dec/e8b/308/dece8b308f74450222deece6fcf9d357.jpg"><br><br>  VGG-Netzwerke verwenden mehrere 3x3-Faltungsschichten, um komplexe Eigenschaften darzustellen.  Beachten Sie die Bl√∂cke 3, 4 und 5 in VGG-E: Um komplexere Eigenschaften zu extrahieren und zu kombinieren, werden 256 √ó 256 und 512 √ó 512 3 √ó 3 Filtersequenzen verwendet.  Dies entspricht einem gro√üen Faltungsklassifikator 512x512 mit drei Schichten!  Dies gibt uns eine Vielzahl von Parametern und hervorragende Lernf√§higkeiten.  Aber es war schwierig, solche Netzwerke zu lernen, ich musste sie in kleinere aufteilen und Schichten nacheinander hinzuf√ºgen.  Der Grund war das Fehlen effektiver Methoden zur Regularisierung von Modellen oder einiger Methoden zur Begrenzung eines gro√üen Suchraums, was durch viele Parameter gef√∂rdert wird. <br><br>  VGG verwendet in vielen Schichten eine gro√üe Anzahl von Eigenschaften, so dass das Training <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">rechenintensiv war</a> .  Die Last kann reduziert werden, indem die Anzahl der Eigenschaften verringert wird, wie dies in den Engpassschichten der Inception-Architektur der Fall ist. <br><br><h3>  Netzwerk im Netzwerk </h3><br>  Die <a href="">Network-in-Network-</a> Architektur (NiN) basiert auf einer einfachen Idee: Verwendung von 1x1-Faltungen zur Erh√∂hung der Kombinatorialit√§t von Eigenschaften in Faltungsschichten. <br><br>  In NiN werden nach jeder Faltung r√§umliche MLP-Schichten verwendet, um die Eigenschaften besser zu kombinieren, bevor sie der n√§chsten Schicht zugef√ºhrt werden.  Es mag den Anschein haben, dass die Verwendung von 1x1-Faltungen den urspr√ºnglichen LeNet-Prinzipien widerspricht, aber in Wirklichkeit k√∂nnen Eigenschaften besser kombiniert werden, als nur mehr Faltungsschichten zu f√ºllen.  Dieser Ansatz unterscheidet sich von der Verwendung von blo√üen Pixeln als Eingabe f√ºr die n√§chste Ebene.  In diesem Fall werden 1x1-Faltungen f√ºr die r√§umliche Kombination von Eigenschaften nach der Faltung im Rahmen von Eigenschaftskarten verwendet, sodass Sie viel weniger Parameter verwenden k√∂nnen, die allen Pixeln dieser Eigenschaften gemeinsam sind! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d9a/d08/e5c/d9ad08e5c699a2a9cf320c4b8b622ba3.jpg"><br><br>  MLP kann die Wirksamkeit einzelner Faltungsschichten erheblich steigern, indem sie zu komplexeren Gruppen zusammengefasst werden.  Diese Idee wurde sp√§ter in anderen Architekturen wie ResNet, Inception und deren Varianten verwendet. <br><br><h3>  GoogLeNet und Inception </h3><br>  Google Christian Szegedy ist besorgt dar√ºber, die Berechnungen in tiefen neuronalen Netzen zu <a href="">verringern,</a> und hat daher <a href="">GoogLeNet erstellt, die erste Inception-Architektur</a> . <br><br>  Bis zum Herbst 2014 waren Deep-Learning-Modelle sehr n√ºtzlich, um Bildinhalte und Frames aus Videos zu kategorisieren.  Viele Skeptiker haben die Vorteile von Deep Learning und neuronalen Netzwerken erkannt, und Internetgiganten, einschlie√ülich Google, sind sehr daran interessiert, effiziente und gro√üe Netzwerke auf ihren Serverkapazit√§ten bereitzustellen. <br><br>  Christian suchte nach M√∂glichkeiten, die Rechenlast in neuronalen Netzen zu reduzieren und die h√∂chste Leistung zu erzielen (z. B. in ImageNet).  Oder den Rechenaufwand beibehalten, aber dennoch die Produktivit√§t steigern. <br><br>  Infolgedessen hat der Befehl ein Inception-Modul erstellt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/abf/d01/a92/abfd01a92262ff6e5b9f23380ba8d9cc.jpg"><br><br>  Auf den ersten Blick ist dies eine parallele Kombination der Faltungsfilter 1x1, 3x3 und 5x5.  Das Highlight war jedoch die Verwendung von Faltungsbl√∂cken 1x1 (NiN), um die Anzahl der Eigenschaften vor dem Servieren in den "teuren" parallelen Bl√∂cken zu verringern.  Normalerweise wird dieser Teil als Engpass bezeichnet. Er wird im n√§chsten Kapitel ausf√ºhrlicher beschrieben. <br><br>  GoogLeNet verwendet einen Stamm ohne Inception-Module als erste Schicht und verwendet au√üerdem ein durchschnittliches Pooling und einen Softmax-Klassifikator √§hnlich wie NiN.  Dieser Klassifikator f√ºhrt im Vergleich zu AlexNet und VGG √§u√üerst wenige Operationen aus.  Es hat auch dazu beigetragen, eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sehr effiziente neuronale Netzwerkarchitektur</a> zu erstellen. <br><br><h3>  Engpassschicht </h3><br>  Diese Schicht reduziert die Anzahl der Eigenschaften (und damit der Operationen) in jeder Schicht, so dass die Geschwindigkeit, mit der das Ergebnis erhalten wird, auf einem hohen Niveau gehalten werden kann.  Vor der √úbertragung von Daten an ‚Äûteure‚Äú Faltungsmodule wird die Anzahl der Eigenschaften beispielsweise um das Vierfache reduziert.  Dies reduziert den Rechenaufwand erheblich, was die Architektur popul√§r gemacht hat. <br><br>  Lass es uns herausfinden.  Angenommen, wir haben 256 Eigenschaften am Eingang und 256 am Ausgang und lassen die Inception-Ebene nur 3x3-Faltungen ausf√ºhren.  Wir erhalten 256 x 256 x 3 x 3 Faltungen (589.000 Operationen der Akkumulationsmultiplikation, dh MAC-Operationen).  Dies kann √ºber unsere Anforderungen an die Rechengeschwindigkeit hinausgehen. Nehmen wir an, eine Ebene wird in 0,5 Millisekunden auf Google Server verarbeitet.  Reduzieren Sie dann die Anzahl der Eigenschaften f√ºr das Falten auf 64 (256/4).  In diesem Fall f√ºhren wir zuerst eine 1x1-Faltung von 256 -&gt; 64, dann eine weitere 64-Faltung in allen Inception-Zweigen durch und wenden dann erneut eine 1x1-Faltung von 64 -&gt; 256 Eigenschaften an.  Anzahl der Operationen: <br><br><ul><li>  256 √ó 64 √ó 1 √ó 1 = 16.000 </li><li>  64 √ó 64 √ó 3 √ó 3 = 36.000 </li><li>  64 √ó 256 √ó 1 √ó 1 = 16.000 </li></ul><br>  Nur etwa 70.000, reduzierte die Anzahl der Operationen um fast das Zehnfache!  Gleichzeitig haben wir die Verallgemeinerung in dieser Ebene nicht verloren.  Engpass-Layer haben im ImageNet-Dataset eine hervorragende Leistung gezeigt und wurden in sp√§teren Architekturen wie ResNet verwendet.  Der Grund f√ºr ihren Erfolg ist, dass die Eingabeeigenschaften korreliert sind. Dies bedeutet, dass Sie Redundanz beseitigen k√∂nnen, indem Sie Eigenschaften korrekt mit 1x1-Faltungen kombinieren.  Und nachdem Sie mit weniger Eigenschaften gefaltet haben, k√∂nnen Sie sie auf der n√§chsten Ebene wieder in einer signifikanten Kombination bereitstellen. <br><br><h3>  Inception V3 (und V2) </h3><br>  Christian und sein Team haben sich als sehr effektive Forscher erwiesen.  Im Februar 2015 wurde die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Batch-normalisierte Inception-</a> Architektur als zweite Version von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inception eingef√ºhrt</a> .  Die Chargennormalisierung berechnet den Mittelwert und die Standardabweichung aller Eigenschaftsverteilungskarten in der Ausgabeebene und normalisiert ihre Antworten mit diesen Werten.  Dies entspricht dem "Aufhellen" der Daten, dh die Antworten aller neuronalen Karten liegen im gleichen Bereich und mit einem Mittelwert von Null.  Dieser Ansatz erleichtert das Lernen, da die n√§chste Ebene keine Offsets von Eingabedaten speichern muss und nur nach den besten Kombinationen von Eigenschaften suchen kann. <br><br>  Im Dezember 2015 wurde eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neue Version der Inception-Module und der entsprechenden Architektur ver√∂ffentlicht</a> .  Der Artikel des Autors erkl√§rt die urspr√ºngliche GoogLeNet-Architektur besser, die viel mehr √ºber die getroffenen Entscheidungen erz√§hlt.  Schl√ºsselideen: <br><br><ul><li>  Maximierung des Informationsflusses im Netzwerk aufgrund des sorgf√§ltigen Gleichgewichts zwischen Tiefe und Breite.  Vor jedem Pooling werden die Eigenschaftskarten erh√∂ht. </li><li>  Mit zunehmender Tiefe nimmt auch die Anzahl der Eigenschaften oder die Schichtbreite systematisch zu. </li><li>  Die Breite jeder Schicht nimmt zu, um die Kombination der Eigenschaften vor der n√§chsten Schicht zu erh√∂hen. </li><li>  Soweit m√∂glich werden nur 3x3 Windungen verwendet.  Vorausgesetzt, dass 5x5- und 7x7-Filter mit mehreren 3x3-Filtern zerlegt werden k√∂nnen <br><br><img src="https://habrastorage.org/getpro/habr/post_images/849/96f/d8c/84996fd8cb1040fbf0a18187313a8a81.jpg"><br><br>  Das neue Inception-Modul sieht folgenderma√üen aus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/975/b0b/ad7/975b0bad7d65a0b37aedf0dc119d03b8.jpg"></li><li>  Filter k√∂nnen auch durch <a href="">gegl√§ttete Windungen</a> in komplexere Module zerlegt werden: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb5/c32/21c/bb5c3221cc8f478de3ac5ef504a13357.jpg"></li><li>  Inception-Module k√∂nnen die Datengr√∂√üe mithilfe von Pooling w√§hrend Inception-Berechnungen reduzieren.  Dies √§hnelt der Durchf√ºhrung einer Faltung mit Schritten parallel zu einer einfachen Pooling-Schicht: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f8b/4c1/263/f8b4c1263b3883d751c7dfe3788110ca.jpg"></li></ul><br>  Inception verwendet die Pooling-Ebene mit Softmax als endg√ºltigen Klassifikator. <br><br><h3>  Resnet </h3><br>  Im Dezember 2015, ungef√§hr zur gleichen Zeit, als die Inception v3-Architektur eingef√ºhrt wurde, kam es zu einer Revolution - sie ver√∂ffentlichten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ResNet</a> .  Es enth√§lt einfache Ideen: Senden Sie die Ausgabe von zwei erfolgreichen Faltungsebenen und umgehen Sie die Eingabe f√ºr die n√§chste Ebene! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/05d/8b8/b8a05d8b89f55e8d06bb2eae79bd648b.jpg"><br><br>  Solche Ideen wurden beispielsweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> bereits vorgeschlagen.  In diesem Fall umgehen die Autoren jedoch ZWEI Ebenen und wenden den Ansatz in gro√üem Ma√üstab an.  Das Umgehen einer Schicht bringt nicht viel, und das Umgehen von zwei ist eine wichtige Erkenntnis.  Dies kann als kleiner Klassifikator angesehen werden, als Netzwerk-in-Netzwerk! <br><br>  Es war auch das erste Beispiel f√ºr das Training eines Netzwerks aus mehreren hundert oder sogar Tausenden von Schichten. <br>  Multilayer ResNet verwendete eine Engpassschicht √§hnlich der in Inception: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0d0/ecf/124/0d0ecf1248874511ae4dbca5f23afcec.jpg"><br><br>  Diese Schicht reduziert die Anzahl der Eigenschaften in jeder Schicht, indem zuerst eine 1x1-Faltung mit einer kleineren Ausgabe (normalerweise ein Viertel der Eingabe), dann eine 3x3-Schicht und dann erneut eine 1x1-Faltung auf eine gr√∂√üere Anzahl von Eigenschaften verwendet wird.  Wie bei Inception-Modulen werden hierdurch Rechenressourcen eingespart und gleichzeitig eine Vielzahl von Eigenschaftskombinationen beibehalten.  Vergleichen Sie mit den komplexeren und weniger offensichtlichen St√§mmen in Inception V3 und V4. <br><br>  ResNet verwendet eine Pooling-Schicht mit Softmax als endg√ºltigem Klassifikator. <br>  Jeden Tag werden zus√§tzliche Informationen zur ResNet-Architektur angezeigt: <br><br><ul><li>  Es kann als ein System von gleichzeitig parallelen und seriellen Modulen betrachtet werden: In vielen Modulen kommt das Inout-Signal parallel und die Ausgangssignale jedes Moduls sind in Reihe geschaltet. </li><li>  ResNet kann als mehrere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ensembles paralleler oder serieller Module betrachtet werden</a> . </li><li>  Es stellte sich heraus, dass ResNet normalerweise mit relativ kleinen Tiefenbl√∂cken von 20 bis 30 Schichten arbeitet, die parallel arbeiten, anstatt nacheinander √ºber die gesamte L√§nge des Netzwerks zu laufen. </li><li>  Da das Ausgangssignal zur√ºckkommt und wie in RNN als Eingang eingespeist wird, kann ResNet als verbessertes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">plausibles Modell der Gro√ühirnrinde angesehen werden</a> . </li></ul><br><h3>  Inception V4 </h3><br>  Christian und sein Team haben sich mit einer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neuen Version von Inception erneut hervorgetan</a> . <br><br>  Das folgende Stammmodul des Inception-Moduls ist das gleiche wie in Inception V3: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48b/955/f38/48b955f385c72d21a20af8517d941580.jpg"><br><br>  In diesem Fall wird das Inception-Modul mit dem ResNet-Modul kombiniert: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f4c/5f2/bd7/f4c5f2bd765082fe56dac5710fc30221.jpg"><br><br>  Diese Architektur erwies sich nach meinem Geschmack als komplizierter, weniger elegant und auch voller undurchsichtiger heuristischer L√∂sungen.  Es ist schwer zu verstehen, warum die Autoren diese oder jene Entscheidungen getroffen haben, und es ist ebenso schwierig, ihnen irgendeine Art von Bewertung zu geben. <br><br>  Daher geht der Preis f√ºr ein sauberes und einfaches neuronales Netzwerk, das leicht zu verstehen und zu √§ndern ist, an ResNet. <br><br><h3>  Squeezenet </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SqueezeNet wurde</a> k√ºrzlich ver√∂ffentlicht.  Dies ist ein neues Remake vieler Konzepte von ResNet und Inception.  Die Autoren haben gezeigt, dass die Verbesserung der Architektur die Netzwerkgr√∂√üe und die Anzahl der Parameter ohne komplexe Komprimierungsalgorithmen reduziert. <br><br><h3>  ENet </h3><br>  Alle Funktionen der neuesten Architekturen werden zu einem sehr effizienten und kompakten Netzwerk kombiniert, das nur sehr wenige Parameter und Rechenleistung verwendet und gleichzeitig hervorragende Ergebnisse liefert.  Die Architektur hie√ü <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ENet</a> und wurde von Adam Paszke ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adam Paszke</a> ) entwickelt.  Zum Beispiel haben wir es verwendet, um Objekte auf dem Bildschirm sehr genau zu markieren und Szenen zu analysieren.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Einige Beispiele von Enet</a> .  Diese Videos beziehen sich nicht auf den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Trainingsdatensatz</a> . <br><br>  Technische Details zu ENet finden Sie hier.  Es ist ein Netzwerk, das auf Encoder und Decoder basiert.  Der Codierer basiert auf dem √ºblichen CNN-Kategorisierungsschema, und der Decodierer ist ein Upsampling-Netzwerk, das zur Segmentierung durch Zur√ºckstreuen der Kategorien auf das Bild in Originalgr√∂√üe entwickelt wurde.  F√ºr die Bildsegmentierung wurden nur neuronale Netze verwendet, keine anderen Algorithmen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/18d/c54/7fa/18dc547fade22215961a848b2170b104.png"><br><br>  Wie Sie sehen k√∂nnen, weist ENet im Vergleich zu allen anderen neuronalen Netzen die h√∂chste spezifische Genauigkeit auf. <br><br>  ENet wurde von Anfang an so konzipiert, dass m√∂glichst wenig Ressourcen verwendet werden.  Infolgedessen belegen der Codierer und der Decodierer zusammen nur 0,7 MB mit einer Genauigkeit von fp16.  Und mit solch einer winzigen Gr√∂√üe ist ENet der Segmentierungsgenauigkeit nicht unterlegen oder anderen rein neuronalen Netzwerkl√∂sungen √ºberlegen. <br><br><h3>  Modulanalyse </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ver√∂ffentlichung einer</a> systematischen Bewertung von CNN-Modulen.  Es stellte sich als vorteilhaft heraus: <br><br><ul><li>  Verwenden Sie die ELU-Nichtlinearit√§t ohne Batch-Normalisierung (Batchnorm) oder ReLU mit Normalisierung. </li><li>  Wenden Sie die gelernte Transformation des RGB-Farbraums an. </li><li>  Verwenden Sie eine Richtlinie f√ºr den linearen Lernratenabfall. </li><li>  Verwenden Sie die Summe der mittleren und maximalen Pooling-Ebene. </li><li>  Verwenden Sie ein 128- oder 256-Minipaket. Wenn dies f√ºr Ihre Grafikkarte zu viel ist, reduzieren Sie die Lerngeschwindigkeit proportional zur Paketgr√∂√üe. </li><li>  Verwenden Sie vollst√§ndig verbundene Schichten als Faltungsschichten und Durchschnittsvorhersagen, um die endg√ºltige L√∂sung zu erhalten. </li><li>  Wenn Sie den Trainingsdatensatz vergr√∂√üern, stellen Sie sicher, dass Sie im Training kein Plateau erreicht haben.  Datenbereinigung ist wichtiger als Gr√∂√üe. </li><li>  Wenn Sie das Eingabebild nicht vergr√∂√üern und den Schritt in nachfolgenden Ebenen verringern k√∂nnen, ist der Effekt ungef√§hr gleich. </li><li>  Wenn Ihr Netzwerk wie in GoogLeNet √ºber eine komplexe und hochoptimierte Architektur verf√ºgt, √§ndern Sie diese mit Vorsicht. </li></ul><br><h3>  Xception </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Xception</a> hat eine einfachere und elegantere Architektur in das Inception-Modul eingef√ºhrt, die nicht weniger effizient ist als ResNet und Inception V4. <br>  So sieht das Xception-Modul aus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/632/40a/deb/63240adebe962726f6d035b5a5d16099.jpg"><br><br>  Jeder wird dieses Netzwerk aufgrund der Einfachheit und Eleganz seiner Architektur m√∂gen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d64/f09/933/d64f099330f0b9290a99202a50863868.jpg"><br><br>  Es enth√§lt 36 Faltungsschritte und √§hnelt ResNet-34.  Gleichzeitig sind Modell und Code wie in ResNet einfach und viel angenehmer als in Inception V4. <br><br>  Eine torch7-Implementierung dieses Netzwerks ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> verf√ºgbar, w√§hrend eine Keras / TF-Implementierung hier verf√ºgbar ist. <br><br>  Seltsamerweise waren die Autoren der j√ºngsten Xception-Architektur auch von <a href="">unserer Arbeit an trennbaren Faltungsfiltern</a> inspiriert. <br><br><h3>  MobileNets </h3><br>  Die neue Architektur von M <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">obileNets</a> wurde im April 2017 ver√∂ffentlicht.  Um die Anzahl der Parameter zu verringern, werden abnehmbare Windungen verwendet, wie in Xception.  In der Arbeit wird auch festgestellt, dass die Autoren die Anzahl der Parameter stark reduzieren konnten: etwa die H√§lfte im Fall von FaceNet.   : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/689/04b/c1e/68904bc1e353888d4fcd54975a064362.jpg"><br><br>         ,         1 (batch of 1)   Titan Xp.      : <br><br><ul><li> resnet18: 0,002871 </li><li> alexnet: 0,001003 </li><li> vgg16: 0,001698 </li><li> squeezenet: 0,002725 </li><li> mobilenet: 0,033251 </li></ul><br>     !        ,     . <br><br><h3>    </h3><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FractalNet</a>   ,      ImageNet        ResNet. <br><br><h3>  </h3><br>  ,           .         ,  . <br><br>   ,         ,       ,   ,       ?  ,       . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>    . <br>  ,        .      ,         . <br><br>         , . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de430524/">https://habr.com/ru/post/de430524/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de430512/index.html">Wie Freiberufler leben: Arbeiten Sie nicht mit allwissenden Kunden zusammen und lassen Sie sich z√∂gern</a></li>
<li><a href="../de430514/index.html">Blockchain Charity - DataArt gewinnt Malta Blockchain Summit Hackathon</a></li>
<li><a href="../de430518/index.html">So rendern Sie den Rahmen von Mittelerde: Schatten von Mordor</a></li>
<li><a href="../de430520/index.html">Einf√ºhrung in Spring Data MongoDB</a></li>
<li><a href="../de430522/index.html">Ben√∂tigen Sie eine Unternehmenskultur in der IT? Gest√§ndnis des Markenmanagers des Krasnodar-Studios Plarium</a></li>
<li><a href="../de430526/index.html">Spielautomaten: Woher kamen sie in der UdSSR und wie sind sie angeordnet?</a></li>
<li><a href="../de430528/index.html">Programmieren mit PyUSB 1.0</a></li>
<li><a href="../de430530/index.html">Mock-Server f√ºr die mobile Testautomatisierung</a></li>
<li><a href="../de430532/index.html">Sicherheit in iOS-Apps</a></li>
<li><a href="../de430534/index.html">Erstellen einer Vorlage f√ºr Zabbix am Beispiel des DVR Trassir SDK</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>