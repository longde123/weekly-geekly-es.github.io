<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🛥️ 🔫 🤵 Architecture AERODISK vAIR ou caractéristiques de la construction d'un cluster national 😠 👩🏿‍🏭 🆒</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Khabrovchans! Nous continuons à vous familiariser avec le système hyperconvergé russe AERODISK vAIR. Cet article se concentrera sur l'archite...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Architecture AERODISK vAIR ou caractéristiques de la construction d'un cluster national</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/475254/"><p><img src="https://habrastorage.org/webt/4c/_3/or/4c_3or6x9ro2_f_g8-waac1oen0.jpeg"></p><br><p>  Bonjour, Khabrovchans!  Nous continuons à vous familiariser avec le système hyperconvergé russe AERODISK vAIR.  Cet article se concentrera sur l'architecture de ce système.  Dans le dernier article, nous avons analysé notre système de fichiers ARDFS, et dans cet article, nous allons passer en revue tous les principaux composants logiciels qui composent vAIR et leurs tâches. </p><a name="habracut"></a><br><p>  Nous commençons la description de l'architecture de bas en haut - du stockage à la gestion. </p><br><h2 id="faylovaya-sistema-ardfs--raft-cluster-driver">  ARDFS + Raft Cluster Driver File System </h2><br><p>  La base de vAIR est le système de fichiers distribué ARDFS, qui combine les disques locaux de tous les nœuds de cluster en un seul pool logique, sur la base duquel les disques virtuels avec l'un ou l'autre schéma de tolérance aux pannes (facteur de réplication ou codage d'effacement) sont formés à partir de blocs virtuels de 4 Mo.  Une description plus détaillée du travail de l'ARDFS est donnée dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article précédent.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><br></a> <br>  Raft Cluster Driver est un service ARDFS interne qui résout le problème du stockage distribué et fiable des métadonnées du système de fichiers. </p><br><p>  Les métadonnées ARDFS sont classiquement divisées en deux classes. </p><br><ul><li>  notifications - informations sur les opérations avec les objets de stockage et informations sur les objets eux-mêmes; </li><li>  informations de service - définition des verrous et informations de configuration pour les nœuds de stockage. </li></ul><br><p>  Le service RCD est utilisé pour diffuser ces données.  Il attribue automatiquement à un nœud le rôle d'un leader dont la tâche est d'obtenir et de diffuser des métadonnées sur les nœuds.  Un leader est la seule véritable source de ces informations.  De plus, le leader organise un battement de cœur, c'est-à-dire  vérifie la disponibilité de tous les nœuds de stockage (cela n'a aucun rapport avec la disponibilité des machines virtuelles, RCD est juste un service de stockage). </p><br><p>  Si, pour une raison quelconque, le leader est devenu indisponible pour l'un des nœuds ordinaires pendant plus d'une seconde, ce nœud ordinaire organise une réélection du leader, demandant la disponibilité du leader à partir d'autres nœuds ordinaires.  S'il y a quorum, le chef est réélu.  Après que l'ancien chef se soit "réveillé", il devient automatiquement un nœud ordinaire, car  le nouveau chef lui envoie l'équipe appropriée. </p><br><p> La logique du RCD lui-même n'est pas nouvelle.  De nombreuses solutions tierces et commerciales et gratuites sont également guidées par cette logique, mais ces solutions ne nous convenaient pas (comme les FS open source existantes), car elles sont assez lourdes et il est très difficile de les optimiser pour nos tâches simples, nous avons donc simplement écrit notre propre Service RCD. <br>  Il peut sembler que le leader est un «col étroit» qui peut ralentir le travail dans les grands clusters par des centaines de nœuds, mais ce n'est pas le cas.  Le processus décrit se produit presque instantanément et «pèse» très peu puisque nous l'avons écrit nous-mêmes et que nous n'avons inclus que les fonctions les plus nécessaires.  De plus, cela se produit de manière complètement automatique, ne laissant que des messages dans les journaux. </p><br><h2 id="masterio--sluzhba-upravleniya-mnogopotochnym-vvodom-vyvodom">  MasterIO - Service de gestion d'E / S multithread </h2><br><p>  Une fois qu'un pool ARDFS avec des disques virtuels est organisé, il peut être utilisé pour les E / S.  À ce stade, la question se pose spécifiquement pour les systèmes hyperconvergés, à savoir: combien de ressources système (CPU / RAM) pouvons-nous donner pour les E / S? </p><br><p>  Dans les systèmes de stockage classiques, ce problème n'est pas si aigu, car la tâche de stockage consiste uniquement à stocker des données (et la plupart des ressources de stockage système peuvent être fournies en toute sécurité sous IO), et les tâches d'hyperconvergence incluent, en plus du stockage, l'exécution de machines virtuelles.  Par conséquent, le GCS nécessite l'utilisation de ressources CPU et RAM principalement pour les machines virtuelles.  Et les E / S? </p><br><p>  Pour résoudre ce problème, vAIR utilise le service de gestion des E / S: MasterIO.  La tâche du service est simple - <del>  "Prenez tout et partagez" </del>  il est garanti de récupérer le nième nombre de ressources système pour les entrées et les sorties et, à partir de celles-ci, de démarrer le nième nombre de flux d'entrée / sortie. </p><br><p>  Au départ, nous voulions fournir un mécanisme «très intelligent» pour l'allocation des ressources aux IO.  Par exemple, s'il n'y a pas de charge sur le stockage, les ressources système peuvent être utilisées pour les machines virtuelles et si la charge apparaît, ces ressources sont supprimées «en douceur» des machines virtuelles dans des limites prédéterminées.  Mais cette tentative s'est soldée par un échec partiel.  Les tests ont montré que si la charge augmente progressivement, alors tout va bien, les ressources (marquées pour une éventuelle suppression) sont progressivement retirées de la VM au profit des E / S.  Mais de brusques rafales de charges de stockage provoquent un retrait pas si «doux» des ressources des machines virtuelles, et par conséquent, les files d'attente s'accumulent sur les processeurs et, par conséquent, <del>  et les loups ont faim et les moutons sont morts </del>  et virtualka se bloquent, et il n'y a aucun IOPS. </p><br><p>  Peut-être qu'à l'avenir, nous reviendrons sur ce problème, mais pour l'instant, nous avons mis en œuvre la délivrance de ressources pour les OI à la manière du bon vieux grand-père. </p><br><p>  Sur la base des données de dimensionnement, l'administrateur pré-alloue le nième nombre de cœurs de CPU et de RAM pour le service MasterIO.  Ces ressources se voient attribuer un monopole, c'est-à-dire  ils ne peuvent en aucun cas être utilisés pour les besoins de la machine virtuelle tant que l'administrateur ne le permet pas.  Les ressources sont réparties de manière égale, c'est-à-dire  la même quantité de ressources système est prélevée sur chaque nœud du cluster.  Tout d'abord, les ressources processeur intéressent MasterIO (la RAM est moins importante), surtout si nous utilisons le codage Erasure. </p><br><p>  Si une erreur s'est produite avec le dimensionnement et que nous avons donné trop de ressources à MasterIO, la situation est facilement résolue en supprimant ces ressources dans le pool de ressources de la machine virtuelle.  Si les ressources sont inactives, elles retourneront presque immédiatement au pool de ressources de la machine virtuelle, mais si ces ressources sont éliminées, vous devrez attendre un certain temps pour que MasterIO les libère en douceur. </p><br><p>  La situation inverse est plus compliquée.  Si nous avions besoin d'augmenter le nombre de cœurs pour MasterIO, et qu'ils sont occupés avec des virtuels, alors nous devons «négocier» avec les virtuels, c'est-à-dire les sélectionner avec des poignées, car en mode automatique dans une situation de forte augmentation de la charge, cette opération est lourde de gels de VM et d'autres comportements capricieux. </p><br><p>  En conséquence, une grande attention doit être accordée au dimensionnement des performances des systèmes hyperconvergés IO (pas seulement les nôtres).  Un peu plus tard, dans l'un des articles, nous promettons d'examiner cette question plus en détail. </p><br><h2 id="gipervizor">  Hyperviseur </h2><br><p>  Hypervisor Aist est responsable de l'exécution des machines virtuelles dans vAIR.  Cet hyperviseur est basé sur l'hyperviseur KVM éprouvé.  En principe, beaucoup de choses ont été écrites sur le travail de KVM, il n'y a donc pas besoin de le peindre, il suffit d'indiquer que toutes les fonctions standard de KVM sont stockées dans Stork et fonctionnent correctement. </p><br><p>  Par conséquent, nous décrirons ici les principales différences par rapport au KVM standard, que nous avons implémenté dans Stork.  La cigogne fait partie du système (hyperviseur préinstallé) et elle est contrôlée à partir de la console commune vAIR via le Web-GUI (versions russe et anglaise) et SSH (évidemment, uniquement en anglais). </p><br><p><img src="https://habrastorage.org/webt/vi/ju/9e/viju9ee4sxkeq-ezvoobttyjvck.png"></p><br><p>  De plus, les configurations d'hyperviseur sont stockées dans la base de données ConfigDB distribuée (à ce sujet un peu plus tard), qui est également un point de contrôle unique.  Autrement dit, vous pouvez vous connecter à n'importe quel nœud du cluster et tout gérer sans avoir besoin d'un serveur de gestion distinct. </p><br><p>  Le module HA que nous avons développé constitue un ajout important à la fonctionnalité KVM standard.  Il s'agit de l'implémentation la plus simple d'un cluster de machines virtuelles haute disponibilité, qui vous permet de redémarrer automatiquement la machine virtuelle sur un autre nœud de cluster en cas de défaillance d'un nœud. </p><br><p>  Une autre fonctionnalité utile est le déploiement en masse de machines virtuelles (pertinent pour les environnements VDI), qui automatisera le déploiement des machines virtuelles avec leur distribution automatique entre les nœuds en fonction de la charge sur eux. </p><br><p>  La distribution de VM entre les nœuds est la base de l'équilibrage de charge automatique (ala DRS).  Cette fonction n'est pas encore disponible dans la version actuelle, mais nous y travaillons activement et elle apparaîtra certainement dans l'une des prochaines mises à jour. </p><br><p>  L'hyperviseur VMware ESXi est pris en charge en option, il est actuellement implémenté à l'aide du protocole iSCSI et la prise en charge NFS est également prévue à l'avenir. </p><br><h2 id="virtualnye-kommutatory">  Commutateurs virtuels </h2><br><p>  Pour la mise en œuvre logicielle des commutateurs, un composant distinct est fourni - Fractal.  Comme dans nos autres composants, nous passons du simple au complexe, donc dans la première version, une commutation simple est implémentée, tandis que le routage et le pare-feu sont accordés à des appareils tiers.  Le principe de fonctionnement est standard.  L'interface physique du serveur est connectée par un pont à l'objet Fractal - un groupe de ports.  Un groupe de ports, à son tour, avec les machines virtuelles souhaitées dans le cluster.  L'organisation des VLAN est prise en charge et dans l'une des prochaines versions, la prise en charge du VxLAN sera ajoutée.  Tous les commutateurs créés sont distribués par défaut, c'est-à-dire  distribués sur tous les nœuds du cluster, de sorte que les machines virtuelles vers lesquelles basculer pour se connecter à la machine virtuelle ne dépendent pas du nœud d'emplacement, cela dépend uniquement de la décision de l'administrateur. </p><br><h2 id="monitoring-i-statistika">  Suivi et statistiques </h2><br><p>  Le composant responsable du suivi et des statistiques (titre de travail Monica) est en fait un clone repensé du système de stockage ENGINE.  À un moment donné, il s'est bien recommandé et nous avons décidé de l'utiliser avec vAIR avec un réglage facile.  Comme tous les autres composants, Monica est exécuté et stocké sur tous les nœuds du cluster en même temps. </p><br><p>  Les responsabilités difficiles de Monica peuvent être décrites comme suit: </p><br><p>  Collecte de données: </p><br><ul><li>  des capteurs matériels (ce qui peut donner du fer sur IPMI); </li><li>  à partir d'objets logiques vAIR (ARDFS, Stork, Fractal, MasterIO et autres objets). </li></ul><br><p><img src="https://habrastorage.org/webt/1c/4k/u8/1c4ku82o_bkanp-348z-jbeioma.png"></p><br><p>  Collecte de données dans une base de données distribuée; </p><br><p>  Interprétation des données sous forme de: </p><br><ul><li>  journaux; </li><li>  Alertes </li><li>  horaires. </li></ul><br><p>  Interaction externe avec des systèmes tiers via les protocoles SMTP (envoi d'alertes par e-mail) et SNMP (interaction avec des systèmes de surveillance tiers). </p><br><p><img src="https://habrastorage.org/webt/2k/-p/9o/2k-p9oah-yrta3n5ti0m5yfm22e.png"></p><br><h2 id="raspredelennaya-baza-konfiguraciy">  Base de configuration distribuée </h2><br><p>  Dans les paragraphes précédents, il a été mentionné que de nombreuses données sont stockées sur tous les nœuds du cluster en même temps.  Pour organiser cette méthode de stockage, une base de données ConfigDB distribuée spéciale est fournie.  Comme son nom l'indique, la base de données stocke les configurations de tous les objets du cluster: hyperviseur, machines virtuelles, module HA, commutateurs, système de fichiers (à ne pas confondre avec la base de données de métadonnées FS, il s'agit d'une autre base de données), ainsi que des statistiques.  Ces données sont stockées de manière synchrone sur tous les nœuds et la cohérence de ces données est une condition préalable au fonctionnement stable de vAIR. </p><br><p>  Un point important: bien que le fonctionnement de ConfigDB soit vital pour le fonctionnement de vAIR, son échec, bien qu'il arrête le cluster, n'affecte pas la cohérence des données stockées dans ARDFS, ce qui à notre avis est un plus pour la fiabilité de la solution dans son ensemble. </p><br><p>  ConfigDB est également un point de gestion unique, vous pouvez donc accéder à n'importe quel nœud du cluster par adresse IP et gérer entièrement tous les nœuds du cluster, ce qui est assez pratique. </p><br><p>  De plus, pour accéder aux systèmes externes, ConfigDB fournit une API Restful à travers laquelle vous pouvez configurer l'intégration avec des systèmes tiers.  Par exemple, nous avons récemment réalisé une intégration pilote avec plusieurs solutions russes dans les domaines du VDI et de la sécurité de l'information.  Lorsque les projets seront terminés, nous serons heureux d'écrire les détails techniques ici. </p><br><h2 id="kartina-v-celom">  L'image entière </h2><br><p>  En conséquence, nous avons deux versions de l'architecture du système. </p><br><p>  Dans le premier cas principal, notre hyperviseur Aist basé sur KVM et nos commutateurs logiciels Fractal sont utilisés. </p><br><p>  <strong>Scénario 1. Vrai</strong> </p><br><p><img src="https://habrastorage.org/webt/0-/pb/7j/0-pb7j1-zdrpqone3dlvw5un4py.png"></p><br><p>  Dans la seconde - option facultative - lorsque vous souhaitez utiliser l'hyperviseur ESXi, le schéma est quelque peu compliqué.  Pour utiliser ESXi, il doit être installé de manière standard sur les lecteurs locaux du cluster.  Ensuite, sur chaque nœud ESXi, la machine virtuelle vAIR MasterVM est installée, qui contient une distribution spéciale vAIR à exécuter en tant que machine virtuelle VMware. </p><br><p>  ESXi offre tous les disques locaux gratuits par transfert direct à MasterVM.  À l'intérieur de MasterVM, ces disques sont déjà formatés en ARDFS et livrés à l'extérieur (ou plutôt, à ESXi) en utilisant le protocole iSCSI (et à l'avenir il y aura également NFS) via les interfaces dédiées dans ESXi.  En conséquence, les machines virtuelles et le réseau de logiciels dans ce cas sont fournis par ESXi. </p><br><p>  <strong>Scénario 2. ESXi</strong> </p><br><p><img src="https://habrastorage.org/webt/no/jf/cv/nojfcvippesbznyxdmzpvnmq2se.png"></p><br><p>  Nous avons donc démonté tous les principaux composants de l'architecture vAIR et leurs tâches.  Dans le prochain article, nous parlerons des fonctionnalités déjà mises en œuvre et des plans pour un avenir proche. </p><br><p>  Nous attendons vos commentaires et suggestions. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr475254/">https://habr.com/ru/post/fr475254/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr475244/index.html">Nouvelles fonctionnalités JavaScript attendues que vous devez connaître</a></li>
<li><a href="../fr475246/index.html">Programmation asynchrone Python: un bref aperçu</a></li>
<li><a href="../fr475248/index.html">L'utilisation de polyfills lors de l'écriture d'applications inter-navigateurs</a></li>
<li><a href="../fr475250/index.html">Comme Redash l'a remarqué et a corrigé un problème qui provoquait une dégradation des performances du code Python</a></li>
<li><a href="../fr475252/index.html">Comment critiquer Microsoft</a></li>
<li><a href="../fr475258/index.html">Une représentation visuelle des élections à Saint-Pétersbourg - la magie de l'habillage vocal</a></li>
<li><a href="../fr475260/index.html">La différence entre une fonction asynchrone et une fonction qui renvoie une promesse</a></li>
<li><a href="../fr475262/index.html">Le condensé de matières fraîches du monde du front-end de la dernière semaine n ° 388 (4-10 novembre 2019)</a></li>
<li><a href="../fr475264/index.html">Des renifleurs qui pourraient: comment la famille FakeSecurity a infecté les boutiques en ligne</a></li>
<li><a href="../fr475266/index.html">Nous inversons les mobiles 1 sous Android. Comment ajouter un peu de fonctionnalité et abandonner quelques soirées</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>