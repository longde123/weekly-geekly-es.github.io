<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìÇ üö¢ üç∑ K√ºnstliche Intelligenz f√ºr allgemeine Zwecke. TK, aktueller Status, Perspektiven üïπÔ∏è üôÖüèæ üå•Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Heutzutage bedeuten die W√∂rter "k√ºnstliche Intelligenz" viele verschiedene Systeme - von einem neuronalen Netzwerk zur Bilderkennung bis zu einem Bot ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>K√ºnstliche Intelligenz f√ºr allgemeine Zwecke. TK, aktueller Status, Perspektiven</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/468379/">  Heutzutage bedeuten die W√∂rter "k√ºnstliche Intelligenz" viele verschiedene Systeme - von einem neuronalen Netzwerk zur Bilderkennung bis zu einem Bot zum Spielen von Quake.  Wikipedia gibt eine wunderbare Definition von KI - dies ist "die Eigenschaft intelligenter Systeme, kreative Funktionen auszuf√ºhren, die traditionell als Vorrecht des Menschen angesehen werden".  Das hei√üt, aus der Definition geht klar hervor, dass eine bestimmte Funktion, wenn sie erfolgreich automatisiert wurde, nicht mehr als k√ºnstliche Intelligenz betrachtet wird. <br><br>  Als jedoch die Aufgabe ‚Äûk√ºnstliche Intelligenz schaffen‚Äú zum ersten Mal gestellt wurde, bedeutete KI etwas anderes.  Dieses Ziel wird jetzt als Starke KI oder Allzweck-KI bezeichnet. <br><a name="habracut"></a><br><h2>  Erkl√§rung des Problems </h2><br>  Nun gibt es zwei bekannte Formulierungen des Problems.  Der erste ist Starke KI.  Die zweite ist eine Allzweck-KI (auch bekannt als Artifical General Intelligence, abgek√ºrzt AGI). <br>  Upd.  In den Kommentaren sagen sie mir, dass dieser Unterschied auf der Ebene der Sprache wahrscheinlicher ist.  Im Russischen bedeutet das Wort "Intelligenz" nicht genau das, was das Wort "Intelligenz" im Englischen bedeutet <br><br>  <b>Eine starke KI</b> ist eine hypothetische KI, die alles kann, was eine Person tun kann.  Es wird normalerweise erw√§hnt, dass er den Turing-Test in der Anfangseinstellung bestehen muss (hmm, bestehen die Leute ihn?), Sich seiner selbst als separate Person bewusst sein und in der Lage sein muss, seine Ziele zu erreichen. <br><br>  Das hei√üt, es ist so etwas wie eine k√ºnstliche Person.  Meiner Meinung nach ist der Nutzen einer solchen KI haupts√§chlich Forschung, da die Definitionen einer starken KI nirgendwo sagen, was ihre Ziele sein werden. <br><br>  <b>AGI oder Allzweck-KI</b> ist eine ‚ÄûMaschine der Ergebnisse‚Äú.  Sie erh√§lt eine bestimmte Zielsetzung am Eingang - und gibt einige Steueraktionen f√ºr Motoren / Laser / Netzwerkkarten / Monitore aus.  Und das Ziel ist erreicht.  Gleichzeitig hat AGI zun√§chst kein Wissen √ºber die Umgebung - nur Sensoren, Aktoren und den Kanal, √ºber den es Ziele setzt.  Das Managementsystem wird als AGI betrachtet, wenn es in einer beliebigen Umgebung Ziele erreichen kann.  Wir haben sie dazu gebracht, ein Auto zu fahren und Unf√§lle zu vermeiden - sie wird damit umgehen.  Wir haben sie unter die Kontrolle eines Kernreaktors gebracht, damit mehr Energie vorhanden ist, aber nicht explodiert - sie kann damit umgehen.  Wir werden einen Briefkasten geben und anweisen, Staubsauger zu verkaufen - werden auch damit fertig.  AGI ist ein L√∂ser f√ºr "inverse Probleme".  Es ist ganz einfach zu √ºberpr√ºfen, wie viele Staubsauger verkauft werden.  Aber herauszufinden, wie man eine Person davon √ºberzeugt, diesen Staubsauger zu kaufen, ist bereits eine Aufgabe f√ºr den Intellekt. <br><br>  In diesem Artikel werde ich √ºber AGI sprechen.  Keine Turing-Tests, kein Selbstbewusstsein, keine k√ºnstlichen Pers√∂nlichkeiten - au√üergew√∂hnlich pragmatische KI und nicht weniger pragmatische Operatoren. <br><br><h2>  Aktueller Stand der Dinge </h2><br>  Jetzt gibt es eine Klasse von Systemen wie Verst√§rkungslernen oder verst√§rktes Lernen.  Dies ist so etwas wie AGI, nur ohne Vielseitigkeit.  Sie k√∂nnen lernen und dadurch Ziele in einer Vielzahl von Umgebungen erreichen.  Dennoch sind sie in jedem Umfeld weit davon entfernt, Ziele zu erreichen. <br><br>  Wie sind Reinforcement Learning-Systeme im Allgemeinen angeordnet und was sind ihre Probleme? <br><br><img src="https://habrastorage.org/webt/oj/3-/dl/oj3-dl6vkgtbhaxeilj7rkl0jxe.png"><br><br>  Jedes RL ist so angeordnet.  Es gibt ein Steuerungssystem, einige Signale √ºber die umgebende Realit√§t treten durch die Sensoren (Zustand) und durch die Leitungsgremien (Aktionen) in das System ein, das auf die umgebende Realit√§t einwirkt.  Belohnung ist ein Signal der Verst√§rkung.  In RL-Systemen wird die Verst√§rkung von au√üerhalb der Steuereinheit gebildet und zeigt an, wie gut die KI mit dem Erreichen des Ziels zurechtkommt.  Wie viele Staubsauger wurden zum Beispiel in letzter Minute verkauft? <br>  Dann wird eine Tabelle aus so etwas gebildet (ich werde es die SAR-Tabelle nennen): <br><br><img src="https://habrastorage.org/webt/lp/oo/ek/lpooekbdpwktkkmvsilzsytsnbo.png"><br><br>  Die Zeitachse ist nach unten gerichtet.  Die Tabelle zeigt alles, was die KI getan hat, alles, was sie gesehen hat und alle Verst√§rkungssignale.  Damit RL etwas Sinnvolles tun kann, muss er normalerweise zun√§chst eine Weile zuf√§llige Bewegungen ausf√ºhren oder sich die Bewegungen einer anderen Person ansehen.  Im Allgemeinen startet RL, wenn die SAR-Tabelle bereits mindestens einige Zeilen enth√§lt. <br>  Was passiert als n√§chstes? <br><br><h3>  Sarsa </h3><br>  Die einfachste Form des verst√§rkenden Lernens. <br><br>  Wir nehmen eine Art maschinelles Lernmodell und sagen unter Verwendung einer Kombination von S und A (Zustand und Aktion) das Gesamt-R f√ºr die n√§chsten Taktzyklen voraus.  Zum Beispiel werden wir sehen, dass (basierend auf der obigen Tabelle), wenn Sie einer Frau sagen: ‚ÄûSei ein Mann, kaufe einen Staubsauger!‚Äú, Dann ist die Belohnung niedrig, und wenn du einem Mann dasselbe sagst, dann hoch. <br><br>  Welche spezifischen Modelle verwendet werden k√∂nnen - ich werde sp√§ter beschreiben, im Moment werde ich nur sagen, dass dies nicht nur neuronale Netze sind.  Sie k√∂nnen Entscheidungsb√§ume verwenden oder sogar eine Funktion in einer Tabellenform definieren. <br><br>  Und dann passiert folgendes.  AI empf√§ngt eine andere Nachricht oder einen Link zu einem anderen Client.  Alle Kundendaten werden von au√üen in die KI eingegeben - wir betrachten den Kundenstamm und den Nachrichtenz√§hler als Teil des Sensorsystems.  Das hei√üt, es bleibt etwas A (Aktion) zuzuweisen und auf Verst√§rkung zu warten.  AI ergreift alle m√∂glichen Ma√ünahmen und sagt dies wiederum voraus (unter Verwendung des gleichen Modells f√ºr maschinelles Lernen) - was passiert, wenn ich das tue?  Was ist, wenn es ist?  Und wie viel Verst√§rkung wird daf√ºr sein?  Und dann f√ºhrt RL die Aktion aus, f√ºr die die maximale Belohnung erwartet wird. <br><br>  Ich habe ein so einfaches und ungeschicktes System in eines meiner Spiele eingef√ºhrt.  SARSA stellt Einheiten im Spiel ein und passt sich im Falle einer √Ñnderung der Spielregeln an. <br><br>  Dar√ºber hinaus gibt es bei allen Arten von verst√§rktem Training einen Rabatt auf Belohnungen und ein Explorations- / Exploit-Dilemma. <br><br>  Das Diskontieren von Belohnungen ist ein solcher Ansatz, wenn RL versucht, nicht den Belohnungsbetrag f√ºr die n√§chsten N Z√ºge zu maximieren, sondern den gewichteten Betrag nach dem Prinzip "100 Rubel sind jetzt besser als 110 in einem Jahr".  Wenn der Abzinsungsfaktor beispielsweise 0,9 betr√§gt und der Planungshorizont 3 betr√§gt, trainieren wir das Modell nicht f√ºr die n√§chsten 3 Taktzyklen, sondern f√ºr R1 * 0,9 + R2 * 0,81 + R3 * 0,729.  Warum ist das notwendig?  Dann brauchen wir diese KI, die irgendwo im Unendlichen einen Gewinn erzielt, nicht.  Wir brauchen eine KI, die hier und jetzt einen Gewinn generiert. <br>  Dilemma erkunden / ausnutzen.  Wenn RL das tut, was sein Modell f√ºr optimal h√§lt, wird es nie erfahren, ob es bessere Strategien gibt.  Exploit ist eine Strategie, bei der RL das tut, was maximale Belohnungen verspricht.  Explore ist eine Strategie, bei der RL etwas unternimmt, um die Umgebung auf der Suche nach besseren Strategien zu erkunden.  Wie implementiere ich effektive Intelligenz?  Beispielsweise k√∂nnen Sie alle paar Takte eine zuf√§llige Aktion ausf√ºhren.  Oder Sie k√∂nnen nicht ein Vorhersagemodell erstellen, sondern mehrere mit leicht unterschiedlichen Einstellungen.  Sie f√ºhren zu unterschiedlichen Ergebnissen.  Je gr√∂√üer der Unterschied ist, desto gr√∂√üer ist der Unsicherheitsgrad dieser Option.  Sie k√∂nnen die Aktion so ausf√ºhren, dass sie den Maximalwert hat: M + k * std, wobei M die durchschnittliche Vorhersage aller Modelle ist, std die Standardabweichung der Vorhersagen ist und k der Neugierkoeffizient ist. <br><br>  <b>Was sind die Nachteile?</b> <br><br>  Nehmen wir an, wir haben Optionen.  Gehen Sie mit dem Auto zum Ziel (das 10 km von uns entfernt ist und die Stra√üe dorthin ist gut) oder gehen Sie zu Fu√ü.  Und dann, nach dieser Wahl, haben wir Optionen - gehen Sie vorsichtig vor oder versuchen Sie, gegen jede S√§ule zu sto√üen. <br><br>  Die Person wird sofort sagen, dass es normalerweise besser ist, ein Auto zu fahren und sich vorsichtig zu verhalten. <br><br>  Aber SARSA ... Er wird sich ansehen, wozu die Entscheidung, mit dem Auto zu fahren, zuvor gef√ºhrt hat.  Aber es f√ºhrte dazu.  In der Phase der ersten Statistik fuhr die KI r√ºcksichtslos und st√ºrzte in der H√§lfte der F√§lle irgendwo ab.  Ja, er kann gut fahren.  Aber wenn er sich f√ºr ein Auto entscheidet, wei√ü er nicht, was er als n√§chstes w√§hlen wird.  Er hat Statistiken - dann w√§hlte er in der H√§lfte der F√§lle die geeignete Option und in der H√§lfte Selbstmord.  Daher ist es im Durchschnitt besser zu laufen. <br><br>  SARSA geht davon aus, dass der Agent dieselbe Strategie einh√§lt, mit der die Tabelle gef√ºllt wurde.  Und handelt auf dieser Basis.  Aber was ist, wenn wir etwas anderes annehmen - dass der Agent in den n√§chsten Schritten die beste Strategie einh√§lt? <br><br><h3>  Q-Learning </h3><br>  Dieses Modell berechnet f√ºr jeden Staat die maximal erreichbare Gesamtbelohnung daraus.  Und er schreibt es in eine spezielle Spalte Q. Das hei√üt, wenn Sie aus dem Zustand S je nach Kurs 2 Punkte oder 1 erhalten k√∂nnen, ist Q (S) gleich 2 (mit einer Vorhersage-Tiefe von 1).  Welche Belohnung aus dem Zustand S erhalten werden kann, lernen wir aus dem Vorhersagemodell Y (S, A).  (S - Zustand, A - Aktion). <br><br>  Dann erstellen wir ein Vorhersagemodell Q (S, A) - das hei√üt, in welchen Zustand Q geht, wenn wir Aktion A von S ausf√ºhren. Und erstellen die n√§chste Spalte in der Tabelle - Q2.  Das hei√üt, das Maximum Q, das aus dem Zustand S erhalten werden kann (wir sortieren alle m√∂glichen A). <br><br>  Dann erstellen wir ein Regressionsmodell Q3 (S, A) - das hei√üt, in den Zustand, mit dem wir Q2 gehen, wenn wir Aktion A von S ausf√ºhren. <br><br>  Usw.  So k√∂nnen wir eine unbegrenzte Prognosetiefe erreichen. <br><br><img src="https://habrastorage.org/webt/ie/e_/gn/iee_gnm9ldz5uiv0dmyj4jni470.jpeg"><br><br>  Im Bild ist R die Verst√§rkung. <br><br>  Und dann w√§hlen wir bei jeder Bewegung die Aktion aus, die das gr√∂√üte Qn verspricht.  Wenn wir diesen Algorithmus auf Schach anwenden w√ºrden, w√ºrden wir so etwas wie einen idealen Minimax erhalten.  Etwas, das fast einer Fehlkalkulation entspricht, bewegt sich in gro√üe Tiefen. <br><br>  Ein h√§ufiges Beispiel f√ºr Q-Learning-Verhalten.  Der J√§ger hat einen Speer und geht von sich aus mit zum B√§ren.  Er wei√ü, dass die √ºberwiegende Mehrheit seiner zuk√ºnftigen Z√ºge eine sehr gro√üe negative Belohnung hat (es gibt viel mehr M√∂glichkeiten zu verlieren als zu gewinnen). Er wei√ü, dass es Z√ºge mit einer positiven Belohnung gibt.  Der J√§ger glaubt, dass er in Zukunft die besten Z√ºge machen wird (und es ist nicht bekannt, welche wie in SARSA), und wenn er die besten Z√ºge macht, wird er den B√§ren besiegen.  Das hei√üt, um zum B√§ren zu gehen, reicht es aus, dass er jedes Element herstellen kann, das f√ºr die Jagd notwendig ist, aber es ist nicht notwendig, Erfahrung mit sofortigem Erfolg zu haben. <br><br>  Wenn der J√§ger im SARSA-Stil handeln w√ºrde, w√ºrde er davon ausgehen, dass seine Handlungen in Zukunft ungef√§hr die gleichen sein w√ºrden wie zuvor (trotz der Tatsache, dass er jetzt ein anderes Wissensgep√§ck hat), und er w√ºrde nur zum B√§ren gehen, wenn er bereits zu ging und er gewann zum Beispiel in&gt; 50% der F√§lle (na ja, oder wenn andere J√§ger in mehr als der H√§lfte der F√§lle gewonnen haben, wenn er aus ihren Erfahrungen lernt). <br><br>  <b>Was sind die Nachteile?</b> <br><br><ol><li>  Das Modell kommt mit der sich √§ndernden Realit√§t nicht zurecht.  Wenn wir unser ganzes Leben lang f√ºr das Dr√ºcken des roten Knopfes ausgezeichnet wurden und sie uns jetzt bestrafen und keine sichtbaren Ver√§nderungen aufgetreten sind ... QL wird dieses Muster f√ºr eine sehr lange Zeit beherrschen. </li><li>  Qn kann eine sehr komplexe Funktion sein.  Um dies zu berechnen, m√ºssen Sie beispielsweise einen Zyklus von N Iterationen scrollen - und es wird nicht schneller funktionieren.  Ein Vorhersagemodell weist normalerweise eine begrenzte Komplexit√§t auf - selbst ein gro√ües neuronales Netzwerk hat eine Komplexit√§tsgrenze, und fast kein maschinelles Lernmodell kann Zyklen drehen. </li><li>  Die Realit√§t hat normalerweise versteckte Variablen.  Wie sp√§t ist es jetzt zum Beispiel?  Es ist leicht herauszufinden, ob wir auf die Uhr schauen, aber sobald wir wegschauen, ist dies bereits eine versteckte Variable.  Um diese nicht beobachtbaren Werte zu ber√ºcksichtigen, muss das Modell nicht nur den aktuellen Status, sondern auch eine Art Verlauf ber√ºcksichtigen.  In QL k√∂nnen Sie dies tun - zum Beispiel, um nicht nur das aktuelle S, sondern auch mehrere vorherige in das Neuron-oder-was-bei-uns-dort einzuspeisen.  Dies geschieht in RL, das Atari-Spiele spielt.  Dar√ºber hinaus k√∂nnen Sie ein wiederkehrendes neuronales Netzwerk f√ºr die Vorhersage verwenden - lassen Sie es nacheinander √ºber mehrere Frames des Verlaufs laufen und berechnen Sie Qn. </li></ol><br><h3>  Modellbasierte Systeme </h3><br>  Aber was ist, wenn wir nicht nur R oder Q vorhersagen, sondern im Allgemeinen alle sensorischen Daten?  Wir werden st√§ndig eine Taschenkopie der Realit√§t haben und in der Lage sein, unsere Pl√§ne darauf zu √ºberpr√ºfen.  In diesem Fall sind wir viel weniger besorgt √ºber die Schwierigkeit, die Q-Funktion zu berechnen.  Ja, f√ºr die Berechnung sind viele Uhren erforderlich. Auf jeden Fall werden wir f√ºr jeden Plan das Prognosemodell wiederholt ausf√ºhren.  Planen Sie 10 Schritte vorw√§rts?  Wir starten das Modell zehnmal und jedes Mal, wenn wir seine Ausgaben seiner Eingabe zuf√ºhren. <br><br>  <b>Was sind die Nachteile?</b> <br><br><ol><li>  Ressourcenintensit√§t.  Angenommen, wir m√ºssen bei jeder Ma√ünahme zwei Alternativen ausw√§hlen.  Dann haben wir f√ºr 10 Taktzyklen 2 ^ 10 = 1024 m√∂gliche Pl√§ne.  Jeder Plan umfasst 10 Modellstarts.  Wenn wir ein Flugzeug mit Dutzenden von Leitungsgremien kontrollieren?  Und simulieren wir die Realit√§t mit einem Zeitraum von 0,1 Sekunden?  M√∂chten Sie einen Planungshorizont f√ºr mindestens ein paar Minuten haben?  Wir m√ºssen das Modell viele Male ausf√ºhren, es gibt viele Prozessortaktzyklen f√ºr eine L√∂sung.  Selbst wenn Sie die Aufz√§hlung von Pl√§nen irgendwie optimieren, gibt es dennoch Gr√∂√üenordnungen mehr Berechnungen als in QL. </li><li>  Das Problem des Chaos.  Einige Systeme sind so konzipiert, dass bereits eine geringe Ungenauigkeit der Eingabesimulation zu einem gro√üen Ausgabefehler f√ºhrt.  Um dem entgegenzuwirken, k√∂nnen Sie mehrere Simulationen der Realit√§t ausf√ºhren - etwas anders.  Sie werden sehr unterschiedliche Ergebnisse liefern, und daraus wird es m√∂glich sein zu verstehen, dass wir uns in der Zone einer solchen Instabilit√§t befinden. </li></ol><br><h2>  Strategie-Aufz√§hlungsmethode </h2><br>  Wenn wir Zugriff auf die Testumgebung f√ºr KI haben, wenn wir sie nicht in der Realit√§t, sondern in einer Simulation ausf√ºhren, k√∂nnen wir die Strategie des Verhaltens unseres Agenten in irgendeiner Form aufschreiben.  Und dann w√§hlen Sie - mit Evolution oder etwas anderem - eine Strategie, die zu maximalem Gewinn f√ºhrt. <br>  ‚ÄûW√§hle eine Strategie‚Äú bedeutet, dass wir zuerst lernen m√ºssen, wie man eine Strategie so aufschreibt, dass sie in den Evolutionsalgorithmus √ºbernommen werden kann.  Das hei√üt, wir k√∂nnen die Strategie mit Programmcode schreiben, aber an einigen Stellen lassen wir die Koeffizienten und lassen sie von der Evolution aufgreifen.  Oder wir k√∂nnen eine Strategie mit einem neuronalen Netzwerk aufschreiben - und die Evolution die Gewichte ihrer Verbindungen erfassen lassen. <br><br>  Das hei√üt, hier gibt es keine Prognose.  Keine SAR-Tabelle.  Wir w√§hlen einfach eine Strategie aus und sie gibt sofort Aktionen aus. <br><br>  Dies ist eine leistungsstarke und effektive Methode. Wenn Sie RL ausprobieren m√∂chten und nicht wissen, wo Sie anfangen sollen, empfehle ich sie.  Dies ist ein sehr billiger Weg, um ‚Äûein Wunder zu sehen‚Äú. <br><br>  <b>Was sind die Nachteile?</b> <br><br><ol><li>  Die F√§higkeit, dieselben Experimente viele Male durchzuf√ºhren, ist erforderlich.  Das hei√üt, wir sollten in der Lage sein, die Realit√§t bis zum Ausgangspunkt zur√ºckzuspulen - zehntausende Male.  Eine neue Strategie ausprobieren. <br><br>  Das Leben bietet selten solche M√∂glichkeiten.  Wenn wir ein Modell des Prozesses haben, an dem wir interessiert sind, k√∂nnen wir normalerweise keine listige Strategie erstellen - wir k√∂nnen einfach einen Plan erstellen, wie bei einem modellbasierten Ansatz, selbst mit stumpfer roher Gewalt. </li><li>  Unvertr√§glichkeit gegen√ºber Erfahrung.  Haben wir eine SAR-Tabelle mit jahrelanger Erfahrung?  Wir k√∂nnen es vergessen, es passt nicht in das Konzept. </li></ol><br><h3>  Eine Methode zur Aufz√§hlung von Strategien, aber "leben" </h3><br>  Die gleiche Aufz√§hlung von Strategien, aber auf lebendige Realit√§t.  Wir versuchen 10 Ma√ünahmen einer Strategie.  Dann misst 10 einen anderen.  Dann 10 Takte des dritten.  Dann w√§hlen wir die aus, bei der es mehr Verst√§rkung gab. <br>  Die besten Ergebnisse f√ºr laufende Humanoide wurden mit dieser Methode erzielt. <br><br><img src="https://habrastorage.org/webt/zh/v_/sn/zhv_snutr8ma1aeenqr3itjojvc.png"><br><br>  F√ºr mich klingt dies etwas unerwartet - es scheint, dass der QL + -Modell-basierte Ansatz mathematisch ideal ist.  Aber nichts dergleichen.  Die Vorteile des Ansatzes sind ungef√§hr die gleichen wie die des vorherigen - aber sie sind weniger ausgepr√§gt, da die Strategien nicht sehr lange getestet werden (wir haben keine Jahrtausende in Bezug auf die Evolution), was bedeutet, dass die Ergebnisse instabil sind.  Dar√ºber hinaus kann auch die Anzahl der Tests nicht auf unendlich angehoben werden - was bedeutet, dass die Strategie in einem nicht sehr komplizierten Bereich von Optionen gesucht werden muss.  Sie wird nicht nur "Stifte" haben, die "verdreht" werden k√∂nnen.  Nun, die Erfahrungsintoleranz wurde nicht abgesagt.  Und im Vergleich zu QL oder modellbasiert nutzen diese Modelle die Erfahrung ineffizient.  Sie brauchen viel mehr Interaktionen mit der Realit√§t als Ans√§tze, die maschinelles Lernen verwenden. <br><br>  Wie Sie sehen k√∂nnen, sollten alle Versuche, theoretisch eine AGI zu erstellen, entweder maschinelles Lernen f√ºr die Vorhersage von Auszeichnungen oder eine Form der parametrischen Notation einer Strategie enthalten, damit Sie diese Strategie mit so etwas wie Evolution aufgreifen k√∂nnen. <br><br>  Dies ist ein starker Angriff auf Personen, die anbieten, KI basierend auf Datenbanken, Logik und konzeptionellen Grafiken zu erstellen.  Wenn Sie, die Bef√ºrworter des symbolischen Ansatzes, dies lesen - willkommen zu den Kommentaren, ich werde froh sein zu wissen, was AGI ohne die oben beschriebene Mechanik tun kann. <br><br><h2>  Modelle f√ºr maschinelles Lernen f√ºr RL </h2><br>  Fast jedes ML-Modell kann f√ºr verst√§rktes Lernen verwendet werden.  Neuronale Netze sind nat√ºrlich gut.  Aber es gibt zum Beispiel KNN.  F√ºr jedes Paar S und A suchen wir nach den √§hnlichsten, aber in der Vergangenheit.  Und wir suchen nach dem, was danach R. Dumm sein wird?  Ja, aber es funktioniert.  Es gibt entscheidende B√§ume - hier ist es besser, einen Spaziergang mit den Schl√ºsselw√∂rtern "Gradientenverst√§rkung" und "entscheidender Wald" zu machen.  K√∂nnen B√§ume komplexe Abh√§ngigkeiten nicht erfassen?  Verwenden Sie Feature-Engineering.  Willst du deine KI n√§her an General?  Verwenden Sie die automatische FE!  Gehen Sie eine Reihe verschiedener Formeln durch, senden Sie sie als Funktionen f√ºr Ihren Boost, verwerfen Sie Formeln, die den Fehler erh√∂hen, und belassen Sie die Formeln, die die Genauigkeit verbessern.  Dann reichen Sie die besten Formeln als Argumente f√ºr die neuen Formeln ein und so weiter. <br><br>  Sie k√∂nnen symbolische Regressionen f√ºr die Vorhersage verwenden, dh einfach Formeln aussortieren, um etwas zu erhalten, das sich Q oder R ann√§hert. Es ist m√∂glich, Algorithmen zu sortieren. Dann erhalten Sie eine sogenannte Solomonov-Induktion, die theoretisch optimal, aber fast sehr schwer zu trainieren ist Approximationen von Funktionen. <br><br>  Neuronale Netze sind jedoch normalerweise ein Kompromiss zwischen Ausdruckskraft und Lernkomplexit√§t.  Die algorithmische Regression nimmt idealerweise jede Abh√§ngigkeit auf - f√ºr Hunderte von Jahren.  Der Entscheidungsbaum wird sehr schnell funktionieren - aber er kann y = a + b nicht extrapolieren.  Ein neuronales Netzwerk ist etwas dazwischen. <br><br><h2>  Entwicklungsperspektiven </h2><br>  Wie kann AGI jetzt genau durchgef√ºhrt werden?  Zumindest theoretisch. <br><br><h3>  Evolution </h3><br>  Wir k√∂nnen viele verschiedene Testumgebungen erstellen und die Entwicklung eines neuronalen Netzwerks starten.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Konfigurationen, die bei allen Versuchen insgesamt mehr Punkte erzielen, werden multipliziert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das neuronale Netzwerk muss √ºber einen Speicher verf√ºgen, und es w√§re w√ºnschenswert, mindestens einen Teil des Speichers in Form eines Bandes wie einer Turing-Maschine oder einer Festplatte zu haben. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das Problem ist, dass man mit Hilfe der Evolution nat√ºrlich so etwas wie RL wachsen lassen kann. </font><font style="vertical-align: inherit;">Aber wie sollte die Sprache aussehen, in der RL kompakt aussieht - damit die Evolution sie findet - und gleichzeitig findet die Evolution keine L√∂sungen wie "Aber werde ich ein Neuron f√ºr 150 Schichten erstellen, damit Sie alle verr√ºckt werden, w√§hrend ich es unterrichte!" . </font><font style="vertical-align: inherit;">Evolution ist wie eine Menge von Analphabeten - es wird Fehler im Code finden und das gesamte System √ºber Bord werfen.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aixi </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie k√∂nnen ein modellbasiertes System erstellen, das auf einer Reihe vieler algorithmischer Regressionen basiert. </font><font style="vertical-align: inherit;">Der Algorithmus ist garantiert vollst√§ndig, was bedeutet, dass es keine Muster gibt, die nicht erfasst werden k√∂nnen. </font><font style="vertical-align: inherit;">Der Algorithmus ist in Code geschrieben - was bedeutet, dass seine Komplexit√§t leicht berechnet werden kann. </font><font style="vertical-align: inherit;">Dies bedeutet, dass es m√∂glich ist, Ihre Hypothesen √ºber das Ger√§t der Welt hinsichtlich der Komplexit√§t mathematisch korrekt zu verfeinern. </font><font style="vertical-align: inherit;">Bei neuronalen Netzen zum Beispiel funktioniert dieser Trick nicht - dort ist die Strafe f√ºr Komplexit√§t sehr indirekt und heuristisch. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es bleibt nur zu lernen, wie man algorithmische Regressionen schnell trainiert. </font><font style="vertical-align: inherit;">Bisher ist das Beste daf√ºr die Evolution, und sie ist unverzeihlich lang.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Seed AI </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es w√§re cool, eine KI zu erstellen, die sich selbst verbessert. Verbessern Sie Ihre F√§higkeit, Probleme zu l√∂sen. Dies mag seltsam erscheinen, aber dieses Problem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wurde bereits f√ºr statische Optimierungssysteme wie die Evolution gel√∂st</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Wenn Sie dies realisieren ... Ist alles √ºber den Aussteller bekannt? Wir werden in sehr kurzer Zeit eine sehr m√§chtige KI bekommen.</font></font><br><br>  Wie kann man das machen? <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie k√∂nnen versuchen, daf√ºr zu sorgen, dass einige der Aktionen in RL die Einstellungen von RL selbst beeinflussen. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Oder geben Sie dem RL-System ein Tool, mit dem Sie neue Vor- und Nachdatenprozessoren f√ºr sich selbst erstellen k√∂nnen. Lassen Sie RL dumm sein, aber es wird in der Lage sein, Taschenrechner, Notebooks und Computer f√ºr sich selbst zu erstellen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine andere M√∂glichkeit besteht darin, mithilfe von Evolution eine Art KI zu erstellen, bei der sich ein Teil der Aktionen auf Codeebene auf das Ger√§t auswirkt. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aber im Moment habe ich keine praktikablen Optionen f√ºr Seed AI gesehen - wenn auch sehr begrenzt. Verstecken sich die Entwickler? Oder sind diese Optionen so schwach, dass sie keine allgemeine Aufmerksamkeit verdient haben und an mir vorbeigegangen sind?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jetzt arbeiten sowohl Google als auch DeepMind haupts√§chlich mit neuronalen Netzwerkarchitekturen. </font><font style="vertical-align: inherit;">Anscheinend wollen sie sich nicht auf die kombinatorische Aufz√§hlung einlassen und versuchen, ihre Ideen f√ºr die Methode der R√ºck√ºbertragung des Fehlers geeignet zu machen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich hoffe, dieser √úbersichtsartikel hat sich als n√ºtzlich erwiesen =) Kommentare sind willkommen, insbesondere Kommentare wie ‚ÄûIch wei√ü, wie man AGI besser macht‚Äú!</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de468379/">https://habr.com/ru/post/de468379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de468363/index.html">Mein Magnum Opus aus der Welt des Mobile Gaming</a></li>
<li><a href="../de468367/index.html">Amazon k√ºndigt Plan zur globalen Erw√§rmung an</a></li>
<li><a href="../de468369/index.html">Wie ich ‚ÄûWildMAN‚Äú erstellt habe - eine Parodie auf viele 8-Bit-Spiele und sie k√ºrzlich auf Android portiert habe</a></li>
<li><a href="../de468371/index.html">Spieldesign zum Leben erwecken. Nahtloser Download oder vollst√§ndiges Eintauchen in God of War 4</a></li>
<li><a href="../de468377/index.html">8 Geschichten √ºber Inner China. Was Ausl√§ndern nicht gezeigt wird</a></li>
<li><a href="../de468381/index.html">Zur√ºck in die Zukunft? Ausstehendes Radiergummi-Quantum</a></li>
<li><a href="../de468383/index.html">Ruby Meme Generator, um Interesse an der Sprache zu wecken</a></li>
<li><a href="../de468385/index.html">Der Desktop ist tot, es lebe der Desktop! Ich sammle habrastatistiki</a></li>
<li><a href="../de468387/index.html">Die Zusammenfassung interessanter Materialien f√ºr den mobilen Entwickler # 316 (vom 16. bis 22. September)</a></li>
<li><a href="../de468389/index.html">Artyom Galonsky, STO-B√ºro des B√ºros: ‚ÄûIch bin gegen so etwas wie einen DevOps-Ingenieur.‚Äú</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>