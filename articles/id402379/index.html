<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ™ğŸ¾ ğŸ’‡ ğŸ½ Apa yang peneliti AI pikirkan tentang kemungkinan risiko yang terkait dengannya ğŸ¤œ â™Šï¸ âœˆï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Saya menjadi tertarik pada risiko yang terkait dengan AI di tahun 2007. Pada saat itu, reaksi kebanyakan orang terhadap topik ini kira-kira seperti in...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apa yang peneliti AI pikirkan tentang kemungkinan risiko yang terkait dengannya</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402379/"> Saya menjadi tertarik pada risiko yang terkait dengan AI di tahun 2007.  Pada saat itu, reaksi kebanyakan orang terhadap topik ini kira-kira seperti ini: "Ini sangat lucu, kembalilah ketika orang lain selain orang-orang Internet akan mempercayainya." <br><br>  Pada tahun-tahun berikutnya, beberapa tokoh yang sangat cerdas dan berpengaruh, termasuk <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bill Gates</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Stephen Hawking,</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Elon Musk</a> , secara publik berbagi keprihatinan mereka tentang risiko AI, diikuti oleh ratusan intelektual lain, dari filsuf Oxford hingga kosmolog dari MIT dan investor dari Silicon Valley .  Dan kita kembali. <br><br>  Kemudian reaksinya berubah menjadi: "Ya, beberapa ilmuwan dan pebisnis dapat mempercayainya, tetapi tidak mungkin mereka akan menjadi ahli nyata di bidang ini yang benar-benar berpengalaman dalam situasi ini." <br><br>  Dari sini muncul pernyataan seperti artikel Science Populer " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bill Gates Takut dengan AI, tetapi Peneliti AI Harus Tahu</a> ": <br><blockquote>  Setelah berbicara dengan peneliti AI - peneliti nyata, yang hampir tidak membuat sistem seperti itu bekerja sama sekali, belum lagi bekerja dengan baik, menjadi jelas bahwa mereka tidak takut bahwa kecerdasan super tiba-tiba akan merayapi mereka, baik sekarang maupun di masa depan .  Terlepas dari semua kisah menakutkan yang diceritakan oleh Mask, para peneliti tidak terburu-buru membangun kamar pelindung dan penghancuran diri dengan hitungan mundur. </blockquote><a name="habracut"></a><br>  Atau, ketika mereka menulis di Fusion.net dalam artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Keberatan tentang robot pembunuh dari seseorang yang benar-benar mengembangkan AI</a> ": <br><blockquote>  Andrew Angie secara profesional mengembangkan sistem AI.  Dia mengajar kursus AI di Stanford, mengembangkan AI di Google, dan kemudian pindah ke mesin pencari Baidu Cina untuk melanjutkan pekerjaannya di garis depan menerapkan AI pada masalah dunia nyata.  Jadi ketika dia mendengar tentang bagaimana Elon Musk atau Stephen Hawking - orang yang tidak secara langsung mengenal teknologi modern - berbicara tentang AI, yang dapat berpotensi menghancurkan umat manusia, orang hampir dapat mendengarnya menutupi wajahnya dengan tangannya. </blockquote><br>  Ramez Naam dari Revolusi Marjinal mengulangi kira-kira hal yang sama dalam artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apa yang peneliti pikirkan tentang risiko AI?</a> ": <br><blockquote>  Elon Musk, Stephen Hawking, dan Bill Gates baru-baru ini menyatakan keprihatinannya bahwa pengembangan AI dapat mengimplementasikan skenario â€œpembunuh AIâ€, dan berpotensi menyebabkan kepunahan umat manusia.  Mereka bukan peneliti AI, dan sejauh yang saya tahu, mereka belum bekerja dengan AI secara langsung.  Apa yang dipikirkan peneliti AI nyata tentang risiko AI? </blockquote><br>  Dia mengutip kata-kata peneliti AI yang dipilih secara khusus, seperti penulis cerita lain - dan kemudian berhenti, tidak menyebutkan pendapat yang berbeda dari ini. <br><br>  Tetapi mereka ada.  Peneliti AI, termasuk para pemimpin di lapangan, telah secara aktif mengungkapkan keprihatinan tentang risiko AI dan di luar intelijen, dan sejak awal.  Saya akan mulai dengan mendaftarkan orang-orang ini, terlepas dari daftar Naam, dan kemudian beralih ke mengapa saya tidak menganggap ini sebagai "diskusi" dalam pengertian klasik yang diharapkan dari daftar bintang-bintang. <br><br>  Kriteria daftar saya adalah sebagai berikut: Saya hanya menyebut peneliti paling bergengsi, atau profesor sains di institut yang baik dengan banyak kutipan makalah ilmiah, atau ilmuwan yang sangat dihormati dari industri yang bekerja untuk perusahaan besar dan memiliki rekam jejak yang baik.  Mereka terlibat dalam AI dan pembelajaran mesin.  Mereka memiliki beberapa pernyataan kuat yang mendukung sudut pandang tertentu mengenai timbulnya singularitas atau risiko serius dari AI dalam waktu dekat.  Beberapa dari mereka memiliki karya tulis atau buku tentang hal ini.  Yang lain hanya mengungkapkan pikiran mereka, percaya bahwa ini adalah topik penting yang layak dipelajari. <br><br>  Jika seseorang tidak setuju dengan dimasukkannya seseorang dalam daftar ini atau percaya bahwa saya lupa sesuatu yang penting, beri tahu saya. <br><br>  * * * * * * * * * * <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Stuart Russell</a> adalah profesor ilmu komputer di Berkeley, pemenang IJCAI Computers And Thought Award, peneliti di Asosiasi Mekanisasi Komputer, peneliti di American Academy of Advanced Scientific Research, direktur Center for Intelligent Systems, pemenang Blaise Pascal Prize, dll.  dll.  Rekan penulis <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">AI: A Modern Approach</a> , sebuah buku teks klasik yang digunakan di 1.200 universitas di seluruh dunia.  Di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs webnya,</a> ia menulis: <br><blockquote>  Di bidang AI, 50 tahun penelitian telah berlangsung di bawah bendera asumsi bahwa semakin pintar semakin baik.  Kepedulian akan manfaat kemanusiaan harus digabungkan dengan ini.  Argumennya sederhana: <br><br>  1. AI kemungkinan berhasil dibuat. <br>  2. Kesuksesan tanpa batas menyebabkan risiko besar dan manfaat besar. <br>  3. Apa yang dapat kita lakukan untuk meningkatkan peluang memperoleh manfaat dan menghindari risiko? <br><br>  Beberapa organisasi sudah menangani masalah-masalah ini, termasuk Institut Masa Depan Kemanusiaan di Oxford, Pusat Studi Risiko Eksistensial di Cambridge (CSER), Institut Studi Kecerdasan Mesin di Berkeley dan Institut Kehidupan Masa Depan di Harvard / MIT (FLI).  Saya di dewan penasihat dengan CSER dan FLI. <br><br>  Sama seperti para peneliti dalam fusi nuklir menganggap masalah membatasi reaksi nuklir sebagai salah satu masalah paling penting di bidangnya, pengembangan bidang AI pasti akan menimbulkan pertanyaan tentang kontrol dan keamanan.  Para peneliti sudah mulai mengajukan pertanyaan, dari yang murni teknis (masalah utama rasionalitas dan utilitas, dll.) Ke yang secara filosofis luas. </blockquote><br>  Di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">edge.org,</a> ia menjelaskan sudut pandang yang serupa: <br><blockquote>  Seperti yang dijelaskan oleh Steve Omohandro, Nick Bostrom dan lainnya, ketidaksesuaian nilai dengan sistem pengambilan keputusan, kemungkinan yang terus berkembang, dapat menyebabkan masalah - bahkan mungkin masalah skala kepunahan, jika mesin lebih mampu daripada manusia.  Beberapa percaya bahwa tidak ada risiko yang dapat diperkirakan bagi umat manusia di abad-abad mendatang, mungkin lupa bahwa perbedaan waktu antara pernyataan percaya diri Rutherford bahwa energi atom tidak pernah dapat diekstraksi dan kurang dari 24 jam berlalu dengan penemuan reaksi berantai nuklir yang dimulai oleh reaksi neutron . </blockquote><br>  Dia juga mencoba menjadi perwakilan dari ide-ide ini di komunitas akademik, yang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menunjukkan</a> : <br><blockquote>  Saya menemukan bahwa orang-orang utama dalam industri ini, yang belum pernah mengungkapkan ketakutan sebelumnya, berpikir kepada diri mereka sendiri bahwa masalah ini perlu ditanggapi dengan sangat serius, dan semakin cepat kita menganggapnya serius, semakin baik. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">David McAllister</a> adalah seorang profesor dan rekan senior di Institut Teknologi Toyota, yang berafiliasi dengan Universitas Chicago, yang sebelumnya bekerja di fakultas MIT dan Institut Cornell.  Dia adalah anggota dari American AI Association, telah menerbitkan lebih dari seratus karya, melakukan penelitian di bidang pembelajaran mesin, teori pemrograman, pengambilan keputusan otomatis, perencanaan AI, linguistik komputasi, dan telah berdampak besar pada algoritma komputer catur Blue Deep yang terkenal.  Menurut sebuah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel</a> di Pittsburgh Tribune Review: <br><blockquote>  Profesor Chicago, David McAllister, menganggap kemunculan kemampuan mesin-mesin cerdas otomatis sepenuhnya yang tak terhindarkan untuk merancang dan membuat versi-versi yang lebih pintar dari diri mereka sendiri, yaitu permulaan suatu peristiwa yang dikenal sebagai singularitas [teknologi].  Singularitas akan memungkinkan mesin menjadi sangat cerdas, mengarah ke "skenario yang sangat berbahaya," katanya. </blockquote><br>  Dalam blognya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Thoughts on Cars</a> , ia menulis: <br><blockquote>  Sebagian besar ilmuwan komputer menolak untuk berbicara tentang kesuksesan nyata dalam AI.  Saya pikir akan lebih masuk akal untuk mengatakan bahwa tidak ada yang bisa memprediksi kapan AI yang sebanding dengan pikiran manusia akan diterima.  John MacArthy pernah mengatakan kepada saya bahwa ketika dia ditanya tentang seberapa cepat AI tingkat manusia akan dibuat, dia menjawab bahwa itu berusia lima hingga lima ratus tahun.  MacArthy pintar.  Mengingat ketidakpastian di bidang ini, masuk akal untuk mempertimbangkan masalah ramah AI ... <br><br>  Pada tahap awal, AI umum akan aman.  Namun, tahap awal OII akan menjadi tempat uji yang sangat baik untuk AI sebagai pelayan atau opsi AI ramah lainnya.  Ben Goertzel juga mengiklankan pendekatan eksperimental dalam posting yang bagus di blog-nya.  Jika era OII yang aman dan tidak terlalu pintar menunggu kita, maka kita akan punya waktu untuk memikirkan masa-masa yang lebih berbahaya. </blockquote><br>  Dia adalah anggota kelompok AAAI Panel On Jangka Panjang AI Futures yang didedikasikan untuk prospek jangka panjang AI, mengetuai komite kontrol jangka panjang, dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">digambarkan sebagai berikut</a> : <br><blockquote>  Makalister berbicara kepada saya tentang pendekatan "singularitas", sebuah peristiwa ketika komputer menjadi lebih pintar daripada orang.  Dia tidak menyebutkan tanggal pasti terjadinya, tetapi mengatakan bahwa ini bisa terjadi dalam beberapa dekade mendatang, dan pada akhirnya pasti akan terjadi.  Inilah pandangannya tentang singularitas.  Dua peristiwa penting akan terjadi: intelijen operasional, di mana kita dapat dengan mudah berbicara dengan komputer, dan reaksi berantai AI, di mana komputer dapat meningkatkan dirinya sendiri tanpa bantuan apa pun, dan kemudian mengulanginya lagi.  Acara pertama yang akan kita perhatikan dalam sistem bantuan otomatis yang akan sangat membantu kita.  Nanti akan menjadi sangat menarik untuk berkomunikasi dengan komputer.  Dan agar komputer dapat melakukan semua yang dapat dilakukan orang, Anda harus menunggu peristiwa kedua terjadi. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Hans Moravek</a> adalah seorang mantan profesor di Institute of Robotics di Carnegie Mellon University, yang dinamai menurut namanya <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">paradoks Moravec</a> , pendiri <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SeeGrid Corporation</a> , yang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">berspesialisasi</a> dalam sistem visi mesin untuk aplikasi industri.  Karyanya, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Sintesis Sensor dalam Jaringan Kepastian Robot Seluler,</a> " telah dikutip lebih dari seribu kali, dan ia diundang untuk menulis artikel untuk British Encyclopedia of Robotics, pada saat artikel-artikel dalam ensiklopedia ditulis oleh para pakar dunia dalam bidang ini, dan bukan ratusan komentator internet anonim. <br><br>  Dia juga penulis <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Robot: Dari Mesin Sederhana ke Pikiran Transendental</a> , yang dijelaskan Amazon sebagai berikut: <br><blockquote>  Dalam buku yang menarik ini, Hans Moravek meramalkan bahwa pada tahun 2040, mesin akan mendekati tingkat intelektual orang, dan pada tahun 2050 mereka akan melampaui kita.  Tetapi sementara Moravec memprediksi akhir era dominasi manusia, visinya tentang peristiwa ini tidak begitu suram.  Dia tidak dipagari dari masa depan di mana mesin menguasai dunia, tetapi menerimanya, dan menjelaskan sudut pandang yang menakjubkan yang menurutnya robot cerdas akan menjadi keturunan evolusi kita.  Moravec percaya bahwa pada akhir proses ini, "ruang maya yang luas akan bersatu dengan manusia super yang tidak manusiawi dan berurusan dengan urusan yang jauh dari manusia, jauh dari masalah manusia dengan bakteri." </blockquote><br>  Shane Leg adalah salah satu pendiri <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DeepMind Technologies</a> , sebuah startup AI yang dibeli pada tahun 2014 seharga $ 500 juta oleh Google.  Dia menerima gelar doktor dari Institute of AI yang dinamai  Dale Moul di Swiss, dan juga bekerja di Divisi Neurobiologi Komputasi.  Gatsby di London.  Di akhir disertasinya, "kecerdasan mesin," ia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menulis</a> : <br><blockquote>  Jika sesuatu muncul yang dapat mendekati kekuatan absolut, itu akan menjadi mesin yang sangat cerdas.  Menurut definisi, dia akan dapat mencapai sejumlah besar tujuan di berbagai lingkungan.  Jika kita hati-hati mempersiapkan kesempatan seperti itu sebelumnya, kita tidak hanya bisa menghindari bencana, tetapi juga memulai era kemakmuran, tidak seperti apa pun yang ada sebelumnya. </blockquote><br>  Dalam wawancara berikutnya, dia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mengatakan</a> : <br><blockquote>  AI sekarang menjadi tempat Internet pada tahun 1988.  Kebutuhan pembelajaran mesin diperlukan dalam aplikasi khusus (mesin pencari seperti Google, dana lindung nilai dan bioinformatika), dan jumlah mereka bertambah setiap tahun.  Saya pikir sekitar pertengahan dekade berikutnya proses ini akan menjadi besar dan nyata.  Boom AI akan berlangsung sekitar 2020, diikuti oleh satu dekade kemajuan pesat, mungkin setelah koreksi pasar.  AI tingkat manusia akan dibuat sekitar pertengahan 2020, meskipun banyak orang tidak akan menerima permulaan acara ini.  Setelah itu, risiko yang terkait dengan AI lanjut akan dipraktikkan.  Saya tidak akan mengatakan tentang "singularitas", tetapi mereka berharap bahwa pada titik tertentu setelah penciptaan hal-hal gila OII akan mulai terjadi.  Itu adalah suatu tempat antara 2025 dan 2040. </blockquote><br>  Dia dan rekan pendiri <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Demis Khasabis</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Mustafa Suleiman</a> menandatangani petisi untuk Institute for Future Life mengenai risiko AI, dan salah satu syarat mereka untuk bergabung dengan Google adalah bahwa perusahaan setuju untuk membentuk <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dewan etika AI</a> untuk mempelajari masalah ini. <br><br>  Steve Omohundro adalah mantan profesor ilmu komputer di University of Illinois, pendiri visi komputer dan kelompok pelatihan di Pusat Studi Sistem Kompleks, dan penemu berbagai perkembangan penting dalam pembelajaran mesin dan visi komputer.  Dia bekerja pada robot membaca bibir, bahasa pemrograman paralel StarLisp, algoritma pembelajaran geometris.  Dia saat ini memimpin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Self-Aware Systems</a> , "tim ilmuwan yang bekerja untuk memastikan bahwa teknologi cerdas bermanfaat bagi umat manusia."  Karyanya, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dasar</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dasar Motivasi AI,</a> " membantu menelurkan domain etika mesin, karena ia mencatat bahwa sistem super-cerdas akan diarahkan ke tujuan yang berpotensi berbahaya.  Dia menulis: <br><blockquote>  Kami telah menunjukkan bahwa semua sistem AI canggih cenderung memiliki serangkaian motivasi inti.  Sangat penting untuk memahami motivasi ini untuk menciptakan teknologi yang memastikan masa depan yang positif bagi umat manusia.  Yudkovsky menyerukan "AI ramah".  Untuk melakukan ini, kita perlu mengembangkan pendekatan ilmiah untuk "pengembangan utilitarian", yang akan memungkinkan kita untuk mengembangkan fungsi yang bermanfaat secara sosial yang akan mengarah pada urutan yang kita inginkan.  Kemajuan pesat dalam kemajuan teknologi menunjukkan bahwa masalah ini akan segera menjadi kritis. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Anda</a> dapat menemukan artikelnya tentang topik "AI rasional untuk kebaikan bersama" di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tautan</a> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Murray Shanahan</a> memegang gelar Ph.D dalam ilmu komputer dari Cambridge, dan sekarang menjadi profesor robotika kognitif di Imperial College London.  Dia menerbitkan karya-karya dalam bidang-bidang seperti robotika, logika, sistem dinamik, neurobiologi komputasi, dan filsafat pikiran.  Dia saat ini sedang mengerjakan buku <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Technological Singularity</a> , yang akan diterbitkan pada bulan Agustus.  Anotasi promosi Amazon adalah sebagai berikut: <br><blockquote>  Shanahan menggambarkan kemajuan teknologi dalam AI, keduanya dibuat di bawah pengaruh pengetahuan dari biologi, dan dikembangkan dari awal.  Dia menjelaskan bahwa ketika AI tingkat manusia diciptakan - tugas yang secara teoritis mungkin, tetapi sulit - transisi ke AI superintelensi akan sangat cepat.  Shanahan merefleksikan apa yang dapat menyebabkan keberadaan mesin-mesin cerdas dalam bidang-bidang seperti kepribadian, tanggung jawab, hak, dan individualitas.  Beberapa perwakilan dari AI yang sangat cerdas dapat diciptakan untuk kepentingan manusia, beberapa dapat di luar kendali.  (Yaitu, Siri atau HAL?) Singularitas mewakili bagi umat manusia baik ancaman eksistensial maupun peluang eksistensial untuk mengatasi keterbatasannya.  Shanahan menjelaskan bahwa jika kita ingin mencapai hasil yang lebih baik, kita perlu membayangkan kedua kemungkinan itu. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Marcus Hutter</a> adalah profesor penelitian ilmu komputer di National Australia University.  Sebelum itu, ia bekerja di Institute of AI yang dinamai  Dale Mole di Swiss dan di Institut Nasional Informatika dan Komunikasi di Australia, dan juga bekerja pada pembelajaran yang dirangsang, penemuan Bayesian, teori kompleksitas komputasi, teori Solomon tentang prediksi induktif, visi komputer, dan profil genetik.  Dia juga menulis banyak tentang singularitas.  Dalam artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bisakah intelijen meledak?</a> " Dia menulis: <br><blockquote>  Abad ini dapat menyaksikan ledakan teknologi, yang skalanya layak disebut singularitas.  Skenario default adalah komunitas yang berinteraksi individu cerdas di dunia maya, disimulasikan pada komputer dengan sumber daya komputasi yang meningkat secara hiperbola.  Ini pasti disertai dengan ledakan kecepatan, diukur dengan waktu fisik, tetapi tidak harus dengan ledakan kecerdasan.  Jika dunia virtual dihuni oleh individu yang bebas dan berinteraksi, tekanan evolusi akan mengarah pada munculnya individu dengan kecerdasan yang meningkat yang akan bersaing untuk sumber daya komputasi.  Titik akhir dari percepatan kecerdasan evolusi ini mungkin adalah komunitas individu yang paling cerdas.  Beberapa aspek dari komunitas tunggal ini secara teoritis dapat dipelajari dengan bantuan alat ilmiah modern.  Jauh sebelum kemunculan singularitas ini, bahkan menempatkan komunitas virtual ini dalam imajinasi kita, orang dapat membayangkan munculnya perbedaan, seperti, misalnya, penurunan tajam nilai individu, yang dapat menyebabkan konsekuensi radikal. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">JÃ¼rgen Schmidhuber</a> adalah profesor AI di Universitas Lugano dan mantan profesor robotika kognitif di Universitas Teknologi Munich.  Dia mengembangkan beberapa jaringan saraf paling maju di dunia, bekerja pada robotika evolusi dan teori kompleksitas komputasi, dan berfungsi sebagai peneliti di Akademi Sains dan Seni Eropa.  Dalam bukunya, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Hipotesis Singularitas,</a> " ia berpendapat bahwa "dengan kelanjutan tren yang ada, kita akan menghadapi ledakan intelektual dalam beberapa dekade mendatang."  Ketika ditanya langsung di Reddit AMA tentang risiko yang terkait dengan AI, dia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menjawab</a> : <br><blockquote>        .    - ,    ?  ,    ,  :   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a>       .     -  .         ,    ,    .  ,           .           .   .              ,  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">temukan tempat untuk bertahan hidup</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . "</font></font></blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Richard Saton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> adalah Profesor dan Anggota Komite iCORE di University of Alberta. </font><font style="vertical-align: inherit;">Dia melayani sebagai peneliti di Asosiasi Pengembangan AI, penulis bersama </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">buku teks paling populer tentang stimulasi pembelajaran</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pelopor metode perbedaan waktu, salah satu yang paling penting dalam bidang ini. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dalam </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">laporannya</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pada sebuah konferensi tentang AI yang diselenggarakan oleh Institute for the Future of Life, Saton berpendapat bahwa "ada peluang nyata bahwa bahkan dengan hidup kita" sebuah AI akan dibuat yang secara intelektual dapat dibandingkan dengan manusia, dan menambahkan bahwa AI ini "tidak akan mematuhi kita," "akan untuk bersaing dan bekerja sama dengan kami, "dan bahwa" jika kita membuat budak yang sangat pintar, kita mendapatkan lawan yang sangat pintar. " Sebagai kesimpulan, ia mengatakan bahwa "kita perlu memikirkan mekanisme (sosial, hukum, politik, budaya) untuk memastikan hasil yang diinginkan", tetapi bahwa "orang-orang biasa pasti akan menjadi kurang penting." Dia juga menyebutkan masalah serupa di </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">presentasi</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Gadsby Institute. Juga dalam buku karya </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glenn Beck</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ada beberapa kalimat seperti itu: "Richard Saton, salah satu spesialis terbesar di AI, memprediksi ledakan kecerdasan di suatu tempat di pertengahan abad ini." </font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrew Davison</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> adalah seorang profesor visi mesin di Imperial College London, pemimpin dalam kelompok visi robot dan Laboratorium Robot Dyson, dan penemu sistem pelokalan dan markup komputerisasi MonoSLAM. Di situs webnya, ia </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">menulis</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>         ,  ,   ,  ,  2006           :            ,          (,   20-30 ).         Â« Â» (   ,    ),           ,    ,        ,     .    , ,      ,        ,     ,    ,      . <br><br>       ,   ,      ,     (       ).   ,        .    Â« Â»    .        ,    ,         ,         . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alan Turing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> tidak perlu diperkenalkan. Turing menemukan dasar matematika ilmu komputasi dan menamainya mesin Turing, kelengkapan Turing dan tes Turing. Goode bekerja dengan Turing di Bletchley Park, membantu menciptakan salah satu komputer pertama, dan menemukan banyak algoritma terkenal, misalnya, algoritma transformasi Fourier diskrit cepat, yang dikenal sebagai algoritma FFT. Dalam pekerjaan mereka, Dapatkah Mobil Digital Berpikir? Turing menulis:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mari kita asumsikan bahwa mesin tersebut dapat dibuat, dan mempertimbangkan konsekuensi dari penciptaannya. </font><font style="vertical-align: inherit;">Tindakan seperti itu tidak diragukan lagi akan menghadapi permusuhan, kecuali kita telah maju dalam toleransi beragama sejak zaman Galileo. </font><font style="vertical-align: inherit;">Oposisi akan terdiri dari kaum intelektual yang takut kehilangan pekerjaan mereka. </font><font style="vertical-align: inherit;">Tetapi kemungkinan bahwa para intelektual akan keliru. </font><font style="vertical-align: inherit;">Dimungkinkan untuk melakukan banyak hal dalam upaya untuk menjaga kecerdasan mereka pada tingkat standar yang ditetapkan oleh mesin, karena setelah dimulainya metode mesin, tidak perlu banyak waktu sampai saat ketika mesin melampaui kemampuan kami yang tidak signifikan. </font><font style="vertical-align: inherit;">Pada tahap tertentu, kita harus berharap bahwa mesin akan mengambil kendali.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Saat bekerja di Atlas Computer Lab di tahun 60-an, Goode mengembangkan ide ini dalam " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Penalaran untuk Mesin Ultra-Cerdas Pertama</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ":</font></font><br><blockquote>   ,  ,       .     â€“     ,       .  ,   ,   Â« Â»,      . ,    â€“   ,    . </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">* * * * * * * * * * * </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mengganggu saya bahwa daftar ini dapat memberikan kesan perselisihan tertentu antara "orang percaya" dan "skeptis" di daerah ini, di mana mereka saling menghancurkan satu sama lain. Tapi saya tidak berpikir begitu. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ketika saya membaca artikel tentang skeptis, saya selalu menemukan dua argumen. Pertama, kita masih sangat jauh dari AI tingkat manusia, belum lagi kecerdasan super, dan tidak ada cara yang jelas untuk mencapai ketinggian seperti itu. Kedua, jika Anda menuntut larangan penelitian AI, Anda idiot. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Saya sepenuhnya setuju dengan kedua poin ini. Seperti para pemimpin pergerakan risiko AI. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Survei di antara para peneliti AI ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Muller &amp; Bostrom, 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) menunjukkan bahwa rata-rata mereka memberikan 50% untuk fakta bahwa AI tingkat manusia akan muncul pada 2040 ode, dan 90% - bahwa itu akan muncul pada 2075. Rata-rata, 75% dari mereka percaya bahwa superintelijen ("kecerdasan mesin, secara serius melebihi kemampuan dari setiap orang di sebagian besar profesi ") akan muncul dalam 30 tahun setelah munculnya AI tingkat manusia. Dan meskipun teknik jajak pendapat ini menimbulkan beberapa keraguan, jika Anda menerima hasilnya, ternyata sebagian besar peneliti AI setuju bahwa sesuatu yang perlu dikhawatirkan akan muncul dalam satu atau dua generasi. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tetapi Luke Muelhauser, direktur Institute for Machine Intelligence, dan Nick Bostrom, direktur Institute for the Future of Humanity, menyatakan bahwa prediksi mereka untuk pengembangan AI jauh lebih lambat daripada yang diprediksi oleh para ilmuwan yang terlibat dalam survei. Jika kamu belajar</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data tentang prediksi AI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dari Stuart Armstrong, dapat dilihat bahwa, secara umum, perkiraan pada saat penampilan AI yang dibuat oleh pendukung AI tidak berbeda dari perkiraan yang dibuat oleh skeptis AI. Selain itu, prediksi jangka panjang dalam tabel ini adalah milik Armstrong sendiri. Namun, Armstrong saat ini bekerja di Institute for the Future of Humanity, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">menarik perhatian pada risiko AI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dan kebutuhan untuk meneliti tujuan superintelijen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perbedaan antara pendukung dan skeptis tidak dalam penilaian mereka tentang kapan kita harus mengharapkan munculnya AI di tingkat manusia, tetapi ketika kita perlu mulai mempersiapkan ini.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yang membawa kita ke poin kedua. Posisi skeptis, tampaknya, adalah bahwa meskipun kita mungkin harus mengirim beberapa orang pintar untuk mengerjakan penilaian awal masalah, sama sekali tidak perlu panik atau melarang studi AI. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Penggemar AI bersikeras bahwa meskipun kami tidak perlu panik atau melarang penelitian AI, mungkin ada baiknya mengirimkan beberapa orang pintar untuk mengerjakan penilaian awal masalah tersebut. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jan Lekun bisa dibilang skeptis paling kuat terhadap risiko AI. Dia banyak dikutip dalam sebuah </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">artikel tentang Sains Populer</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dalam sebuah </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">posting tentang Revolusi Marjinal</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dan dia juga berbicara dengan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KDNuggets</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dan </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tentang "masalah singularitas" yang tak terhindarkan ", yang ia sendiri gambarkan sebagai" begitu jauh sehingga fiksi ilmiah dapat ditulis tentang mereka. " </font><font style="vertical-align: inherit;">Tetapi ketika diminta untuk mengklarifikasi posisinya, ia menyatakan:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elon Musk sangat khawatir tentang ancaman eksistensial terhadap kemanusiaan (itulah sebabnya ia membangun roket untuk mengirim orang untuk menjajah planet lain). </font><font style="vertical-align: inherit;">Dan meskipun risiko pemberontakan AI sangat kecil dan sangat jauh ke masa depan, kita perlu memikirkannya, mengembangkan tindakan pencegahan dan aturan. </font><font style="vertical-align: inherit;">Sama seperti komite bioetika yang muncul pada 1970-an dan 1980-an, sebelum meluasnya penggunaan genetika, kita membutuhkan komite etika AI. </font><font style="vertical-align: inherit;">Tapi, seperti yang ditulis Yoshua Benjio, kita masih punya banyak waktu.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz adalah pakar lain yang sering disebut sebagai corong utama skeptisisme dan keterbatasan. </font><font style="vertical-align: inherit;">Pandangannya dijelaskan dalam artikel seperti " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Direktur Penelitian Microsoft percaya bahwa AI yang tidak terkendali tidak akan membunuh kita,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " dan " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz dari Microsoft percaya bahwa AI tidak boleh takut</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ." </font><font style="vertical-align: inherit;">Tapi inilah yang dia katakan dalam wawancara panjang dengan NPR:</font></font><br><blockquote> Keist: Horvitz ragu bahwa sekretaris virtual akan pernah berubah menjadi sesuatu yang akan menaklukkan dunia.  Dia mengatakan bahwa ini diharapkan untuk sebuah layang-layang berkembang menjadi Boeing 747. Apakah ini berarti bahwa itu mengolok-olok singularitas? <br><br>  Horvitz: Tidak.  Saya pikir ada campuran konsep, dan saya sendiri juga punya perasaan campur aduk. <br><br>  Keist: Khususnya, karena ide-ide seperti singularitas, Horvits dan pakar AI lainnya semakin berusaha untuk menangani masalah etika yang mungkin timbul dengan AI yang ditargetkan secara sempit di tahun-tahun mendatang.  Mereka juga mengajukan pertanyaan yang lebih futuristik.  Misalnya, bagaimana saya bisa membuat tombol shutdown darurat untuk komputer yang dapat berubah sendiri? <br><br>  Horvits: Saya benar-benar percaya bahwa taruhannya cukup tinggi untuk menghabiskan waktu dan energi secara aktif mencari solusi, bahkan jika kemungkinan kejadian seperti itu kecil. </blockquote><br>  Ini umumnya bertepatan dengan posisi banyak agitator risiko AI yang paling bersemangat.  Dengan teman seperti itu, musuh tidak diperlukan. <br><br>  Artikel Slate, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Jangan Takut AI,</a> " juga mengejutkan menempatkan hal-hal dalam cahaya yang tepat: <br><blockquote>  Seperti yang dinyatakan Musk sendiri, solusi untuk risiko AI terletak pada kolaborasi para ilmuwan dan pembuat hukum yang bijaksana dan rasional.  Namun, cukup sulit untuk memahami bagaimana berbicara tentang "setan" dapat membantu mencapai tujuan mulia ini.  Dia bahkan bisa merintanginya. <br><br>  Pertama, ada lubang besar dalam gagasan skrip Skynet.  Meskipun para peneliti di bidang ilmu komputer percaya bahwa alasan Mask adalah "tidak sepenuhnya gila," mereka masih terlalu jauh dari dunia di mana hype tentang AI menyamarkan kenyataan AI yang sedikit kurang bahwa ilmuwan komputer kita dihadapkan dengan. <br><br>  Ian Lekun, kepala lab AI di Facebook, merangkum ide ini dalam sebuah posting di Google+ pada 2013: Hype ini merugikan AI.  Selama lima dekade terakhir, hype telah membunuh AI empat kali.  Dia perlu dihentikan. "Lekun dan yang lainnya benar-benar takut pada hype. Kegagalan untuk memenuhi harapan yang tinggi yang dipaksakan oleh fiksi ilmiah mengarah pada pemotongan serius dalam anggaran untuk penelitian AI. </blockquote><br>  Para ilmuwan yang bekerja di AI adalah orang-orang pintar.  Mereka tidak tertarik jatuh ke dalam perangkap politik klasik, di mana mereka akan dibagi ke dalam kamp dan saling menuduh panik atau burung unta.  Tampaknya, mereka berusaha menemukan keseimbangan antara kebutuhan untuk memulai pekerjaan pendahuluan terkait dengan ancaman yang menjulang di suatu tempat di kejauhan, dan risiko menyebabkan sensasi kuat yang akan menghantam mereka. <br><br>  Saya tidak ingin mengatakan bahwa tidak ada perbedaan pendapat tentang seberapa cepat Anda perlu mulai mengatasi masalah ini.  Pada dasarnya itu semua bermuara pada apakah mungkin untuk mengatakan bahwa "kita akan menyelesaikan masalah ketika kita menemukannya," atau untuk mengharapkan lepas landas yang tak terduga, karena semuanya akan keluar dari kendali, dan untuk itu, oleh karena itu, kita perlu mempersiapkan di muka.  Saya melihat lebih sedikit bukti daripada saya ingin bahwa mayoritas peneliti AI dengan pendapat mereka sendiri memahami kemungkinan kedua.  Apa yang bisa saya katakan, bahkan jika dalam sebuah artikel tentang Revolusi Marjinal seorang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ahli dikutip</a> mengatakan bahwa kecerdasan super tidak menimbulkan ancaman besar, karena "komputer pintar tidak dapat menetapkan tujuan untuk diri mereka sendiri", meskipun siapa pun yang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">membaca Bostrom</a> tahu bahwa seluruh masalahnya. <br><br>  Masih ada gunung pekerjaan yang harus dilakukan.  Tetapi hanya untuk tidak secara khusus memilih artikel di mana "ahli AI nyata tidak khawatir tentang kecerdasan super." </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id402379/">https://habr.com/ru/post/id402379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id402367/index.html">Bagaimana cara berhenti membayar untuk roaming, atau Dengan satu nomor di seluruh dunia</a></li>
<li><a href="../id402369/index.html">Bagaimana mengukur kecepatan printer 3D - ujung panasnya. Dan tidak hanya kecepatan</a></li>
<li><a href="../id402373/index.html">Apa yang memberi "Genetika Mikrobiota"</a></li>
<li><a href="../id402375/index.html">8 kilowatt Sakelar AC 4 saluran dengan pengukuran konsumsi. Bagian 1</a></li>
<li><a href="../id402377/index.html">Apa yang dipikirkan smartphone Anda tentang pengisian daya USB mobil</a></li>
<li><a href="../id402381/index.html">Bagaimana cara merekrut astronot</a></li>
<li><a href="../id402383/index.html">Saw, Shura: bagaimana kami merancang aplikasi pelacak anjing Mishiko</a></li>
<li><a href="../id402385/index.html">Mengapa Anda harus mengharapkan booming dalam pembuatan robot untuk tempat komersial</a></li>
<li><a href="../id402387/index.html">Pena 3D untuk printer 3D</a></li>
<li><a href="../id402389/index.html">MPAA dan RIAA berencana untuk memulihkan data dari hard drive yang gagal berbagi file Megaupload</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>