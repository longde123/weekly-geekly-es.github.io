<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ö∞Ô∏è üõå ü§í Neuronale Netze und tiefes Lernen, Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen üíÉ ü§úüèø üçò</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hinweis 
 Hier ist eine √úbersetzung von Michael Nielsens kostenlosem Online-Buch Neural Networks and Deep Learning, das unter der Creative Commons Att...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze und tiefes Lernen, Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456738/"><h3>  Hinweis </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/d1f/7ac/ee5/d1f7acee5600a381c43f05c7df9c439a.jpg" alt="Michael Nielsen" align="left">  Hier ist eine √úbersetzung von Michael Nielsens kostenlosem Online-Buch Neural Networks and Deep Learning, das unter der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Creative Commons Attribution-NonCommercial 3.0 Unported License vertrieben wird</a> .  Die Motivation f√ºr die Erstellung war die erfolgreiche Erfahrung bei der √úbersetzung eines Programmierlehrbuchs, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Expressive JavaScript</a> .  Das Buch √ºber neuronale Netze ist ebenfalls sehr beliebt, Autoren von englischsprachigen Artikeln zitieren es aktiv.  Ich habe ihre √úbersetzungen nicht gefunden, au√üer der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úbersetzung des Anfangs des ersten Kapitels mit Abk√ºrzungen</a> . <br><br>  Wer sich beim Autor des Buches bedanken m√∂chte, kann dies auf seiner <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Seite</a> per √úberweisung per PayPal oder Bitcoin tun.  Um den √úbersetzer auf Habr√© zu unterst√ºtzen, gibt es ein Formular "um den Autor zu unterst√ºtzen". <br><br><div class="spoiler">  <b class="spoiler_title">Inhalt</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 2: Funktionsweise des Backpropagation-Algorithmus</a> </li><li>  Kapitel 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Verbesserung der Methode zum Trainieren neuronaler Netze</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: Warum tr√§gt die Regularisierung dazu bei, die Umschulung zu reduzieren?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 3: Wie w√§hlt man Hyperparameter f√ºr neuronale Netze?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen k√∂nnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 5: Warum sind tiefe neuronale Netze so schwer zu trainieren?</a> </li><li>  Kapitel 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: J√ºngste Fortschritte bei der Bilderkennung</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nachwort: Gibt es einen einfachen Algorithmus zum Erstellen von Intelligenz?</a> </li></ul></div></div><br><h2>  Einf√ºhrung </h2><br>  In diesem Tutorial erfahren Sie ausf√ºhrlich √ºber Konzepte wie: <br><br><ul><li>  Neuronale Netze - ein hervorragendes Software-Paradigma, das unter dem Einfluss der Biologie erstellt wurde und es dem Computer erm√∂glicht, anhand von Beobachtungen zu lernen. </li><li>  Deep Learning ist ein leistungsf√§higer Satz von Trainingstechniken f√ºr neuronale Netze. </li></ul><br>  Neuronale Netze (NS) und Deep Learning (GO) bieten heute die beste L√∂sung f√ºr viele Probleme in den Bereichen Bilderkennung, Sprache und Verarbeitung nat√ºrlicher Sprache.  In diesem Tutorial lernen Sie viele der Schl√ºsselkonzepte kennen, die NS und GO zugrunde liegen. <br><a name="habracut"></a><br><h2>  Worum geht es in diesem Buch? </h2><br>  NS ist eines der besten Software-Paradigmen, die jemals vom Menschen erfunden wurden.  Mit einem Standard-Programmieransatz teilen wir dem Computer mit, was zu tun ist, teilen gro√üe Aufgaben in viele kleine auf und bestimmen genau die Aufgaben, die der Computer problemlos ausf√ºhren kann.  Im Falle der Nationalversammlung hingegen sagen wir dem Computer nicht, wie das Problem zu l√∂sen ist.  Er selbst lernt dies auf der Grundlage von "Beobachtungen" der Daten und "erfindet" seine eigene L√∂sung f√ºr das Problem. <br><br>  Automatisiertes datenbasiertes Lernen klingt vielversprechend.  Bis 2006 wussten wir jedoch nicht, wie wir die Nationalversammlung so ausbilden sollten, dass sie mit Ausnahme einiger Sonderf√§lle √ºber die traditionelleren Ans√§tze hinausgehen konnte.  Im Jahr 2006 wurden Trainingstechniken der sogenannten  tiefe neuronale Netze (GNS).  Jetzt sind diese Techniken als Deep Learning (GO) bekannt.  Sie wurden weiterentwickelt, und heute haben GNS und GO bei vielen wichtigen Aufgaben im Zusammenhang mit Computer Vision, Spracherkennung und Verarbeitung nat√ºrlicher Sprache erstaunliche Ergebnisse erzielt.  In gro√üem Umfang werden sie von Unternehmen wie Google, Microsoft und Facebook bereitgestellt. <br><br>  Der Zweck dieses Buches ist es, Ihnen zu helfen, die Schl√ºsselkonzepte neuronaler Netze, einschlie√ülich moderner GO-Techniken, zu beherrschen.  Nachdem Sie mit dem Tutorial gearbeitet haben, schreiben Sie einen Code, der NS und GO verwendet, um komplexe Probleme der Mustererkennung zu l√∂sen.  Sie haben eine Grundlage f√ºr den Einsatz von NS und Zivilschutz bei der L√∂sung Ihrer eigenen Probleme. <br><br><h3>  Prinzipbasierter Ansatz </h3><br>  Eine der √úberzeugungen, die dem Buch zugrunde liegen, ist, dass es besser ist, ein solides Verst√§ndnis der Schl√ºsselprinzipien der Nationalversammlung und der Zivilgesellschaft zu erlangen, als Wissen aus einer langen Liste verschiedener Ideen zu gewinnen.  Wenn Sie die wichtigsten Ideen gut verstehen, werden Sie schnell andere neue Materialien verstehen.  In der Sprache des Programmierers k√∂nnen wir sagen, dass wir die grundlegende Syntax, Bibliotheken und Datenstrukturen der neuen Sprache studieren werden.  M√∂glicherweise erkennen Sie nur einen kleinen Teil der gesamten Sprache - viele Sprachen verf√ºgen √ºber immense Standardbibliotheken - Sie k√∂nnen jedoch neue Bibliotheken und Datenstrukturen schnell und einfach verstehen. <br><br>  Daher ist dieses Buch kategorisch kein Lehrmaterial zur Verwendung einer bestimmten Bibliothek f√ºr die Nationalversammlung.  Wenn Sie nur lernen m√∂chten, wie man mit der Bibliothek arbeitet, lesen Sie das Buch nicht!  Finden Sie die Bibliothek, die Sie ben√∂tigen, und arbeiten Sie mit Schulungsunterlagen und Dokumentationen.  Aber denken Sie daran: Obwohl dieser Ansatz den Vorteil hat, das Problem sofort zu l√∂sen, reicht es nicht aus, wenn Sie nur verstehen m√∂chten, was genau in der Nationalversammlung geschieht, wenn Sie Ideen beherrschen m√∂chten, die in vielen Jahren relevant sein werden Modebibliothek.  Sie m√ºssen die verl√§sslichen und langfristigen Ideen verstehen, die der Arbeit der Nationalversammlung zugrunde liegen.  Technologie kommt und geht und Ideen halten ewig. <br><br><h3>  Praktischer Ansatz </h3><br>  Wir werden die Grundprinzipien am Beispiel einer bestimmten Aufgabe untersuchen: einem Computer das Erkennen handgeschriebener Zahlen beibringen.  Mit herk√∂mmlichen Programmierans√§tzen ist diese Aufgabe √§u√üerst schwer zu l√∂sen.  Wir k√∂nnen es jedoch recht gut mit einem einfachen NS und mehreren Dutzend Codezeilen ohne spezielle Bibliotheken l√∂sen.  Dar√ºber hinaus werden wir dieses Programm schrittweise verbessern und dabei immer mehr Schl√ºsselideen zur Nationalversammlung und zum Zivilschutz einbeziehen. <br><br>  Dieser praktische Ansatz bedeutet, dass Sie einige Programmiererfahrung ben√∂tigen.  Sie m√ºssen jedoch kein professioneller Programmierer sein.  Ich habe Python-Code (Version 2.7) geschrieben, der auch dann klar sein sollte, wenn Sie keine Python-Programme geschrieben haben.  W√§hrend des Studiums erstellen wir eine eigene Bibliothek f√ºr die Nationalversammlung, die Sie f√ºr Experimente und Weiterbildungen verwenden k√∂nnen.  Der gesamte Code kann hier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">heruntergeladen werden</a> .  Wenn Sie das Buch fertiggestellt haben oder gerade lesen, k√∂nnen Sie eine der vollst√§ndigeren Bibliotheken f√ºr die Nationalversammlung ausw√§hlen, die f√ºr die Verwendung in diesen Projekten angepasst sind. <br><br>  Die mathematischen Anforderungen zum Verst√§ndnis des Materials sind recht durchschnittlich.  Die meisten Kapitel haben mathematische Teile, aber normalerweise sind es elementare Algebra- und Funktionsgraphen.  Manchmal verwende ich fortgeschrittenere Mathematik, aber ich habe das Material so strukturiert, dass Sie es verstehen k√∂nnen, auch wenn Ihnen einige Details entgehen.  Der gr√∂√üte Teil der Mathematik wird in Kapitel 2 verwendet, das ein wenig Matanalyse und lineare Algebra erfordert.  F√ºr diejenigen, denen sie nicht vertraut sind, beginne ich Kapitel 2 mit einer Einf√ºhrung in die Mathematik.  Wenn Sie Schwierigkeiten haben, √ºberspringen Sie einfach das Kapitel bis zur Nachbesprechung.  Mach dir auf keinen Fall Sorgen. <br><br>  Ein Buch ist selten gleichzeitig auf ein Verst√§ndnis der Prinzipien und einen praktischen Ansatz ausgerichtet.  Ich glaube jedoch, dass es besser ist, auf der Grundlage der Grundgedanken der Nationalversammlung zu studieren.  Wir werden Arbeitscode schreiben und nicht nur abstrakte Theorie studieren, sondern Sie k√∂nnen diesen Code untersuchen und erweitern.  Auf diese Weise verstehen Sie die Grundlagen, sowohl Theorie als auch Praxis, und k√∂nnen weiter lernen. <br><br><h3>  √úbungen und Aufgaben </h3><br>  Autoren technischer B√ºcher warnen den Leser oft, dass er einfach alle √úbungen absolvieren und alle Probleme l√∂sen muss.  Wenn ich solche Warnungen vorlese, wirken sie immer etwas seltsam.  Wird mir etwas Schlimmes passieren, wenn ich keine √úbungen mache und Probleme l√∂se?  Nein, nat√ºrlich.  Ich werde nur durch weniger tiefes Verst√§ndnis Zeit sparen.  Manchmal lohnt es sich.  Manchmal nicht. <br><br>  Was ist es wert, mit diesem Buch gemacht zu werden?  Ich rate Ihnen, die meisten √úbungen zu absolvieren, aber nicht die meisten Aufgaben zu l√∂sen. <br><br>  Die meisten √úbungen m√ºssen abgeschlossen werden, da dies grundlegende √úberpr√ºfungen f√ºr ein angemessenes Verst√§ndnis des Materials sind.  Wenn Sie die √úbung nicht relativ einfach durchf√ºhren k√∂nnen, m√ºssen Sie etwas Grundlegendes √ºbersehen haben.  Nat√ºrlich, wenn Sie wirklich in irgendeiner Art von √úbung stecken - lassen Sie es fallen, vielleicht ist dies eine Art kleines Missverst√§ndnis, oder vielleicht habe ich etwas schlecht formuliert.  Wenn Ihnen die meisten √úbungen jedoch Schwierigkeiten bereiten, m√ºssen Sie das vorherige Material h√∂chstwahrscheinlich erneut lesen. <br><br>  Aufgaben sind eine andere Sache.  Sie sind schwieriger als √úbungen, und mit einigen werden Sie es schwer haben.  Das ist √§rgerlich, aber nat√ºrlich ist Geduld angesichts einer solchen Entt√§uschung der einzige Weg, das Thema wirklich zu verstehen und aufzunehmen. <br><br>  Ich empfehle daher nicht, alle Probleme zu l√∂sen.  Besser noch - holen Sie sich Ihr eigenes Projekt.  M√∂glicherweise m√∂chten Sie NS verwenden, um Ihre Musiksammlung zu klassifizieren.  Oder um den Wert von Aktien vorherzusagen.  Oder etwas anderes.  Aber finden Sie ein interessantes Projekt f√ºr Sie.  Und dann k√∂nnen Sie die Aufgaben aus dem Buch ignorieren oder sie lediglich als Inspiration f√ºr die Arbeit an Ihrem Projekt verwenden.  Bei Problemen mit Ihrem eigenen Projekt lernen Sie mehr als nur mit einer beliebigen Anzahl von Aufgaben zu arbeiten.  Emotionales Engagement ist ein Schl√ºsselfaktor f√ºr die Meisterleistung. <br><br>  Nat√ºrlich, obwohl Sie vielleicht kein solches Projekt haben.  Es ist in Ordnung.  L√∂sen Sie Aufgaben, f√ºr die Sie eine intrinsische Motivation empfinden.  Verwenden Sie Material aus dem Buch, um Ideen f√ºr pers√∂nliche kreative Projekte zu finden. <br><br><h2>  Kapitel 1 </h2><br>  Das menschliche visuelle System ist eines der Weltwunder.  Betrachten Sie die folgende Folge handgeschriebener Zahlen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><br>  Die meisten Leute werden sie leicht lesen, wie 504192. Aber diese Einfachheit t√§uscht.  In jeder Gehirnh√§lfte hat eine Person einen prim√§ren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">visuellen Kortex</a> , auch als V1 bekannt, der 140 Millionen Neuronen und zig Milliarden Verbindungen zwischen ihnen enth√§lt.  Gleichzeitig ist nicht nur V1 am menschlichen Sehen beteiligt, sondern eine ganze Reihe von Gehirnregionen - V2, V3, V4 und V5 -, die an einer immer komplexer werdenden Bildverarbeitung beteiligt sind.  Wir haben einen Supercomputer in unseren K√∂pfen, der seit Hunderten von Millionen von Jahren von der Evolution abgestimmt und perfekt darauf ausgelegt ist, die sichtbare Welt zu verstehen.  Das Erkennen handgeschriebener Zahlen ist nicht so einfach.  Es ist nur so, dass wir Menschen erstaunlicherweise √ºberraschend gut erkennen, was unsere Augen uns zeigen.  Aber fast alle diese Arbeiten werden unbewusst ausgef√ºhrt.  Und normalerweise legen wir keinen Wert darauf, welche schwierige Aufgabe unsere visuellen Systeme l√∂sen. <br><br>  Die Schwierigkeit, visuelle Muster zu erkennen, wird deutlich, wenn Sie versuchen, ein Computerprogramm zu schreiben, um Zahlen wie die oben genannten zu erkennen.  Was in unserer Ausf√ºhrung einfach erscheint, erweist sich pl√∂tzlich als √§u√üerst komplex.  Das einfache Konzept, wie wir die Formen erkennen - ‚Äûdie Neun hat oben eine Schleife und unten rechts den vertikalen Balken‚Äú - ist f√ºr einen algorithmischen Ausdruck gar nicht so einfach.  Wenn Sie versuchen, diese Regeln klar zu formulieren, geraten Sie schnell in einen Sumpf von Ausnahmen, Fallstricken und besonderen Anl√§ssen.  Die Aufgabe scheint hoffnungslos. <br><br>  NS-Ansatz zur L√∂sung des Problems auf andere Weise.  Die Idee ist, die vielen handgeschriebenen Zahlen zu nehmen, die als Lehrbeispiele bekannt sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a67/2ff/15c/a672ff15c58d9672b9f5d2b427a20eb6.png"><br><br>  und entwickeln Sie ein System, das aus diesen Beispielen lernen kann.  Mit anderen Worten, die Nationalversammlung verwendet Beispiele, um automatisch handschriftliche Regeln zur Erkennung von Ziffern zu erstellen.  Dar√ºber hinaus kann das Netzwerk durch Erh√∂hen der Anzahl von Trainingsbeispielen mehr √ºber handschriftliche Zahlen erfahren und deren Genauigkeit verbessern.  Obwohl ich oben nur 100 Fallstudien zitiert habe, k√∂nnen wir vielleicht ein besseres Handschrifterkennungssystem mit Tausenden oder sogar Millionen und Milliarden von Fallstudien erstellen. <br><br>  In diesem Kapitel werden wir ein Computerprogramm schreiben, das das NS-Lernen zum Erkennen handgeschriebener Zahlen implementiert.  Das Programm wird nur 74 Zeilen lang sein und keine speziellen Bibliotheken f√ºr die Nationalversammlung verwenden.  Dieses kurze Programm kann jedoch handschriftliche Zahlen mit einer Genauigkeit von mehr als 96% erkennen, ohne dass ein menschliches Eingreifen erforderlich ist.  Dar√ºber hinaus werden wir in zuk√ºnftigen Kapiteln Ideen entwickeln, mit denen die Genauigkeit auf 99% oder mehr verbessert werden kann.  Tats√§chlich leisten die besten kommerziellen NS so gute Arbeit, dass sie von Banken zur Bearbeitung von Schecks und von der Post zur Erkennung von Adressen verwendet werden. <br><br>  Wir konzentrieren uns auf die Erkennung von Handschriften, da dies ein gro√üartiger Prototyp einer Aufgabe zum Studium von NS ist.  Ein solcher Prototyp ist ideal f√ºr uns: Es ist eine schwierige Aufgabe (das Erkennen handgeschriebener Zahlen ist keine leichte Aufgabe), aber nicht so kompliziert, dass eine √§u√üerst komplexe L√∂sung oder immense Rechenleistung erforderlich ist.  Dar√ºber hinaus ist dies eine gro√üartige M√∂glichkeit, komplexere Techniken wie GO zu entwickeln.  Daher werden wir in dem Buch st√§ndig auf die Aufgabe der Handschrifterkennung zur√ºckkommen.  Sp√§ter werden wir diskutieren, wie diese Ideen auf andere Aufgaben des Computer-Sehens, auf die Spracherkennung, die Verarbeitung nat√ºrlicher Sprache und andere Bereiche angewendet werden k√∂nnen. <br><br>  Wenn der Zweck dieses Kapitels nur darin bestand, ein Programm zum Erkennen handgeschriebener Zahlen zu schreiben, w√§re das Kapitel nat√ºrlich viel k√ºrzer!  Dabei werden wir jedoch viele Schl√ºsselideen im Zusammenhang mit NS entwickeln, darunter zwei wichtige Arten von k√ºnstlichen Neuronen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Perzeptron</a> und Sigmoidneuron) und den Standard-NS-Lernalgorithmus, den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">stochastischen Gradientenabstieg</a> .  Im Text konzentriere ich mich darauf, zu erkl√§ren, warum alles so gemacht wird, und Ihr Verst√§ndnis der Nationalversammlung zu formen.  Dies erfordert ein l√§ngeres Gespr√§ch, als wenn ich nur die grundlegenden Mechanismen des Geschehens vorgestellt h√§tte, aber es kostet ein tieferes Verst√§ndnis, das Sie haben werden.  Unter anderem - am Ende des Kapitels werden Sie verstehen, was ein Zivilschutz ist und warum er so wichtig ist. <br><br><h3>  Perceptrons </h3><br>  Was ist ein neuronales Netzwerk?  Zun√§chst werde ich √ºber eine Art k√ºnstliches Neuron sprechen, das Perzeptron.  Perceptrons wurden in den 1950er und 60er Jahren vom Wissenschaftler <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Frank Rosenblatt</a> erfunden, inspiriert von den fr√ºhen Arbeiten von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Warren McCallock</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Walter Pitts</a> .  Heutzutage werden andere Modelle k√ºnstlicher Neuronen h√§ufiger verwendet - in diesem Buch, und die meisten modernen Arbeiten zu NS verwenden haupts√§chlich das Sigmoidmodell des Neurons.  Wir werden sie bald treffen.  Um zu verstehen, warum Sigmoidneuronen auf diese Weise definiert werden, lohnt es sich, Zeit mit der Analyse des Perzeptrons zu verbringen. <br><br>  Wie funktionieren Perzeptrone?  Das Perzeptron empf√§ngt mehrere Bin√§rzahlen x <sub>1</sub> , x <sub>2</sub> , ... und gibt eine Bin√§rzahl an: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/293/1f3/f5c/2931f3f5c6125d14262954583868f959.png"><br><br>  In diesem Beispiel hat das Perzeptron drei Eingangsnummern, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Im Allgemeinen kann es mehr oder weniger davon geben.  Rosenblatt schlug eine einfache Regel zur Berechnung des Ergebnisses vor.  Er f√ºhrte Gewichte w <sub>1</sub> , w <sub>2</sub> und reelle Zahlen ein und dr√ºckte die Bedeutung der entsprechenden Eingabezahlen f√ºr die Ergebnisse aus.  Die Ausgabe eines Neurons, 0 oder 1, wird dadurch bestimmt, ob eine gewichtete Summe kleiner oder gr√∂√üer als ein bestimmter Schwellenwert ist [Schwellenwert] <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mtext" id="MJXp-Span-2">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-3"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-5"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-6" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-7" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-8"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-9" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-10" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-11"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-13" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.764ex" height="2.057ex" viewBox="0 -520.7 4634.5 885.9" role="img" focusable="false" style="vertical-align: -0.848ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(2562,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(3670,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="809" y="-213"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-1"> \ sum_j w_jx_j </script>  .  Wie Gewichte ist der Schwellenwert eine reelle Zahl, ein Parameter eines Neurons.  In mathematischen Begriffen: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-14"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-15">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-16">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-20">t</span><span class="MJXp-mo" id="MJXp-Span-21" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-22">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27">n</span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29">F</span><span class="MJXp-mrow" id="MJXp-Span-30"><span class="MJXp-mo" id="MJXp-Span-31" style="margin-left: 0em; margin-right: 0em;">√§</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-34">e</span></span><span class="MJXp-mn" id="MJXp-Span-35">0</span><span class="MJXp-mtext" id="MJXp-Span-36">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-38">f</span><span class="MJXp-mtext" id="MJXp-Span-39">&nbsp;</span><span class="MJXp-mtext" id="MJXp-Span-40">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-42">u</span><span class="MJXp-msubsup" id="MJXp-Span-43"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-44" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-45" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-46"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-47" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-48" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-49"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-50" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-51" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mtext" id="MJXp-Span-52">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55">q</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56">S</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59">w</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-61">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-62">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-63">e</span><span class="MJXp-mspace" id="MJXp-Span-64" style="width: 0em; height: 0em;"></span><span class="MJXp-mn" id="MJXp-Span-65">1</span><span class="MJXp-mtext" id="MJXp-Span-66">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-67">w</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-68">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-69">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-70">n</span><span class="MJXp-mtext" id="MJXp-Span-71">&nbsp;</span><span class="MJXp-mtext" id="MJXp-Span-72">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-73">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-74">u</span><span class="MJXp-msubsup" id="MJXp-Span-75"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-76" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-77" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-78"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-79" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-80" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-81"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-82" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-83" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-84" style="margin-left: 0.333em; margin-right: 0.333em;">&gt;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-85">S</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-86">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-88">w</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-89">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-90">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-91">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-92">e</span><span class="MJXp-mtext" id="MJXp-Span-93">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-94">E</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-95">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-96">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-97">e</span><span class="MJXp-mrow" id="MJXp-Span-98"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-99">F</span><span class="MJXp-mrow" id="MJXp-Span-100"><span class="MJXp-mo" id="MJXp-Span-101" style="margin-left: 0em; margin-right: 0em;">√§</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-102">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-103">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-104">e</span></span><span class="MJXp-mtext" id="MJXp-Span-105">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-106">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-107">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-108">g</span><span class="MJXp-mrow" id="MJXp-Span-109"><span class="MJXp-mn" id="MJXp-Span-110">1</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="76.867ex" height="6.033ex" viewBox="0 -780.1 33095.6 2597.7" role="img" focusable="false" style="vertical-align: -4.222ex; max-width: 638px;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(6097,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6F" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-75" x="485" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-74" x="1058" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-70" x="1419" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-75" x="1923" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-74" x="2495" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMAIN-3D" x="3134" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-62" x="4441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="4870" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-67" x="5337" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-69" x="5817" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6E" x="6163" y="0"></use><g transform="translate(6763,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-46" x="0" y="0"></use><g transform="translate(749,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">√§</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="1164" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="1462" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="1761" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMAIN-30" x="8991" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-69" x="9742" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-66" x="10087" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-73" x="11138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-75" x="11607" y="0"></use><g transform="translate(12180,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(13450,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(14558,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="15772" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="16071" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-71" x="16537" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-53" x="16998" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-63" x="17643" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-68" x="18077" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="18653" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="19370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="19836" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="20135" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="20433" y="0"></use></g><g transform="translate(6442,-1452)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMAIN-31" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="750" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="1467" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6E" x="1933" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6E" x="2534" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-73" x="3634" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-75" x="4104" y="0"></use><g transform="translate(4676,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(5946,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(7054,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMAIN-3E" x="8296" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-53" x="9353" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-63" x="9998" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-68" x="10432" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="11008" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="11725" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="12191" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="12490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="12788" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-45" x="13505" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6E" x="14269" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-64" x="14870" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="15393" y="0"></use><g transform="translate(15860,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-46" x="0" y="0"></use><g transform="translate(749,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">√§</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="1164" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="1462" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="1761" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-74" x="18338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-61" x="18699" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-67" x="19229" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMAIN-31" x="19709" y="0"></use></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> output = \ begin {F√§lle} 0 ~ if ~ \ sum_j w_jx_j \ leq Schwelle \\ 1 ~ wenn ~ \ sum_j w_jx_j> Schwelle \ Ende {F√§lle} \ tag {1} </script></p><br><br>  Das ist die ganze Beschreibung des Perzeptrons! <br><br>  Dies ist das grundlegende mathematische Modell.  Ein Perzeptron kann als Entscheidungstr√§ger betrachtet werden, indem Beweise abgewogen werden.  Lassen Sie mich Ihnen ein nicht sehr realistisches, aber einfaches Beispiel geben.  Nehmen wir an, das Wochenende steht vor der T√ºr und Sie haben geh√∂rt, dass in Ihrer Stadt ein K√§sefestival stattfinden wird.  Du magst K√§se und versuchst zu entscheiden, ob du zum Festival gehst oder nicht.  Sie k√∂nnen eine Entscheidung treffen, indem Sie drei Faktoren abw√§gen: <br><br><ol><li>  Ist das Wetter gut? </li><li>  M√∂chte Ihr Partner mit Ihnen gehen? </li><li>  Ist das Festival weit entfernt von √∂ffentlichen Verkehrsmitteln?  (Du hast kein Auto) </li></ol><br>  Diese drei Faktoren k√∂nnen als bin√§re Variablen x <sub>1</sub> , x <sub>2</sub> , x <sub>3 dargestellt werden</sub> .  Zum Beispiel x <sub>1</sub> = 1, wenn das Wetter gut ist, und 0, wenn es schlecht ist.  x <sub>2</sub> = 1, wenn Ihr Partner gehen m√∂chte, und 0, wenn nicht.  Gleiches gilt f√ºr x <sub>3</sub> . <br><br>  Nehmen wir an, Sie sind so ein Fan von K√§se, dass Sie bereit sind, zum Festival zu gehen, auch wenn Ihr Partner nicht daran interessiert ist und es schwierig ist, dorthin zu gelangen.  Aber vielleicht hasst du nur schlechtes Wetter und bei schlechtem Wetter gehst du nicht zum Festival.  Sie k√∂nnen Perzeptrone verwenden, um einen solchen Entscheidungsprozess zu modellieren.  Eine M√∂glichkeit besteht darin, das Gewicht w <sub>1</sub> = 6 f√ºr das Wetter und w <sub>2</sub> = 2, w <sub>3</sub> = 2 f√ºr andere Bedingungen zu w√§hlen.  Ein gr√∂√üerer Wert von w <sub>1</sub> bedeutet, dass das Wetter f√ºr Sie viel wichtiger ist als die Frage, ob Ihr Partner zu Ihnen kommt oder die N√§he des Festivals zu einem Zwischenstopp.  Angenommen, Sie w√§hlen den Schwellenwert 5 f√ºr das Perzeptron. Mit diesen Optionen implementiert das Perzeptron das gew√ºnschte Entscheidungsmodell und gibt 1 bei gutem Wetter und 0 bei schlechtem Wetter.  Der Wunsch des Partners und die N√§he des Anschlags beeinflussen den Ausgabewert nicht. <br><br>  Durch √Ñndern von Gewichten und Schwellenwerten k√∂nnen wir verschiedene Entscheidungsmodelle erhalten.  Nehmen wir zum Beispiel die Schwelle 3. Dann entscheidet das Perzeptron, dass Sie zum Festival gehen m√ºssen, entweder bei sch√∂nem Wetter oder wenn sich das Festival in der N√§he einer Bushaltestelle befindet und Ihr Partner sich bereit erkl√§rt, mit Ihnen zu gehen.  Mit anderen Worten, das Modell ist anders.  Wenn Sie die Schwelle senken, m√∂chten Sie mehr zum Festival gehen. <br><br>  Offensichtlich ist das Perzeptron kein vollst√§ndiges menschliches Entscheidungsmodell!  Dieses Beispiel zeigt jedoch, wie ein Perzeptron verschiedene Arten von Beweisen abw√§gen kann, um Entscheidungen zu treffen.  Es scheint m√∂glich, dass ein komplexes Netzwerk von Perzeptronen sehr komplexe Entscheidungen treffen kann: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f1/853/0d8/5f18530d8e17d640b3146924f7666032.png"><br><br>  In diesem Netzwerk trifft die erste Spalte von Perzeptronen - was wir die erste Schicht von Perzeptronen nennen - drei sehr einfache Entscheidungen, wobei die eingegebenen Beweise abgewogen werden.  Was ist mit Perzeptronen aus der zweiten Schicht?  Jeder von ihnen trifft eine Entscheidung und w√§gt die Ergebnisse der ersten Entscheidungsebene ab.  Auf diese Weise kann das Perzeptron der zweiten Schicht eine Entscheidung auf einer komplexeren und abstrakteren Ebene treffen als das Perzeptron der ersten Schicht.  Und noch komplexere Entscheidungen k√∂nnen von Perzeptronen auf der dritten Ebene getroffen werden.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Auf diese Weise kann ein mehrschichtiges Netzwerk von Perzeptronen komplexe Entscheidungen treffen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Als ich das Perzeptron bestimmte, sagte ich √ºbrigens, dass es nur einen Ausgabewert hat. </font><font style="vertical-align: inherit;">Im oberen Netzwerk sehen die Perzeptrone jedoch so aus, als h√§tten sie mehrere Ausgabewerte. </font><font style="vertical-align: inherit;">Tats√§chlich haben sie nur einen Ausweg. </font><font style="vertical-align: inherit;">Viele Ausgabepfeile sind nur eine bequeme M√∂glichkeit, um zu zeigen, dass die Ausgabe des Perzeptrons als Eingabe mehrerer anderer Perzeptrone verwendet wird. </font><font style="vertical-align: inherit;">Dies ist weniger umst√§ndlich als das Zeichnen eines einzelnen Verzweigungsausgangs. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vereinfachen wir die Beschreibung von Perzeptronen. </font><font style="vertical-align: inherit;">Zustand</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-111"><span class="MJXp-msubsup" id="MJXp-Span-112"><span class="MJXp-mo" id="MJXp-Span-113" style="margin-left: 0.111em; margin-right: 0.05em;">‚àë</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-114" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-115"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-116" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-117" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-118"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-119" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-120" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-121" style="margin-left: 0.333em; margin-right: 0.333em;">&gt;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-122">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-123">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-124">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-125">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-126">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-127">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-128">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-129">d</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="20.1ex" height="3.142ex" viewBox="0 -832 8654.3 1352.7" role="img" focusable="false" style="vertical-align: -1.209ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJSZ1-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1494" y="-405"></use><g transform="translate(1614,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(2723,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMAIN-3E" x="3964" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-74" x="5021" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-72" x="5382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-65" x="5834" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-73" x="6300" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-68" x="6770" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6F" x="7346" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6C" x="7832" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-64" x="8130" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-3">\sum_j w_jx_j > treshold</script><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ungeschickt, und wir k√∂nnen uns auf zwei √Ñnderungen am Datensatz einigen, um ihn zu vereinfachen. </font><font style="vertical-align: inherit;">Das erste ist aufzunehmen</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-130"><span class="MJXp-msubsup" id="MJXp-Span-131"><span class="MJXp-mo" id="MJXp-Span-132" style="margin-left: 0.111em; margin-right: 0.05em;">‚àë</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-133" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-134"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-135" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-136" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-137"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-138" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-139" style="vertical-align: -0.4em;">j</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.564ex" height="3.142ex" viewBox="0 -832 3687.2 1352.7" role="img" focusable="false" style="vertical-align: -1.209ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJSZ1-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1494" y="-405"></use><g transform="translate(1614,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(2723,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="809" y="-213"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-4">\sum_j w_jx_j</script><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> als skalares Produkt, </font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-140"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-141">w</span><span class="MJXp-mo" id="MJXp-Span-142" style="margin-left: 0.267em; margin-right: 0.267em;">‚ãÖ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-143">x</span><span class="MJXp-mo" id="MJXp-Span-144" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-145"><span class="MJXp-mo" id="MJXp-Span-146" style="margin-left: 0.111em; margin-right: 0.05em;">‚àë</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-147" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-148"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-149" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-150" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-151"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-152" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-153" style="vertical-align: -0.4em;">j</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="16.335ex" height="3.142ex" viewBox="0 -832 7033.2 1352.7" role="img" focusable="false" style="vertical-align: -1.209ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMAIN-22C5" x="938" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-78" x="1439" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMAIN-3D" x="2289" y="0"></use><g transform="translate(3346,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJSZ1-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1494" y="-405"></use></g><g transform="translate(4960,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(6069,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhgeIRAozlr5aJIwAX1F8KzIj8eDwg#MJMATHI-6A" x="809" y="-213"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-5">w \cdot x = \sum_j w_jx_j</script><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wobei w und x Vektoren sind, deren Komponenten Gewichte bzw. Eingabedaten sind. </font><font style="vertical-align: inherit;">Die zweite besteht darin, die Schwelle auf einen anderen Teil der Ungleichung zu √ºbertragen und durch einen Wert zu ersetzen, der als Perzeptronverschiebung [Bias] bekannt ist.</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-154"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-155">b</span><span class="MJXp-mo" id="MJXp-Span-156" style="margin-left: 0.333em; margin-right: 0.333em;">‚â°</span><span class="MJXp-mo" id="MJXp-Span-157" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-158">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-159">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-160">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-161">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-162">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-163">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-164">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-165">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-166">d</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-6">b \equiv ‚àíthreshold</script>  .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wenn wir die Verschiebung anstelle eines Schwellenwerts verwenden, k√∂nnen wir die Perzeptronregel neu schreiben: </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-167"><span class="MJXp-mtable" id="MJXp-Span-168"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-169" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-170" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-171">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-172">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-173">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-174">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-175">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-176">t</span><span class="MJXp-mo" id="MJXp-Span-177" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mrow" id="MJXp-Span-178"><span class="MJXp-mo" id="MJXp-Span-179" style="margin-left: 0em; margin-right: 0em; vertical-align: -0.472em;"><span class="MJXp-right MJXp-scale5" style="font-size: 2.889em; margin-left: -0.22em;">{</span></span><span class="MJXp-mtable" id="MJXp-Span-180"><span><span class="MJXp-mtr" id="MJXp-Span-181" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-182" style="text-align: left;"><span class="MJXp-mn" id="MJXp-Span-183">0</span><span class="MJXp-mtext" id="MJXp-Span-184">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-185">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-186">f</span><span class="MJXp-mtext" id="MJXp-Span-187">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-188">w</span><span class="MJXp-mo" id="MJXp-Span-189" style="margin-left: 0.267em; margin-right: 0.267em;">‚ãÖ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-190">x</span><span class="MJXp-mo" id="MJXp-Span-191" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-192">b</span><span class="MJXp-mo" id="MJXp-Span-193" style="margin-left: 0.333em; margin-right: 0.333em;">‚â§</span><span class="MJXp-mn" id="MJXp-Span-194">0</span></span></span><span class="MJXp-mtr" id="MJXp-Span-195" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-196" style="padding-top: 0.2em; text-align: left;"><span class="MJXp-mn" id="MJXp-Span-197">1</span><span class="MJXp-mtext" id="MJXp-Span-198">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-199">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-200">f</span><span class="MJXp-mtext" id="MJXp-Span-201">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-202">w</span><span class="MJXp-mo" id="MJXp-Span-203" style="margin-left: 0.267em; margin-right: 0.267em;">‚ãÖ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-204">x</span><span class="MJXp-mo" id="MJXp-Span-205" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-206">b</span><span class="MJXp-mo" id="MJXp-Span-207" style="margin-left: 0.333em; margin-right: 0.333em;">&gt;</span><span class="MJXp-mn" id="MJXp-Span-208">0</span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-209" style="margin-left: 0em; margin-right: 0em;"></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-7"> output = \begin{cases} 0 ~ if ~ w \cdot x + b \leq 0 \\ 1 ~ if ~ w \cdot x + b > 0 \end{cases} \tag{2} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Versatz kann als Ma√ü daf√ºr dargestellt werden, wie einfach es ist, am Ausgang des Perzeptrons einen Wert von 1 zu erhalten. In biologischer Hinsicht ist die Verschiebung ein Ma√ü daf√ºr, wie einfach es ist, das Perzeptron zur Aktivierung zu bringen. Ein Perzeptron mit einer sehr gro√üen Vorspannung ist extrem einfach zu geben 1. Bei einer sehr gro√üen negativen Vorspannung ist dies jedoch schwierig. Offensichtlich ist die Einf√ºhrung von Bias eine kleine √Ñnderung in der Beschreibung von Perzeptronen, aber sp√§ter werden wir sehen, dass dies zu einer weiteren Vereinfachung der Aufzeichnung f√ºhrt. Daher werden wir weiterhin nicht den Schwellenwert verwenden, sondern immer den Offset.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich habe Perzeptrone im Hinblick auf die Methode zum Abw√§gen von Beweisen f√ºr die Entscheidungsfindung beschrieben. Eine andere Methode ihrer Verwendung ist die Berechnung elementarer logischer Funktionen, die wir normalerweise als Hauptberechnungen betrachten, wie z. B. AND, OR und NAND. Nehmen wir zum Beispiel an, wir haben ein Perzeptron mit zwei Eing√§ngen, deren Gewicht jeweils -2 betr√§gt und dessen Versatz 3 betr√§gt. Hier ist es: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/530/d45/9a9/530d459a9262c475caf057106a500ddc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eingang 00 ergibt Ausgang 1, weil (‚àí2) ‚àó 0 + (- 2 ) ‚àó 0 + 3 = 3 ist gr√∂√üer als Null. Dieselben Berechnungen besagen, dass die Eing√§nge 01 und 10 1 ergeben. Aber 11 am Eingang ergibt 0 am Ausgang, da (‚àí2) ‚àó 1 + (- 2) ‚àó 1 + 3 = ‚àí1, kleiner als Null. Daher implementiert unser Perzeptron die NAND-Funktion!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieses Beispiel zeigt, dass Perzeptrone verwendet werden k√∂nnen, um grundlegende Logikfunktionen zu berechnen. Tats√§chlich k√∂nnen wir Perzeptron-Netzwerke verwenden, um alle logischen Funktionen im Allgemeinen zu berechnen. Tatsache ist, dass das </font><font style="vertical-align: inherit;">NAND- </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Logikgatter</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> universell f√ºr Berechnungen ist - es ist m√∂glich, beliebige Berechnungen auf seiner Basis zu erstellen. Beispielsweise k√∂nnen Sie NAND-Gatter verwenden, um eine Schaltung zu erstellen, die zwei Bits x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> hinzuf√ºgt </font><font style="vertical-align: inherit;">. Berechnen Sie dazu die bitweise Summe</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-210"><span class="MJXp-msubsup" id="MJXp-Span-211"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-212" style="margin-right: 0.05em;">x</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-213" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-214" style="margin-left: 0.267em; margin-right: 0.267em;">‚äï</span><span class="MJXp-msubsup" id="MJXp-Span-215"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-216" style="margin-right: 0.05em;">x</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-217" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-8"> x_1 \oplus x_2 </script><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sowie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">das √úbertragsflag</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , das 1 ist, wenn sowohl x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> als auch x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1 sind - das hei√üt, das √úbertragsflag ist einfach das Ergebnis der bitweisen Multiplikation x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d25/af6/afe/d25af6afec499649a0b338f2ccc67f63.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um das √§quivalente Netzwerk von Perzeptronen zu erhalten, ersetzen wir alle NAND-Gatter durch Perzeptrone durch zwei Eingaben, deren Gewicht jeweils -2 betr√§gt, und mit einem Versatz von 3. Hier ist das resultierende Netzwerk. Beachten Sie, dass ich das Perzeptron entsprechend dem unteren rechten Ventil bewegt habe, um das Zeichnen von Pfeilen zu vereinfachen:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/222/5b4/71b/2225b471b11b7357777ebc024bd287c2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein bemerkenswerter Aspekt dieses Perzeptron-Netzwerks ist, dass der Ausgang des am weitesten links stehenden zweimal als Eingang nach unten verwendet wird. Als ich das Modell des Perzeptrons definierte, erw√§hnte ich nicht die Zul√§ssigkeit eines solchen Doppelausgangsschemas an derselben Stelle. In der Tat ist es nicht wirklich wichtig. Wenn wir dies nicht zulassen m√∂chten, k√∂nnen wir einfach zwei Zeilen mit einer Gewichtung von -2 zu einer mit einer Gewichtung von -4 kombinieren. (Wenn Ihnen dies nicht offensichtlich erscheint, h√∂ren Sie auf und beweisen Sie es sich selbst). Nach dieser √Ñnderung sieht das Netzwerk wie folgt aus: Alle nicht zugewiesenen Gewichte sind gleich -2, alle Offsets sind gleich 3 und ein Gewicht -4 ist markiert: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/151/d84/dbd/151d84dbdf944cc9016ed656ba70e424.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine solche Aufzeichnung von Perzeptronen, die eine Ausgabe, aber keine Eingaben haben:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/41c/8e3/e46/41c8e3e46587bf536bab96b8427e9bcd.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ist nur eine Abk√ºrzung. Dies bedeutet nicht, dass er keine Eingaben hat. Um dies zu verstehen, nehmen wir an, wir haben ein Perzeptron ohne Eingaben. Dann w√§re die gewichtete Summe ‚àë </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> immer Null, also w√ºrde das Perzeptron 1 f√ºr b&gt; 0 und 0 f√ºr b ‚â§ 0 geben. Das hei√üt, das Perzeptron w√ºrde nur einen festen Wert geben und nicht das, was wir brauchen (x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> im Beispiel) oben). Es ist besser, die eingegebenen Perzeptrone nicht als Perzeptrone zu betrachten, sondern als spezielle Einheiten, die einfach definiert werden, um die gew√ºnschten Werte x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ... </font><font style="vertical-align: inherit;">zu erzeugen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das Addiererbeispiel zeigt, wie ein Perzeptron-Netzwerk verwendet werden kann, um eine Schaltung zu simulieren, die viele NAND-Gatter enth√§lt. Und da diese Tore f√ºr Berechnungen universell sind, sind Perzeptrone f√ºr Berechnungen universell. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die rechnerische Vielseitigkeit von Perzeptronen ist sowohl ermutigend als auch entt√§uschend. Es ist ermutigend sicherzustellen, dass das Perzeptron-Netzwerk genauso leistungsf√§hig ist wie jedes andere Computerger√§t. Entt√§uschend, was den Eindruck erweckt, dass Perzeptrone nur eine neue Art von NAND-Logikgatter sind. So lala Entdeckung!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Situation ist jedoch tats√§chlich besser. </font><font style="vertical-align: inherit;">Es stellt sich heraus, dass wir Trainingsalgorithmen entwickeln k√∂nnen, mit denen die Gewichte und Verschiebungen des Netzwerks von k√ºnstlichen Neuronen automatisch angepasst werden k√∂nnen. </font><font style="vertical-align: inherit;">Diese Anpassung erfolgt als Reaktion auf externe Reize, ohne dass ein Programmierer direkt eingreifen muss. </font><font style="vertical-align: inherit;">Diese Lernalgorithmen erm√∂glichen es uns, k√ºnstliche Neuronen auf eine Weise zu verwenden, die sich radikal von gew√∂hnlichen Logikgattern unterscheidet. </font><font style="vertical-align: inherit;">Anstatt eine Schaltung explizit von NAND-Gattern und anderen zu registrieren, k√∂nnen unsere neuronalen Netze einfach lernen, wie man Probleme l√∂st, manchmal solche, f√ºr die es √§u√üerst schwierig w√§re, eine regul√§re Schaltung direkt zu entwerfen.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sigmoidneuronen </font></font></h3><br>  Lernalgorithmen sind gro√üartig.  Wie kann man jedoch einen solchen Algorithmus f√ºr ein neuronales Netzwerk entwickeln?  Angenommen, wir haben ein Netzwerk von Perzeptronen, mit denen wir uns in der L√∂sung eines Problems schulen m√∂chten.  Angenommen, die Eingabe in das Netzwerk kann Pixel eines gescannten Bildes einer handgeschriebenen Ziffer sein.  Und wir m√∂chten, dass das Netzwerk die Gewichte und Offsets kennt, die zur korrekten Klassifizierung der Zahlen erforderlich sind.  Um zu verstehen, wie ein solches Training funktionieren kann, stellen wir uns vor, dass wir ein bestimmtes Gewicht (oder eine bestimmte Tendenz) im Netzwerk geringf√ºgig √§ndern.  Wir m√∂chten, dass diese kleine √Ñnderung zu einer kleinen √Ñnderung der Netzwerkleistung f√ºhrt.  Wie wir bald sehen werden, erm√∂glicht diese Eigenschaft das Lernen.  Schematisch wollen wir Folgendes (offensichtlich ist ein solches Netzwerk zu einfach, um Handschrift zu erkennen!): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2cf/d97/76f/2cfd9776fdb27a7106bdf2a94d76eb46.png"><br><br>  Wenn eine kleine √Ñnderung des Gewichts (oder der Verzerrung) zu einer kleinen √Ñnderung des Ausgabeergebnisses f√ºhren w√ºrde, k√∂nnten wir die Gewichte und Verzerrungen so √§ndern, dass sich unser Netzwerk ein wenig n√§her an dem verh√§lt, was wir wollen.  Nehmen wir zum Beispiel an, das Netzwerk hat das Bild f√§lschlicherweise "8" zugewiesen, obwohl es "9" h√§tte sein sollen.  Wir k√∂nnten herausfinden, wie eine kleine √Ñnderung des Gewichts und der Verschiebung vorgenommen werden kann, damit das Netzwerk der Klassifizierung des Bildes als ‚Äû9‚Äú ein St√ºck n√§her kommt.  Und dann w√ºrden wir dies wiederholen und immer wieder Gewichte und Verschiebungen √§ndern, um das beste und beste Ergebnis zu erzielen.  Das Netzwerk w√ºrde lernen. <br><br>  Das Problem ist, dass dies nicht der Fall ist, wenn sich Perzeptrone im Netzwerk befinden.  Eine kleine √Ñnderung der Gewichte oder der Verschiebung eines Perzeptrons kann manchmal zu einer √Ñnderung seiner Ausgabe in das Gegenteil f√ºhren, beispielsweise von 0 auf 1. Ein solcher Flip kann das Verhalten des restlichen Netzwerks auf sehr komplizierte Weise √§ndern.  Und selbst wenn jetzt unsere ‚Äû9‚Äú richtig erkannt wird, hat sich das Verhalten des Netzwerks mit allen anderen Bildern wahrscheinlich auf eine schwer zu kontrollierende Weise vollst√§ndig ge√§ndert.  Aus diesem Grund ist es schwer vorstellbar, wie wir Gewichte und Offsets schrittweise anpassen k√∂nnen, damit sich das Netzwerk allm√§hlich dem gew√ºnschten Verhalten n√§hert.  Vielleicht gibt es einen klugen Weg, um dieses Problem zu umgehen.  Es gibt jedoch keine einfache L√∂sung f√ºr das Problem, ein Netzwerk von Perzeptronen zu lernen. <br><br>  Dieses Problem kann umgangen werden, indem ein neuer Typ eines k√ºnstlichen Neurons namens Sigmoid-Neuron eingef√ºhrt wird.  Sie √§hneln Perzeptronen, sind jedoch so modifiziert, dass kleine √Ñnderungen der Gewichte und Offsets nur zu geringen √Ñnderungen der Ausgabe f√ºhren.  Dies ist eine grundlegende Tatsache, die es dem Netzwerk von Sigmoidneuronen erm√∂glicht, zu lernen. <br><br>  Lassen Sie mich ein Sigmoid-Neuron beschreiben.  Wir werden sie auf die gleiche Weise wie Perzeptrone zeichnen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2cd/0b3/ccf/2cd0b3ccf36d0dabf96fd15649c29f90.png"><br><br>  Es hat den gleichen Eingang x <sub>1</sub> , x <sub>2</sub> , ... Aber anstatt gleich 0 oder 1 zu sein, k√∂nnen diese Eing√§nge einen beliebigen Wert im Bereich von 0 bis 1 haben. Beispielsweise ist ein Wert von 0,638 eine g√ºltige Eingabe f√ºr Sigmoidneuron (CH).  Genau wie das Perzeptron hat SN Gewichte f√ºr jeden Eingang, w <sub>1</sub> , w <sub>2</sub> , ... und die Gesamtvorspannung b.  Sein Ausgabewert ist jedoch nicht 0 oder 1. Es ist œÉ (w‚ãÖx + b), wobei œÉ das Sigmoid ist. <br><br>  √úbrigens wird œÉ manchmal als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">logistische Funktion bezeichnet</a> , und diese Klasse von Neuronen wird als logistische Neuronen bezeichnet.  Es ist n√ºtzlich, sich an diese Terminologie zu erinnern, da diese Begriffe von vielen Menschen verwendet werden, die mit neuronalen Netzen arbeiten.  Wir werden uns jedoch an die Sigmoid-Terminologie halten. <br><br>  Die Funktion ist wie folgt definiert: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-218"><span class="MJXp-mtext" id="MJXp-Span-219">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-220">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-221">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-222">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-223">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-224">a</span><span class="MJXp-mo" id="MJXp-Span-225" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-226">z</span><span class="MJXp-mo" id="MJXp-Span-227" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-228">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-229">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-230">q</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-231">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-232">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-233">v</span><span class="MJXp-mtext" id="MJXp-Span-234">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-235">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-236">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-237">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-238">c</span><span class="MJXp-mrow" id="MJXp-Span-239"><span class="MJXp-mn" id="MJXp-Span-240">1</span></span><span class="MJXp-mrow" id="MJXp-Span-241"><span class="MJXp-mn" id="MJXp-Span-242">1</span><span class="MJXp-mo" id="MJXp-Span-243" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-244"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-245" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-246" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-247">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-248">z</span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-249">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-250">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-251">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-252">g</span><span class="MJXp-mrow" id="MJXp-Span-253"><span class="MJXp-mn" id="MJXp-Span-254">3</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-9"> \ sigma (z) \ equiv \ frac {1} {1 + e ^ {- z}} \ tag {3} </script></p><br><br>  In unserem Fall wird der Ausgabewert des Sigmoidneurons mit den Eingabedaten x <sub>1</sub> , x <sub>2</sub> , ... durch die Gewichte w <sub>1</sub> , w <sub>2</sub> , ... und den Versatz b wie folgt betrachtet: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-255"><span class="MJXp-mtext" id="MJXp-Span-256">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-257">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-258">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-259">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-260">c</span><span class="MJXp-mrow" id="MJXp-Span-261"><span class="MJXp-mn" id="MJXp-Span-262">1</span></span><span class="MJXp-mrow" id="MJXp-Span-263"><span class="MJXp-mn" id="MJXp-Span-264">1</span><span class="MJXp-mo" id="MJXp-Span-265" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-266">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-267">x</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-268">p</span><span class="MJXp-mo" id="MJXp-Span-269" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mo" id="MJXp-Span-270" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-271">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-272">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-273">u</span><span class="MJXp-msubsup" id="MJXp-Span-274"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-275" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-276" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-277"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-278" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-279" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-280"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-281" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-282" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-283" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-284">b</span><span class="MJXp-mo" id="MJXp-Span-285" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mtext" id="MJXp-Span-286">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-287">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-289">g</span><span class="MJXp-mrow" id="MJXp-Span-290"><span class="MJXp-mn" id="MJXp-Span-291">4</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-10"> \ frac {1} {1 + exp (- \ sum_j w_jx_j - b)} \ tag {4} </script></p><br><br>  Auf den ersten Blick scheint CH v√∂llig anders zu sein als Neuronen.  Das algebraische Aussehen eines Sigmoid kann verwirrend und dunkel erscheinen, wenn Sie nicht damit vertraut sind.  Tats√§chlich gibt es viele √Ñhnlichkeiten zwischen Perzeptronen und SN, und die algebraische Form eines Sigmoid erweist sich eher als technisches Detail als als ernstes Hindernis f√ºr das Verst√§ndnis. <br><br>  Um die √Ñhnlichkeiten zum Perzeptronmodell zu verstehen, nehmen wir an, dass z ‚â° w ‚ãÖ x + b eine gro√üe positive Zahl ist.  Dann ist e - z ‚â§ 0, daher ist œÉ (z) ‚â§ 1. Mit anderen Worten, wenn z = w ‚â§ x + b gro√ü und positiv ist, betr√§gt die SN-Ausbeute ungef√§hr 1, wie im Perzeptron.  Angenommen, z = w ‚ãÖ x + b ist gro√ü mit einem Minuszeichen.  Dann ist e - z ‚Üí ‚àû und œÉ (z) ‚âà 0. F√ºr gro√ües z mit einem Minuszeichen n√§hert sich das Verhalten des SN also auch dem Perzeptron.  Und nur wenn w ‚ãÖ x + b eine durchschnittliche Gr√∂√üe hat, werden gravierende Abweichungen vom Perzeptronmodell beobachtet. <br><br>  Was ist mit der algebraischen Form von œÉ?  Wie verstehen wir ihn?  Tats√§chlich ist die genaue Form von œÉ nicht so wichtig - die Form der Funktion im Diagramm ist wichtig.  Da ist sie: <br><br><img src="https://habrastorage.org/webt/sm/u4/jv/smu4jvbwuriryrfojrpt-ukdr6w.png"><br><br>  Dies ist eine reibungslose Version der Schrittfunktion: <br><br><img src="https://habrastorage.org/webt/2i/p0/a-/2ip0a-3cmpstyiwrglx_michl3m.png"><br><br>  Wenn œÉ schrittweise w√§re, w√§re das SN ein Perzeptron, da es abh√§ngig vom Vorzeichen w ‚ãÖ x + b 0 oder 1 Ausgang haben w√ºrde (tats√§chlich ergibt das Perzeptron bei z = 0 0 und die Schrittfunktion - 1 an diesem Punkt m√ºsste also die Funktion ge√§ndert werden). <br><br>  Mit der reellen Funktion œÉ erhalten wir ein gegl√§ttetes Perzeptron.  Und die Hauptsache hier ist die Gl√§tte der Funktion, nicht ihre genaue Form.  Gl√§tte bedeutet, dass kleine √Ñnderungen der Œîw <sub>j</sub> -Gewichte und Œ¥b-Offsets kleine √Ñnderungen der Œî-Ausgabe des Ausgangs ergeben.  Die Algebra sagt uns, dass die Œî-Ausgabe wie folgt gut angen√§hert ist: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-292"><span class="MJXp-mtext" id="MJXp-Span-293">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-294">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-295">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-296">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-297">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-298">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-299">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-300">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-301">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-302">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-303">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-304">t</span><span class="MJXp-mtext" id="MJXp-Span-305">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-306">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-307">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-308">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-309">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-310">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-311">x</span><span class="MJXp-mtext" id="MJXp-Span-312">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-313">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-314">u</span><span class="MJXp-msubsup" id="MJXp-Span-315"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-316" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-317" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mtext" id="MJXp-Span-318">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-319">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-320">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-321">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-322">c</span><span class="MJXp-mrow" id="MJXp-Span-323"><span class="MJXp-mtext" id="MJXp-Span-324">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-325">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-326">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-327">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-328">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-329">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-330">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-331">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-332">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-333">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-334">A</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-335">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-336">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-337">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-338">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-339">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-340">e</span></span><span class="MJXp-mrow" id="MJXp-Span-341"><span class="MJXp-mtext" id="MJXp-Span-342">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-343">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-344">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-345">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-346">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-347">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-348">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-349">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-350">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-351">e</span><span class="MJXp-msubsup" id="MJXp-Span-352"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-353" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-354" style="vertical-align: -0.4em;">j</span></span></span><span class="MJXp-mtext" id="MJXp-Span-355">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-356">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-357">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-358">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-359">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-360">a</span><span class="MJXp-msubsup" id="MJXp-Span-361"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-362" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-363" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-364" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-365">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-366">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-367">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-368">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-369">c</span><span class="MJXp-mrow" id="MJXp-Span-370"><span class="MJXp-mtext" id="MJXp-Span-371">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-372">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-373">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-374">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-375">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-376">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-377">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-378">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-380">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-381">A</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-382">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-383">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-384">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-385">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-386">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387">e</span></span><span class="MJXp-mrow" id="MJXp-Span-388"><span class="MJXp-mtext" id="MJXp-Span-389">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-390">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-391">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-392">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-393">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-394">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-395">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-396">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-397">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-398">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-399">b</span></span><span class="MJXp-mtext" id="MJXp-Span-400">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-401">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-402">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-403">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-404">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-405">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-406">b</span><span class="MJXp-mtext" id="MJXp-Span-407">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-408">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-409">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-410">g</span><span class="MJXp-mrow" id="MJXp-Span-411"><span class="MJXp-mn" id="MJXp-Span-412">5</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-11"> \ Delta output \ approx \ sum_j \ frac {\ partielle Ausgabe} {\ partielle w_j} \ Delta w_j + \ frac {\ partielle Ausgabe} {\ partielle b} \ Delta b \ tag {5} </script></p><br><br>  Wenn die Summation √ºber allen Gewichten w <sub>j</sub> liegt und ‚àÇoutput / ‚àÇw <sub>j</sub> und ‚àÇoutput / ‚àÇb die partiellen Ableitungen der Ausgabe in Bezug auf w <sub>j</sub> bzw. b bezeichnen.  Keine Panik, wenn Sie sich in der Gesellschaft privater Derivate unsicher f√ºhlen!  Obwohl die Formel mit all diesen partiellen Ableitungen kompliziert aussieht, sagt sie tats√§chlich etwas ganz Einfaches (und N√ºtzliches) aus: Œîoutput ist eine lineare Funktion, die von den Gewichten und Vorspannungen von Œîw <sub>j</sub> und Œîb abh√§ngt.  Seine Linearit√§t macht es einfach, kleine √Ñnderungen der Gewichte und Offsets auszuw√§hlen, um jede gew√ºnschte kleine Ausgangsvorspannung zu erzielen.  Obwohl SNs im qualitativen Verhalten Perzeptronen √§hnlich sind, erleichtern sie das Verst√§ndnis, wie die Ausgabe durch √Ñndern von Gewichten und Verschiebungen ge√§ndert werden kann. <br><br>  Wenn die allgemeine Form œÉ f√ºr uns wichtig ist und nicht ihre genaue Form, warum verwenden wir dann eine solche Formel (3)?  Tats√§chlich werden wir sp√§ter manchmal Neuronen betrachten, deren Ausgabe f (w ‚ãÖ x + b) ist, wobei f () eine andere Aktivierungsfunktion ist.  Die Hauptsache, die sich √§ndert, wenn sich die Funktion √§ndert, ist der Wert der partiellen Ableitungen in Gleichung (5).  Es stellt sich heraus, dass die Verwendung von œÉ bei der Berechnung dieser partiellen Ableitungen die Algebra erheblich vereinfacht, da Exponenten bei der Differenzierung sehr sch√∂ne Eigenschaften haben.  In jedem Fall wird œÉ h√§ufig bei der Arbeit mit neuronalen Netzen verwendet, und am h√§ufigsten wird in diesem Buch eine solche Aktivierungsfunktion verwendet. <br><br>  Wie ist das Ergebnis der Arbeit von CH zu interpretieren?  Offensichtlich besteht der Hauptunterschied zwischen den Perzeptronen und dem CH darin, dass der CH nicht nur 0 oder 1 ausgibt. Ihre Ausgabe kann eine beliebige reelle Zahl von 0 bis 1 sein, sodass Werte wie 0,173 oder 0,689 g√ºltig sind.  Dies kann beispielsweise n√ºtzlich sein, wenn der Ausgabewert beispielsweise die durchschnittliche Helligkeit der Pixel des am Eingang des NS empfangenen Bildes anzeigen soll.  Aber manchmal kann es unpraktisch sein.  Angenommen, die Netzwerkausgabe soll sagen, dass "Bild 9 eingegeben wurde" oder "Eingabebild nicht 9".  Offensichtlich w√§re es einfacher, wenn die Ausgabewerte 0 oder 1 w√§ren, wie bei einem Perzeptron.  In der Praxis k√∂nnen wir uns jedoch darauf einigen, dass ein Ausgabewert von mindestens 0,5 am Eingang ‚Äû9‚Äú und ein Wert unter 0,5 ‚Äûnicht 9‚Äú bedeutet.  Ich werde immer ausdr√ºcklich auf das Bestehen solcher Vereinbarungen hinweisen. <br><br>  √úbungen <br><br><ul><li>  CH simuliert Perzeptrone, Teil 1 </li></ul><br>  Angenommen, wir nehmen alle Gewichte und Offsets aus einem Netzwerk von Perzeptronen und multiplizieren sie mit einer positiven Konstante c&gt; 0.  Zeigen Sie, dass sich das Netzwerkverhalten nicht √§ndert. <br><br><ul><li>  CH simuliert Perzeptrone, Teil 2 </li></ul><br>  Nehmen wir an, wir haben die gleiche Situation wie im vorherigen Problem - ein Netzwerk von Perzeptronen.  Angenommen, die Eingabedaten f√ºr das Netzwerk sind ausgew√§hlt.  Wir brauchen keinen bestimmten Wert, die Hauptsache ist, dass er fest ist.  Angenommen, die Gewichte und Verschiebungen sind so, dass w‚ãÖx + b ‚â† 0 ist, wobei x der Eingabewert eines beliebigen Perzeptrons des Netzwerks ist.  Jetzt ersetzen wir alle Perzeptrone im Netzwerk durch SN und multiplizieren die Gewichte und Verschiebungen mit der positiven Konstante c&gt; 0.  Zeigen Sie, dass im Grenzwert c ‚Üí ‚àû das Verhalten des Netzwerks vom SN genau das gleiche ist wie das der Perzeptron-Netzwerke.  Wie wird diese Aussage verletzt, wenn f√ºr eines der Perzeptrone w‚ãÖx + b = 0 ist? <br><br><h3>  Neuronale Netzwerkarchitektur </h3><br>  Im n√§chsten Abschnitt werde ich ein neuronales Netzwerk vorstellen, das eine gute Klassifizierung handgeschriebener Zahlen erm√∂glicht.  Zuvor ist es hilfreich, die Terminologie zu erl√§utern, mit der wir auf verschiedene Teile des Netzwerks verweisen k√∂nnen.  Angenommen, wir haben das folgende Netzwerk: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/472/521/00b/47252100b91b8e1a527796217a6ed0fa.png"><br><br>  Wie bereits erw√§hnt, wird die am weitesten links liegende Schicht im Netzwerk als Eingabeschicht bezeichnet, und ihre Neuronen werden als Eingangsneuronen bezeichnet.  Die am weitesten rechts stehende oder Ausgangsschicht enth√§lt Ausgangsneuronen oder, wie in unserem Fall, ein Ausgangsneuron.  Die mittlere Schicht wird als versteckt bezeichnet, da ihre Neuronen weder eingegeben noch ausgegeben werden.  Der Begriff "versteckt" mag etwas mysteri√∂s klingen - als ich ihn zum ersten Mal h√∂rte, entschied ich, dass er eine tiefe philosophische oder mathematische Bedeutung haben sollte - er bedeutet jedoch nur "weder Eingang noch Ausgang".  Das obige Netzwerk hat nur eine versteckte Schicht, aber einige Netzwerke haben mehrere versteckte Schichten.  Im folgenden vierschichtigen Netzwerk gibt es beispielsweise zwei versteckte Schichten: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/26e/be9/c5f/26ebe9c5f4c7f61413cfa43e67151734.png"><br><br>  Dies mag verwirrend sein, aber aus historischen Gr√ºnden werden solche mehrschichtigen Netzwerke manchmal als mehrschichtige Perzeptrone, MLPs, bezeichnet, obwohl sie eher aus Sigmoidneuronen als aus Perzeptronen bestehen.  Ich werde eine solche Terminologie nicht verwenden, weil sie verwirrend ist, aber ich muss vor ihrer Existenz warnen. <br><br>  Das Entwerfen von Eingabe- und Ausgabeebenen ist manchmal eine einfache Aufgabe.  Nehmen wir zum Beispiel an, wir versuchen festzustellen, ob die handschriftliche Zahl "9" bedeutet oder nicht.  Eine nat√ºrliche Netzwerkschaltung codiert die Helligkeit der Bildpixel in den Eingangsneuronen.  Wenn das Bild schwarzwei√ü mit einer Gr√∂√üe von 64 x 64 Pixel ist, haben wir 64 x 64 = 4096 Eingangsneuronen mit einer Helligkeit im Bereich von 0 bis 1. Die Ausgabeebene enth√§lt nur ein Neuron, dessen Wert kleiner als 0,5 bedeutet, dass "Ein" Die Eingabe war nicht 9 ", aber Werte mehr bedeuten, dass" die Eingabe 9 war ". <br><br>  W√§hrend das Entwerfen von Eingabe- und Ausgabeebenen oft eine einfache Aufgabe ist, kann das Entwerfen versteckter Ebenen eine schwierige Kunst sein.  Insbesondere ist es nicht m√∂glich, den Prozess der Entwicklung versteckter Ebenen mit wenigen einfachen Faustregeln zu beschreiben.  Forscher der Nationalversammlung haben viele heuristische Regeln f√ºr das Design verborgener Schichten entwickelt, die dazu beitragen, das gew√ºnschte Verhalten neuronaler Netze zu erreichen.  Eine solche Heuristik kann beispielsweise verwendet werden, um zu verstehen, wie ein Kompromiss zwischen der Anzahl der verborgenen Ebenen und der f√ºr das Training des Netzwerks verf√ºgbaren Zeit erzielt werden kann.  Sp√§ter werden wir auf einige dieser Regeln sto√üen. <br><br>  Bisher haben wir NSs diskutiert, bei denen die Ausgabe einer Schicht als Eingabe f√ºr die n√§chste verwendet wird.  Solche Netze werden neuronale Netze mit direkter Verteilung genannt.  Dies bedeutet, dass es keine Schleifen im Netzwerk gibt - Informationen werden immer weitergeleitet und niemals zur√ºckgemeldet.  Wenn wir Schleifen h√§tten, w√ºrden wir auf Situationen sto√üen, in denen das Eingabesigmoid von der Ausgabe abh√§ngen w√ºrde.  Es w√§re schwer zu verstehen, und wir erlauben solche Schleifen nicht. <br><br>  Es gibt jedoch andere Modelle k√ºnstlicher NS, bei denen R√ºckkopplungsschleifen verwendet werden k√∂nnen.  Diese Modelle werden als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wiederkehrende neuronale Netze</a> (RNS) bezeichnet.  Die Idee dieser Netzwerke ist, dass ihre Neuronen f√ºr begrenzte Zeitr√§ume aktiviert werden.  Diese Aktivierung kann andere Neutronen stimulieren, die etwas sp√§ter auch f√ºr eine begrenzte Zeit aktiviert werden k√∂nnen.  Dies f√ºhrt zur Aktivierung der folgenden Neuronen, und im Laufe der Zeit erhalten wir eine Kaskade aktivierter Neuronen.  Schleifen in solchen Modellen stellen keine Probleme dar, da die Ausgabe eines Neurons seinen Eintritt zu einem sp√§teren Zeitpunkt und nicht sofort beeinflusst. <br><br>  RNSs waren nicht so einflussreich wie NSs der direkten Verteilung, insbesondere weil Trainingsalgorithmen f√ºr RNSs bisher weniger Potenzial haben.  Das RNS bleibt jedoch weiterhin √§u√üerst interessant.  Im Geiste der Arbeit sind sie dem Gehirn viel n√§her als NS der direkten Verteilung.  Es ist m√∂glich, dass das RNS wichtige Probleme l√∂sen kann, die mit Hilfe der Direktverteilung NS mit gro√üen Schwierigkeiten gel√∂st werden k√∂nnen.  Um den Umfang unserer Studie einzuschr√§nken, werden wir uns jedoch auf die am weitesten verbreitete NS der Direktverteilung konzentrieren. <br><br><h3>  Einfaches Netzwerk zur Klassifizierung von Tinte </h3><br>  Nachdem wir die neuronalen Netze definiert haben, kehren wir zur Handschrifterkennung zur√ºck.  Die Aufgabe, handgeschriebene Zahlen zu erkennen, kann in zwei Unteraufgaben unterteilt werden.  Zun√§chst m√∂chten wir einen Weg finden, ein Bild mit vielen Ziffern in eine Folge von Einzelbildern aufzuteilen, von denen jedes eine Ziffer enth√§lt.  Zum Beispiel m√∂chten wir das Bild teilen <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><br>  in sechs getrennte <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b36/135/6ae/b361356aec440b1dbf77c8dedbc6f9b6.png"><br><br>  Wir Menschen k√∂nnen dieses Segmentierungsproblem leicht l√∂sen, aber es ist f√ºr ein Computerprogramm schwierig, das Bild korrekt aufzuteilen.  Nach der Segmentierung muss das Programm jede einzelne Ziffer klassifizieren.  So m√∂chten wir beispielsweise, dass unser Programm die erste Ziffer erkennt <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e5a/c2a/808/e5ac2a808e18ac02ba0f09b2052fff4f.png"><br><br>  es ist 5. <br><br>  Wir werden uns darauf konzentrieren, ein Programm zur L√∂sung des zweiten Problems zu erstellen, der Klassifizierung einzelner Zahlen.  Es stellt sich heraus, dass das Problem der Segmentierung nicht so schwer zu l√∂sen ist, sobald wir einen guten Weg finden, einzelne Ziffern zu klassifizieren.  Es gibt viele Ans√§tze zur L√∂sung des Segmentierungsproblems.  Eine davon besteht darin, viele verschiedene Arten der Bildsegmentierung unter Verwendung des Klassifikators einzelner Ziffern auszuprobieren und jeden Versuch zu bewerten.  Die Versuchssegmentierung wird sehr gesch√§tzt, wenn der Klassifizierer einzelner Ziffern mit der Klassifizierung aller Segmente vertraut ist, und niedrig, wenn er Probleme in einem oder mehreren Segmenten hat.  Die Idee ist, dass wenn der Klassifikator irgendwo Probleme hat, dies h√∂chstwahrscheinlich bedeutet, dass die Segmentierung falsch ist.  Diese Idee und andere Optionen k√∂nnen f√ºr eine gute L√∂sung des Segmentierungsproblems verwendet werden.  Anstatt uns um die Segmentierung zu k√ºmmern, konzentrieren wir uns auf die Entwicklung eines NS, der eine interessantere und komplexere Aufgabe l√∂sen kann, n√§mlich das Erkennen einzelner handgeschriebener Zahlen. <br><br>  Um einzelne Ziffern zu erkennen, verwenden wir NS aus drei Ebenen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/350/3dc/e6b3503dcc1e2848980d17ad8c193018.png"><br><br>  Die Eingangsnetzwerkschicht enth√§lt Neuronen, die verschiedene Werte der Eingangspixel codieren.  Wie im n√§chsten Abschnitt angegeben wird, bestehen unsere Trainingsdaten aus vielen Bildern gescannter handgeschriebener Ziffern mit einer Gr√∂√üe von 28 x 28 Pixel, sodass die Eingabeebene 28 x 28 = 784 Neuronen enth√§lt.  Der Einfachheit halber habe ich die meisten 784 Neuronen im Diagramm nicht angegeben.  Eingehende Pixel sind Schwarzwei√ü, wobei ein Wert von 0,0 Wei√ü anzeigt, 1,0 Schwarz anzeigt und Zwischenwerte zunehmend dunklere Graustufen anzeigen. <br><br>  Die zweite Schicht des Netzwerks ist ausgeblendet.  Wir bezeichnen die Anzahl der Neuronen in dieser Schicht n und werden mit verschiedenen Werten von n experimentieren.  Das obige Beispiel zeigt eine kleine verborgene Schicht, die nur n = 15 Neuronen enth√§lt. <br><br>  Es gibt 10 Neuronen in der Ausgabeschicht des Netzwerks.  Wenn das erste Neuron aktiviert ist, dh sein Ausgabewert ist ‚âà 1, bedeutet dies, dass das Netzwerk glaubt, dass die Eingabe 0 war. Wenn das zweite Neuron aktiviert ist, glaubt das Netzwerk, dass die Eingabe 1 war. Und so weiter.  Genau genommen nummerieren wir die Ausgangsneuronen von 0 bis 9 und schauen uns an, welche von ihnen den maximalen Aktivierungswert hatten.  Wenn dies beispielsweise Neuron Nr. 6 ist, dann glaubt unser Netzwerk, dass die Eingabe Nummer 6 war. Und so weiter. <br><br>  Sie fragen sich vielleicht, warum wir zehn Neuronen verwenden m√ºssen.  Schlie√ülich wollen wir wissen, welche Ziffer von 0 bis 9 dem Eingabebild entspricht.  Es w√§re nat√ºrlich, nur 4 Ausgangsneuronen zu verwenden, von denen jedes einen Bin√§rwert annehmen w√ºrde, abh√§ngig davon, ob sein Ausgangswert n√§her an 0 oder 1 liegt. Vier Neuronen w√§ren ausreichend, da 2 <sup>4</sup> = 16, mehr als 10 m√∂gliche Werte.  Warum sollte unser Netzwerk 10 Neuronen verwenden?  Ist das unwirksam?  Die Basis daf√ºr ist empirisch;  Wir k√∂nnen beide Varianten des Netzwerks ausprobieren, und es stellt sich heraus, dass f√ºr diese Aufgabe ein Netzwerk mit 10 Ausgangsneuronen besser darauf trainiert ist, Zahlen zu erkennen als ein Netzwerk mit 4.  Es bleibt jedoch die Frage, warum 10 Ausgangsneuronen besser sind.  Gibt es eine Heuristik, die uns im Voraus sagen w√ºrde, dass 10 Ausgangsneuronen anstelle von 4 verwendet werden sollten? <br><br>  Um zu verstehen, warum, ist es n√ºtzlich, dar√ºber nachzudenken, was ein neuronales Netzwerk tut.  Betrachten Sie zun√§chst die Option mit 10 Ausgangsneuronen.  Wir konzentrieren uns auf das erste Ausgangsneuron, das versucht zu entscheiden, ob das eingehende Bild Null ist.  Er tut dies, indem er Beweise aus einer verborgenen Schicht abw√§gt.  Was machen versteckte Neuronen?  Angenommen, das erste Neuron in der verborgenen Schicht bestimmt, ob das Bild so etwas enth√§lt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/650/c35/402/650c3540204f98b406e25078ddc8742a.png"><br><br>  Er kann dies tun, indem er Pixeln, die zu diesem Bild passen, gro√üe Gewichte und dem Rest kleine Gewichte zuweist.  Nehmen wir auf die gleiche Weise an, dass das zweite, dritte und vierte Neuron in der verborgenen Schicht suchen, ob das Bild √§hnliche Fragmente enth√§lt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d94/4c3/cea/d944c3cea54d91106d2ee2198d04c801.png"><br><br>  Wie Sie vielleicht erraten haben, ergeben diese vier Fragmente zusammen das Bild 0, das wir zuvor gesehen haben: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f99/a20/1e5/f99a201e557bacdbe39190a6f913b49e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn also die vier versteckten Neuronen aktiviert sind, k√∂nnen wir daraus schlie√üen, dass die Zahl 0 ist. Dies ist nat√ºrlich nicht der einzige Beweis daf√ºr, dass dort 0 angezeigt wurde - wir k√∂nnen 0 auf viele andere Arten erhalten (indem wir diese Bilder leicht verschieben oder leicht verzerren). Wir k√∂nnen jedoch mit Sicherheit sagen, dass wir zumindest in diesem Fall den Schluss ziehen k√∂nnen, dass am Eingang 0 war.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn wir davon ausgehen, dass das Netzwerk so funktioniert, k√∂nnen wir plausibel erkl√§ren, warum es besser ist, 10 Ausgangsneuronen anstelle von 4 zu verwenden. Wenn wir 4 Ausgangsneuronen h√§tten, w√ºrde das erste Neuron versuchen, zu entscheiden, was das bedeutendste Bit der eingehenden Ziffer ist. Und es gibt keine einfache M√∂glichkeit, das wichtigste Bit mit den oben angegebenen einfachen Formen zu verkn√ºpfen. Es ist kaum vorstellbar, warum Teile der Form einer Ziffer in irgendeiner Weise mit dem wichtigsten Teil der Ausgabe zusammenh√§ngen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All dies wird jedoch nur durch Heuristiken unterst√ºtzt. </font><font style="vertical-align: inherit;">Nichts spricht daf√ºr, dass ein dreischichtiges Netzwerk wie gesagt funktionieren sollte und versteckte Neuronen einfache Komponenten von Formen finden sollten. </font><font style="vertical-align: inherit;">Vielleicht findet der knifflige Lernalgorithmus einige Gewichte, mit denen wir nur 4 Ausgangsneuronen verwenden k√∂nnen. </font><font style="vertical-align: inherit;">Als Heuristik funktioniert meine Methode jedoch gut und kann Ihnen viel Zeit bei der Entwicklung einer guten NS-Architektur sparen.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √úbungen </font></font></h3><br><ul><li>      ,      .          ,     .         . ,   3   ,       (  )     0,99,       0,01. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/f5b/4af/2e9/f5b4af2e9acf846ab8cc5c60dac20c03.png"><br><br><h3>     </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben also das NA-Schema - wie lernt man, Zahlen zu erkennen? Das erste, was wir brauchen, sind Trainingsdaten, die sogenannten Trainingsdatensatz. Wir werden das </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MNIST-Kit verwenden,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> das Zehntausende gescannter Bilder handgeschriebener Zahlen und deren korrekte Klassifizierung enth√§lt. Der Name MNIST wurde aufgrund der Tatsache erhalten, dass es sich um eine modifizierte Teilmenge der beiden Datens√§tze handelt, die von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NIST</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dem US-amerikanischen National Institute of Standards and Technology, </font><font style="vertical-align: inherit;">gesammelt wurden </font><font style="vertical-align: inherit;">. Hier einige Bilder von MNIST: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b36/135/6ae/b361356aec440b1dbf77c8dedbc6f9b6.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies sind die gleichen Zahlen, die zu Beginn des Kapitels als Erkennungsaufgabe angegeben wurden. Wenn wir die NS √ºberpr√ºfen, werden wir sie nat√ºrlich bitten, die falschen Bilder zu erkennen, die bereits im Trainingsset enthalten waren!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MNIST-Daten bestehen aus zwei Teilen. Das erste enth√§lt 60.000 Bilder f√ºr das Training. Dies sind gescannte Manuskripte von 250 Personen, von denen die H√§lfte Angestellte des US Census Bureau und die andere H√§lfte Sch√ºler waren. Die Bilder sind schwarzwei√ü und messen 28 x 28 Pixel. Der zweite Teil des MNIST-Datensatzes besteht aus 10.000 Bildern zum Testen des Netzwerks. Dies ist auch ein Schwarzwei√übild mit 28 x 28 Pixel. Wir werden diese Daten verwenden, um zu bewerten, wie gut das Netzwerk gelernt hat, Zahlen zu erkennen. Um die Qualit√§t der Bewertung zu verbessern, wurden diese Zahlen von weiteren 250 Personen herangezogen, die nicht an der Aufzeichnung des Schulungssatzes teilnahmen (obwohl dies auch Mitarbeiter des B√ºros und Sch√ºler waren). Dies hilft uns sicherzustellen, dass unser System die Handschrift von Personen erkennen kann, die es w√§hrend des Trainings nicht getroffen hat.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Trainingseingabe wird mit x bezeichnet. Es ist zweckm√§√üig, jedes Eingabebild x als einen Vektor mit 28x28 = 784 Messungen zu behandeln. Jeder Wert innerhalb des Vektors gibt die Helligkeit eines Pixels im Bild an. Wir werden den Ausgabewert als y = y (x) bezeichnen, wobei y ein zehndimensionaler Vektor ist. Wenn zum Beispiel ein bestimmtes Trainingsbild x 6 enth√§lt, ist y (x) = (0,0,0,0,0,0,1,0,0,0) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> der Vektor, den wir ben√∂tigen. T ist eine Transponierungsoperation, die einen Zeilenvektor in einen Spaltenvektor verwandelt. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir wollen einen Algorithmus finden, der es uns erm√∂glicht, nach solchen Gewichten und Offsets zu suchen, so dass sich die Netzwerkausgabe f√ºr alle Trainingseingaben x y (x) n√§hert. Um die Ann√§herung an dieses Ziel zu quantifizieren, definieren wir eine Kostenfunktion (manchmal auch </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">als Verlustfunktion bezeichnet)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">;; Im Buch werden wir die Kostenfunktion verwenden, aber einen anderen Namen beachten):</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-413"><span class="MJXp-mtable" id="MJXp-Span-414"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-415" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-416" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-417">C</span><span class="MJXp-mo" id="MJXp-Span-418" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-419">w</span><span class="MJXp-mo" id="MJXp-Span-420" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-421">b</span><span class="MJXp-mo" id="MJXp-Span-422" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-423" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mfrac" id="MJXp-Span-424" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-425">1</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-426">2</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-427">n</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-428"><span class=""><span class="MJXp-mo" id="MJXp-Span-429" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">‚àë</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-430" style="margin-left: 0px;">x</span></span></span><span class="MJXp-mrow" id="MJXp-Span-431"><span class="MJXp-mo" id="MJXp-Span-432" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mrow" id="MJXp-Span-433"><span class="MJXp-mo" id="MJXp-Span-434" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-435">y</span><span class="MJXp-mo" id="MJXp-Span-436" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-437">x</span><span class="MJXp-mo" id="MJXp-Span-438" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-439" style="margin-left: 0em; margin-right: 0.222em;">‚Äì</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-440">a</span><span class="MJXp-mrow" id="MJXp-Span-441"><span class="MJXp-mo" id="MJXp-Span-442" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-msubsup" id="MJXp-Span-443"><span class="MJXp-mrow" id="MJXp-Span-444" style="margin-right: 0.05em;"><span class="MJXp-mo" id="MJXp-Span-445" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-446" style="vertical-align: 0.5em;">2</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-12"> C(w, b) = \frac{1}{2n} \sum_x || y(x) ‚Äì a ||^2 \tag{6} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hier bezeichnet w einen Satz von Netzwerkgewichten, b ist ein Satz von Offsets, n ist die Anzahl von Trainingseingabedaten, a ist der Vektor von Ausgangsdaten, wenn x Eingabedaten sind, und die Summe durchl√§uft alle Trainingseingabedaten x. Die Ausgabe h√§ngt nat√ºrlich von x, w und b ab, aber der Einfachheit halber habe ich diese Abh√§ngigkeit nicht bezeichnet. Die Notation || v || bedeutet die L√§nge des Vektors v. Wir werden C eine quadratische Kostenfunktion nennen; manchmal wird es auch als Standardfehler oder MSE bezeichnet. Wenn Sie sich C genau ansehen, k√∂nnen Sie sehen, dass es nicht negativ ist, da alle Mitglieder der Summe nicht negativ sind. Au√üerdem werden die Kosten von C (w, b) klein, dh C (w, b) ‚â§ 0, genau dann, wenn y (x) f√ºr alle Trainingseingabedaten x ungef√§hr gleich dem Ausgangsvektor a ist. Unser Algorithmus funktionierte also gut, wenn es uns gelang, Gewichte und Offsets so zu finden, dass C (w, b) ‚âà 0 war. Und umgekehrt funktionierte er schlecht, wenn C (w,b) gro√ü - Dies bedeutet, dass y (x) f√ºr eine gro√üe Menge an Eingabe nicht mit der Ausgabe √ºbereinstimmt. Es stellt sich heraus, dass das Ziel des Trainingsalgorithmus darin besteht, die Kosten von C (w, b) als Funktion von Gewichten und Offsets zu minimieren. Mit anderen Worten, wir m√ºssen eine Reihe von Gewichten und Offsets finden, die den Wert der Kosten minimieren. Wir werden dies mit einem Algorithmus tun, der Gradientenabstieg genannt wird.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Warum brauchen wir einen quadratischen Wert? Interessieren wir uns nicht haupts√§chlich f√ºr die Anzahl der vom Netzwerk korrekt erkannten Bilder? Ist es m√∂glich, diese Zahl einfach direkt zu maximieren und den Zwischenwert des quadratischen Wertes nicht zu minimieren? Das Problem ist, dass die Anzahl der korrekt erkannten Bilder keine glatte Funktion der Gewichte und Offsets des Netzwerks ist. Zum gr√∂√üten Teil √§ndern kleine √Ñnderungen der Gewichte und Offsets nicht die Anzahl der korrekt erkannten Bilder. Aus diesem Grund ist es schwer zu verstehen, wie Gewichte und Vorspannungen ge√§ndert werden k√∂nnen, um die Effizienz zu verbessern. Wenn wir eine reibungslose Kostenfunktion verwenden, ist es f√ºr uns leicht zu verstehen, wie kleine √Ñnderungen an Gewichten und Offsets vorgenommen werden k√∂nnen, um die Kosten zu verbessern. Daher konzentrieren wir uns zuerst auf den quadratischen Wert und untersuchen dann die Klassifizierungsgenauigkeit.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selbst wenn wir bedenken, dass wir eine reibungslose Kostenfunktion verwenden m√∂chten, k√∂nnte es Sie dennoch interessieren, warum wir die quadratische Funktion f√ºr Gleichung (6) gew√§hlt haben. Ist es nicht m√∂glich, es willk√ºrlich zu w√§hlen? Wenn wir eine andere Funktion w√§hlen w√ºrden, w√ºrden wir vielleicht einen v√∂llig anderen Satz von Minimierungsgewichten und Offsets erhalten? Eine vern√ºnftige Frage, und sp√§ter werden wir die Kostenfunktion erneut untersuchen und einige Korrekturen daran vornehmen. Die quadratische Kostenfunktion eignet sich jedoch hervorragend, um die grundlegenden Dinge beim Erlernen von NS zu verstehen. Daher werden wir uns vorerst daran halten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zusammenfassend: Unser Ziel beim Training von NS ist es, Gewichte und Offsets zu finden, die die quadratische Kostenfunktion C (w, b) minimieren. Die Aufgabe ist gut gestellt, hat aber bisher viele ablenkende Strukturen - die Interpretation von w und b als Gewichte und Offsets, die im Hintergrund verborgene Funktion œÉ, die Wahl der Netzwerkarchitektur, MNIST und so weiter. Es stellt sich heraus, dass wir viel verstehen k√∂nnen, den gr√∂√üten Teil dieser Struktur ignorieren und uns nur auf den Aspekt der Minimierung konzentrieren. Daher werden wir vorerst die spezielle Form der Kostenfunktion, die Kommunikation mit der Nationalversammlung usw. vergessen. Stattdessen werden wir uns vorstellen, dass wir nur eine Funktion mit vielen Variablen haben und diese minimieren wollen. Wir werden eine Technologie namens Gradientenabstieg entwickeln, mit der solche Probleme gel√∂st werden k√∂nnen. Und dann kommen wir zu einer bestimmten Funktion zur√ºck,was wir f√ºr die Nationalversammlung minimieren wollen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nehmen wir an, wir versuchen, eine Funktion C (v) zu minimieren. Es kann eine beliebige Funktion mit reellen Werten vieler Variablen sein. V = v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ... Beachten Sie, dass ich die Notation w und b durch v ersetzt habe, um zu zeigen, dass es sich um eine beliebige Funktion handeln kann - wir sind nicht mehr von HC besessen. Es ist n√ºtzlich sich vorzustellen, dass eine Funktion C nur zwei Variablen hat - v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/fff/752/1ad/fff7521ad0e339cb68eceace0f200697.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir m√∂chten herausfinden, wo C ein globales Minimum erreicht. Nat√ºrlich k√∂nnen wir mit der oben gezeichneten Funktion den Graphen studieren und das Minimum finden. In diesem Sinne habe ich Ihnen vielleicht eine zu einfache Funktion gegeben! Im allgemeinen Fall kann C eine komplexe Funktion vieler Variablen sein, und es ist normalerweise unm√∂glich, nur den Graphen zu betrachten und das Minimum zu finden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine M√∂glichkeit, das Problem zu l√∂sen, besteht darin, mithilfe der Algebra das Minimum analytisch zu ermitteln. Wir k√∂nnen die Ableitungen berechnen und versuchen, sie zu verwenden, um das Extremum zu finden. Wenn wir Gl√ºck haben, funktioniert dies, wenn C eine Funktion von einer oder zwei Variablen ist. Bei einer gro√üen Anzahl von Variablen wird dies jedoch zu einem Albtraum. Und f√ºr NSs ben√∂tigen wir oft viel mehr Variablen - f√ºr die gr√∂√üten NSs h√§ngen die Kostenfunktionen auf komplexe Weise von Milliarden von Gewichten und Verschiebungen ab. Die Verwendung von Algebra zur Minimierung dieser Funktionen schl√§gt fehl!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Nachdem ich festgestellt hatte, dass es f√ºr uns bequemer w√§re, C als Funktion zweier Variablen zu betrachten, sagte ich zweimal in zwei Abs√§tzen: ‚ÄûJa, aber was ist, wenn es sich um eine Funktion einer viel gr√∂√üeren Anzahl von Variablen handelt?‚Äú Ich entschuldige mich. Glauben Sie uns, dass es wirklich n√ºtzlich sein wird, C als Funktion darzustellen Bei zwei Variablen f√§llt dieses Bild manchmal auseinander, weshalb die beiden vorherigen Abs√§tze ben√∂tigt wurden. F√ºr mathematische √úberlegungen ist es h√§ufig erforderlich, mehrere intuitive Darstellungen zu jonglieren und gleichzeitig zu lernen, wann die Darstellung verwendet werden kann und wann nicht ZYA.)</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Okay, das bedeutet, dass Algebra nicht funktioniert. Gl√ºcklicherweise gibt es eine gro√üartige Analogie, die einen gut funktionierenden Algorithmus bietet. Wir stellen uns unsere Funktion als Tal vor. Mit dem neuesten Zeitplan wird es nicht so schwierig sein. Und wir stellen uns einen Ball vor, der am Hang des Tals entlang rollt. Unsere Erfahrung zeigt uns, dass der Ball irgendwann ganz nach unten rutschen wird. Vielleicht k√∂nnen wir diese Idee nutzen, um das Minimum einer Funktion zu finden? Wir w√§hlen zuf√§llig den Startpunkt f√ºr einen imagin√§ren Ball aus und simulieren dann die Bewegung des Balls, als w√ºrde er auf den Grund des Tals rollen. Wir k√∂nnen diese Simulation einfach verwenden, indem wir die Ableitungen (und m√∂glicherweise die zweiten Ableitungen) von C z√§hlen - sie erz√§hlen uns alles √ºber die lokale Form des Tals und damit dar√ºber, wie unser Ball rollt.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Basierend auf dem, was Sie geschrieben haben, k√∂nnten Sie denken, dass wir Newtons Bewegungsgleichungen f√ºr den Ball aufschreiben, die Auswirkungen von Reibung und Schwerkraft ber√ºcksichtigen und so weiter. Tats√§chlich werden wir dieser Analogie mit dem Ball nicht so nahe kommen - wir entwickeln einen Algorithmus zur Minimierung von C und keine exakte Simulation der Gesetze der Physik! Diese Analogie sollte unsere Vorstellungskraft anregen und unser Denken nicht einschr√§nken. Anstatt in die komplexen Details der Physik einzutauchen, stellen wir die Frage: Wenn wir f√ºr einen Tag zu Gott ernannt w√ºrden, w√ºrden wir unsere eigenen Gesetze der Physik erstellen und dem Ball sagen, wie er rollen soll, welches Gesetz oder welche Bewegungsgesetze wir w√§hlen w√ºrden, damit der Ball immer weiter rollt Talboden? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um das Problem zu kl√§ren, werden wir dar√ºber nachdenken, was passiert, wenn wir den Ball um eine kleine Strecke Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in Richtung von v </font><sub><font style="vertical-align: inherit;">1</font></sub><font style="vertical-align: inherit;"> bewegen</font></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und einen kleinen Abstand Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in Richtung von v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Die Algebra sagt uns, dass sich C wie folgt √§ndert:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-447"><span class="MJXp-mtable" id="MJXp-Span-448"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-449" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-450" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-451">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-452">C</span><span class="MJXp-mo" id="MJXp-Span-453" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mfrac" id="MJXp-Span-454" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-455">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-456">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-457">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-458"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-459" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-460" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-461">Œî</span><span class="MJXp-msubsup" id="MJXp-Span-462"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-463" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-464" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-465" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mfrac" id="MJXp-Span-466" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-467">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-468">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-469">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-470"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-471" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-472" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-473">Œî</span><span class="MJXp-msubsup" id="MJXp-Span-474"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-475" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-476" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-13"> \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2 \tag{7} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir werden einen Weg finden, solche Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> so </font><font style="vertical-align: inherit;">zu w√§hlen </font><font style="vertical-align: inherit;">, dass ŒîC kleiner als Null ist; </font><font style="vertical-align: inherit;">Das hei√üt, wir werden sie so ausw√§hlen, dass der Ball nach unten rollt. </font><font style="vertical-align: inherit;">Um zu verstehen, wie dies zu tun ist, ist es n√ºtzlich, Œîv als den Vektor der √Ñnderungen zu definieren, dh Œîv ‚â° (Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , wobei T die Transponierungsoperation ist, die Zeilenvektoren in Spaltenvektoren umwandelt. </font><font style="vertical-align: inherit;">Wir definieren auch den Gradientenvektor C als partielle Ableitungen (‚àÇS / ‚àÇV </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ‚àÇS / ‚àÇV </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Wir bezeichnen den Gradientenvektor mit ‚àá:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-477"><span class="MJXp-mtable" id="MJXp-Span-478"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-479" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-480" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-481">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-482">C</span><span class="MJXp-mo" id="MJXp-Span-483" style="margin-left: 0.333em; margin-right: 0.333em;">‚â°</span><span class="MJXp-mo" id="MJXp-Span-484" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mfrac" id="MJXp-Span-485" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-486">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-487">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-488">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-489"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-490" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-491" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-492" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mfrac" id="MJXp-Span-493" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-494">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-495">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-496">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-497"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-498" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-499" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-500"><span class="MJXp-mo" id="MJXp-Span-501" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-502" style="vertical-align: 0.5em;">T</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-14"> \nabla C \equiv (\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2})^T \tag{8} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bald werden wir die √Ñnderung von ŒîC durch Œîv und den Gradienten ‚àáC umschreiben. In der Zwischenzeit m√∂chte ich etwas klarstellen, weshalb die Leute oft am Gef√§lle h√§ngen. Wenn sie sich zum ersten Mal mit ‚àáC trafen, verstehen die Leute manchmal nicht, wie sie das Symbol wahrnehmen sollen ‚àá. Was bedeutet das konkret? Tats√§chlich k√∂nnen Sie ‚àáC sicher als ein einzelnes mathematisches Objekt betrachten - einen zuvor definierten Vektor - der einfach mit zwei Zeichen geschrieben wird. Unter diesem Gesichtspunkt ist ‚àá wie das Schwenken einer Flagge, die besagt, dass "‚àáC ein Gradientenvektor ist". Es gibt fortgeschrittenere Gesichtspunkte, unter denen ‚àá als unabh√§ngige mathematische Einheit betrachtet werden kann (zum Beispiel als Differenzierungsoperator), aber wir brauchen sie nicht. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mit solchen Definitionen kann Ausdruck (7) wie folgt umgeschrieben werden:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-503"><span class="MJXp-mtable" id="MJXp-Span-504"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-505" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-506" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-507">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-508">C</span><span class="MJXp-mo" id="MJXp-Span-509" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mi" id="MJXp-Span-510">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-511">C</span><span class="MJXp-mo" id="MJXp-Span-512" style="margin-left: 0.267em; margin-right: 0.267em;">‚ãÖ</span><span class="MJXp-mi" id="MJXp-Span-513">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-514">v</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-15"> \Delta C \approx \nabla C \cdot \Delta v \tag{9} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diese Gleichung hilft zu erkl√§ren, warum ‚àáC als Gradientenvektor bezeichnet wird: Sie verbindet die √Ñnderungen in v mit den √Ñnderungen in C, genau wie von einer Entit√§t namens Gradienten erwartet. </font><font style="vertical-align: inherit;">[dt. </font><font style="vertical-align: inherit;">Gradient - Abweichung / ca. </font><font style="vertical-align: inherit;">trans.] Es ist jedoch interessanter, dass diese Gleichung es uns erm√∂glicht zu sehen, wie man Œîv so w√§hlt, dass ŒîC negativ ist. </font><font style="vertical-align: inherit;">Nehmen wir an, wir w√§hlen</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-515"><span class="MJXp-mtable" id="MJXp-Span-516"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-517" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-518" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-519">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-520">v</span><span class="MJXp-mo" id="MJXp-Span-521" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-522" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-523">Œ∑</span><span class="MJXp-mi" id="MJXp-Span-524">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-525">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-16"> \Delta v = - \eta \nabla C \tag{10} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dabei ist Œ∑ ein kleiner positiver Parameter (Lerngeschwindigkeit). </font><font style="vertical-align: inherit;">Dann sagt uns Gleichung (9), dass ŒîC ‚âà - Œ∑ ‚àáC ‚ãÖC = - Œ∑ || ‚àáC ||</font></font> <sup>2</sup> .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seit || ‚àáC || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚â• 0, dies stellt sicher, dass ŒîC ‚â§ 0, dh C, die ganze Zeit abnimmt, wenn wir v √§ndern, wie in (10) vorgeschrieben (nat√ºrlich als Teil der N√§herung aus Gleichung (9)). Und genau das brauchen wir! Daher nehmen wir Gleichung (10), um das "Bewegungsgesetz" des Balls in unserem Gradientenabstiegsalgorithmus zu bestimmen. Das hei√üt, wir werden Gleichung (10) verwenden, um den Œîv-Wert zu berechnen, und dann werden wir den Ball auf diesen Wert bewegen:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-526"><span class="MJXp-mtable" id="MJXp-Span-527"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-528" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-529" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-530">v</span><span class="MJXp-mo" id="MJXp-Span-531" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msup" id="MJXp-Span-532"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-533" style="margin-right: 0.05em;">v</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-534" style="vertical-align: 0.5em;">‚Ä≤</span></span><span class="MJXp-mo" id="MJXp-Span-535" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-536">v</span><span class="MJXp-mo" id="MJXp-Span-537" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-538">Œ∑</span><span class="MJXp-mi" id="MJXp-Span-539">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-540">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-17"> v \rightarrow v' = v - \eta \nabla C \tag{11} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dann wenden wir diese Regel erneut f√ºr den n√§chsten Schritt an. Wenn wir die Wiederholung fortsetzen, werden wir C senken, bis wir hoffentlich ein globales Minimum erreichen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zusammenfassend l√§sst sich sagen, dass der Gradientenabstieg durch sequentielle Berechnung des Gradienten ‚àá C und die anschlie√üende Verschiebung in die entgegengesetzte Richtung erfolgt, was zu einem ‚ÄûSturz‚Äú entlang des Talhangs f√ºhrt. Dies kann wie folgt visualisiert werden: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f7/495/966/5f749596634bc20923f5f8a3e49a3b9f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Beachten Sie, dass mit dieser Regel der Gradientenabstieg keine echte physische Bewegung reproduziert. Im wirklichen Leben hat der Ball einen Impuls, der es ihm erm√∂glichen kann, √ºber den Hang zu rollen oder sogar f√ºr einige Zeit aufzurollen. Erst nach der Arbeit der Reibungskraft wird garantiert, dass der Ball das Tal hinunter rollt. Unsere Auswahlregel Œîv sagt nur "runter". Eine ziemlich gute Regel, um das Minimum zu finden!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Damit der Gradientenabstieg korrekt funktioniert, m√ºssen wir einen ausreichend kleinen Wert der Lerngeschwindigkeit Œ∑ w√§hlen, damit Gleichung (9) eine gute Ann√§herung darstellt. Andernfalls kann sich herausstellen, dass ŒîC&gt; 0 ist - nichts Gutes! Gleichzeitig ist es nicht erforderlich, dass Œ∑ zu klein ist, da dann die √Ñnderungen von Œîv winzig sind und der Algorithmus zu langsam arbeitet. In der Praxis √§ndert sich Œ∑ so, dass Gleichung (9) eine gute Ann√§herung ergibt und der Algorithmus nicht zu langsam arbeitet. Sp√§ter werden wir sehen, wie es funktioniert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich erkl√§rte den Gradientenabstieg, wenn Funktion C nur von zwei Variablen abhing. Aber alles funktioniert auf die gleiche Weise, wenn C eine Funktion vieler Variablen ist. Angenommen, sie hat m Variablen, v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ..., v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Dann ist die √Ñnderung von &amp; Dgr; C, die durch eine kleine √Ñnderung von &amp; Dgr; v = (&amp; Dgr; v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ..., </font><font style="vertical-align: inherit;">&amp; Dgr </font><font style="vertical-align: inherit;">; v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T verursacht</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> wird</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-541"><span class="MJXp-mtable" id="MJXp-Span-542"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-543" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-544" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-545">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-546">C</span><span class="MJXp-mo" id="MJXp-Span-547" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mi" id="MJXp-Span-548">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-549">C</span><span class="MJXp-mo" id="MJXp-Span-550" style="margin-left: 0.267em; margin-right: 0.267em;">‚ãÖ</span><span class="MJXp-mi" id="MJXp-Span-551">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-552">v</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-18"> \Delta C \approx \nabla C \cdot \Delta v \tag{12} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> wobei der Gradient ‚àáC der Vektor ist </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-553"><span class="MJXp-mtable" id="MJXp-Span-554"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-555" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-556" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-557">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-558">C</span><span class="MJXp-mo" id="MJXp-Span-559" style="margin-left: 0.333em; margin-right: 0.333em;">‚â°</span><span class="MJXp-mo" id="MJXp-Span-560" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mfrac" id="MJXp-Span-561" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-562">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-563">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-564">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-565"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-566" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-567" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-568" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-569" style="margin-left: 0em; margin-right: 0em;">‚Ä¶</span><span class="MJXp-mo" id="MJXp-Span-570" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mfrac" id="MJXp-Span-571" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-572">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-573">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-574">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-575"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-576" style="margin-right: 0.05em;">v</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-577" style="vertical-align: -0.4em;">m</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-578"><span class="MJXp-mo" id="MJXp-Span-579" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-580" style="vertical-align: 0.5em;">T</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-19"> \nabla C \equiv (\frac{\partial C}{\partial v_1},‚Ä¶, \frac{\partial C}{\partial v_m})^T \tag{13} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wie bei zwei Variablen k√∂nnen wir w√§hlen </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-581"><span class="MJXp-mtable" id="MJXp-Span-582"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-583" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-584" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-585">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-586">v</span><span class="MJXp-mo" id="MJXp-Span-587" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-588" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-589">Œ∑</span><span class="MJXp-mi" id="MJXp-Span-590">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-591">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-20"> \Delta v = - \eta \nabla C \tag{14} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und stellen Sie sicher, dass unser ungef√§hrer Ausdruck (12) f√ºr ŒîC negativ ist. </font><font style="vertical-align: inherit;">Dies gibt uns die M√∂glichkeit, den Gradienten auf ein Minimum zu reduzieren, selbst wenn C eine Funktion vieler Variablen ist, und die Aktualisierungsregel immer wieder anzuwenden.</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-592"><span class="MJXp-mtable" id="MJXp-Span-593"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-594" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-595" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-596">v</span><span class="MJXp-mo" id="MJXp-Span-597" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msup" id="MJXp-Span-598"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-599" style="margin-right: 0.05em;">v</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-600" style="vertical-align: 0.5em;">‚Ä≤</span></span><span class="MJXp-mo" id="MJXp-Span-601" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-602">v</span><span class="MJXp-mo" id="MJXp-Span-603" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-604">Œ∑</span><span class="MJXp-mi" id="MJXp-Span-605">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-606">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-21-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-21"> v \rightarrow v' = v - \eta \nabla C \tag{15} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diese Aktualisierungsregel kann als definierender Gradientenabstiegsalgorithmus betrachtet werden. Es gibt uns eine Methode zum wiederholten √Ñndern der Position von v auf der Suche nach dem Minimum der Funktion C. Diese Regel funktioniert nicht immer - verschiedene Dinge k√∂nnen schief gehen und verhindern, dass der Gradientenabstieg das globale Minimum von C findet - wir werden in den folgenden Kapiteln auf diesen Punkt zur√ºckkommen. In der Praxis funktioniert der Gradientenabstieg jedoch h√§ufig sehr gut, und wir werden sehen, dass dies in der Nationalversammlung ein wirksamer Weg ist, um die Kostenfunktion zu minimieren und daher das Netzwerk zu trainieren.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In gewissem Sinne kann der Gradientenabstieg als optimale minimale Suchstrategie angesehen werden. </font><font style="vertical-align: inherit;">Angenommen, wir versuchen, Œîv in eine Position zu bewegen, um C zu minimieren. Dies entspricht der Minimierung von ŒîC ‚âà ‚àáC ‚ãÖ Œîv. </font><font style="vertical-align: inherit;">Wir werden die Schrittgr√∂√üe so begrenzen, dass || Œîv || </font><font style="vertical-align: inherit;">= Œµ f√ºr eine kleine Konstante Œµ&gt; 0. Mit anderen Worten, wir m√∂chten eine kleine Strecke fester Gr√∂√üe zur√ºcklegen und versuchen, die Bewegungsrichtung zu finden, die C so weit wie m√∂glich verringert. </font><font style="vertical-align: inherit;">Es kann bewiesen werden, dass die Wahl von Œîv zur Minimierung von ‚àáC ‚ãÖ Œîv Œîv = -Œ∑‚àáC ist, wobei Œ∑ = Œµ / || ‚àáC || durch die Einschr√§nkung || Œîv || bestimmt wird </font><font style="vertical-align: inherit;">= Œµ. </font><font style="vertical-align: inherit;">Der Gradientenabstieg kann daher als ein Weg angesehen werden, kleine Schritte in die Richtung zu unternehmen, in der C am meisten abnimmt.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √úbungen </font></font></h3><br><ul><li>     . :      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  ‚Äî </a> , ,  ,      . </li><li>     ,      ,       .  ,       ?            ? </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Menschen haben viele Optionen f√ºr den Gradientenabstieg untersucht, einschlie√ülich solcher, die einen echten physischen Ball genauer reproduzieren. Solche Optionen haben ihre Vorteile, aber auch einen gro√üen Nachteil: Die Notwendigkeit, die zweiten partiellen Ableitungen von C zu berechnen, die viele Ressourcen verbrauchen k√∂nnen. Um dies zu verstehen, nehmen wir an, dass wir alle zweiten partiellen Ableitungen ‚àÇ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> berechnen </font><font style="vertical-align: inherit;">m√ºssen </font><font style="vertical-align: inherit;">. Wenn die Variablen v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Millionen sind, m√ºssen wir ungef√§hr eine Billion (eine Million Quadrat) zweite partielle Ableitungen berechnen (tats√§chlich eine halbe Billion, da ‚àÇ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = ‚àÇ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Aber du hast die Essenz verstanden). Dies erfordert viele Rechenressourcen. Es gibt Tricks, um dies zu vermeiden, und die Suche nach Alternativen zum Gradientenabstieg ist ein Bereich aktiver Forschung. In diesem Buch werden wir jedoch den Gradientenabstieg und seine Varianten als Hauptansatz f√ºr das Erlernen von NS verwenden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie wenden wir Gradientenabstieg auf NA-Lernen an? Wir m√ºssen es verwenden, um nach Gewichten w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und Offsets b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zu suchen </font><font style="vertical-align: inherit;">, die die Kostengleichung (6) minimieren. Schreiben wir die Aktualisierungsregel f√ºr den Gradientenabstieg neu, indem wir die Variablen v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> durch </font><font style="vertical-align: inherit;">Gewichte und Offsets </font><font style="vertical-align: inherit;">ersetzen </font><font style="vertical-align: inherit;">. Mit anderen Worten, jetzt hat unsere ‚ÄûPosition‚Äú die Komponenten w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , und der Gradientenvektor ‚àáC hat die entsprechenden Komponenten ‚àÇC / ‚àÇw</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und ‚àÇC / ‚àÇb </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Nachdem wir unsere Update-Regel mit neuen Komponenten geschrieben haben, erhalten wir:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-607"><span class="MJXp-mtable" id="MJXp-Span-608"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-609" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-610" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-611"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-612" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-613" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-614" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msubsup" id="MJXp-Span-615"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-616" style="margin-right: 0.05em;">w</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-618">‚Ä≤</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-617">k</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-619" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-620"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-621" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-622" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-623" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-624">Œ∑</span><span class="MJXp-mfrac" id="MJXp-Span-625" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-626">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-627">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-628">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-629"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-630" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-631" style="vertical-align: -0.4em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-22-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-22"> w_k \rightarrow w'_k = w_k - \eta \frac{\partial C}{\partial w_k} \tag{16} </script></p><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-632"><span class="MJXp-mtable" id="MJXp-Span-633"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-634" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-635" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-636"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-637" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-638" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-639" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msubsup" id="MJXp-Span-640"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-641" style="margin-right: 0.05em;">b</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-643">‚Ä≤</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-642">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-644" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-645"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-646" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-647" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-648" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-649">Œ∑</span><span class="MJXp-mfrac" id="MJXp-Span-650" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-651">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-652">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-653">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-654"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-655" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-656" style="vertical-align: -0.4em;">l</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-23-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-23"> b_l \rightarrow b'_l = b_l - \eta \frac{\partial C}{\partial b_l} \tag{17} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Durch erneutes Anwenden dieser Aktualisierungsregel k√∂nnen wir ‚Äûbergab rollen‚Äú und mit etwas Gl√ºck die Mindestkostenfunktion finden. Mit anderen Worten, diese Regel kann verwendet werden, um die Nationalversammlung auszubilden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt mehrere Hindernisse f√ºr die Anwendung der Gradientenabstiegsregel. Wir werden sie in den folgenden Kapiteln genauer untersuchen. Aber im Moment m√∂chte ich nur ein Problem erw√§hnen. Um es zu verstehen, kehren wir zum quadratischen Wert in Gleichung (6) zur√ºck. Beachten Sie, dass diese Kostenfunktion wie </font><font style="vertical-align: inherit;">folgt </font><font style="vertical-align: inherit;">aussieht: C = 1 / n ‚àë </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font><font style="vertical-align: inherit;">dh </font><font style="vertical-align: inherit;">es sind die durchschnittlichen Kosten C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚â° (|| y (x) - a || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) / 2 f√ºr einzelne Trainingsbeispiele. In der Praxis m√ºssen wir zur Berechnung des Gradienten ‚àáC die Gradienten ‚àáC </font><sub><font style="vertical-align: inherit;">x</font></sub><font style="vertical-align: inherit;"> berechnen</font></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">getrennt f√ºr jede Trainingseingabe x und dann mitteln, ‚àáC = 1 / n ‚àë </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àáC </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Leider wird es sehr lange dauern, wenn die Menge an Eingaben sehr gro√ü ist, und ein solches Training wird langsam sein. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um das Lernen zu beschleunigen, k√∂nnen Sie den stochastischen Gradientenabstieg verwenden. Die Idee ist, den ‚àáC-Gradienten durch Berechnen von ‚àáC </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> f√ºr eine kleine Zufallsstichprobe von Trainingseingaben </font><font style="vertical-align: inherit;">n√§herungsweise zu berechnen </font><font style="vertical-align: inherit;">. Durch die Berechnung ihres Durchschnitts k√∂nnen wir schnell eine gute Sch√§tzung des tats√§chlichen Gradienten ‚àáC erhalten, was dazu beitr√§gt, den Gradientenabstieg und damit das Training zu beschleunigen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine genauere Formulierung des stochastischen Gradientenabfalls erfolgt durch Zufallsstichprobe einer kleinen Anzahl von m Trainingseingabedaten. </font><font style="vertical-align: inherit;">Wir werden diese zuf√§lligen Daten X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , .., X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nennen und es ein Minipaket nennen. </font><font style="vertical-align: inherit;">Wenn die Stichprobengr√∂√üe m gro√ü genug ist, liegt der Durchschnittswert von ‚àáC </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nahe genug am Durchschnitt aller ‚àáCx, d. H.</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-657"><span class="MJXp-mtable" id="MJXp-Span-658"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-659" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-660" style="text-align: center;"><span class="MJXp-mfrac" id="MJXp-Span-661" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-662"><span class="MJXp-mo" id="MJXp-Span-663" style="margin-left: 0.111em; margin-right: 0.05em;">‚àë</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-668">m</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-664"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-665">j</span><span class="MJXp-mo" id="MJXp-Span-666">=</span><span class="MJXp-mn" id="MJXp-Span-667">1</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-669">‚àá</span><span class="MJXp-msubsup" id="MJXp-Span-670"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-671" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-672" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-673"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-674" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-675" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-676">m</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-677" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mfrac" id="MJXp-Span-678" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-679"><span class="MJXp-mo" id="MJXp-Span-680" style="margin-left: 0.111em; margin-right: 0.05em;">‚àë</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-681" style="vertical-align: -0.4em;">x</span></span><span class="MJXp-mi" id="MJXp-Span-682">‚àá</span><span class="MJXp-msubsup" id="MJXp-Span-683"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-684" style="margin-right: 0.05em;">C</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-685" style="vertical-align: -0.4em;">x</span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-686">n</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-687" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi" id="MJXp-Span-688">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-689">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-24-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-24"> \frac{\sum^m_{j=1} \nabla C_{X_j}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C \tag{18} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dabei geht der zweite Betrag √ºber den gesamten Satz von Trainingsdaten. </font><font style="vertical-align: inherit;">Durch den Austausch von Teilen erhalten wir</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-690"><span class="MJXp-mtable" id="MJXp-Span-691"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-692" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-693" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-694">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-695">C</span><span class="MJXp-mo" id="MJXp-Span-696" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mfrac" id="MJXp-Span-697" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-698">1</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-699">m</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-700"><span><span class="MJXp-over"><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-706" style="margin-right: 0px; margin-left: 0px;">m</span></span><span class=""><span class="MJXp-mo" id="MJXp-Span-701" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">‚àë</span></span></span></span></span><span class=" MJXp-script"><span class="MJXp-mrow" id="MJXp-Span-702" style="margin-left: 0px;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-703">j</span><span class="MJXp-mo" id="MJXp-Span-704">=</span><span class="MJXp-mn" id="MJXp-Span-705">1</span></span></span></span><span class="MJXp-mi" id="MJXp-Span-707">‚àá</span><span class="MJXp-msubsup" id="MJXp-Span-708"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-709" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-710" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-711"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-712" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-713" style="vertical-align: -0.4em;">j</span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-25-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-25"> \nabla C \approx \frac{1}{m} \sum^m_{j=1} \nabla C_{X_j} \tag{19} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies best√§tigt, dass wir den Gesamtgradienten sch√§tzen k√∂nnen, indem wir die Gradienten f√ºr ein zuf√§llig ausgew√§hltes Minipack berechnen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um dies direkt mit dem Training von NS in Verbindung zu bringen, nehmen wir an, dass w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> die Gewichte und Verschiebungen unseres NS bezeichnen. </font><font style="vertical-align: inherit;">Dann w√§hlt der stochastische Gradientenabstieg ein zuf√§lliges Minipaket von Eingabedaten aus und lernt daraus</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-714"><span class="MJXp-mtable" id="MJXp-Span-715"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-716" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-717" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-718"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-719" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-720" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-721" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msubsup" id="MJXp-Span-722"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-723" style="margin-right: 0.05em;">w</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-725">‚Ä≤</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-724">k</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-726" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-727"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-728" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-729" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-730" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mfrac" id="MJXp-Span-731" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-732">Œ∑</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-733">m</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-734"><span class=""><span class="MJXp-mo" id="MJXp-Span-735" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">‚àë</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-736" style="margin-left: 0px;">j</span></span></span><span class="MJXp-mfrac" id="MJXp-Span-737" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-738">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-739"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-740" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-741" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-742"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-743" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-744" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-745">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-746"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-747" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-748" style="vertical-align: -0.4em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-26-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-26"> w_k \rightarrow w'_k = w_k - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20} </script></p><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-749"><span class="MJXp-mtable" id="MJXp-Span-750"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-751" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-752" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-753"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-754" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-755" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-756" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msubsup" id="MJXp-Span-757"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-758" style="margin-right: 0.05em;">b</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-760">‚Ä≤</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-759">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-761" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-762"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-763" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-764" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-765" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mfrac" id="MJXp-Span-766" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-767">Œ∑</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-768">m</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-769"><span class=""><span class="MJXp-mo" id="MJXp-Span-770" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">‚àë</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-771" style="margin-left: 0px;">j</span></span></span><span class="MJXp-mfrac" id="MJXp-Span-772" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-773">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-774"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-775" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-776" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-777"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-778" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-779" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-780">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-781"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-782" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-783" style="vertical-align: -0.4em;">l</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-27-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-27"> b_l \rightarrow b'_l = b_l - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l} \tag{21} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wo ist die Summe aller Trainingsbeispiele X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> im aktuellen Minipaket? </font><font style="vertical-align: inherit;">Dann w√§hlen wir ein anderes zuf√§lliges Minipaket aus und studieren es. </font><font style="vertical-align: inherit;">Und so weiter, bis wir alle Trainingsdaten ersch√∂pft haben, was als Ende der Trainings√§ra bezeichnet wird. </font><font style="vertical-align: inherit;">In diesem Moment beginnen wir eine neue √Ñra des Lernens.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√úbrigens ist anzumerken, dass sich die Vereinbarungen bez√ºglich der Skalierung der Kostenfunktion und der Aktualisierung der Gewichte und Offsets in einem Minipaket unterscheiden. In Gleichung (6) haben wir die Kostenfunktion 1 / n-mal skaliert. Manchmal lassen die Leute 1 / n weg, indem sie die Kosten f√ºr einzelne Trainingsbeispiele addieren, anstatt den Durchschnitt zu berechnen. Dies ist n√ºtzlich, wenn die Gesamtzahl der Trainingsbeispiele nicht im Voraus bekannt ist. Dies kann beispielsweise passieren, wenn zus√§tzliche Daten in Echtzeit angezeigt werden. Auf die gleiche Weise lassen die Mini-Paket-Aktualisierungsregeln (20) und (21) manchmal das 1 / m-Mitglied vor der Summe weg. Konzeptionell hat dies keine Auswirkungen, da es einer √Ñnderung der Lerngeschwindigkeit Œ∑ entspricht. Beim Vergleich verschiedener Werke lohnt es sich jedoch, darauf zu achten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein stochastischer Gradientenabstieg kann als politische Abstimmung betrachtet werden: Es ist viel einfacher, eine Stichprobe in Form eines Minipakets zu entnehmen, als einen Gradientenabstieg auf eine vollst√§ndige Stichprobe anzuwenden - genau wie eine Umfrage am Ausgang eines Standorts einfacher ist als eine vollst√§ndige Wahl durchzuf√ºhren. Wenn unser Trainingssatz beispielsweise wie MNIST eine Gr√∂√üe von n = 60.000 hat und wir eine Stichprobe eines Minipakets mit der Gr√∂√üe m = 10 erstellen, beschleunigen wir die Gradientensch√§tzung um das 6000-fache! Nat√ºrlich wird die Sch√§tzung nicht ideal sein - es wird statistische Schwankungen geben -, aber sie muss nicht ideal sein: Wir m√ºssen uns nur in die Richtung bewegen, in der C abnimmt, was bedeutet, dass wir den Gradienten nicht genau berechnen m√ºssen. In der Praxis ist der stochastische Gradientenabstieg eine g√§ngige und leistungsstarke Unterrichtstechnik f√ºr die Nationalversammlung und die Grundlage der meisten Unterrichtstechnologien, die wir als Teil des Buches entwickeln werden.</font></font><br><br><h3>  </h3><br><ul><li>       -  1.  ,    x         w <sub>k</sub> ‚Üí w‚Ä≤ <sub>k</sub> = w <sub>k</sub> ‚àí Œ∑ ‚àÇC <sub>x</sub> / ‚àÇw <sub>k</sub>  b <sub>l</sub> ‚Üí b‚Ä≤ <sub>l</sub> = b <sub>l</sub> ‚àí Œ∑ ‚àÇC <sub>x</sub> / ‚àÇb <sub>l</sub> .              .  Usw.   ,  -,   .  -            ( ).       -         -  20. </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lassen Sie mich diesen Abschnitt mit einer Diskussion √ºber ein Thema beenden, das manchmal Menschen st√∂rt, die zum ersten Mal auf einen Gradientenabstieg gesto√üen sind. In NS ist der Wert von C eine Funktion vieler Variablen - aller Gewichte und Offsets - und bestimmt gewisserma√üen die Oberfl√§che in einem sehr mehrdimensionalen Raum. Die Leute beginnen zu denken: "Ich muss all diese zus√§tzlichen Dimensionen visualisieren." Und sie beginnen sich Sorgen zu machen: "Ich kann nicht in vier Dimensionen navigieren, ganz zu schweigen von f√ºnf (oder f√ºnf Millionen)." Haben sie eine besondere Qualit√§t, die die ‚Äûechte‚Äú Supermathematik hat? Nat√ºrlich nicht. Selbst professionelle Mathematiker k√∂nnen sich den vierdimensionalen Raum nicht gut vorstellen - wenn √ºberhaupt. Sie gehen zu Tricks und entwickeln andere M√∂glichkeiten, um darzustellen, was passiert. Genau das haben wir getan:Wir haben die algebraische (und nicht die visuelle) Darstellung von ŒîC verwendet, um zu verstehen, wie man sich bewegt, damit C abnimmt. Menschen, die gute Arbeit mit einer Vielzahl von Dimensionen leisten, haben eine gro√üe Bibliothek √§hnlicher Techniken im Kopf. Unser algebraischer Trick ist nur ein Beispiel. Diese Techniken sind m√∂glicherweise nicht so einfach wie wir es gewohnt sind, wenn wir drei Dimensionen visualisieren. Wenn Sie jedoch eine Bibliothek √§hnlicher Techniken erstellen, beginnen Sie, in h√∂heren Dimensionen gut zu denken. Ich werde nicht auf Details eingehen, aber wenn Sie interessiert sind, m√∂gen Sie vielleichtWas sind wir gewohnt, wenn wir drei Dimensionen visualisieren, aber wenn Sie eine Bibliothek √§hnlicher Techniken erstellt haben, beginnen Sie, in h√∂heren Dimensionen gut zu denken. Ich werde nicht auf Details eingehen, aber wenn Sie interessiert sind, m√∂gen Sie vielleichtWas sind wir gewohnt, wenn wir drei Dimensionen visualisieren, aber wenn Sie eine Bibliothek √§hnlicher Techniken erstellt haben, beginnen Sie, in h√∂heren Dimensionen gut zu denken. Ich werde nicht auf Details eingehen, aber wenn Sie interessiert sind, m√∂gen Sie vielleicht</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine Diskussion einiger dieser Techniken durch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> professionelle Mathematiker, die es gewohnt sind, in h√∂heren Dimensionen zu denken. </font><font style="vertical-align: inherit;">Obwohl einige der besprochenen Techniken recht komplex sind, sind die meisten der besten Antworten intuitiv und f√ºr jedermann zug√§nglich.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Implementierung eines Netzwerks zur Klassifizierung von Zahlen </font></font></h3><br>  Ok, jetzt schreiben wir ein Programm, das lernt, handgeschriebene Ziffern anhand des stochastischen Gradientenabfalls und der Trainingsdaten von MNIST zu erkennen.  Wir werden dies mit einem kurzen Programm in Python 2.7 tun, das nur aus 74 Zeilen besteht!  Als erstes m√ºssen wir die MNIST-Daten herunterladen.  Wenn Sie git verwenden, k√∂nnen Sie sie erhalten, indem Sie das Repository dieses Buches klonen: <br><br> <code>git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git</code> <br> <br>  Wenn nicht, laden Sie den Code <a href="">√ºber den Link</a> herunter. <br><br>  Als ich zuvor MNIST-Daten erw√§hnte, sagte ich √ºbrigens, dass sie in 60.000 Trainingsbilder und 10.000 Testbilder unterteilt sind.  Dies ist die offizielle Beschreibung von MNIST.  Wir werden die Daten etwas anders brechen.  Wir werden die √úberpr√ºfungsbilder unver√§ndert lassen, aber den Schulungssatz in zwei Teile aufteilen: 50.000 Bilder, mit denen wir die Nationalversammlung schulen, und einzelne 10.000 Bilder zur zus√§tzlichen Best√§tigung.  Wir werden sie zwar nicht verwenden, aber sp√§ter werden sie uns n√ºtzlich sein, wenn wir die Konfiguration einiger Hyperparameter des NS verstehen - die Lerngeschwindigkeit usw. -, die unser Algorithmus nicht direkt ausw√§hlt.  Obwohl best√§tigende Daten nicht Teil der urspr√ºnglichen MNIST-Spezifikation sind, verwenden viele MNIST auf diese Weise, und im Bereich HC ist die Verwendung best√§tigender Daten √ºblich.  Wenn ich jetzt von den ‚ÄûMNIST-Trainingsdaten‚Äú spreche, meine ich unsere 50.000 Karitnoks, nicht die urspr√ºnglichen 60.000. <br><br>  Zus√§tzlich zu MNIST-Daten ben√∂tigen wir eine Python-Bibliothek namens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Numpy</a> f√ºr schnelle lineare Algebra-Berechnungen.  Wenn Sie es nicht haben, k√∂nnen Sie es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vom Link nehmen</a> . <br><br>  Bevor ich Ihnen das gesamte Programm vorstelle, m√∂chte ich die Hauptfunktionen des Codes f√ºr NS erl√§utern.  Den zentralen Platz nimmt die Netzwerkklasse ein, mit der wir die Nationalversammlung vertreten.  Hier ist der Initialisierungscode f√ºr das Netzwerkobjekt: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, sizes)</span></span></span><span class="hljs-function">:</span></span> self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Das Gr√∂√üenarray enth√§lt die Anzahl der Neuronen in den entsprechenden Schichten.  Wenn wir also ein Netzwerkobjekt mit zwei Neuronen in der ersten Schicht, drei Neuronen in der zweiten Schicht und einem Neuron in der dritten Schicht erstellen m√∂chten, schreiben wir es folgenderma√üen: <br><br><pre> <code class="python hljs">net = Network([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre> <br>  Die Offsets und Gewichte im Netzwerkobjekt werden zuf√§llig mit der Numpy-Funktion np.random.randn initialisiert, die eine Gau√üsche Verteilung mit einer mathematischen Erwartung von 0 und einer Standardabweichung von 1 erzeugt. Diese zuf√§llige Initialisierung gibt unserem stochastischen Gradientenabstiegsalgorithmus einen Ausgangspunkt.  In den folgenden Kapiteln finden Sie die besten M√∂glichkeiten zum Initialisieren von Gewichten und Offsets. Dies reicht jedoch vorerst aus.  Beachten Sie, dass der Netzwerkinitialisierungscode davon ausgeht, dass die erste Schicht von Neuronen eingegeben wird, und ihnen keine Verzerrung zuweist, da sie nur zur Berechnung der Ausgabe verwendet werden. <br><br>  Beachten Sie auch, dass Offsets und Gewichte als Array von Numpy-Matrizen gespeichert werden.  Zum Beispiel ist net.weights [1] eine Numpy-Matrix, die die Gewichte speichert, die die zweite und dritte Schicht von Neuronen verbinden (dies ist nicht die erste und zweite Schicht, da in Python die Nummerierung der Elemente des Arrays von Grund auf neu erfolgt).  Da das Schreiben von net.weights [1] zu lange dauert, bezeichnen wir diese Matrix als w.  Dies ist eine solche Matrix, dass w <sub>jk</sub> das Gewicht der Verbindung zwischen dem k-ten Neuron in der zweiten Schicht und dem j-ten Neuron in der dritten Schicht ist.  Eine solche Reihenfolge der Indizes j und k mag seltsam erscheinen - w√§re es nicht logischer, sie zu tauschen?  Der gro√üe Vorteil einer solchen Aufzeichnung besteht jedoch darin, dass der Aktivierungsvektor der dritten Schicht von Neuronen erhalten wird: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-784"><span class="MJXp-msup" id="MJXp-Span-785"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-786" style="margin-right: 0.05em;">a</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-787" style="vertical-align: 0.5em;">‚Ä≤</span></span><span class="MJXp-mo" id="MJXp-Span-788" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-789">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-790">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-791">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-792">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-793">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-794">a</span><span class="MJXp-mo" id="MJXp-Span-795" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-796">w</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-797">a</span><span class="MJXp-mo" id="MJXp-Span-798" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-799">b</span><span class="MJXp-mo" id="MJXp-Span-800" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-801">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-802">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-803">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-804">g</span><span class="MJXp-mrow" id="MJXp-Span-805"><span class="MJXp-mn" id="MJXp-Span-806">22</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-28-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-28"> a '= \ sigma (wa + b) \ tag {22} </script></p><br><br>  Schauen wir uns diese ziemlich reiche Gleichung an.  a ist der Aktivierungsvektor der zweiten Schicht von Neuronen.  Um a 'zu erhalten, multiplizieren wir a mit der Gewichtsmatrix w und addieren den Verschiebungsvektor b.  Dann wenden wir das Sigmoid œÉ Element f√ºr Element auf jedes Element des Vektors wa + b an (dies wird als Vektorisierung der Funktion œÉ bezeichnet).  Es ist leicht zu √ºberpr√ºfen, ob Gleichung (22) das gleiche Ergebnis wie Regel (4) f√ºr die Berechnung eines Sigmoidneurons liefert. <br><br><h3>  √úbung </h3><br><ul><li>  Schreiben Sie Gleichung (22) in Komponentenform und stellen Sie sicher, dass sie das gleiche Ergebnis wie Regel (4) f√ºr die Berechnung eines Sigmoidneurons liefert. </li></ul><br>  Vor diesem Hintergrund ist es einfach, Code zu schreiben, der die Ausgabe eines Netzwerkobjekts berechnet.  Wir beginnen mit der Definition eines Sigmoid: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sigmoid</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0</span></span>/(<span class="hljs-number"><span class="hljs-number">1.0</span></span>+np.exp(-z))</code> </pre> <br>  Beachten Sie, dass Numpy, wenn der z-Parameter ein Numpy-Vektor oder -Array ist, das Sigmoid-Element automatisch anwendet, dh in Vektorform. <br><br>  F√ºgen Sie der Netzwerkklasse eine direkte Weitergabemethode hinzu, die a aus dem Netzwerk als Eingabe verwendet und die entsprechende Ausgabe zur√ºckgibt.  Es wird angenommen, dass der Parameter a (n, 1) Numpy ndarray ist, kein Vektor (n,).  Hier ist n die Anzahl der Eingangsneuronen.  Wenn Sie versuchen, den Vektor (n,) zu verwenden, erhalten Sie seltsame Ergebnisse. <br><br>  Das Verfahren wendet einfach Gleichung (22) auf jede Schicht an: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">feedforward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, a)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""       "a"""</span></span><span class="hljs-string"><span class="hljs-string">" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a</span></span></code> </pre> <br>  Nat√ºrlich brauchen wir sie von Netzwerkobjekten, um sie zu lernen.  Dazu geben wir ihnen die SGD-Methode, die einen stochastischen Gradientenabstieg implementiert.  Hier ist sein Code.  An einigen Stellen ist es ziemlich mysteri√∂s, aber im Folgenden werden wir es genauer analysieren. <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SGD</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, training_data, epochs, mini_batch_size, eta, test_data=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    -    . training_data ‚Äì   "(x, y)",       .       .  test_data ,          ,     .     ,    . """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> test_data: n_test = len(test_data) n = len(training_data) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">0</span></span>, n, mini_batch_size)] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> mini_batch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mini_batches: self.update_mini_batch(mini_batch, eta) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> test_data: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Epoch {0}: {1} / {2}"</span></span>.format( j, self.evaluate(test_data), n_test) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Epoch {0} complete"</span></span>.format(j)</code> </pre> <br>  training_data ist eine Liste von Tupeln "(x, y)", die die Trainingseingabe und die gew√ºnschte Ausgabe darstellen.  Die Variablen epochs und mini_batch_size geben die Anzahl der zu lernenden Epochen und die Gr√∂√üe der zu verwendenden Minipakete an.  eta - Lerngeschwindigkeit, Œ∑.  Wenn test_data festgelegt ist, wird das Netzwerk nach jeder √Ñra anhand der Verifizierungsdaten ausgewertet und der aktuelle Fortschritt wird angezeigt.  Dies ist n√ºtzlich, um den Fortschritt zu verfolgen, verlangsamt jedoch die Arbeit erheblich. <br><br>  Der Code funktioniert so.  In jeder Epoche mischt er zun√§chst versehentlich Trainingsdaten und zerlegt sie dann in Minipakete der richtigen Gr√∂√üe.  Dies ist eine einfache M√∂glichkeit, eine Stichprobe von Trainingsdaten zu erstellen.  Dann wenden wir f√ºr jeden mini_batch einen Schritt des Gradientenabfalls an.  Dies erfolgt √ºber den Code self.update_mini_batch (mini_batch, eta), der die Netzwerkgewichte und -vers√§tze gem√§√ü einer Iteration des Gradientenabfalls aktualisiert, wobei nur Trainingsdaten in mini_batch verwendet werden.  Hier ist der Code f√ºr die update_mini_batch-Methode: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update_mini_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, mini_batch, eta)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    ,          -. mini_batch ‚Äì    (x, y),  eta ‚Äì  ."""</span></span> nabla_b = [np.zeros(b.shape) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.biases] nabla_w = [np.zeros(w.shape) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.weights] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> nb, dnb <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> nw, dnw <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w, nw <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b, nb <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.biases, nabla_b)]</code> </pre> <br>  Der gr√∂√üte Teil der Arbeit wird von der Leitung erledigt. <br><br><pre> <code class="python hljs"> delta_nabla_b, delta_nabla_w = self.backprop(x, y)</code> </pre> <br>  Es ruft einen Backpropagation-Algorithmus auf - eine schnelle Methode, um den Gradienten einer Kostenfunktion zu berechnen.  Update_mini_batch berechnet diese Steigungen einfach f√ºr jedes Trainingsbeispiel aus mini_batch und aktualisiert dann self.weights und self.biases. <br><br>  Bisher werde ich keinen Code f√ºr self.backprop demonstrieren.  Wir werden im n√§chsten Kapitel mehr √ºber Backpropagation erfahren, und es wird self.backprop-Code geben.  Angenommen, es verh√§lt sich vorerst wie angegeben und gibt einen geeigneten Gradienten f√ºr die mit dem Trainingsbeispiel x verbundenen Kosten zur√ºck. <br><br>  Schauen wir uns das gesamte Programm an, einschlie√ülich erkl√§render Kommentare.  Mit Ausnahme der Funktion self.backprop spricht das Programm f√ºr sich selbst - die Hauptarbeit erledigen self.SGD und self.update_mini_batch.  Die self.backprop-Methode verwendet mehrere zus√§tzliche Funktionen, um den Gradienten zu berechnen, n√§mlich sigmoid_prime, der die Ableitung des Sigmoid berechnet, und self.cost_derivative, die ich hier nicht beschreiben werde.  Sie k√∂nnen sich ein Bild davon machen, indem Sie sich den Code und die Kommentare ansehen.  Im n√§chsten Kapitel werden wir sie genauer betrachten.  Denken Sie daran, dass das Programm zwar lang erscheint, der gr√∂√üte Teil des Codes jedoch aus Kommentaren besteht, die das Verst√§ndnis erleichtern.  Tats√§chlich besteht das Programm selbst nur aus 74 Nicht-Codezeilen - nicht leer und keine Kommentare.  Der gesamte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code ist auf GitHub verf√ºgbar</a> . <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" network.py ~~~~~~~~~~           .      .     ,    .   ,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #   import random #   import numpy as np class Network(object): def __init__(self, sizes): """  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].               0    1. ,      ,       ,        . """ self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] def feedforward(self, a): """   ,  ``a`` -  .""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """    -    . training_data ‚Äì   "(x, y)",       .       .  test_data ,          ,     .     ,    . """ if test_data: n_test = len(test_data) n = len(training_data) for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoch {0}: {1} / {2}".format( j, self.evaluate(test_data), n_test) else: print "Epoch {0} complete".format(j) def update_mini_batch(self, mini_batch, eta): """    ,          -. mini_batch ‚Äì    (x, y),  eta ‚Äì  .""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """  ``(nabla_b, nabla_w)``,      C_x. ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #   activation = x activations = [x] #      zs = [] #     z- for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) #   delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) """ l      ,      . l = 1    , l = 2 ‚Äì ,   .    ,   python      .""" for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): """    ,      .    ‚Äì          .""" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) def cost_derivative(self, output_activations, y): """    ( C_x /  a)   .""" return (output_activations-y) ####   def sigmoid(z): """.""" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): """ .""" return sigmoid(z)*(1-sigmoid(z))</span></span></code> </pre> <br>  Wie gut erkennt das Programm handschriftliche Zahlen?  Beginnen wir mit dem Laden der MNIST-Daten.  Wir werden dies mit dem kleinen Hilfsprogramm mnist_loader.py tun, das ich unten beschreiben werde.  F√ºhren Sie die folgenden Befehle in der Python-Shell aus: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper()</code> </pre> <br>  Dies kann nat√ºrlich in einem separaten Programm erfolgen, aber wenn Sie parallel zu einem Buch arbeiten, ist dies einfacher. <br><br>  Richten Sie nach dem Herunterladen der MNIST-Daten ein Netzwerk von 30 versteckten Neuronen ein.  Wir werden dies tun, nachdem wir das oben beschriebene Programm importiert haben, das als Netzwerk bezeichnet wird: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network &gt;&gt;&gt; net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])</code> </pre> <br>  Schlie√ülich verwenden wir den stochastischen Gradientenabstieg f√ºr das Training von Trainingsdaten f√ºr 30 Epochen mit einer Minipaketgr√∂√üe von 10 und einer Lerngeschwindigkeit von Œ∑ = 3,0: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">3.0</span></span>, test_data=test_data)</code> </pre> <br>  Wenn Sie Code parallel zum Lesen eines Buches ausf√ºhren, beachten Sie, dass die Ausf√ºhrung einige Minuten dauert.  Ich schlage vor, Sie starten alles, lesen weiter und √ºberpr√ºfen regelm√§√üig, was das Programm produziert.  Wenn Sie es eilig haben, k√∂nnen Sie die Anzahl der Epochen verringern, indem Sie die Anzahl der versteckten Neuronen verringern oder nur einen Teil der Trainingsdaten verwenden.  Der endg√ºltige Arbeitscode funktioniert schneller: Diese Python-Skripte sollen Ihnen helfen, die Funktionsweise des Netzwerks zu verstehen, und sind nicht leistungsstark!  Und nat√ºrlich kann das Netzwerk nach dem Training auf fast jeder Computerplattform sehr schnell arbeiten.  Wenn wir dem Netzwerk beispielsweise eine gute Auswahl an Gewichten und Offsets beibringen, kann es problemlos f√ºr die Arbeit mit JavaScript in einem Webbrowser oder als native Anwendung auf einem mobilen Ger√§t portiert werden.  In jedem Fall wird ungef√§hr die gleiche Schlussfolgerung aus dem Programm gezogen, das das neuronale Netzwerk trainiert.  Sie schreibt die Anzahl der korrekt erkannten Testbilder nach jeder Trainings√§ra.  Wie Sie sehen, erreicht es auch nach einer √Ñra eine Genauigkeit von 9.129 von 10.000, und diese Zahl w√§chst weiter: <br><br> <code>Epoch 0: 9129 / 10000 <br> Epoch 1: 9295 / 10000 <br> Epoch 2: 9348 / 10000 <br> ... <br> Epoch 27: 9528 / 10000 <br> Epoch 28: 9542 / 10000 <br> Epoch 29: 9534 / 10000</code> <br> <br>  Es stellt sich heraus, dass das trainierte Netzwerk einen Prozentsatz der korrekten Klassifizierung von maximal 95 - 95,42% ergibt!  Ein ziemlich vielversprechender erster Versuch.  Ich warne Sie, dass Ihr Code nicht unbedingt genau die gleichen Ergebnisse liefert, da wir das Netzwerk mit zuf√§lligen Gewichten und Offsets initialisieren.  F√ºr dieses Kapitel habe ich den besten von drei Versuchen ausgew√§hlt. <br><br>  Lassen Sie uns das Experiment neu starten, indem Sie die Anzahl der versteckten Neuronen auf 100 √§ndern. Wenn Sie den Code gleichzeitig mit dem Lesen ausf√ºhren, denken Sie daran, dass dies ziemlich viel Zeit in Anspruch nimmt (auf meinem Computer dauert jede √Ñra mehrere zehn Sekunden). Daher ist es besser, parallel zu lesen mit Codeausf√ºhrung. <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">3.0</span></span>, test_data=test_data)</code> </pre> <br>  Dies verbessert nat√ºrlich das Ergebnis auf 96,59%.  Zumindest in diesem Fall hilft die Verwendung von mehr versteckten Neuronen, bessere Ergebnisse zu erzielen. <br><br>  Die R√ºckmeldungen der Leser deuten darauf hin, dass die Ergebnisse dieses Experiments sehr unterschiedlich sind und einige Lernergebnisse viel schlechter sind.  Verwenden Sie Techniken aus Kapitel 3, um die Vielfalt der Arbeitseffizienz von einem Lauf zum anderen erheblich zu verringern. <br><br>  Um diese Genauigkeit zu erreichen, musste ich nat√ºrlich eine bestimmte Anzahl von Epochen f√ºr das Lernen, die Gr√∂√üe des Minipakets und die Lerngeschwindigkeit Œ∑ ausw√§hlen.  Wie oben erw√§hnt, werden sie als Hyperparameter unserer Nationalversammlung bezeichnet, um sie von einfachen Parametern (Gewichte und Offsets) zu unterscheiden, die der Algorithmus w√§hrend des Trainings anpasst.  Wenn wir Hyperparameter schlecht w√§hlen, erhalten wir schlechte Ergebnisse.  Nehmen wir zum Beispiel an, wir haben die Lernrate Œ∑ = 0,001 gew√§hlt: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, test_data=test_data)</code> </pre> <br>  Die Ergebnisse sind viel weniger beeindruckend: <br><br> <code>Epoch 0: 1139 / 10000 <br> Epoch 1: 1136 / 10000 <br> Epoch 2: 1135 / 10000 <br> ... <br> Epoch 27: 2101 / 10000 <br> Epoch 28: 2123 / 10000 <br> Epoch 29: 2142 / 10000</code> <br> <br>  Sie k√∂nnen jedoch feststellen, dass die Netzwerkeffizienz im Laufe der Zeit langsam zunimmt.  Dies deutet darauf hin, dass Sie versuchen k√∂nnen, die Lerngeschwindigkeit beispielsweise auf 0,01 zu erh√∂hen.  In diesem Fall sind die Ergebnisse besser, was darauf hinweist, dass die Geschwindigkeit weiter erh√∂ht werden muss (wenn die √Ñnderung die Situation verbessert, √§ndern Sie sich weiter!).  Wenn Sie dies mehrmals tun, werden wir schlie√ülich zu Œ∑ = 1,0 (und manchmal sogar 3,0) gelangen, was unseren fr√ºheren Experimenten nahe kommt.  Obwohl wir Hyperparameter anfangs schlecht ausgew√§hlt haben, haben wir zumindest gen√ºgend Informationen gesammelt, um unsere Parameterauswahl verbessern zu k√∂nnen. <br><br>  Im Allgemeinen ist das Debuggen von NA eine komplizierte Angelegenheit.  Dies ist insbesondere dann der Fall, wenn die Auswahl der anf√§nglichen Hyperparameter zu Ergebnissen f√ºhrt, die das zuf√§llige Rauschen nicht √ºberschreiten.  Angenommen, wir versuchen, eine erfolgreiche Architektur mit 30 Neuronen zu verwenden, √§ndern jedoch die Lerngeschwindigkeit auf 100,0: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">100.0</span></span>, test_data=test_data)</code> </pre> <br>  Am Ende stellt sich heraus, dass wir zu weit gegangen sind und zu viel Geschwindigkeit genommen haben: <br><br> <code>Epoch 0: 1009 / 10000 <br> Epoch 1: 1009 / 10000 <br> Epoch 2: 1009 / 10000 <br> Epoch 3: 1009 / 10000 <br> ... <br> Epoch 27: 982 / 10000 <br> Epoch 28: 982 / 10000 <br> Epoch 29: 982 / 10000</code> <br> <br>  Stellen Sie sich nun vor, wir n√§hern uns dieser Aufgabe zum ersten Mal.  Nat√ºrlich wissen wir aus fr√ºhen Experimenten, dass es richtig w√§re, die Lerngeschwindigkeit zu verringern.  Wenn wir uns dieser Aufgabe jedoch zum ersten Mal n√§hern w√ºrden, h√§tten wir keine Ergebnisse, die uns zur richtigen L√∂sung f√ºhren k√∂nnten.  K√∂nnten wir anfangen zu denken, dass wir vielleicht die falschen Anfangsparameter f√ºr Gewichte und Offsets gew√§hlt haben und es f√ºr das Netzwerk schwierig ist zu lernen?  Oder haben wir nicht gen√ºgend Trainingsdaten, um ein aussagekr√§ftiges Ergebnis zu erzielen?  Vielleicht haben wir nicht genug Epochen gewartet?  Vielleicht kann ein neuronales Netzwerk mit einer solchen Architektur einfach nicht lernen, handgeschriebene Zahlen zu erkennen?  Vielleicht ist die Lerngeschwindigkeit zu langsam?  Wenn Sie sich der Aufgabe zum ersten Mal n√§hern, haben Sie nie Vertrauen. <br><br>  Daraus lohnt es sich, eine Lektion zu lernen, dass das Debuggen von NS keine triviale Aufgabe ist, und dies ist wie das regul√§re Programmieren Teil der Kunst.  Sie m√ºssen diese Debugging-Technik lernen, um gute Ergebnisse von NS zu erhalten.  Im Allgemeinen m√ºssen wir eine Heuristik entwickeln, um gute Hyperparameter und eine gute Architektur auszuw√§hlen.  Wir werden dies im Buch ausf√ºhrlich diskutieren, einschlie√ülich der Auswahl der obigen Hyperparameter. <br><br><h3>  √úbung </h3><br><ul><li>  Versuchen Sie, ein Netzwerk aus nur zwei Schichten - Eingabe und Ausgabe ohne Verstecktheit - mit 784 bzw. 10 Neuronen zu erstellen.  Trainieren Sie das Netzwerk mit stochastischem Gef√§lle.  Welche Klassifizierungsgenauigkeit erhalten Sie? </li></ul><br>  Ich habe zuvor die Details zum Laden von MNIST-Daten √ºbersprungen.  Es passiert ganz einfach.  Hier ist der Code, um das Bild zu vervollst√§ndigen.  Datenstrukturen werden in den Kommentaren beschrieben - alles ist einfach, Tupel und Arrays von Numpy ndarray-Objekten (wenn Sie mit solchen Objekten nicht vertraut sind, stellen Sie sie sich als Vektoren vor). <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" mnist_loader ~~~~~~~~~~~~      MNIST.       ``load_data``  ``load_data_wrapper``.  , ``load_data_wrapper`` -  ,     . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import cPickle import gzip #  import numpy as np def load_data(): """  MNIST   ,  ,    . ``training_data``      .    .  numpy ndarray  50 000 .   ‚Äì     numpy ndarray  784 ,  28 * 28 = 784    MNIST.  ‚Äì  numpy ndarray  50 000 .   ‚Äì   0  9   ,    . ``validation_data``  ``test_data`` ,    10 000 .    ,           ``training_data``.    - ``load_data_wrapper()``. """ f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data) def load_data_wrapper(): """ ,  ``(training_data, validation_data, test_data)``.   ``load_data``,         .  , ``training_data`` -    50 000    , ``(x, y)``. ``x`` -  784- numpy.ndarray,   . ``y`` -  10- numpy.ndarray,   ,     ``x``. ``validation_data``  ``test_data`` -  ,   10 000    , ``(x, y)``. ``x`` -  784- numpy.ndarray,   ,  ``y`` -   ,  ,   ( ),  ``x``. ,  ,           .         .""" tr_d, va_d, te_d = load_data() training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data) def vectorized_result(j): """ 10-    1.0   j     .      (0..9)     .""" e = np.zeros((10, 1)) e[j] = 1.0 return e</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ich sagte, dass unser Programm ziemlich gute Ergebnisse erzielt. </font></font> Was bedeutet das?<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gut im Vergleich zu was? Es ist n√ºtzlich, die Ergebnisse einiger einfacher, grundlegender Tests zu haben, mit denen Sie einen Vergleich anstellen k√∂nnen, um zu verstehen, was ‚Äûgute Ergebnisse‚Äú bedeuten. Das einfachste Basislevel w√§re nat√ºrlich eine zuf√§llige Vermutung. Dies kann in etwa 10% der F√§lle erfolgen. Und wir zeigen ein viel besseres Ergebnis! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was ist mit einem weniger trivialen Basislevel? Schauen wir uns an, wie dunkel das Bild ist. Beispielsweise ist Bild 2 normalerweise dunkler als Bild 1, einfach weil es mehr dunkle Pixel hat, wie in den folgenden Beispielen gezeigt:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f59/0de/5b7/f590de5b7bc6581b9854b2013e5013de.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daraus folgt, dass wir die durchschnittliche Dunkelheit f√ºr jede Ziffer von 0 bis 9 berechnen k√∂nnen. Wenn wir ein neues Bild erhalten, berechnen wir dessen Dunkelheit und wir vermuten, dass es eine Zahl mit der n√§chsten durchschnittlichen Dunkelheit zeigt. Dies ist eine einfache Prozedur, die einfach zu programmieren ist, daher werde ich keinen Code schreiben - bei Interesse </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">liegt sie auf GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Dies ist jedoch eine ernsthafte Verbesserung im Vergleich zu zuf√§lligen Vermutungen - der Code erkennt 2.225 von 10.000 Bildern korrekt, dh er ergibt eine Genauigkeit von 22,25%.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist nicht schwer, andere Ideen zu finden, die eine Genauigkeit im Bereich von 20 bis 50% erreichen. Nachdem Sie etwas mehr gearbeitet haben, k√∂nnen Sie 50% √ºberschreiten. Um jedoch eine viel gr√∂√üere Genauigkeit zu erzielen, ist es besser, ma√ügebliche MO-Algorithmen zu verwenden. Probieren wir einen der bekanntesten Algorithmen aus, die Support Vector Method oder SVM. Wenn Sie mit SVM nicht vertraut sind, machen Sie sich keine Sorgen, wir m√ºssen diese Details nicht verstehen. Wir verwenden nur eine Python-Bibliothek namens scikit-learn, die eine einfache Schnittstelle zur schnellen C-Bibliothek f√ºr SVM bietet, die als </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LIBSVM bekannt ist</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn wir den SVM-Klassifikator scikit-learn mit den Standardeinstellungen ausf√ºhren, erhalten wir die korrekte Klassifizierung von 9.435 von 10.000 (der Code </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ist unter dem Link verf√ºgbar</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) Dies ist bereits eine gro√üe Verbesserung gegen√ºber dem naiven Ansatz, Bilder nach Dunkelheit zu klassifizieren. Dies bedeutet, dass SVM ungef√§hr so ‚Äã‚Äãgut funktioniert wie unser NS, nur ein bisschen schlechter. In den folgenden Kapiteln werden wir uns mit neuen Techniken vertraut machen, mit denen wir unsere NS so verbessern k√∂nnen, dass sie die SVM deutlich √ºbertreffen.</font></font><br><br>  Das ist aber noch nicht alles.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ergebnis 9 435 von 10 000 aus scikit-learn ist f√ºr die Standardeinstellungen angegeben. SVM verf√ºgt √ºber viele Parameter, die angepasst werden k√∂nnen, und Sie k√∂nnen nach Parametern suchen, die dieses Ergebnis verbessern. Ich werde nicht auf Details eingehen, sie k√∂nnen in dem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Artikel von</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andreas M√ºller </font><font style="vertical-align: inherit;">gelesen werden </font><font style="vertical-align: inherit;">. Er zeigte, dass durch Arbeiten zur Optimierung der Parameter eine Genauigkeit von mindestens 98,5% erreicht werden kann. Mit anderen Worten, eine gut abgestimmte SVM macht nur eine Ziffer aus 70 Fehlern. Ein gutes Ergebnis! Kann NA mehr erreichen? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es stellt sich heraus, dass sie es k√∂nnen. Heute √ºberholt ein gut abgestimmter NS jede andere bekannte Technologie in der MNIST-L√∂sung, einschlie√ülich SVM. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rekord f√ºr 2013</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">9.979 von 10.000 Bildern korrekt klassifiziert. Und wir werden die meisten daf√ºr verwendeten Technologien in diesem Buch sehen. Diese Genauigkeit liegt nahe am Menschen und √ºbertrifft sie vielleicht sogar, da einige Bilder von MNIST selbst f√ºr Menschen schwer zu entziffern sind, zum Beispiel: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/def/4b3/b37/def4b3b37d8d510615d60a372a06ad47.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich denke, Sie werden zustimmen, dass es schwierig ist, sie zu klassifizieren! Bei solchen Bildern im MNIST-Datensatz ist es √ºberraschend, dass der NS alle 10.000 Bilder mit Ausnahme von 21 korrekt erkennen kann. Normalerweise glauben Programmierer, dass das L√∂sen einer solch komplexen Aufgabe einen komplexen Algorithmus erfordert. Aber auch die NS ist am </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Werk</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Rekordhalter verwendet ziemlich einfache Algorithmen, bei denen es sich um kleine Abweichungen von den in diesem Kapitel untersuchten handelt. </font><font style="vertical-align: inherit;">Die gesamte Komplexit√§t wird w√§hrend des Trainings automatisch basierend auf den Trainingsdaten angezeigt. </font><font style="vertical-align: inherit;">In gewissem Sinne ist die Moral unserer Ergebnisse und derjenigen, die in komplexeren Werken enthalten sind, die f√ºr einige Aufgaben</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> komplexer Algorithmus ‚â§ einfacher Trainingsalgorithmus + gute Trainingsdaten </font></font></blockquote><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Zu tiefem Lernen </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obwohl unser Netzwerk eine beeindruckende Leistung zeigt, wird dies auf mysteri√∂se Weise erreicht. </font><font style="vertical-align: inherit;">Gewichte und Mischnetzwerke werden automatisch erkannt. </font><font style="vertical-align: inherit;">Wir haben also keine vorgefertigte Erkl√§rung daf√ºr, wie das Netzwerk das tut, was es tut. </font><font style="vertical-align: inherit;">Gibt es eine M√∂glichkeit, die Grundprinzipien der Klassifizierung durch ein Netzwerk handgeschriebener Zahlen zu verstehen? </font><font style="vertical-align: inherit;">Und ist es angesichts dieser M√∂glichkeiten m√∂glich, das Ergebnis zu verbessern?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir formulieren diese Fragen strenger um: Nehmen wir an, dass sich die NS in einigen Jahrzehnten in k√ºnstliche Intelligenz (KI) verwandeln wird. Werden wir verstehen, wie diese KI funktioniert? Vielleicht bleiben die Netzwerke mit ihren Gewichten und Offsets f√ºr uns unverst√§ndlich, da sie automatisch zugewiesen werden. In den ersten Jahren der KI-Forschung hofften die Menschen, dass der Versuch, KI zu schaffen, uns auch helfen w√ºrde, die Prinzipien zu verstehen, die der Intelligenz zugrunde liegen, und vielleicht sogar die Arbeit des menschlichen Gehirns. Am Ende kann sich jedoch herausstellen, dass wir weder das Gehirn noch die Funktionsweise der KI verstehen werden! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um diese Probleme zu l√∂sen, erinnern wir uns an die Interpretation k√ºnstlicher Neuronen, die ich zu Beginn des Kapitels gegeben habe - dass dies eine M√∂glichkeit ist, Beweise abzuw√§gen. Angenommen, wir m√∂chten feststellen, ob sich das Gesicht einer Person auf einem Bild befindet:</font></font><br><br><img src="https://habrastorage.org/webt/up/2e/af/up2eafnwrrpph5xdai4oapmgw8m.jpeg"><br><br><img src="https://habrastorage.org/webt/tu/8i/an/tu8ianduufnfebbjnqaibtzskyy.jpeg"><br><br><img src="https://habrastorage.org/webt/wf/tj/4j/wftj4j86hzytgwyypbw_eqk0jte.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieses Problem kann auf die gleiche Weise wie die Handschrifterkennung angegangen werden: Die Verwendung von Bildpixeln als Eingabe f√ºr den NS und die Ausgabe des NS ist ein Neuron, das sagt: "Ja, das ist ein Gesicht" oder "Nein, das ist kein Gesicht" ". </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Angenommen, wir tun dies, ohne jedoch einen Lernalgorithmus zu verwenden. Wir werden versuchen, manuell ein Netzwerk zu erstellen und die entsprechenden Gewichte und Offsets auszuw√§hlen. Wie k√∂nnen wir das angehen? Wenn wir die Nationalversammlung vergessen, k√∂nnen wir die Aufgabe f√ºr einen Moment in Unteraufgaben unterteilen: Hat das Bild der Augen in der oberen linken Ecke? Gibt es ein Auge in der oberen rechten Ecke? Gibt es eine mittlere Nase? Gibt es einen Mund in der Mitte? Gibt es oben Haare?</font></font> Usw. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn die Antworten auf mehrere dieser Fragen positiv sind oder sogar ‚Äûwahrscheinlich ja‚Äú, schlie√üen wir, dass das Bild m√∂glicherweise ein Gesicht hat. Umgekehrt, wenn die Antworten nein sind, gibt es wahrscheinlich keine Person.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist nat√ºrlich eine ungef√§hre Heuristik und weist viele M√§ngel auf. Vielleicht ist das ein kahlk√∂pfiger Mann, und er hat keine Haare. Vielleicht k√∂nnen wir nur einen Teil des Gesichts oder das Gesicht in einem Winkel sehen, so dass einige Teile des Gesichts geschlossen sind. Die Heuristik legt jedoch nahe, dass wir, wenn wir Teilprobleme mit Hilfe neuronaler Netze l√∂sen k√∂nnen, m√∂glicherweise NSs f√ºr die Gesichtserkennung erstellen k√∂nnen, indem wir Netze f√ºr Unteraufgaben kombinieren. Das Folgende ist eine m√∂gliche Architektur eines solchen Netzwerks, in dem Subnetze durch Rechtecke angezeigt werden. Dies ist kein v√∂llig realistischer Ansatz zur L√∂sung des Gesichtserkennungsproblems: Er wird ben√∂tigt, um die Arbeit neuronaler Netze intuitiv zu verstehen. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/61b/dd0/ef6/61bdd0ef651ebc30afff87ed56204bd0.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In den Rechtecken gibt es Unteraufgaben: Hat das Bild der Augen in der oberen linken Ecke? Gibt es ein Auge in der oberen rechten Ecke? Gibt es eine mittlere Nase? Gibt es einen Mund in der Mitte? Gibt es oben Haare? Usw.</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist m√∂glich, dass Subnetze auch in Komponenten zerlegt werden k√∂nnen. Nehmen Sie die Frage nach einem Auge in der oberen linken Ecke. Es kann unterschieden werden in Fragen wie: "Gibt es eine Augenbraue?", "Gibt es Wimpern?", "Gibt es eine Pupille?" usw. Nat√ºrlich sollten Fragen Informationen √ºber den Ort enthalten - "Befindet sich die Augenbraue oben links √ºber der Pupille?" Usw. - aber vereinfachen wir dies zun√§chst. Daher kann das Netzwerk, das die Frage nach der Anwesenheit des Auges beantwortet, in die Komponenten zerlegt werden: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d87/871/6ff/d878716ff0ecad83cfa5b0e9c54a866e.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Gibt es eine Augenbraue?", "Gibt es Wimpern?", "Gibt es eine Pupille?"</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diese Fragen k√∂nnen in kleinen Schritten in kleine Schichten unterteilt werden. Daher werden wir mit Subnetzen arbeiten, die so einfache Fragen beantworten, dass sie auf Pixelebene leicht zerlegt werden k√∂nnen. Diese Fragen k√∂nnen beispielsweise das Vorhandensein oder Fehlen einfacher Formen an bestimmten Stellen des Bildes betreffen. Einzelne Neuronen, die mit Pixeln assoziiert sind, k√∂nnen auf sie reagieren.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das Ergebnis ist ein Netzwerk, das sehr komplexe Fragen - ob sich eine Person im Bild befindet - in sehr einfache Fragen zerlegt, die auf der Ebene der einzelnen Pixel beantwortet werden k√∂nnen. Sie wird dies durch eine Abfolge von vielen Ebenen tun, in denen die ersten Ebenen sehr einfache und spezifische Fragen zum Bild beantworten und die letzteren eine Hierarchie komplexerer und abstrakterer Konzepte erstellen. Netzwerke mit einer solchen Mehrschichtstruktur - zwei oder mehr verborgene Schichten - werden als Deep Neural Networks (GNS) bezeichnet.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nat√ºrlich habe ich nicht dar√ºber gesprochen, wie man dieses rekursive Subnetz macht. Es wird definitiv unpraktisch sein, Gewichte und Offsets manuell auszuw√§hlen. Wir m√∂chten Trainingsalgorithmen verwenden, damit das Netzwerk automatisch Gewichte und Offsets - und durch sie die Hierarchien von Konzepten - basierend auf Trainingsdaten lernt. In den 1980er und 1990er Jahren versuchten Forscher, stochastischen Gradientenabstieg und Backpropagation zu verwenden, um GNS zu trainieren. Leider gelang es ihnen mit Ausnahme einiger spezieller Architekturen nicht. Die Netzwerke trainierten, aber sehr langsam, und in der Praxis war es zu langsam, um irgendwie verwendet zu werden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seit 2006 wurden verschiedene Technologien entwickelt, um STS zu trainieren. Sie basieren auf stochastischem Gradientenabstieg und R√ºckausbreitung, enthalten aber auch neue Ideen. Sie durften viel tiefere Netzwerke trainieren - heute trainieren die Leute leise Netzwerke mit 5-10 Schichten. Und es stellt sich heraus, dass sie viele Probleme viel besser l√∂sen als flache NS, dh Netzwerke mit einer verborgenen Schicht. Der Grund ist nat√ºrlich, dass STS eine komplexe Hierarchie von Konzepten erstellen kann. Dies √§hnelt der Verwendung modularer Schemata und Abstraktionsideen durch Programmiersprachen, um komplexe Computerprogramme zu erstellen. Um einen tiefen NS mit einem flachen NS zu vergleichen, muss man ungef√§hr eine Programmiersprache vergleichen, die Funktionsaufrufe mit einer Sprache ausf√ºhren kann, die dies nicht tut. Abstraktion in der NS sieht nicht so aus wie in Programmiersprachen,hat aber die gleiche Bedeutung.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456738/">https://habr.com/ru/post/de456738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456714/index.html">Gleitkommazahlen verstehen (Teil 0)</a></li>
<li><a href="../de456716/index.html">Nicht sehr gro√üe Datenmengen</a></li>
<li><a href="../de456722/index.html">PostgreSQL-Rezepte: Asynchroner Taskplaner</a></li>
<li><a href="../de456730/index.html">Buch "{Sie kennen JS nicht} Typen und Grammatikkonstruktionen"</a></li>
<li><a href="../de456736/index.html">PostgreSQL-Rezepte: cURL: Abrufen, Ver√∂ffentlichen und ... E-Mail</a></li>
<li><a href="../de456744/index.html">10 Probleme, die ich mit Erinnerungen auf meinem Smartphone gel√∂st habe</a></li>
<li><a href="../de456746/index.html">Big Data - gro√üe Verantwortung, gro√üer Stress und viel Geld</a></li>
<li><a href="../de456748/index.html">2003 Thermodrucker von einem Flohmarkt: Was kann er 2019 tun?</a></li>
<li><a href="../de456754/index.html">GitOps: Vergleichen von Pull- und Push-Methoden</a></li>
<li><a href="../de456756/index.html">Warum √§ndert CockroachDB die Open Source-Lizenz?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>