<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üï£ üé≤ üë©üèø‚Äçüé® Richard Hamming: Cap√≠tulo 13. Teoria da Informa√ß√£o üåÉ üíπ üõÄüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="N√≥s conseguimos! 

 "O objetivo deste curso √© prepar√°-lo para o seu futuro t√©cnico." 
 Oi Habr. Lembre-se do incr√≠vel artigo "Voc√™ e seu trabalho" (+2...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Richard Hamming: Cap√≠tulo 13. Teoria da Informa√ß√£o</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422205/">  N√≥s conseguimos! <br><br><blockquote>  "O objetivo deste curso √© prepar√°-lo para o seu futuro t√©cnico." </blockquote><br><img src="https://habrastorage.org/getpro/habr/post_images/d67/6ff/9ea/d676ff9eadd2a38b0948de76bbf27fd4.jpg" alt="imagem" align="right">  Oi Habr.  Lembre-se do incr√≠vel artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"Voc√™ e seu trabalho"</a> (+219, 2588 marcado, 429k leituras)? <br><br>  Portanto, Hamming (sim, sim, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">c√≥digos de Hamming com</a> auto-verifica√ß√£o e auto-corre√ß√£o) tem um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">livro</a> inteiro escrito com base em suas palestras.  Estamos traduzindo, porque o homem est√° falando de neg√≥cios. <br><br>  Este livro n√£o √© apenas sobre TI, √© um livro sobre o estilo de pensamento de pessoas incrivelmente legais.  <i>‚ÄúIsso n√£o √© apenas uma carga de pensamento positivo;</i>  <i>descreve condi√ß√µes que aumentam as chances de fazer um √≥timo trabalho. ‚Äù</i> <br><br>  <i>Obrigado pela tradu√ß√£o para Andrei Pakhomov.</i> <br><br>  A Teoria da Informa√ß√£o foi desenvolvida por C.E. Shannon no final da d√©cada de 1940.  A ger√™ncia do Bell Labs insistiu que ele a chamasse de "Teoria da Comunica√ß√£o", porque  esse √© um nome muito mais preciso.  Por raz√µes √≥bvias, o nome "Teoria da Informa√ß√£o" tem um impacto significativamente maior no p√∫blico, ent√£o Shannon o escolheu, e √© o que sabemos at√© hoje.  O pr√≥prio nome sugere que a teoria lida com informa√ß√µes, o que a torna importante, pois estamos penetrando mais profundamente na era da informa√ß√£o.  Neste cap√≠tulo, abordarei algumas conclus√µes b√°sicas dessa teoria, darei evid√™ncias n√£o estritas, mas intuitivas, de algumas das disposi√ß√µes separadas dessa teoria, para que voc√™ entenda o que √© a "Teoria da Informa√ß√£o", onde voc√™ pode aplic√°-la e onde n√£o . <br><a name="habracut"></a><br>  Primeiro de tudo, o que √© "informa√ß√£o"?  Shannon identifica informa√ß√µes com incerteza.  Ele escolheu o logaritmo negativo da probabilidade de um evento como uma medida quantitativa das informa√ß√µes que voc√™ recebe quando um evento ocorre com probabilidade p.  Por exemplo, se eu lhe disser que o tempo em Los Angeles est√° nublado, ent√£o p est√° pr√≥ximo de 1, o que, em geral, n√£o nos fornece muita informa√ß√£o.  Mas se eu disser que chove em Monterey em junho, haver√° incerteza nesta mensagem e ela conter√° mais informa√ß√µes.  Um evento confi√°vel n√£o cont√©m nenhuma informa√ß√£o, pois o log 1 = 0. <br><br>  Vamos nos debru√ßar sobre isso com mais detalhes.  Shannon acreditava que uma medida quantitativa de informa√ß√£o deve ser uma fun√ß√£o cont√≠nua da probabilidade de um evento p e, para eventos independentes, deve ser aditiva - a quantidade de informa√ß√£o obtida como resultado de dois eventos independentes deve ser igual √† quantidade de informa√ß√£o obtida como resultado de um evento conjunto.  Por exemplo, o resultado de um lan√ßamento de dados e moedas √© geralmente considerado como eventos independentes.  Vamos traduzir o que foi dito acima para a linguagem da matem√°tica.  Se I (p) √© a quantidade de informa√ß√£o contida no evento com probabilidade p, ent√£o, para um evento conjunto que consiste em dois eventos independentes x com probabilidade p <sub>1</sub> e y com probabilidade p <sub>2</sub> obtemos <br><br><img src="https://habrastorage.org/webt/yi/hc/2d/yihc2dbw1d5_rlv2rptbyqs1mww.jpeg" alt="imagem"><br>  <i>(eventos independentes x e y)</i> <br><br>  Essa √© a equa√ß√£o funcional de Cauchy, verdadeira para todos os p <sub>1</sub> e p2.  Para resolver essa equa√ß√£o funcional, suponha que <br><br>  p <sub>1</sub> = p <sub>2</sub> = p, <br><br>  d√° <br><br><img src="https://habrastorage.org/webt/dk/rj/gf/dkrjgfn-xucnz-twcyhq3l6xefw.jpeg" alt="imagem"><br><br>  Se p <sub>1</sub> = p <sup>2</sup> ep <sub>2</sub> = p, ent√£o <br><br><img src="https://habrastorage.org/webt/ux/-s/cj/ux-scjh2wymlpjj-hpnsnzrgsly.jpeg" alt="imagem"><br><br>  etc.  Estendendo esse processo usando o m√©todo padr√£o para exponenciais para todos os n√∫meros racionais m / n, o seguinte √© verdadeiro <br><br><img src="https://habrastorage.org/webt/gt/_t/ro/gt_trop25xscgj5m3c1-k0nxiwe.jpeg" alt="imagem"><br><br>  A partir da continuidade assumida da medida de informa√ß√£o, conclui-se que a fun√ß√£o logar√≠tmica √© a √∫nica solu√ß√£o cont√≠nua para a equa√ß√£o funcional de Cauchy. <br><br>  Na teoria da informa√ß√£o, √© habitual tomar a base do logaritmo de 2, para que a escolha bin√°ria contenha exatamente 1 bit de informa√ß√£o.  Portanto, as informa√ß√µes s√£o medidas pela f√≥rmula <br><br><img src="https://habrastorage.org/webt/wx/ix/ow/wxixowgi6tookkpcslvwgmu6bpg.jpeg" alt="imagem"><br><br>  Vamos fazer uma pausa e ver o que aconteceu acima.  Antes de tudo, n√£o definimos o conceito de ‚Äúinforma√ß√£o‚Äù, apenas definimos uma f√≥rmula para sua medida quantitativa. <br><br>  Em segundo lugar, essa medida depende da incerteza e, embora seja suficientemente adequada para m√°quinas - por exemplo, sistemas telef√¥nicos, r√°dio, televis√£o, computadores etc. - n√£o reflete uma atitude humana normal em rela√ß√£o √† informa√ß√£o. <br><br>  Terceiro, essa √© uma medida relativa, depende do estado atual do seu conhecimento.  Se voc√™ observar o fluxo de "n√∫meros aleat√≥rios" do gerador de n√∫meros aleat√≥rios, presume que cada pr√≥ximo n√∫mero √© indefinido, mas se souber a f√≥rmula para calcular "n√∫meros aleat√≥rios", o pr√≥ximo n√∫mero ser√° conhecido e, portanto, n√£o conter√° informa√ß√£o. <br><br>  Assim, a defini√ß√£o dada por Shannon para informa√ß√µes √©, em muitos casos, adequada para m√°quinas, mas n√£o parece corresponder √† compreens√£o humana da palavra.  Por esse motivo, a "Teoria da Informa√ß√£o" deve ser chamada de "Teoria da Comunica√ß√£o".  No entanto, √© tarde demais para alterar as defini√ß√µes (gra√ßas √†s quais a teoria ganhou sua popularidade inicial e que ainda faz as pessoas pensarem que essa teoria lida com "informa√ß√µes"), ent√£o temos que atend√™-las, mas voc√™ precisa entender claramente at√© que ponto a defini√ß√£o de informa√ß√£o dada por Shannon est√° longe de seu senso comum.  As informa√ß√µes de Shannon lidam com algo completamente diferente, a saber, incerteza. <br><br>  √â nisso que voc√™ precisa pensar quando oferece alguma terminologia.  Qu√£o consistente √© a defini√ß√£o proposta, por exemplo, a defini√ß√£o dada por Shannon, com a sua ideia original e qual a diferen√ßa?  Quase n√£o existe um termo que reflita com precis√£o sua vis√£o anterior do conceito, mas, no final, √© a terminologia usada que reflete o significado do conceito; portanto, formalizar algo atrav√©s de defini√ß√µes claras sempre faz algum barulho. <br><br>  Considere um sistema cujo alfabeto consiste em s√≠mbolos q com probabilidades pi.  Nesse caso, a <i>quantidade m√©dia de informa√ß√µes</i> no sistema (seu valor esperado) √©: <br><br><img src="https://habrastorage.org/webt/wj/ss/83/wjss83z5fnynnyurbcftgokak6e.jpeg" alt="imagem"><br><br>  Isso √© chamado de entropia do sistema de distribui√ß√£o de probabilidade {pi}.  Usamos o termo ‚Äúentropia‚Äù porque a mesma forma matem√°tica surge na termodin√¢mica e na mec√¢nica estat√≠stica.  √â por isso que o termo "entropia" cria em torno de si uma aura de import√¢ncia, que, em √∫ltima an√°lise, n√£o se justifica.  A mesma forma matem√°tica de nota√ß√£o n√£o implica a mesma interpreta√ß√£o de caracteres! <br><br>  A entropia da distribui√ß√£o de probabilidade desempenha um papel importante na teoria da codifica√ß√£o.  A desigualdade de Gibbs para duas distribui√ß√µes de probabilidade diferentes pi e qi √© uma das consequ√™ncias importantes dessa teoria.  Ent√£o temos que provar que <br><br><img src="https://habrastorage.org/webt/jx/1v/c-/jx1vc-ac5t4kzyxhpplqfb20bmu.jpeg" alt="imagem"><br><br>  A prova √© baseada em um gr√°fico √≥bvio, fig.  13.I, que mostra que <br><br><img src="https://habrastorage.org/webt/gn/_o/i1/gn_oi1vqrmafw6k5v0g043jpz30.jpeg" alt="imagem"><br><br>  e a igualdade √© alcan√ßada apenas para x = 1. Aplicamos a desigualdade a cada soma da soma do lado esquerdo: <br><br><img src="https://habrastorage.org/webt/rg/wl/5o/rgwl5owcdte29pj1ycvixfgdogo.jpeg" alt="imagem"><br><br>  Se o alfabeto do sistema de comunica√ß√£o consiste em q caracteres, pegando a probabilidade de transmiss√£o de cada caractere qi = 1 / q e substituindo q, obtemos da desigualdade de Gibbs <br><br><img src="https://habrastorage.org/webt/qo/dz/gv/qodzgviyhmwnovomxokegxao54e.jpeg" alt="imagem"><br><br><img src="https://habrastorage.org/webt/8n/2m/y9/8n2my9t_5vd7vktnvdezxi0p1q4.jpeg" alt="imagem"><br><br>  <i>Figura 13.I</i> <br><br>  Isso sugere que, se a probabilidade de transmitir todos os caracteres q for igual e igual a -1 / q, a entropia m√°xima ser√° ln q; caso contr√°rio, a desigualdade ser√° mantida. <br><br>  No caso de um c√≥digo decodificado exclusivamente, temos a desigualdade Kraft <br><br><img src="https://habrastorage.org/webt/0-/8k/5l/0-8k5lsd16wlmzgwmmzvxgvlbr4.jpeg" alt="imagem"><br><br>  Agora, se definirmos pseudo-probabilidades <br><br><img src="https://habrastorage.org/webt/s9/qc/fv/s9qcfv1ywfj7_zkfvwm4jna8cqw.jpeg" alt="imagem"><br><br>  onde √© claro <img src="https://habrastorage.org/webt/is/_u/vw/is_uvwjifihdmybm68w_ayyfk-q.jpeg" alt="imagem">  = 1, que decorre da desigualdade de Gibbs, <br><br><img src="https://habrastorage.org/webt/my/hy/s0/myhys0cvxd6kgzu7f8r10vtnlym.jpeg" alt="imagem"><br><br>  e aplicar alguma √°lgebra (lembre-se de que K ‚â§ 1, para que possamos omitir o termo logar√≠tmico e, possivelmente, fortalecer a desigualdade posteriormente), obtemos <br><br><img src="https://habrastorage.org/webt/v0/i4/yo/v0i4yoxhwj8grndqupxwbo4zppw.jpeg" alt="imagem"><br><br>  onde L √© o comprimento m√©dio do c√≥digo. <br><br>  Assim, entropia √© o limite m√≠nimo para qualquer c√≥digo de caractere com uma palavra de c√≥digo m√©dia L. Esse √© o teorema de Shannon para um canal sem interfer√™ncia. <br><br>  Agora, consideramos o principal teorema das limita√ß√µes dos sistemas de comunica√ß√£o nos quais a informa√ß√£o √© transmitida na forma de um fluxo de bits e ru√≠dos independentes.  Sup√µe-se que a probabilidade da transmiss√£o correta de um bit seja P&gt; 1/2 e a probabilidade de o valor do bit ser invertido durante a transmiss√£o (ocorre um erro) √© Q = 1 - P. Por conveni√™ncia, assumimos que os erros s√£o independentes e a probabilidade de erro √© a mesma para cada envio bits - ou seja, h√° "ru√≠do branco" no canal de comunica√ß√£o. <br><br>  A maneira como temos um longo fluxo de n bits codificados em uma √∫nica mensagem √© uma extens√£o n-dimensional de um c√≥digo de um bit.  Determinaremos o valor de n posteriormente.  Considere uma mensagem que consiste em n bits como um ponto no espa√ßo n dimensional.  Como temos um espa√ßo n-dimensional - e por simplicidade, assumimos que cada mensagem tem a mesma probabilidade de ocorr√™ncia - existem M mensagens poss√≠veis (M tamb√©m ser√° determinado posteriormente), portanto, a probabilidade de qualquer mensagem enviada √© igual a <br><br><img src="https://habrastorage.org/webt/vs/gj/e5/vsgje5adnr3222jgs5qqlii1hga.jpeg" alt="imagem"><br><br><img src="https://habrastorage.org/webt/s8/mu/br/s8mubr4blaju8evwtbsgqxjvqye.jpeg" alt="imagem"><br>  (remetente) <br>  <i>Quadro 13.II</i> <br><br>  Em seguida, considere a id√©ia de largura de banda do canal.  Sem entrar em detalhes, a capacidade do canal √© definida como a quantidade m√°xima de informa√ß√µes que podem ser transmitidas com seguran√ßa pelo canal de comunica√ß√£o, levando em considera√ß√£o o uso da codifica√ß√£o mais eficiente.  N√£o h√° argumento de que mais informa√ß√µes possam ser transmitidas atrav√©s do canal de comunica√ß√£o do que sua capacidade.  Isso pode ser provado para um canal sim√©trico bin√°rio (que usamos no nosso caso).  A capacidade do canal para envio bit a bit √© definida como <br><br><img src="https://habrastorage.org/webt/pi/ek/76/piek76b-zyqfhs7k2qxzhleky9y.jpeg" alt="imagem"><br><br>  onde, como antes, P √© a probabilidade de n√£o haver erro em nenhum bit enviado.  Ao enviar n bits independentes, a capacidade do canal √© determinada como <br><br><img src="https://habrastorage.org/webt/nv/7x/67/nv7x67ybwwcisk8pxntmligj3ky.jpeg" alt="imagem"><br><br>  Se estivermos pr√≥ximos da largura de banda do canal, devemos enviar quase um volume de informa√ß√µes para cada um dos caracteres ai, i = 1, ..., M. Dado que a probabilidade de ocorr√™ncia de cada caractere ai √© 1 / M, obtemos <br><br><img src="https://habrastorage.org/webt/ne/mf/om/nemfom_mjugxea6infrm7asm4la.jpeg" alt="imagem"><br><br>  quando enviamos qualquer uma das M mensagens equiprob√°veis ‚Äã‚Äãai, temos <br><br><img src="https://habrastorage.org/webt/mt/c-/si/mtc-siba4teyqt0flsyzqs3-eja.jpeg" alt="imagem"><br><br>  Ao enviar n bits, esperamos que ocorram erros nQ.  Na pr√°tica, para uma mensagem que consiste em n bits, teremos aproximadamente nQ erros na mensagem recebida.  Para n grande, varia√ß√£o relativa (varia√ß√£o = largura da distribui√ß√£o) <br>  a distribui√ß√£o do n√∫mero de erros ser√° mais estreita com o aumento de n. <br><br>  Ent√£o, do lado do transmissor, eu pego a mensagem ai para enviar e desenhar uma esfera ao redor com um raio <br><br><img src="https://habrastorage.org/webt/de/j8/h9/dej8h9wxzm0ktckfoksyksxdcm0.jpeg" alt="imagem"><br><br>  que √© um pouco maior em uma quantidade igual a e2 do que o n√∫mero esperado de erros Q (Figura 13.II).  Se n for grande o suficiente, haver√° uma probabilidade arbitrariamente pequena da apar√™ncia do ponto de mensagem bj no lado do receptor, que vai al√©m dessa esfera.  Vamos tra√ßar a situa√ß√£o, como eu a vejo do ponto de vista do transmissor: temos quaisquer raios da mensagem transmitida ai para a mensagem recebida bj com uma probabilidade de erro igual a (ou quase igual a) a distribui√ß√£o normal, atingindo um m√°ximo em nQ.  Para qualquer e2, n √© t√£o grande que a probabilidade de que o ponto resultante bj, indo al√©m da minha esfera, seja t√£o pequena quanto voc√™ desejar. <br><br>  Agora considere a mesma situa√ß√£o de sua parte (Fig. 13.III).  No lado do receptor, existe uma esfera S (r) do mesmo raio r em torno do ponto recebido bj no espa√ßo n-dimensional, de modo que se a mensagem recebida bj estiver dentro da minha esfera, a mensagem que eu enviei estar√° dentro da sua esfera. <br><br>  Como pode ocorrer um erro?  Pode ocorrer um erro nos casos descritos na tabela abaixo: <br><br><img src="https://habrastorage.org/webt/ap/jj/ce/apjjcepa80evnhjxgc_swmf62z4.jpeg" alt="imagem"><br><br>  <i>Figura 13.III</i> <br><br><img src="https://habrastorage.org/webt/qt/m3/v8/qtm3v8viaw9cw291jxn11t9ya0i.jpeg" alt="imagem"><br><br>  Aqui vemos que, se na esfera constru√≠da em torno do ponto recebido, houver pelo menos mais um ponto correspondente a uma poss√≠vel mensagem n√£o codificada enviada, ocorreu um erro durante a transmiss√£o, pois voc√™ n√£o pode determinar qual dessas mensagens foi transmitida.  A mensagem enviada n√£o cont√©m erros apenas se o ponto correspondente a ela estiver na esfera e n√£o houver outros pontos poss√≠veis neste c√≥digo que estejam na mesma esfera. <br><br>  Temos uma equa√ß√£o matem√°tica para a probabilidade de um erro Re se ai foi enviado <br><br><img src="https://habrastorage.org/webt/uu/fr/fo/uufrfozpmxs0asmf4sjlinmk9rk.jpeg" alt="imagem"><br><br>  Podemos jogar fora o primeiro fator no segundo mandato, tomando-o como 1. Assim, obtemos a desigualdade <br><br><img src="https://habrastorage.org/webt/oh/qt/_9/ohqt_97ig6afppntwqmz7ifd-fs.jpeg" alt="imagem"><br><br>  Obviamente, <br><br><img src="https://habrastorage.org/webt/ac/hx/ty/achxtyvmg-aq2yirdswhnsm4olk.jpeg" alt="imagem"><br><br>  portanto <br><br><img src="https://habrastorage.org/webt/ym/27/5p/ym275poqvhr2e8jyuajtui9a4ky.jpeg" alt="imagem"><br><br>  reaplicar ao √∫ltimo membro √† direita <br><br><img src="https://habrastorage.org/webt/hd/4v/9g/hd4v9gnlnbiyjcjv-grtzvsslsg.jpeg" alt="imagem"><br><br>  Se n for tomado em tamanho suficiente, o primeiro termo poder√° ser arbitrariamente pequeno, digamos, menor que um determinado n√∫mero d.  Portanto, temos <br><br><img src="https://habrastorage.org/webt/lt/ey/hf/lteyhffjmrrzy8pjblxrsigpkzu.jpeg" alt="imagem"><br><br>  Agora vamos ver como voc√™ pode criar um c√≥digo de substitui√ß√£o simples para codificar M mensagens consistindo em n bits.  N√£o tendo id√©ia de como criar o c√≥digo (os c√≥digos de corre√ß√£o de erros ainda n√£o foram inventados), Shannon escolheu a codifica√ß√£o aleat√≥ria.  Jogue uma moeda para cada um dos n bits da mensagem e repita o processo para M mensagens.  Tudo o que voc√™ precisa fazer √© o sorteio de moedas nM, para que seja poss√≠vel <br><br><img src="https://habrastorage.org/webt/ii/ke/ss/iikessacb5q-xgdhr5btks6otss.jpeg" alt="imagem"><br><br>  dicion√°rios de c√≥digo com a mesma probabilidade de ¬ΩnM.  Obviamente, o processo aleat√≥rio de cria√ß√£o de um livro de c√≥digos significa que h√° uma probabilidade de duplicatas, bem como pontos de c√≥digo, que estar√£o pr√≥ximos um do outro e, portanto, ser√£o uma fonte de erros prov√°veis.  √â necess√°rio provar que, se isso n√£o ocorrer com uma probabilidade maior que qualquer pequeno n√≠vel de erro selecionado, o n fornecido ser√° grande o suficiente. <br>  O ponto decisivo √© que Shannon calculou a m√©dia de todos os livros de c√≥digos poss√≠veis para encontrar o erro m√©dio!  Usaremos o s√≠mbolo Av [.] Para indicar a m√©dia no conjunto de todos os poss√≠veis dicion√°rios de c√≥digos aleat√≥rios.  A m√©dia sobre a constante d, √© claro, fornece uma constante, pois para calcular a m√©dia de cada termo coincide com qualquer outro termo na soma <br><br><img src="https://habrastorage.org/webt/-j/gh/_z/-jgh_zkjbnuzjdj39bz03euss_g.jpeg" alt="imagem"><br><br>  que pode ser aumentado (M - 1 vai para M) <br><br><img src="https://habrastorage.org/webt/ma/3u/wz/ma3uwzaee8iaohanrz5qsgvspnq.jpeg" alt="imagem"><br><br>  Para qualquer mensagem em particular, ao calcular a m√©dia de todos os livros de c√≥digos, a codifica√ß√£o √© executada em todos os valores poss√≠veis; portanto, a probabilidade m√©dia de um ponto em uma esfera √© a raz√£o entre o volume da esfera e o volume total de espa√ßo.  O escopo da esfera <br><br><img src="https://habrastorage.org/webt/ry/pu/qb/rypuqbtnwvzsd4ei3_-6sxsot3c.jpeg" alt="imagem"><br><br>  onde s = Q + e2 &lt;1/2 e ns deve ser um n√∫mero inteiro. <br><br>  O √∫ltimo termo √† direita √© o maior nesta soma.  Primeiro, estimamos seu valor pela f√≥rmula de Stirling para fatoriais.  Em seguida, examinamos o coeficiente de redu√ß√£o do termo √† sua frente, observe que esse coeficiente aumenta quando se move para a esquerda e, portanto, podemos: (1) limitar o valor da soma √† soma da progress√£o geom√©trica com esse coeficiente inicial, (2) expandir a progress√£o geom√©trica de ns membros para n√∫mero infinito de termos, (3) calcule a soma da progress√£o geom√©trica infinita (√°lgebra padr√£o, nada significativo) e, finalmente, obtenha o valor limite (para um n suficientemente grande): <br><br><img src="https://habrastorage.org/webt/6f/jo/lr/6fjolr9tmeljs-eglmaxup0yrli.jpeg" alt="imagem"><br><br>  Observe como a entropia H (s) apareceu na identidade binomial.  Observe que a expans√£o nas s√©ries de Taylor H (s) = H (Q + e2) fornece uma estimativa obtida levando em considera√ß√£o apenas a primeira derivada e ignorando todas as outras.  Agora vamos coletar a express√£o final: <br><br><img src="https://habrastorage.org/webt/9h/-u/wd/9h-uwd1ukrxttavrdy91ifezgpw.jpeg" alt="imagem"><br><br>  onde <br><br><img src="https://habrastorage.org/webt/bm/h4/bb/bmh4bbgfnl2h9g0-omxft9b8lbe.jpeg" alt="imagem"><br><br>  Tudo o que precisamos fazer √© escolher e2 para que e3 &lt;e1, e ent√£o o √∫ltimo termo seja arbitrariamente pequeno, por n suficientemente grande.  Portanto, o erro PE m√©dio pode ser obtido arbitrariamente pequeno com uma capacidade de canal arbitrariamente pr√≥xima de C. <br>  Se a m√©dia de todos os c√≥digos tiver um erro suficientemente pequeno, pelo menos um c√≥digo deve ser adequado; portanto, existe pelo menos um sistema de codifica√ß√£o adequado.  Esse √© um resultado importante obtido por Shannon - ‚Äúteorema de Shannon para um canal com interfer√™ncia‚Äù, embora deva-se notar que ele o provou para um caso muito mais geral do que para um canal sim√©trico bin√°rio simples que eu usei.  Para o caso geral, os c√°lculos matem√°ticos s√£o muito mais complicados, mas as id√©ias n√£o s√£o t√£o diferentes, com muita frequ√™ncia, usando o exemplo de um caso especial, pode-se revelar o verdadeiro significado do teorema. <br><br>  Vamos criticar o resultado.  Repetimos repetidamente: "Para n suficientemente grande".  Mas qu√£o grande √© n?  Muito, muito grande, se voc√™ realmente deseja estar simultaneamente pr√≥ximo da largura de banda do canal e garantir a transfer√™ncia correta de dados!  T√£o grande que, na verdade, voc√™ ter√° que esperar muito tempo para acumular uma mensagem de tantos bits para codific√°-la mais tarde.  Nesse caso, o tamanho do dicion√°rio de c√≥digo aleat√≥rio ser√° enorme (afinal, esse dicion√°rio n√£o pode ser representado em uma forma menor que a lista completa de todos os bits Mn, enquanto n e M s√£o muito grandes)! <br><br>  Os c√≥digos de corre√ß√£o de erros evitam aguardar uma mensagem muito longa, com sua subsequente codifica√ß√£o e decodifica√ß√£o por meio de grandes livros de c√≥digos, porque evitam os livros de c√≥digos em si e, em vez disso, usam c√°lculos convencionais.  Em uma teoria simples, esses c√≥digos, em regra, perdem a capacidade de se aproximar da capacidade do canal e, ao mesmo tempo, mant√™m uma taxa de erro razoavelmente baixa, mas quando o c√≥digo corrige um grande n√∫mero de erros, eles mostram bons resultados.  Em outras palavras, se voc√™ est√° colocando algum tipo de capacidade de canal para corre√ß√£o de erros, deve usar a op√ß√£o de corre√ß√£o de erros na maioria das vezes, ou seja, em cada mensagem enviada, um grande n√∫mero de erros deve ser corrigido, caso contr√°rio, voc√™ perde essa capacidade em v√£o. <br><br>  Al√©m disso, o teorema provado acima ainda n√£o tem sentido!  Isso mostra que sistemas de transmiss√£o eficientes devem usar esquemas de codifica√ß√£o sofisticados para sequ√™ncias de bits muito longas.  Um exemplo s√£o os sat√©lites que voaram para fora do planeta exterior;  √† medida que se afastam da Terra e do Sol, s√£o for√ßados a corrigir cada vez mais erros no bloco de dados: alguns sat√©lites usam baterias solares, que fornecem cerca de 5 watts, outros usam fontes de energia at√¥mica que fornecem a mesma energia.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A fraca pot√™ncia da fonte de alimenta√ß√£o, o tamanho pequeno das placas transmissoras e o tamanho limitado das placas receptoras na Terra, a grande dist√¢ncia que o sinal deve percorrer - tudo isso requer o uso de c√≥digos com alto n√≠vel de corre√ß√£o de erros para construir um sistema de comunica√ß√£o eficaz.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Voltamos ao espa√ßo n-dimensional que usamos na prova acima. Discutindo isso, mostramos que quase todo o volume da esfera est√° concentrado perto da superf√≠cie externa; portanto, quase certamente o sinal enviado ser√° localizado na superf√≠cie da esfera constru√≠da em torno do sinal recebido, mesmo com um raio relativamente pequeno dessa esfera. Portanto, n√£o √© surpreendente que o sinal recebido ap√≥s a corre√ß√£o de um n√∫mero arbitrariamente grande de erros, nQ, seja arbitrariamente pr√≥ximo do sinal sem erros. A capacidade do canal de comunica√ß√£o, que examinamos anteriormente, √© a chave para entender esse fen√¥meno. Observe que essas esferas constru√≠das para c√≥digos de corre√ß√£o de erros de Hamming n√£o se sobrep√µem. Um grande n√∫mero de medi√ß√µes praticamente ortogonais no espa√ßo n-dimensional mostrapor que podemos encaixar as esferas M em um espa√ßo com uma leve sobreposi√ß√£o. Se voc√™ permitir uma sobreposi√ß√£o pequena e arbitrariamente pequena, o que pode levar apenas a um pequeno n√∫mero de erros durante a decodifica√ß√£o, voc√™ poder√° obter um arranjo denso de esferas no espa√ßo. Hamming garantiu um certo n√≠vel de corre√ß√£o de erros, Shannon - uma baixa probabilidade de erro, mas ao mesmo tempo mantendo a largura de banda real arbitrariamente pr√≥xima da capacidade do canal de comunica√ß√£o, o que os c√≥digos de Hamming n√£o podem fazer.mas, ao mesmo tempo, a preserva√ß√£o da taxa de transfer√™ncia real √© arbitrariamente pr√≥xima da capacidade do canal de comunica√ß√£o, o que os c√≥digos de Hamming n√£o podem fazer.mas, ao mesmo tempo, a preserva√ß√£o da taxa de transfer√™ncia real √© arbitrariamente pr√≥xima da capacidade do canal de comunica√ß√£o, o que os c√≥digos de Hamming n√£o podem fazer.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A teoria da informa√ß√£o n√£o fala sobre como projetar um sistema eficaz, mas indica a dire√ß√£o do movimento em dire√ß√£o a sistemas de comunica√ß√£o eficazes. Essa √© uma ferramenta valiosa para a constru√ß√£o de sistemas de comunica√ß√£o entre m√°quinas, mas, como observado anteriormente, n√£o tem muito a ver com a maneira como as pessoas trocam informa√ß√µes entre si. A extens√£o em que a heran√ßa biol√≥gica √© semelhante aos sistemas de comunica√ß√£o t√©cnica √© simplesmente desconhecida; portanto, atualmente n√£o est√° claro como a teoria da informa√ß√£o se aplica aos genes. N√£o temos escolha a n√£o ser tentar, e se o sucesso nos mostrar a natureza semelhante a uma m√°quina desse fen√¥meno, o fracasso apontar√° para outros aspectos significativos da natureza da informa√ß√£o.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">N√£o vamos nos distrair muito. Vimos que todas as defini√ß√µes iniciais, em maior ou menor grau, deveriam expressar a ess√™ncia de nossas cren√ßas iniciais, mas s√£o caracterizadas por um certo grau de distor√ß√£o e, portanto, n√£o s√£o aplic√°veis. √â tradicionalmente aceito que, em √∫ltima an√°lise, a defini√ß√£o que usamos define realmente a ess√™ncia; mas apenas nos diz como processar as coisas e n√£o faz sentido algum. A abordagem postulativa, t√£o aclamada nos c√≠rculos matem√°ticos, deixa muito a desejar na pr√°tica.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agora, veremos um exemplo de testes de QI, em que a defini√ß√£o √© t√£o c√≠clica quanto voc√™ gosta e, como resultado, o engana. Um teste √© criado para medir a intelig√™ncia. Depois disso, ele √© revisado para ser o mais consistente poss√≠vel e, em seguida, √© publicado e calibrado de maneira simples, para que a "intelig√™ncia" medida seja normalmente distribu√≠da (√© claro, ao longo da curva de calibra√ß√£o). Todas as defini√ß√µes devem ser cruzadas, n√£o apenas quando s√£o propostas pela primeira vez, mas muito mais tarde, quando s√£o usadas nas conclus√µes feitas. At√© que ponto os limites de defini√ß√£o s√£o adequados para a tarefa em quest√£o? Com que frequ√™ncia as defini√ß√µes dadas nas mesmas condi√ß√µes come√ßam a ser aplicadas em condi√ß√µes bastante diferentes? Isso √© bastante comum!Nas humanidades que voc√™ inevitavelmente encontrar√° em sua vida, isso acontece com mais frequ√™ncia.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Assim, um dos objetivos desta apresenta√ß√£o da teoria da informa√ß√£o, al√©m de demonstrar sua utilidade, era alert√°-lo sobre esse perigo ou demonstrar como us√°-lo para obter o resultado desejado. H√° muito se nota que as defini√ß√µes iniciais determinam o que voc√™ encontra no final, em uma extens√£o muito maior do que parece. As defini√ß√µes iniciais exigem que voc√™ preste muita aten√ß√£o n√£o apenas em qualquer nova situa√ß√£o, mas tamb√©m em √°reas com as quais voc√™ trabalha h√° muito tempo. Isso permitir√° que voc√™ entenda at√© que ponto os resultados obtidos s√£o uma tautologia e n√£o algo √∫til.</font></font><br><br>      ,       .   ,   ,     ,    !      ,   . <br><br> <i> ...</i> <br><br> <i>    ,     ‚Äî       magisterludi2016@yandex.ru</i> <br><br> ,         ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">¬´The Dream Machine:   ¬ª</a> ) <br><br> <b> </b> ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> ,     </a> . ( <i>  10 ,  20  </i> ) <br><br><div class="spoiler"> <b class="spoiler_title">    </b> <div class="spoiler_text"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pref√°cio</font></font></a> <br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Introdu√ß√£o √† arte de fazer ci√™ncia e engenharia: aprendendo a aprender (28 de mar√ßo de 1995) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tradu√ß√£o: Cap√≠tulo 1</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúFundamentos da revolu√ß√£o digital (discreta)‚Äù (30 de mar√ßo de 1995) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 2. Fundamentos da revolu√ß√£o digital (discreta)</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúHist√≥ria dos computadores - hardware‚Äù (31 de mar√ßo de 1995) </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">Cap√≠tulo 3.</font></a><font style="vertical-align: inherit;"> Hist√≥ria do computador - hardware</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúHist√≥ria dos computadores - software‚Äù (4 de abril de 1995) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 4. Hist√≥ria dos computadores - software</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hist√≥ria dos Computadores - Aplica√ß√µes (6 de abril de 1995) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 5. Hist√≥ria do Computador - Aplica√ß√£o Pr√°tica</font></font></a> </li><li> ¬´Artificial Intelligence ‚Äî Part I¬ª (April 7, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 6.   ‚Äî 1</a> </li><li> ¬´Artificial Intelligence ‚Äî Part II¬ª (April 11, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 7.   ‚Äî II</a> </li><li> ¬´Artificial Intelligence III¬ª (April 13, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 8.  -III</a> </li><li> ¬´n-Dimensional Space¬ª (April 14, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 9. N- </a> </li><li> ¬´Coding Theory ‚Äî The Representation of Information, Part I¬ª (April 18, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 10.   ‚Äî I</a> </li><li> ¬´Coding Theory ‚Äî The Representation of Information, Part II¬ª (April 20, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 11.   ‚Äî II</a> </li><li> ¬´Error-Correcting Codes¬ª (April 21, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 12.    </a> </li><li> ¬´Information Theory¬ª (April 25, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 13.  </a> </li><li> ¬´Digital Filters, Part I¬ª (April 27, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 14.   ‚Äî 1</a> </li><li> ¬´Digital Filters, Part II¬ª (April 28, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 15.   ‚Äî 2</a> </li><li> ¬´Digital Filters, Part III¬ª (May 2, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 16.   ‚Äî 3</a> </li><li> ¬´Digital Filters, Part IV¬ª (May 4, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> 17.   ‚Äî IV</a> </li><li>  ‚ÄúSimula√ß√£o, Parte I‚Äù (5 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 18. Modelagem - I</a> </li><li>  ‚ÄúSimula√ß√£o, Parte II‚Äù (9 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 19. Modelagem - II</a> </li><li>  ‚ÄúSimula√ß√£o, Parte III‚Äù (11 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 20. Modelagem - III</a> </li><li>  Fibra √≥tica (12 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 21. Fibra √≥tica</a> </li><li>  Instru√ß√£o Assistida por Computador (16 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 22. Aprendizado</a> Assistido por Computador <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">(CAI)</a> </li><li>  Matem√°tica (18 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 23. Matem√°tica</a> </li><li>  Mec√¢nica Qu√¢ntica (19 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 24. Mec√¢nica Qu√¢ntica</a> </li><li>  Criatividade (23 de maio de 1995).  Tradu√ß√£o: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 25. Criatividade</a> </li><li>  ‚ÄúPeritos‚Äù (25 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 26. Peritos</a> </li><li>  ‚ÄúDados n√£o confi√°veis‚Äù (26 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 27. Dados inv√°lidos</a> </li><li>  Engenharia de sistemas (30 de maio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 28. Engenharia de sistemas</a> </li><li>  ‚ÄúVoc√™ consegue o que mede‚Äù (1 de junho de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 29.</a> Voc√™ obt√©m o que mede </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚ÄúComo sabemos o que sabemos‚Äù</a> (2 de junho de 1995) <i>traduz em fatias de 10 minutos</i> </li><li>  Hamming, "Voc√™ e sua pesquisa" (6 de junho de 1995).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tradu√ß√£o: voc√™ e seu trabalho</a> </li></ol><br>     ,     ‚Äî       magisterludi2016@yandex.ru <br><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt422205/">https://habr.com/ru/post/pt422205/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt422195/index.html">O uso de ACS na minera√ß√£o</a></li>
<li><a href="../pt422197/index.html">Dizemos uma palavra sobre o revezamento</a></li>
<li><a href="../pt422199/index.html">Semana 33 de seguran√ßa: por quem o monitor oscila?</a></li>
<li><a href="../pt422201/index.html">China, deixe-me escrever?</a></li>
<li><a href="../pt422203/index.html">Clicker DIY</a></li>
<li><a href="../pt422207/index.html">Monstros ap√≥s as f√©rias: AMD Threadripper 2990WX 32-Core e 2950X 16-Core (parte 4)</a></li>
<li><a href="../pt422209/index.html">Monstros ap√≥s as f√©rias: AMD Threadripper 2990WX 32-Core e 2950X 16-Core (parte 5)</a></li>
<li><a href="../pt422211/index.html">Bela estrutura de componentes na nuvem do Microsoft Azure</a></li>
<li><a href="../pt422213/index.html">Um dia sem JavaScript: o que poderia dar errado?</a></li>
<li><a href="../pt422217/index.html">N√£o √© realmente s√©rio sobre hospedagem ou como verificar a adequa√ß√£o do host</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>