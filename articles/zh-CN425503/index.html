<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤚🏿 🖐🏾 💆🏻 Cassandra Sink用于Spark结构化流 👆🏼 💄 ⏮️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="几个月前，我开始研究Spark，在某个时候，我面临着将结构化流计算保存在Cassandra数据库中的问题。 

 在本文中，我给出了一个简单的示例，该示例创建和使用Cassandra Sink进行Spark结构化流传输。 我希望该帖子对最近开始使用Spark Structured Streaming...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cassandra Sink用于Spark结构化流</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425503/"> 几个月前，我开始研究Spark，在某个时候，我面临着将结构化流计算保存在Cassandra数据库中的问题。 <br><br> 在本文中，我给出了一个简单的示例，该示例创建和使用Cassandra Sink进行Spark结构化流传输。 我希望该帖子对最近开始使用Spark Structured Streaming并想知道如何将计算结果上载到数据库的人员有用。 <br><br> 该应用程序的想法非常简单-接收和解析来自Kafka的消息，成对执行简单的转换并将结果保存在cassandra中。 <br><a name="habracut"></a><br><h3> 结构化流的优点 </h3><br> 您可以在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">文档中</a>阅读有关结构化流的更多信息。 简而言之，结构化流是基于Spark SQL引擎的可扩展的流信息处理引擎。 它允许您使用Dataset / DataFrame聚合数据，计算窗口函数，连接等。也就是说，结构化流允许您使用良好的旧SQL处理数据流。 <br><br><h3> 怎么了 </h3><br>  Spark结构化流的稳定版本于2017年发布。 也就是说，这是一个相当新的API，可实现基本功能，但某些事情必须由我们自己完成。 例如，结构化流传输具有用于将输出写入文件，图块，控制台或内存的标准功能，但为了将数据保存到数据库，您必须使用结构化流传输中可用的<i>foreach</i>接收器并实现<i>ForeachWriter</i>接口。  <b>从Spark 2.3.1开始，此功能只能在Scala和Java中实现</b> 。 <br><br> 我假定读者已经知道结构化流一般如何工作，知道如何实现必要的转换，并且现在准备将结果上传到数据库。 如果上述步骤中的某些步骤不清楚，则官方文档可以作为学习结构化流媒体的一个很好的起点。 在本文中，当您需要将结果保存到数据库中时，我想着重介绍最后一步。 <br><br> 下面，我将描述用于结构化流的Cassandra接收器的示例实现，并说明如何在集群中运行它。 完整的代码<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在这里</a> 。 <br><br> 当我第一次遇到上述问题时， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">这个项目</a>非常有用。 但是，如果读者刚刚开始使用结构化流并且正在寻找有关如何将数据上传到cassandra的简单示例，则似乎有点复杂。 另外，该项目被编写为以本地模式工作，并且需要进行一些更改才能在集群中运行。 <br><br> 我还想举例说明如何使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">JDBC</a>将数据保存到<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">MongoDB</a>和任何其他数据库。 <br><br><h3> 简单的解决方案 </h3><br> 要将数据上传到外部系统，必须使用<i>foreach</i>接收器。  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在这里</a>阅读更多有关此的内容。 简而言之，必须实现<i>ForeachWriter</i>接口。 也就是说，有必要确定如何打开连接，如何处理每条数据以及如何在处理结束时关闭连接。 源代码如下： <br><br><pre><code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraSinkForeach</span></span></span><span class="hljs-class">(</span><span class="hljs-params"></span><span class="hljs-class"><span class="hljs-params"></span>) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ForeachWriter</span></span></span><span class="hljs-class">[org.apache.spark.sql.</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Row</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This class implements the interface ForeachWriter, which has methods that get called // whenever there is a sequence of rows generated as output val cassandraDriver = new CassandraDriver(); def open(partitionId: Long, version: Long): Boolean = { // open connection println(s"Open connection") true } def process(record: org.apache.spark.sql.Row) = { println(s"Process new $record") cassandraDriver.connector.withSessionDo(session =&gt; session.execute(s""" insert into ${cassandraDriver.namespace}.${cassandraDriver.foreachTableSink} (fx_marker, timestamp_ms, timestamp_dt) values('${record(0)}', '${record(1)}', '${record(2)}')""") ) } def close(errorOrNull: Throwable): Unit = { // close the connection println(s"Close connection") } }</span></span></code> </pre> <br> 我稍后将描述<i>CassandraDriver</i>的定义和输出表的结构，但是现在，让我们仔细看看上面的代码是如何工作的。 为了从Spark连接到Kasandra，我创建了一个<i>CassandraDriver</i>对象，该对象提供对<i>CassandraConnector</i> （由<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">DataStax</a>开发的连接器）的访问。  CassandraConnector负责打开和关闭与数据库的连接，因此我仅在<i>CassandraSinkForeach</i>类的<i>open</i>和<i>close</i>方法中显示调试消息。 <br><br> 从主应用程序调用以上代码，如下所示： <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sink = parsed .writeStream .queryName(<span class="hljs-string"><span class="hljs-string">"KafkaToCassandraForeach"</span></span>) .outputMode(<span class="hljs-string"><span class="hljs-string">"update"</span></span>) .foreach(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CassandraSinkForeach</span></span>()) .start()</code> </pre><br> 为数据的每一行创建<i>CassandraSinkForeach</i> ，因此每个工作节点都将其部分行插入数据库中。 也就是说，每个工作节点都执行<i>val cassandraDriver = new CassandraDriver（）;</i> 这是CassandraDriver的样子： <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraDriver</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This object will be used in CassandraSinkForeach to connect to Cassandra DB from an executor. // It extends SparkSessionBuilder so to use the same SparkSession on each node. val spark = buildSparkSession import spark.implicits._ val connector = CassandraConnector(spark.sparkContext.getConf) // Define Cassandra's table which will be used as a sink /* For this app I used the following table: CREATE TABLE fx.spark_struct_stream_sink ( fx_marker text, timestamp_ms timestamp, timestamp_dt date, primary key (fx_marker)); */ val namespace = "fx" val foreachTableSink = "spark_struct_stream_sink" }</span></span></code> </pre><br> 让我们仔细看看<i>spark</i>对象。  <i>SparkSessionBuilder</i>的代码如下： <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Serializable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Build a spark session. Class is made serializable so to get access to SparkSession in a driver and executors. // Note here the usage of @transient lazy val def buildSparkSession: SparkSession = { @transient lazy val conf: SparkConf = new SparkConf() .setAppName("Structured Streaming from Kafka to Cassandra") .set("spark.cassandra.connection.host", "ec2-52-23-103-178.compute-1.amazonaws.com") .set("spark.sql.streaming.checkpointLocation", "checkpoint") @transient lazy val spark = SparkSession .builder() .config(conf) .getOrCreate() spark } }</span></span></code> </pre><br> 在每个工作节点上， <i>SparkSessionBuilder</i>提供对在驱动程序上创建的<i>SparkSession的</i>访问。 为了使这种访问成为可能，必须序列化<i>SparkSessionBuilder</i>并使用<i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=" class="user_link">瞬态</a> lazy val</i> ，它允许序列化系统在程序初始化时忽略<i>conf</i>和<i>spark</i>对象，直到访问对象为止。 因此，在程序启动时， <i>buildSparkSession被</i>序列化并发送到每个工作节点，但是仅当工作节点正在访问<i>conf</i>和<i>spark</i>对象时，才允许<i>conf</i>和<i>spark</i>对象。 <br><br> 现在让我们看一下主要的应用程序代码： <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">KafkaToCassandra</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Main body of the app. It also extends SparkSessionBuilder. def main(args: Array[String]) { val spark = buildSparkSession import spark.implicits._ // Define location of Kafka brokers: val broker = "ec2-18-209-75-68.compute-1.amazonaws.com:9092,ec2-18-205-142-57.compute-1.amazonaws.com:9092,ec2-50-17-32-144.compute-1.amazonaws.com:9092" /*Here is an example massage which I get from a Kafka stream. It contains multiple jsons separated by \n {"timestamp_ms": "1530305100936", "fx_marker": "EUR/GBP"} {"timestamp_ms": "1530305100815", "fx_marker": "USD/CHF"} {"timestamp_ms": "1530305100969", "fx_marker": "EUR/CHF"} {"timestamp_ms": "1530305100011", "fx_marker": "USD/CAD"} */ // Read incoming stream val dfraw = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", broker) .option("subscribe", "currency_exchange") .load() val schema = StructType( Seq( StructField("fx_marker", StringType, false), StructField("timestamp_ms", StringType, false) ) ) val df = dfraw .selectExpr("CAST(value AS STRING)").as[String] .flatMap(_.split("\n")) val jsons = df.select(from_json($"value", schema) as "data").select("data.*") // Process data. Create a new date column val parsed = jsons .withColumn("timestamp_dt", to_date(from_unixtime($"timestamp_ms"/1000.0, "yyyy-MM-dd HH:mm:ss.SSS"))) .filter("fx_marker != ''") // Output results into a database val sink = parsed .writeStream .queryName("KafkaToCassandraForeach") .outputMode("update") .foreach(new CassandraSinkForeach()) .start() sink.awaitTermination() } }</span></span></code> </pre><br> 当发送应用程序执行时， <i>buildSparkSession被</i>序列化并发送到工作节点，但是<i>conf</i>和<i>spark</i>对象仍未解析。 然后，驱动程序在<i>KafkaToCassandra</i>内部创建一个spark对象，并在工作节点之间分配工作。 每个工作节点都从Kafka读取数据，对记录的接收部分进行简单的转换，当工作节点准备好将结果写入数据库时​​，它允许<i>conf</i>和<i>spark</i>对象，从而获得对在驱动程序上创建的<i>SparkSession的</i>访问权限。 <br><br><h3> 如何构建和运行应用程序？ </h3><br> 当我从PySpark迁移到Scala时，花了我一段时间才弄清楚如何构建应用程序。 因此，我在项目中包含了Maven <i>pom.xml</i> 。 读者可以通过运行<i>mvn package</i>命令使用Maven构建应用程序。 可以将应用程序发送给执行后 <br><br><pre> <code class="bash hljs">./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1,datastax:spark-cassandra-connector:2.3.0-s_2.11 --class com.insight.app.CassandraSink.KafkaToCassandra --master spark://ec2-18-232-26-53.compute-1.amazonaws.com:7077 target/cassandra-sink-0.0.1-SNAPSHOT.jar</code> </pre><br> 为了构建和运行应用程序，有必要用您自己的AWS机器名称替换（即替换所有类似ec2-xx-xxx-xx-xx.compute-1.amazonaws.com的名称）。 <br><br> 火花和结构化流特别是对我来说是一个新主题，因此，我将非常感谢读者提出意见，讨论和更正。 </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN425503/">https://habr.com/ru/post/zh-CN425503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN425489/index.html">自动化：夸大的机器人威胁</a></li>
<li><a href="../zh-CN425493/index.html">为IPTV Beeline配置MikroTik hAP mini</a></li>
<li><a href="../zh-CN425497/index.html">Tutu PHP聚会＃2：现场直播</a></li>
<li><a href="../zh-CN425499/index.html">HyperX Impact DDR4-可以的SO-DIMM！ 还是为什么在笔记本电脑中以3200 MHz的频率存储64 GB内存？</a></li>
<li><a href="../zh-CN425501/index.html">从A到Z在Android上进行A / B测试</a></li>
<li><a href="../zh-CN425505/index.html">Linux内核启动过程分析</a></li>
<li><a href="../zh-CN425507/index.html">Parsim Wikipedia负责4个团队的NLP任务</a></li>
<li><a href="../zh-CN425511/index.html">Rotativa应用程序的非显而易见的功能，用于在ASP.NET MVC应用程序中生成PDF</a></li>
<li><a href="../zh-CN425515/index.html">Apple阻止对新MacBook型号进行独立维修</a></li>
<li><a href="../zh-CN425517/index.html">Yandex如何使用雷达和卫星创建全球降水预报</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>