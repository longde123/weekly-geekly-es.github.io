<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòÑ üéØ ‚¨õÔ∏è Ich sehe, es bedeutet, dass ich existiere: Deep Learning Review in Computer Vision (Teil 2) üíÖüèª ü•É üëÅ‚Äçüó®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir verstehen weiterhin moderne Magie (Computer Vision). Teil 2 bedeutet nicht, dass Sie zuerst Teil 1 lesen m√ºssen. Teil 2 bedeutet, dass jetzt alles...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ich sehe, es bedeutet, dass ich existiere: Deep Learning Review in Computer Vision (Teil 2)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/458190/"> Wir verstehen weiterhin moderne Magie (Computer Vision).  Teil 2 bedeutet nicht, dass Sie zuerst Teil 1 lesen m√ºssen. Teil 2 bedeutet, dass jetzt alles ernst ist - wir m√∂chten die volle Leistung neuronaler Netze im Sehen verstehen.  Erkennung, Verfolgung, Segmentierung, Haltungsbewertung, Aktionserkennung ... Die modischsten und coolsten Architekturen, Hunderte von Ebenen und Dutzende brillanter Ideen warten bereits unter dem Schnitt auf Sie! <br><br><img src="https://habrastorage.org/webt/yt/nk/uu/ytnkuundiudek47rjvlmlujrrm4.jpeg"><br><a name="habracut"></a><br><h2>  In der letzten Serie </h2><br>  Ich m√∂chte Sie daran erinnern, dass wir uns im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Teil</a> mit Faltungs-Neuronalen Netzen und deren Visualisierung sowie mit den Aufgaben der Klassifizierung von Bildern und der Konstruktion ihrer effektiven Darstellungen (Einbettungen) vertraut gemacht haben.  Wir haben sogar die Aufgaben der Gesichtserkennung und Neuidentifizierung von Personen besprochen. <br><br>  Selbst im vorherigen Artikel haben wir √ºber verschiedene Arten von Architekturen gesprochen (ja, dieselben Tablets <s>, die ich einen Monat lang erstellt habe</s> ), und hier hat Google keine Zeit verschwendet: Sie haben eine weitere extrem schnelle und genaue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">EfficientNet-</a> Architektur ver√∂ffentlicht.  Sie haben es mit dem <abbr title="Suche nach neuronaler Architektur">NAS</abbr> und dem speziellen Compound Scaling-Verfahren erstellt.  Schauen Sie sich den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel an</a> , es lohnt sich. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9m/_h/5c/9m_h5cc1tsxs7bfkainm5zom-wg.jpeg" width="500"></div><br>  In der Zwischenzeit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">animieren</a> einige Forscher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gesichter</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">suchen in Filmen nach K√ºssen</a> . Wir werden uns mit dringlicheren Problemen befassen. <br><br>  Hier sagen die Leute: "Bilderkennung".  Aber was ist ‚ÄûAnerkennung‚Äú?  Was ist "Verstehen (Szene)"?  Meiner Meinung nach h√§ngen die Antworten auf diese Fragen davon ab, was genau wir ‚Äûerkennen‚Äú und was genau wir ‚Äûverstehen‚Äú wollen.  Wenn wir k√ºnstliche Intelligenz aufbauen, die Informationen √ºber die Welt aus dem visuellen Strom genauso effizient (oder sogar besser) wie Menschen extrahiert, m√ºssen wir von Aufgaben, von Bed√ºrfnissen ausgehen.  Historisch gesehen kann die moderne ‚ÄûErkennung‚Äú und das ‚ÄûVerst√§ndnis der Szene‚Äú in mehrere spezifische Aufgaben unterteilt werden: Klassifizierung, Erkennung, Verfolgung, Bewertung von K√∂rperhaltungen und Gesichtspunkten, Segmentierung, Erkennung von Aktionen auf dem Video und Beschreibung des Bildes im Text.  Dieser Artikel konzentriert sich auf die ersten beiden Aufgaben aus der Liste (Ups, Spoiler des dritten Teils). Der aktuelle Plan lautet also wie folgt: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Finde mich, wenn du kannst: Objekterkennung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gesichtserkennung: Nicht gefangen - kein Dieb</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Viele Buchstaben: Texterkennung (und Erkennung)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Video und Tracking: in einem Stream</a> </li></ol><br>  Lass uns rocken, Superstars! <br><br><a name="1"></a><h2>  Finde mich, wenn du kannst: Objekterkennung </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-q/9e/on/-q9eonan6thdv5jivk8kq0h7gm0.jpeg" width="700"></div><br>  Die Aufgabe klingt also einfach - es wird ein Bild gegeben, auf dem Sie Objekte vordefinierter Klassen finden m√ºssen (Person, Buch, Apfel, artesisch-normannischer Basset-Griffon usw.).  Um dieses Problem mit Hilfe neuronaler Netze zu l√∂sen, stellen wir es in Bezug auf Tensoren und maschinelles Lernen. <br><br>  Wir erinnern uns, dass ein Farbbild ein Tensor ist (H, W, 3) (wenn wir uns nicht erinnern, dh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1</a> ).  Fr√ºher wussten wir nur, wie man das gesamte Bild klassifiziert, aber jetzt ist es unser Ziel, die Positionen von interessierenden Objekten (Pixelkoordinaten) im Bild und deren Klassen vorherzusagen. <br><br>  Die Schl√ºsselidee dabei ist, zwei Probleme gleichzeitig zu l√∂sen - Klassifizierung und Regression.  Wir verwenden ein neuronales Netzwerk, um die Koordinaten zu regressieren und die darin enthaltenen Objekte zu klassifizieren. <br><br><div class="spoiler">  <b class="spoiler_title">Klassifizierung?</b>  <b class="spoiler_title">Regression?</b> <div class="spoiler_text">  Ich m√∂chte Sie daran erinnern, dass wir √ºber die Aufgaben des maschinellen Lernens sprechen.  Im <b>Klassifizierungsproblem</b> fungieren <b>Klassenbeschriftungen</b> als Qualit√§t von echten Beschriftungen f√ºr Objekte, und wir sagen die Klasse des Objekts voraus.  Im <b>Regressionsproblem</b> sind <b>reelle Zahlen reelle Zahlen</b> , und wir sagen die Zahl voraus (zum Beispiel: Gewicht, Gr√∂√üe, Gehalt, Anzahl der Charaktere, die in der n√§chsten Serie des Game of Thrones sterben ...).  Im Detail - Sie sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">herzlich willkommen zur 3. Vorlesung der DLSchool (FPMI MIPT)</a> . <br></div></div><br>  Die Koordinaten des Objekts k√∂nnen jedoch im Allgemeinen auf verschiedene Arten formalisiert werden. In DL gibt es drei Hauptmethoden: <i>Erkennung</i> ( <abbr title="Rechtecke, die Objekte begrenzen">K√§stchen mit</abbr> Objekten), <i>Bewertung der K√∂rperhaltung</i> (Schl√ºsselpunkte von Objekten) und <i>Segmentierung</i> (‚ÄûMasken‚Äú von Objekten).  <abbr title="Rechtecke, die Objekte begrenzen"><b>Lassen Sie uns nun</b></abbr> √ºber die Vorhersage pr√§ziser <abbr title="Rechtecke, die Objekte begrenzen"><b>Begrenzungsrahmen</b></abbr> , Punkte und Segmentierung sprechen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ki/wt/xm/kiwtxmvvhmwsvqn3_5dovlmp8w8.jpeg" width="500"></div><br>  Grunds√§tzlich sind Erkennungsdatens√§tze mit K√§stchen im Format gekennzeichnet: ‚ÄûKoordinaten der oberen linken und unteren rechten Ecke f√ºr jedes Objekt in jedem Bild‚Äú (dieses Format wird auch als <abbr title="&quot;Tlbr&quot;">oben links, unten rechts bezeichnet</abbr> ), und die meisten neuronalen Netzwerkans√§tze sagen diese Koordinaten voraus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uo/zs/tj/uozstjspdifxpslvqyuurauxs2g.png" width="500"></div><br><div class="spoiler">  <b class="spoiler_title">Informationen zu Datens√§tzen und Metriken im Erkennungsproblem</b> <div class="spoiler_text">  Nach dem Festlegen der Aufgabe ist es am besten zu sehen, welche Daten f√ºr das Training verf√ºgbar sind und welche Metriken zur Messung der Qualit√§t verwendet werden.  Dar√ºber spreche ich langsam in der ersten H√§lfte der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">13. Vorlesung der Deep Learning School</a> (bei x2.0 ist es die h√∂chste). <br></div></div><br>  Bevor wir uns mit den Arten neuronaler Netze zur Erkennung befassen, √ºberlegen wir uns gemeinsam, wie das Problem der Erkennung von Bildern gel√∂st werden kann.  Wenn wir ein bestimmtes Objekt im Bild finden m√∂chten, wissen wir wahrscheinlich ungef√§hr, wie es aussieht und welchen Bereich das Bild einnehmen soll (obwohl es sich √§ndern kann). <br><br><div class="spoiler">  <b class="spoiler_title">Erkennung von Grund auf neu erfinden</b> <div class="spoiler_text">  Der naive und einfachste Ansatz w√§re, einfach einen "Vorlagensuch" -Algorithmus zu erstellen: Lassen Sie das Bild 100 x 100 Pixel gro√ü sein, und wir suchen nach einem Fu√üball.  Es sei ein Kugelmuster von 20x20 Pixel.  Nehmen Sie diese Vorlage und wir werden sie wie eine Faltung im gesamten Bild durchgehen und den Unterschied von Pixel zu Pixel z√§hlen.  So funktioniert <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Template Matching</a> (h√§ufig wird eine Art Korrelation anstelle eines pixelweisen Unterschieds verwendet). <br><br>  Wenn es keine Vorlage gibt, aber einen Klassifikator f√ºr neuronale Netze, k√∂nnen wir dies tun: Wir gehen durch ein Fenster fester Gr√∂√üe im Bild und sagen die Klasse des aktuellen Bildbereichs voraus.  Dann sagen wir einfach, dass die wahrscheinlichsten Bereiche der Objekte diejenigen sind, in denen der Klassifikator sicher geantwortet hat.  Auf diese Weise k√∂nnen wir das Problem l√∂sen, dass das Objekt anders aussieht (da es darauf trainiert wurde, eine sehr unterschiedliche Stichprobe zu klassifizieren). <br><br>  Aber dann taucht ein Problem auf - die Objekte auf den Bildern haben unterschiedliche Gr√∂√üen.  Der gleiche Fu√üball kann sich in der gesamten H√∂he / Breite des Bildes befinden oder weit am Ziel liegen und nur 10 bis 20 Pixel von 1000 nehmen. Ich m√∂chte den Brute-Force-Algorithmus schreiben: Wir durchlaufen einfach die Fenstergr√∂√üen.  Angenommen, wir haben 100x200 Pixel, dann gehen wir zu einem 2x2-, 2X3-, 3x2-, 2x4-, 4x2-, 3x3-Fenster ..., 3x4, 4x3 ... Ich denke, Sie verstehen, dass die Anzahl der m√∂glichen Fenster 100 * 200 betr√§gt und jedes von uns durch das Bild geht Durchf√ºhren von Klassifizierungsoperationen (100-W_window) * (200-H_window), was viel Zeit in Anspruch nimmt.  Ich f√ºrchte, wir werden nicht warten, bis ein solcher Algorithmus funktioniert. <br><br>  Sie k√∂nnen nat√ºrlich je nach Objekt die charakteristischsten Fenster ausw√§hlen, aber es funktioniert auch sehr lange, und wenn es schnell ist, ist es unwahrscheinlich, dass es genau ist - in realen Anwendungen gibt es wahnsinnig viele Variationen in der Gr√∂√üe von Objekten in den Bildern. <br></div></div><br>  Au√üerdem werde ich mich manchmal auf eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neue √úberpr√ºfung des Erkennungsbereichs ab Januar 2019 verlassen</a> (Bilder werden auch davon stammen).  Dies ist nur ein Muss, wenn Sie schnell einen m√∂glichst umfassenden Einblick in DL bei der Erkennung erhalten m√∂chten. <br><br>  Einer der ersten Artikel zur Erkennung und Lokalisierung mit CNN war <a href="">Overfeat</a> .  Die Autoren behaupten, dass sie zuerst ein neuronales Netzwerk zur Erkennung in ImageNet verwendet haben, um das Problem neu zu formulieren und den Verlust zu √§ndern.  Der Ansatz war √ºbrigens fast durchgehend (siehe unten das √úberfeature-Schema). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8o/5v/tv/8o5vtvhgukkn7frba0nx8yltfis.png" width="700"></div><br>  Die n√§chste wichtige Architektur war das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf</a> <b>Regionen</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">basierende Convolutional Neural Network</a> ( <b>RCNN</b> ), das 2014 von Forschern von <abbr>FAIR</abbr> erfunden wurde.  Das Wesentliche ist, dass es zun√§chst viele der sogenannten ‚ÄûRegionen von Interesse‚Äú (RoIs) vorhersagt, in denen sich m√∂glicherweise Objekte befinden k√∂nnen (mithilfe des Selective Search-Algorithmus), diese klassifiziert und die Koordinaten der Boxen mithilfe von CNN verfeinert. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2r/2p/kp/2r2pkpcoysglv4z_v-ll_y14mqw.png" width="700"></div><br>  Eine solche Pipeline hat zwar das gesamte System verlangsamt, da wir jede Region durch das neuronale Netzwerk gef√ºhrt haben (wir haben tausende Male weitergeleitet).  Ein Jahr sp√§ter r√ºstete derselbe FAIR Ross Girshick RCNN auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fast-RCNN auf</a> .  Hier bestand die Idee darin, die selektive Suche und die Netzwerkvorhersage auszutauschen: Zuerst leiten wir das gesamte Bild durch ein vorab trainiertes neuronales Netzwerk und dann prognostizieren wir interessierende Regionen √ºber die vom Backbone-Netzwerk ausgegebene Feature-Map (z. B. unter Verwendung derselben selektiven Suche, aber es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">andere Algorithmen</a> ).  Es war immer noch ziemlich langsam, viel langsamer als in Echtzeit (im Moment gehen wir davon aus, dass Echtzeit weniger als 40 Millisekunden pro Bild betr√§gt). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tn/uc/d-/tnucd-y6i7tr4edgjeudtrsj16u.png" width="700"></div><br>  Die Geschwindigkeit wurde vor allem nicht von CNN, sondern vom Box-Generierungsalgorithmus selbst beeinflusst. Daher wurde beschlossen, ihn durch ein zweites neuronales Netzwerk zu ersetzen - das Region Proposal Network ( <b>RPN</b> ) -, das trainiert wird, um die interessierenden Regionen von Objekten vorherzusagen.  So <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erschien Faster-RCNN</a> (ja, sie haben offensichtlich lange nicht an den Namen gedacht).  Schema: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6v/h_/ye/6vh_yee2zrsflyhh8jdvtm_bbgy.png" width="700"></div><br>  Dann gab es eine weitere Verbesserung in Form von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">R-FCN</a> , wir werden nicht im Detail dar√ºber sprechen, aber ich m√∂chte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mask-RCNN erw√§hnen</a> .  Mask-RCNN ist ein einzigartiges Netzwerk, das erste neuronale Netzwerk, das gleichzeitig das <b>Problem der Erkennung und</b> Instanzsegmentierung l√∂st. Es sagt die genauen Masken (Silhouetten) von Objekten in Begrenzungsrahmen voraus.  Ihre Idee ist eigentlich recht einfach: Es gibt zwei Zweige: zur Erkennung und zur Segmentierung, und Sie m√ºssen das Netzwerk f√ºr beide Aufgaben gleichzeitig trainieren.  Die Hauptsache ist, Daten markiert zu haben.  Mask-RCNN selbst ist Faster-RCNN sehr √§hnlich: Das Backbone ist das gleiche, aber am Ende gibt es zwei <b>‚ÄûK√∂pfe‚Äú</b> (wie die <b>letzten Schichten des</b> neuronalen Netzwerks oft genannt werden) f√ºr zwei verschiedene Aufgaben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7w/ig/hq/7wighq6ox7tptik5f_7d7cez2hg.png" width="700"></div><br>  Dies waren die sogenannten <b>zweistufigen</b> (oder <b>regionalen</b> ) Ans√§tze.  Parallel dazu wurden Analoga f√ºr die DL-Detektion entwickelt - <b>einstufige</b> Ans√§tze.  Dazu geh√∂ren neuronale Netze wie: SSD (Single-Shot Detector), YOLO (You Only Look Once), DSOD (Deeply Supervised Object Detector), RFBNet (Receptive Field Block Network) und viele andere (siehe Karte unten) von <a href="">dieses Repository</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/lc/-y/uelc-yeav4_avjdkycglf9uwrmm.png" width="750"></div><br>  Einstufige Ans√§tze verwenden im Gegensatz zu zweistufigen Ans√§tzen keinen separaten Algorithmus zum Erzeugen von Boxen, sondern sagen einfach mehrere Boxkoordinaten f√ºr jede Feature-Map voraus, die von einem Faltungs-Neuronalen Netzwerk erzeugt wird.  YOLO verh√§lt sich √§hnlich, SSD ist etwas anders, aber es gibt nur eine Idee: Eine 1x1-Faltung sagt viele Zahlen aus den empfangenen Feature-Maps detailliert voraus, wir sind uns jedoch im Voraus einig, welche Zahl dies bedeutet. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qj/ml/w_/qjmlw_ympdcib6jkpfdjdvcirdy.png" width="600"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jr/i3/oy/jri3oymb48sv5vq9dwwdxbndszg.png" width="600"></div><br>  Zum Beispiel sagen wir aus einer Feature-Map voraus, dass die Gr√∂√üe von 13x13x256 eine Feature-Map mit 13x13x (4 * (5 + 80)) Zahlen ist, wobei wir in der Tiefe 85 Zahlen f√ºr 4 Boxen vorhersagen: Die ersten 4 Zahlen in der Sequenz sind immer die Koordinaten der Box, die 5 .. - Vertrauen in das Boxen und 80 Zahlen - die Wahrscheinlichkeiten jeder der Klassen (Klassifikation).  Dies ist notwendig, um dann die notwendigen Zahlen den notwendigen Verlusten zu unterwerfen und das neuronale Netz richtig zu trainieren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ao/xi/o2/aoxio2rty3hgpu2f8mduomvs9nu.png" width="800"></div><br>  Ich m√∂chte darauf aufmerksam machen, dass die Qualit√§t der Arbeit des Detektors von der Qualit√§t des neuronalen Netzwerks abh√§ngt, um Merkmale ( <b>dh ein neuronales Backbone-Netzwerk</b> ) zu extrahieren.  Normalerweise wird diese Rolle von einer der Architekturen gespielt, √ºber die ich in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fr√ºheren Artikel gesprochen habe</a> (ResNet, SENet usw.), aber manchmal entwickeln Autoren ihre eigenen optimaleren Architekturen (z. B. Darknet-53 in YOLOv3) oder Modifikationen (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Feature Pyramid Pooling)</a> (FPN)). <br><br>  Ich stelle erneut fest, dass wir das Netzwerk gleichzeitig auf Klassifizierung und Regression trainieren.  In der Community wird dies als Verlust mehrerer Aufgaben bezeichnet: Die Summe der Verluste f√ºr mehrere Aufgaben (mit einigen Koeffizienten) wird in einem Verlust angezeigt. <br><br><div class="spoiler">  <b class="spoiler_title">Nachrichten mit f√ºhrendem Multitask-Verlust</b> <div class="spoiler_text">  Bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Machines Can See 2019 verwendete</a> einer der Redner den Multi-Task-Verlust f√ºr 7 Aufgaben gleichzeitig <s>, Carl</s> .  Es stellte sich heraus, dass einige Aufgaben urspr√ºnglich als Gegengewicht zueinander gesetzt wurden und ein ‚ÄûKonflikt‚Äú entstand, der das Netzwerk daran hinderte, besser zu lernen, als wenn es f√ºr jede Aufgabe einzeln trainiert w√ºrde.  Schlussfolgerung: Wenn Sie den Verlust mehrerer Aufgaben verwenden, stellen Sie sicher, dass dieselben Aufgaben nicht mit der Anweisung in Konflikt stehen (z. B. kann die Vorhersage der Grenzen von Objekten und ihrer internen Segmentierung sich gegenseitig st√∂ren, da diese Dinge auf unterschiedlichen Vorzeichen im Netzwerk beruhen k√∂nnen).  Der Autor hat dies umgangen, indem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">er f√ºr jede Aufgabe separate Squeeze-and-Excitation-Bl√∂cke hinzugef√ºgt hat</a> . <br></div></div><br>  K√ºrzlich erschienen Artikel aus dem Jahr 2019, in denen die Autoren ein noch besseres Geschwindigkeits- / Genauigkeitsverh√§ltnis in der Erkennungsaufgabe unter Verwendung einer <b>punktbasierten Box-Vorhersage</b> deklarieren.  Ich spreche von den Artikeln <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûObjekte als Punkte‚Äú</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûCornerNet-Lite‚Äú</a> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ExtremeNet</a> ist eine Modifikation von CornerNet.  Es scheint, dass sie jetzt bei der Erkennung mithilfe neuronaler Netze als SOTA bezeichnet werden k√∂nnen (dies ist jedoch nicht genau). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wk/qf/mr/wkqfmrenwm3u5f6c6bzinjcffga.png" width="900"></div><br>  Wenn meine Erkl√§rung der Detektoren pl√∂tzlich immer noch chaotisch und unverst√§ndlich schien, diskutiere ich sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserem Video</a> langsam.  Vielleicht solltest du es zuerst sehen. <br><br>  Im Folgenden habe ich Tabellen neuronaler Netze zur Erkennung mit Links zum Code und einer kurzen Beschreibung der Chips jedes Netzes angegeben.  Ich habe versucht, nur die Netzwerke zu sammeln, die wirklich wichtig sind (zumindest ihre Ideen), um heute eine gute Vorstellung von der Objekterkennung zu haben: <br><br><div class="spoiler">  <b class="spoiler_title">Neuronale Netzdetektoren (zweistufig)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schl√ºsselidee </th><th>  Code </th></tr><tr><td>  2013-2014 </td><td>  <a href="">RCNN</a> </td><td>  Erzeugung von Regionen von Interesse und Vorhersage des neuronalen Netzwerks von Klassen in ihnen </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fast-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">F√ºhren Sie das Bild zuerst durch das Netzwerk und generieren Sie dann interessierende Regionen</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schneller-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verwenden Sie RPN, um interessierende Regionen zu generieren</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">R-FCN</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vollst√§ndig faltungsorientierter Ansatz, anstatt interessierende Regionen zu erzeugen</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mask-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zwei ‚ÄûK√∂pfe‚Äú zum gleichzeitigen L√∂sen von zwei Aufgaben, RoI-Align</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Keras, TF</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Argumentation-RCNN</a> </td><td>  Verbesserung der Qualit√§t von RCNN durch Erstellung eines Diagramms semantischer Beziehungen von Objekten </td><td>  --- ---. </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Neuronale Netzdetektoren (einstufig)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schl√ºsselidee </th><th>  Code </th></tr><tr><td>  2013-2014 </td><td>  <a href="">√úberfeature</a> </td><td>  einer der ersten Detektoren f√ºr neuronale Netze </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">C ++ (mit Wrappern f√ºr andere Sprachen)</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SSD</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sehr flexibler einstufiger Ansatz, der jetzt in vielen Anwendungen verwendet wird</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yolo</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine Idee √§hnlich der von SSD entwickelt sich parallel und ist nicht weniger beliebt (es gibt neue Versionen).</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">C ++</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">YOLOv2 (auch bekannt als YOLO9000)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine Reihe von Verbesserungen f√ºr YOLO</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">YOLOv3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine Reihe von Verbesserungen f√ºr YOLOv2</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2017-2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DSOD</a> </td><td>  Deep Supervision Idea und DenseNet Ideas </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2017-2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RFBNet</a> </td><td>  Faltungsfilter werden basierend auf der Struktur des menschlichen visuellen Systems ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RF-</a> Block) genau ausgew√§hlt. </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Neuronale Netzdetektoren (Sonstiges)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schl√ºsselidee </th><th>  Code </th></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RetinaNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">spezieller Fokusverlust zur L√∂sung des Problems des Klassenungleichgewichts</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Keras</a> </td></tr><tr><td>  2014-2015 </td><td>  <a href="">SPP</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Modul, mit dem Sie effektiv mit Bildern unterschiedlicher Gr√∂√üe arbeiten k√∂nnen</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Keras</a> </td></tr><tr><td>  2016-2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FPN</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verf√ºgen √ºber Pyramiden zur besseren Erkennung von Objekten unterschiedlicher Gr√∂√üe</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NAS-FPN</a> </td><td>  Finden der besten FPN mit der Suche nach neuronaler Architektur </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Daedalus</a> </td><td>  wie man den Detektor mit einem gegnerischen Angriff bricht </td><td>  --- ---. </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Neuronale Netzdetektoren (punktbasiert)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schl√ºsselidee </th><th>  Code </th></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Centernet</a> </td><td>  Ein neuer Ansatz zur Erkennung, mit dem das Problem der gleichzeitigen Suche nach Punkten, Feldern und 3D-Feldern schnell und effizient gel√∂st werden kann </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cornernet</a> </td><td>  Vorhersage von K√§stchen basierend auf Paaren von Eckpunkten </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CornerNet-Lite</a> </td><td>  beschleunigtes Cornernet </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ExtremeNet</a> </td><td>  Vorhersage von ‚Äûextremen‚Äú Punkten von Objekten (geometrisch genaue Grenzen) </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Um zu verstehen, wie die Geschwindigkeit / Qualit√§t jeder Architektur korreliert, k√∂nnen Sie sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Test</a> oder seine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">popul√§rere Version</a> ansehen. <br><br>  Architektur ist in Ordnung, aber Erkennung ist in erster Linie eine praktische Aufgabe.  "Haben Sie nicht hundert Netzwerke, aber mindestens eines funktioniert" - das ist meine Botschaft.  Es gibt Links zum Code in der obigen Tabelle, aber ich pers√∂nlich sto√üe selten darauf, Detektoren direkt aus Repositorys zu starten (zumindest mit dem Ziel einer weiteren Bereitstellung in der Produktion).  Am h√§ufigsten wird hierf√ºr eine Bibliothek verwendet, beispielsweise die TensorFlow-Objekterkennungs-API (siehe den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">praktischen Teil meiner Lektion</a> ) oder eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bibliothek von Forschern von CUHK</a> .  Ich mache Sie auf einen weiteren Supertisch aufmerksam (Sie m√∂gen sie, oder?): <br><br><div class="spoiler">  <b class="spoiler_title">Bibliotheken zum Ausf√ºhren von Erkennungsmodellen</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Titel </th><th>  Die Autoren </th><th>  Beschreibung </th><th>  Implementierte neuronale Netze </th><th>  Framework </th></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Detectron</a> </td><td>  Facebook AI Research </td><td>  Facebook-Repository mit verschiedenen Modellcodes zur Erkennung und Bewertung der K√∂rperhaltung </td><td>  Alle regional </td><td>  Caffe2 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TF-Objekterkennungs-API</a> </td><td>  TensorFlow-Team </td><td>  Viele gebrauchsfertige Modelle (Gewichte sind angegeben) </td><td>  Alle regionalen und SSDs (mit unterschiedlichen Backbones) </td><td>  Tensorflow </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Darkflow</a> </td><td>  thtrieu </td><td>  Gebrauchsfertige YOLO- und YOLOv2-Implementierungen </td><td>  Alle YOLO-Typen (mit √Ñnderungen) au√üer YOLOv3 </td><td>  Tensorflow </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mmdetektion</a> </td><td>  √ñffnen Sie MMLab (CUHK) </td><td>  Eine gro√üe Anzahl von Detektoren auf PyTorch finden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sie in ihrem Artikel</a> </td><td>  Fast alle Modelle au√üer der YOLO-Familie </td><td>  Pytorch </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Darknet (modifiziert)</a> </td><td>  AlexAB </td><td>  Bequeme Implementierung von YOLOv3 mit vielen Verbesserungen am <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">urspr√ºnglichen Repository</a> </td><td>  YOLOv3 </td><td>  C ++ </td></tr></tbody></table></div><br></div></div><br>  Oft m√ºssen Sie ein Objekt nur einer Klasse erkennen, das jedoch spezifisch und sehr variabel ist.  Zum Beispiel, um alle Gesichter auf dem Foto zu erkennen (zur weiteren √úberpr√ºfung / Z√§hlung von Personen), um ganze Personen zu erkennen (zur erneuten Identifizierung / Z√§hlung / Verfolgung) oder um Text in der Szene zu erkennen (zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OCR</a> / √úbersetzung von W√∂rtern auf dem Foto).  Im Allgemeinen funktioniert der ‚Äûnormale‚Äú Erkennungsansatz hier bis zu einem gewissen Grad, aber jede dieser Unteraufgaben hat ihre eigenen Tricks, um die Qualit√§t zu verbessern. <br><br><a name="2"></a><h2>  Gesichtserkennung: Nicht gefangen - kein Dieb </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ic/ul/rp/iculrpbc7niyrdxg1yk_8r82nsw.jpeg" width="700"></div><br>  Hier tritt eine gewisse Spezifit√§t auf, da Gesichter h√§ufig einen relativ kleinen Teil des Bildes einnehmen.  Au√üerdem schauen die Leute nicht immer in die Kamera, oft ist das Gesicht nur von der Seite sichtbar.  Einer der ersten Ans√§tze zur Gesichtserkennung war der ber√ºhmte Viola-Jones-Detektor, der auf Haar-Kaskaden basiert und bereits 2001 erfunden wurde. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hc/tf/zn/hctfzn0xudbedi_aymhmlcwkamu.png" width="400"></div><br>  Neuronale Netze <s>waren damals nicht in Mode,</s> sie waren immer noch nicht so stark in der Vision, aber der gute alte handgefertigte Ansatz hat seine Aufgabe erf√ºllt.  Es wurden verschiedene Arten von speziellen Filtermasken aktiv verwendet, die dazu beitrugen, Gesichtsbereiche aus dem Bild und ihren Zeichen zu extrahieren, und diese Zeichen wurden dann dem AdaBoost-Klassifikator √ºbermittelt.  √úbrigens funktioniert diese Methode wirklich gut und jetzt ist sie schnell genug und startet sofort <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mit OpenCV</a> .  Der Nachteil dieses Detektors besteht darin, dass er nur Gesichter sieht, die frontal an der Kamera angebracht sind.  Man muss sich nur ein wenig umdrehen und die Stabilit√§t der Erkennung wird verletzt. <br><br>  F√ºr solch komplexere F√§lle k√∂nnen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dlib verwenden</a> .  Dies ist C ++ - eine Bibliothek, in der viele Bildverarbeitungsalgorithmen implementiert sind, auch zur Gesichtserkennung. <br><br>  Von den neuronalen Netzwerkans√§tzen bei der Gesichtserkennung ist das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kaskadierte Multitask-CNN (MTCNN)</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MatLab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TensorFlow</a> ) von besonderer Bedeutung.  Im Allgemeinen wird es jetzt aktiv verwendet (im selben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Facenet</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/80/wn/vu/80wnvuf59poodmodzswcgjlyjt4.jpeg" width="400"></div><br>  Die Idee von MTCNN besteht darin, drei neuronale Netze nacheinander (daher eine <b>‚ÄûKaskade‚Äú</b> ) zu verwenden, um die Position eines Gesichts und seiner singul√§ren Punkte vorherzusagen.  In diesem Fall gibt es genau 5 spezielle Punkte im Gesicht: das linke Auge, das rechte Auge, den linken Rand der Lippen, den rechten Rand der Lippen und der Nase.  Das erste neuronale Netzwerk aus der Kaskade ( <abbr title="Proposal Net">P-Net</abbr> ) wird verwendet, um potenzielle Regionen des Gesichts zu erzeugen.  Das zweite ( <abbr title="Refine Net">R-Net</abbr> ) - um die Koordinaten der empfangenen Boxen zu verbessern.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das dritte ( </font></font><abbr title="Facial Landmarks Net"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O-Net</font></font></abbr><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) neuronale Netzwerk regressiert erneut die Koordinaten der Boxen und sagt zus√§tzlich 5 Schl√ºsselpunkte des Gesichts voraus. </font><font style="vertical-align: inherit;">Dieses Netzwerk ist eine Mehrfachaufgabe, da drei Aufgaben gel√∂st sind: Regression von Boxpunkten, Klassifizierung von Gesicht / Nicht-Gesicht f√ºr jede Box und Regression von Gesichtspunkten. </font><font style="vertical-align: inherit;">Dar√ºber hinaus erledigt MTCNN alles in Echtzeit, dh es ben√∂tigt weniger als 40 ms pro Bild.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/iz/d5/5eizd5lwag9umfo1ccypep42eik.jpeg" width="800"></div><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie lesen Sie immer noch keine Artikel mit ArXiv selbst?</font></font></b> <div class="spoiler_text"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Fall empfehle ich, den </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Originalartikel √ºber MTCNN</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zu lesen </font><font style="vertical-align: inherit;">, wenn Sie bereits Hintergrundinformationen zu Faltungsnetzwerken haben. </font><font style="vertical-align: inherit;">Dieser Artikel umfasst nur </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5 Seiten</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , enth√§lt jedoch alle Informationen, die Sie zum Verst√§ndnis des Ansatzes ben√∂tigen. </font><font style="vertical-align: inherit;">Probieren Sie es aus, es wird sich verz√∂gern :)</font></font><br></div></div><br>   State-of-the-Art   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dual Shot Face Detector (DSFD)</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FaceBoxes</a> . FaceBoxes      CPU (!),  DSFD    (   2019 ). DSFD  ,  MTCNN,          ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dilated convolutions</a> ),        . ,  dilated convolutions            .    DSFD (,   ?). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xv/or/it/xvoritceua0_ocjteyvm4xvisbq.jpeg"></div><br>     <b></b> ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> ,      . <br><br><a name="3"></a><h2>  :  ( )  </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qn/v9/wo/qnv9woeqru3dioeennrkvalkjqg.png" width="500"></div><br>     .  , ,   bounding box',    (   ),    .     ,   , ,       recognition-,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">    </a> . <br><br>       bounding box',        ,    ( ).     , , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">EAST-</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/j3/hd/bj/j3hdbj-xozs4voec8ekiz4k1eo0.png" width="500"></div><br><br>  EAST-  ,      ,    : <br><br><ol><li> Text Score Map' (     ) </li><li>     </li><li>        </li></ol><br>  ,      (  ),  .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv-</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dj/vt/uu/djvtuu36dzinoratlumncsgb5t8.png" width="700"></div><br><br>    (    )  ,    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TextBoxes++</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SegLinks</a> ,  EAST,   ,    . <br><br>         ,  <b></b>     .       ‚Äî    .     ,      ,   ,          . , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MORAN</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  PyTorch</a> )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ASTER</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  TensorFlow</a> )     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vt/ot/nc/vtotncz1d1zhhsiytuzmarta9ta.png" width="700"></div><br><br>    - ,          : CNN  RNN.       ,     .    MORAN':     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hu/qx/_2/huqx_2jakjcggsgacvhj9a8vce4.png" width="300"></div><br>       EAST', -       ,           .  ,           ,     . <br><br>   <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a></b>   ,  / .      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spatial Transformet Network (STN)</a> ,              (,       ,    ).   / STN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6y/z1/p9/6yz1p942ensgepwirqrtzpcnb7q.jpeg" width="700"></div><br>   STN    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> (  ,  )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  PyTorch</a> . <br><br>  MORAN (     )    ‚Äî      ,        <b> </b>  x   y,     ,      .    <i><abbr title="Korrektur, Korrektur">rectification</abbr></i> ,         ( <i>rectifier'</i> ).         : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/n4/az/du/n4azduih5p7wd9sx-tqhpgy-p20.png" width="300"></div><br>  Zus√§tzlich zu den Ans√§tzen zur ‚Äûmodularen‚Äú Texterkennung (Erkennungsnetzwerk -&gt; Erkennungsnetzwerk) gibt es jedoch eine End-to-End-Architektur: Die Eingabe ist ein Bild, und die Ausgabe ist eine Erkennung, und der Text wird in ihnen erkannt.  Und das alles ist eine einzige Pipeline, die beide Aufgaben gleichzeitig lernt.  In dieser Richtung gibt es die beeindruckende Arbeit des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fast Oriented Text Spotting mit einem Unified Network ( <b>FOTS</b> ) ( <b>PyTorch-</b></a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> ), bei der die Autoren auch feststellen, dass der End-to-End-Ansatz doppelt so schnell ist wie ‚ÄûErkennung + Erkennung‚Äú.  Unten sehen Sie das FOTS-Diagramm f√ºr neuronale Netze. Eine besondere Rolle spielt der RoiRotate-Block, aufgrund dessen es m√∂glich ist, Gradienten aus dem Netzwerk zur Erkennung auf das neuronale Netzwerk zur Erkennung zu √ºbertragen (dies ist wirklich komplizierter als es scheint). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/a5/xh/ch/a5xhchryrwf-zpfudielmab9pqu.png" width="800"></div><br>  √úbrigens findet jedes Jahr die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICDAR-</a> Konferenz <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">statt</a> , an der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehrere Wettbewerbe</a> zur Erkennung von Text in verschiedenen Bildern teilnehmen. <br><br><h3>  Aktuelle Probleme bei der Erkennung </h3><br>  Meiner Meinung nach ist das Hauptproblem bei der Erkennung jetzt nicht die Qualit√§t des Detektormodells, sondern die Daten: Die Markierung ist normalerweise lang und teuer, insbesondere wenn viele Klassen erkannt werden m√ºssen (aber es gibt √ºbrigens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein Beispiel f√ºr eine L√∂sung</a> f√ºr 500 Klassen).  Daher widmen sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viele Arbeiten</a> nun der Erzeugung der plausibelsten Daten ‚Äûsynthetisch‚Äú und dem Erhalt von Markups ‚Äûkostenlos‚Äú.  Unten ist ein Bild aus <s>meinem Diplom eines</s> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikels von Nvidia</a> , der sich speziell mit der Erzeugung synthetischer Daten befasst. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6k/mn/wh/6kmnwhvc1pgahzwtjloa4dy9sbm.png" width="800"></div><br>  Trotzdem ist es gro√üartig, dass wir jetzt sicher sagen k√∂nnen, wo auf dem Bild was sein soll.  Und wenn wir zum Beispiel die Menge von etwas auf dem Rahmen berechnen wollen, dann reicht es aus, dies zu erkennen und die Anzahl der K√§stchen herauszugeben.  Bei der Erkennung von Personen funktioniert das √ºbliche YOLO auch gut, nur die Hauptsache ist, viele Daten einzureichen.  Der gleiche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Darkflow ist</a> geeignet, und die Klasse ‚ÄûMensch‚Äú ist in fast allen wichtigen Erkennungsdatens√§tzen enthalten.  Wenn wir also mit der Kamera die Anzahl der Personen z√§hlen m√∂chten, die beispielsweise an einem Tag vorbeigekommen sind, oder die Anzahl der Waren, die eine Person in einem Gesch√§ft aufgenommen hat, erkennen wir einfach die Menge und geben sie aus ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/m2/ts/pl/m2tsplbgrnmoauvlghy8ivq2jdm.jpeg" width="700"></div><br>  H√∂r auf  Wenn wir jedoch Personen in jedem Bild von der Kamera erkennen m√∂chten, k√∂nnen wir ihre Anzahl in einem Bild und in zwei Bildern berechnen - nicht mehr, da wir nicht sagen k√∂nnen, wo genau sich welche Person befindet.  Wir brauchen einen Algorithmus, mit dem wir genau eindeutige Personen im Videostream z√§hlen k√∂nnen.  Es mag ein Algorithmus zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erneuten Identifizierung sein</a> , aber wenn es um Video und Erkennung geht, ist es eine S√ºnde, keine Tracking-Algorithmen zu verwenden. <br><br><a name="4"></a><h3>  Video und Tracking: in einem Stream </h3><br>  Bisher haben wir nur √ºber Aufgaben in Bildern gesprochen, aber das Interessanteste passiert im Video.  Um die gleiche Erkennung von Handlungen zu l√∂sen, m√ºssen wir nicht nur die sogenannte <i>r√§umliche</i> Komponente verwenden, sondern auch die <i>zeitliche</i> , da Video eine <i>zeitliche</i> Folge von Bildern ist. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qm/da/pa/qmdapao0kcytnxhorj7htm6susy.jpeg" width="700"></div><br>  Tracking ist ein Analogon zur Bilderkennung, jedoch f√ºr Video.  Das hei√üt, wir m√∂chten dem Netzwerk beibringen, nicht das Boxen im Bild vorherzusagen, sondern ein Tracklet in der Zeit (was im Wesentlichen eine Folge von Boxen ist).  Unten sehen Sie ein Beispiel f√ºr ein Bild, das die ‚ÄûSchw√§nze‚Äú zeigt - die Spuren dieser Personen im Video. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xz/yv/c_/xzyvc_oumhrnf_-bmf8gz5l_hli.png" width="600"></div><br>  Lassen Sie uns dar√ºber nachdenken, wie Sie das Tracking-Problem l√∂sen k√∂nnen.  Es gebe ein Video und seine Bilder Nr. 1 und Nr. 2.  Betrachten wir bisher nur ein Objekt - wir verfolgen einen Ball.  Bei Bild 1 k√∂nnen wir einen Detektor verwenden, um es zu erkennen.  Beim zweiten k√∂nnen wir auch einen Ball erkennen, und wenn er alleine da ist, ist alles in Ordnung: Wir sagen, dass das Boxen auf dem vorherigen Frame das Boxen des gleichen Balls wie auf Frame # 2 ist.  Sie k√∂nnen auch mit den verbleibenden Frames unterhalb des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GIFs</a> des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pyimagesearch-</a> Vision- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurses fortfahren</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_w/et/ak/_wetakdpybemefhert6cxslqaju.gif" width="600"></div><br>  √úbrigens, um Zeit zu sparen, k√∂nnen wir das neuronale Netzwerk nicht im zweiten Frame starten, sondern einfach die Box des Balls aus dem ersten Frame ‚Äûherausschneiden‚Äú und im zweiten Frame oder Pixel f√ºr Pixel nach genau der gleichen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Korrelation</a> suchen.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Korrelations-Tracker</a> verwenden diesen Ansatz. Sie gelten als einfach und mehr oder weniger zuverl√§ssig, wenn es sich um einfache F√§lle wie ‚ÄûVerfolgen eines Balls vor der Kamera in einem leeren Raum‚Äú handelt.  Diese Aufgabe wird auch als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">visuelle Objektverfolgung bezeichnet</a> .  Unten sehen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein Beispiel f√ºr die Arbeit des</a> Korrelations-Trackers am Beispiel einer Person. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cv/fp/qd/cvfpqdd5ghelxfpucxrejxq6vpm.gif" width="600"></div><br>  Wenn es jedoch mehrere Erkennungen / Personen gibt, m√ºssen Sie in der Lage sein, die Felder aus Bild 1 und Bild 2 abzugleichen.  Die erste Idee, die mir in den Sinn kommt, ist zu versuchen, die Box mit der Box <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">abzustimmen</a> , die den gr√∂√üten Schnittbereich ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">IoU</a> ) aufweist.  Richtig, bei mehreren √ºberlappenden Erkennungen ist ein solcher Tracker instabil, sodass Sie noch mehr Informationen verwenden m√ºssen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qx/i2/t1/qxi2t1ejpnir0h52ip-23zpf4ti.png" width="600"></div><br>  Der Ansatz mit IoU basiert nur auf den <i>‚Äûgeometrischen‚Äú</i> Erkennungszeichen, dh es wird lediglich versucht, diese anhand der N√§he zu Frames zu vergleichen.  Wir verf√ºgen jedoch √ºber ein ganzes Bild (in diesem Fall sogar √ºber zwei), und wir k√∂nnen die Tatsache nutzen, dass sich in diesen Erkennungen <i>‚Äûvisuelle‚Äú Zeichen befinden</i> .  Au√üerdem haben wir eine Erkennungshistorie f√ºr jede Person, die es uns erm√∂glicht, ihre n√§chste Position auf der Grundlage von Geschwindigkeit und Bewegungsrichtung genauer vorherzusagen. Dies kann bedingt als <i>‚Äûphysische‚Äú Zeichen bezeichnet werden</i> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mc/ay/se/mcaysee7xq25-j6hnvjalhfy5z0.gif" width="700"></div><br>  Einer der ersten Echtzeit-Tracker, der absolut zuverl√§ssig und in der Lage war, schwierige Situationen zu bew√§ltigen, wurde 2016 ver√∂ffentlicht. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Simple Online und Realtime Traker (SORT)</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-Code</a> ).  SORT verwendete keine visuellen Zeichen und neuronalen Netze, sondern sch√§tzte nur eine Reihe von Parametern jeder Box in jedem Frame: die aktuelle Geschwindigkeit (x und y getrennt) und Gr√∂√üe (H√∂he und Breite).  Das Seitenverh√§ltnis einer Box wird immer von der ersten Erkennung dieser Box an √ºbernommen.  Ferner werden die Geschwindigkeiten unter Verwendung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kalman-Filtern</a> vorhergesagt (sie sind in der Welt der Signalverarbeitung im Allgemeinen gut und leicht), die Schnittmatrix der Boxen durch IoU wird erstellt und die Erkennungen werden durch den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ungarischen Algorithmus</a> zugewiesen. <br><br>  Wenn es Ihnen so vorkommt, als sei die Mathematik schon ein bisschen viel geworden, dann wird in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Artikel</a> alles auf zug√§ngliche Weise erkl√§rt (das ist mittel :). <br><br>  Bereits 2017 wurde eine Modifikation von SORT in Form von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepSORT</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code f√ºr TensorFlow</a> ) ver√∂ffentlicht.  DeepSORT hat bereits begonnen, das neuronale Netzwerk zum Extrahieren visueller Zeichen zu verwenden, um Kollisionen aufzul√∂sen.  Die Qualit√§t des Trackings ist gewachsen - nicht umsonst gilt es heute als einer der besten Online-Tracker. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/it/ny/15itnyht32oes3n-keaqk36jnpq.gif" width="800"></div><br>  Das Gebiet der Verfolgung entwickelt sich tats√§chlich aktiv: Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tracker mit siamesischen neuronalen Netzen</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tracker mit RNN</a> .  Halten Sie Ihren Finger am Puls der Zeit, denn an jedem Tag kann eine noch genauere und schnellere Architektur herauskommen (oder ist bereits herausgekommen).  √úbrigens ist es sehr praktisch, solche Dinge auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PapersWithCode</a> zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verfolgen</a> . Es gibt immer Links zu Artikeln und Code f√ºr sie (falls vorhanden). <br><br><h3>  Nachwort </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jb/yh/bf/jbyhbf2ovu_ynurp_t_d9_hi4vg.jpeg" width="600"></div><br><br>  Wir haben wirklich viel erlebt und viel gelernt.  Aber Computer Vision ist ein extrem weites Gebiet, und ich bin eine extrem hartn√§ckige Person.  Aus diesem Grund werden wir Sie im dritten Artikel dieses Zyklus sehen (wird es der letzte sein? Wer wei√ü ...), in dem wir die Segmentierung, die Beurteilung der K√∂rperhaltung, die Erkennung von Aktionen auf einem Video und die Erstellung einer Beschreibung aus einem Bild unter Verwendung neuronaler Netze ausf√ºhrlicher diskutieren werden. <br><br>  PS Ich m√∂chte Vadim Gorbatschow f√ºr seine wertvollen Ratschl√§ge und Kommentare bei der Vorbereitung dieses und des vorherigen Artikels meinen besonderen Dank aussprechen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458190/">https://habr.com/ru/post/de458190/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458176/index.html">Die Exaflop-Barriere wird 2021 √ºberwunden</a></li>
<li><a href="../de458180/index.html">Kea-basierter Failover-DHCP-Server</a></li>
<li><a href="../de458182/index.html">Wir lesen VKontakte √ºber RSS</a></li>
<li><a href="../de458184/index.html">Haxe und PHP: statische Eingabe, Pfeilfunktionen, Metaprogrammierung und vieles mehr</a></li>
<li><a href="../de458186/index.html">WAL in PostgreSQL: 1. Puffercache</a></li>
<li><a href="../de458202/index.html">Schauen Sie sich einfach SObjectizer an, wenn Sie Actors oder CSP in Ihrem C ++ - Projekt verwenden m√∂chten</a></li>
<li><a href="../de458204/index.html">So bewerten Sie die Speicherleistung unter Linux: Benchmarking mit offenen Tools</a></li>
<li><a href="../de458206/index.html">Erhabener Text 3 f√ºr das Site-Layout. Passen Sie das Erscheinungsbild an und installieren Sie Plugins. Anf√§ngerleitfaden</a></li>
<li><a href="../de458208/index.html">Digitale Veranstaltungen in Moskau vom 01. bis 07. Juli</a></li>
<li><a href="../de458214/index.html">Pentest-Labor "Pentestit Test Lab 12" - volle Passage</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>