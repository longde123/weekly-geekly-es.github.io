<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😄 🎯 ⬛️ Ich sehe, es bedeutet, dass ich existiere: Deep Learning Review in Computer Vision (Teil 2) 💅🏻 🥃 👁‍🗨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir verstehen weiterhin moderne Magie (Computer Vision). Teil 2 bedeutet nicht, dass Sie zuerst Teil 1 lesen müssen. Teil 2 bedeutet, dass jetzt alles...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ich sehe, es bedeutet, dass ich existiere: Deep Learning Review in Computer Vision (Teil 2)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/458190/"> Wir verstehen weiterhin moderne Magie (Computer Vision).  Teil 2 bedeutet nicht, dass Sie zuerst Teil 1 lesen müssen. Teil 2 bedeutet, dass jetzt alles ernst ist - wir möchten die volle Leistung neuronaler Netze im Sehen verstehen.  Erkennung, Verfolgung, Segmentierung, Haltungsbewertung, Aktionserkennung ... Die modischsten und coolsten Architekturen, Hunderte von Ebenen und Dutzende brillanter Ideen warten bereits unter dem Schnitt auf Sie! <br><br><img src="https://habrastorage.org/webt/yt/nk/uu/ytnkuundiudek47rjvlmlujrrm4.jpeg"><br><a name="habracut"></a><br><h2>  In der letzten Serie </h2><br>  Ich möchte Sie daran erinnern, dass wir uns im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Teil</a> mit Faltungs-Neuronalen Netzen und deren Visualisierung sowie mit den Aufgaben der Klassifizierung von Bildern und der Konstruktion ihrer effektiven Darstellungen (Einbettungen) vertraut gemacht haben.  Wir haben sogar die Aufgaben der Gesichtserkennung und Neuidentifizierung von Personen besprochen. <br><br>  Selbst im vorherigen Artikel haben wir über verschiedene Arten von Architekturen gesprochen (ja, dieselben Tablets <s>, die ich einen Monat lang erstellt habe</s> ), und hier hat Google keine Zeit verschwendet: Sie haben eine weitere extrem schnelle und genaue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">EfficientNet-</a> Architektur veröffentlicht.  Sie haben es mit dem <abbr title="Suche nach neuronaler Architektur">NAS</abbr> und dem speziellen Compound Scaling-Verfahren erstellt.  Schauen Sie sich den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel an</a> , es lohnt sich. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9m/_h/5c/9m_h5cc1tsxs7bfkainm5zom-wg.jpeg" width="500"></div><br>  In der Zwischenzeit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">animieren</a> einige Forscher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gesichter</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">suchen in Filmen nach Küssen</a> . Wir werden uns mit dringlicheren Problemen befassen. <br><br>  Hier sagen die Leute: "Bilderkennung".  Aber was ist „Anerkennung“?  Was ist "Verstehen (Szene)"?  Meiner Meinung nach hängen die Antworten auf diese Fragen davon ab, was genau wir „erkennen“ und was genau wir „verstehen“ wollen.  Wenn wir künstliche Intelligenz aufbauen, die Informationen über die Welt aus dem visuellen Strom genauso effizient (oder sogar besser) wie Menschen extrahiert, müssen wir von Aufgaben, von Bedürfnissen ausgehen.  Historisch gesehen kann die moderne „Erkennung“ und das „Verständnis der Szene“ in mehrere spezifische Aufgaben unterteilt werden: Klassifizierung, Erkennung, Verfolgung, Bewertung von Körperhaltungen und Gesichtspunkten, Segmentierung, Erkennung von Aktionen auf dem Video und Beschreibung des Bildes im Text.  Dieser Artikel konzentriert sich auf die ersten beiden Aufgaben aus der Liste (Ups, Spoiler des dritten Teils). Der aktuelle Plan lautet also wie folgt: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Finde mich, wenn du kannst: Objekterkennung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gesichtserkennung: Nicht gefangen - kein Dieb</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Viele Buchstaben: Texterkennung (und Erkennung)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Video und Tracking: in einem Stream</a> </li></ol><br>  Lass uns rocken, Superstars! <br><br><a name="1"></a><h2>  Finde mich, wenn du kannst: Objekterkennung </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-q/9e/on/-q9eonan6thdv5jivk8kq0h7gm0.jpeg" width="700"></div><br>  Die Aufgabe klingt also einfach - es wird ein Bild gegeben, auf dem Sie Objekte vordefinierter Klassen finden müssen (Person, Buch, Apfel, artesisch-normannischer Basset-Griffon usw.).  Um dieses Problem mit Hilfe neuronaler Netze zu lösen, stellen wir es in Bezug auf Tensoren und maschinelles Lernen. <br><br>  Wir erinnern uns, dass ein Farbbild ein Tensor ist (H, W, 3) (wenn wir uns nicht erinnern, dh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1</a> ).  Früher wussten wir nur, wie man das gesamte Bild klassifiziert, aber jetzt ist es unser Ziel, die Positionen von interessierenden Objekten (Pixelkoordinaten) im Bild und deren Klassen vorherzusagen. <br><br>  Die Schlüsselidee dabei ist, zwei Probleme gleichzeitig zu lösen - Klassifizierung und Regression.  Wir verwenden ein neuronales Netzwerk, um die Koordinaten zu regressieren und die darin enthaltenen Objekte zu klassifizieren. <br><br><div class="spoiler">  <b class="spoiler_title">Klassifizierung?</b>  <b class="spoiler_title">Regression?</b> <div class="spoiler_text">  Ich möchte Sie daran erinnern, dass wir über die Aufgaben des maschinellen Lernens sprechen.  Im <b>Klassifizierungsproblem</b> fungieren <b>Klassenbeschriftungen</b> als Qualität von echten Beschriftungen für Objekte, und wir sagen die Klasse des Objekts voraus.  Im <b>Regressionsproblem</b> sind <b>reelle Zahlen reelle Zahlen</b> , und wir sagen die Zahl voraus (zum Beispiel: Gewicht, Größe, Gehalt, Anzahl der Charaktere, die in der nächsten Serie des Game of Thrones sterben ...).  Im Detail - Sie sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">herzlich willkommen zur 3. Vorlesung der DLSchool (FPMI MIPT)</a> . <br></div></div><br>  Die Koordinaten des Objekts können jedoch im Allgemeinen auf verschiedene Arten formalisiert werden. In DL gibt es drei Hauptmethoden: <i>Erkennung</i> ( <abbr title="Rechtecke, die Objekte begrenzen">Kästchen mit</abbr> Objekten), <i>Bewertung der Körperhaltung</i> (Schlüsselpunkte von Objekten) und <i>Segmentierung</i> („Masken“ von Objekten).  <abbr title="Rechtecke, die Objekte begrenzen"><b>Lassen Sie uns nun</b></abbr> über die Vorhersage präziser <abbr title="Rechtecke, die Objekte begrenzen"><b>Begrenzungsrahmen</b></abbr> , Punkte und Segmentierung sprechen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ki/wt/xm/kiwtxmvvhmwsvqn3_5dovlmp8w8.jpeg" width="500"></div><br>  Grundsätzlich sind Erkennungsdatensätze mit Kästchen im Format gekennzeichnet: „Koordinaten der oberen linken und unteren rechten Ecke für jedes Objekt in jedem Bild“ (dieses Format wird auch als <abbr title="&quot;Tlbr&quot;">oben links, unten rechts bezeichnet</abbr> ), und die meisten neuronalen Netzwerkansätze sagen diese Koordinaten voraus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uo/zs/tj/uozstjspdifxpslvqyuurauxs2g.png" width="500"></div><br><div class="spoiler">  <b class="spoiler_title">Informationen zu Datensätzen und Metriken im Erkennungsproblem</b> <div class="spoiler_text">  Nach dem Festlegen der Aufgabe ist es am besten zu sehen, welche Daten für das Training verfügbar sind und welche Metriken zur Messung der Qualität verwendet werden.  Darüber spreche ich langsam in der ersten Hälfte der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">13. Vorlesung der Deep Learning School</a> (bei x2.0 ist es die höchste). <br></div></div><br>  Bevor wir uns mit den Arten neuronaler Netze zur Erkennung befassen, überlegen wir uns gemeinsam, wie das Problem der Erkennung von Bildern gelöst werden kann.  Wenn wir ein bestimmtes Objekt im Bild finden möchten, wissen wir wahrscheinlich ungefähr, wie es aussieht und welchen Bereich das Bild einnehmen soll (obwohl es sich ändern kann). <br><br><div class="spoiler">  <b class="spoiler_title">Erkennung von Grund auf neu erfinden</b> <div class="spoiler_text">  Der naive und einfachste Ansatz wäre, einfach einen "Vorlagensuch" -Algorithmus zu erstellen: Lassen Sie das Bild 100 x 100 Pixel groß sein, und wir suchen nach einem Fußball.  Es sei ein Kugelmuster von 20x20 Pixel.  Nehmen Sie diese Vorlage und wir werden sie wie eine Faltung im gesamten Bild durchgehen und den Unterschied von Pixel zu Pixel zählen.  So funktioniert <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Template Matching</a> (häufig wird eine Art Korrelation anstelle eines pixelweisen Unterschieds verwendet). <br><br>  Wenn es keine Vorlage gibt, aber einen Klassifikator für neuronale Netze, können wir dies tun: Wir gehen durch ein Fenster fester Größe im Bild und sagen die Klasse des aktuellen Bildbereichs voraus.  Dann sagen wir einfach, dass die wahrscheinlichsten Bereiche der Objekte diejenigen sind, in denen der Klassifikator sicher geantwortet hat.  Auf diese Weise können wir das Problem lösen, dass das Objekt anders aussieht (da es darauf trainiert wurde, eine sehr unterschiedliche Stichprobe zu klassifizieren). <br><br>  Aber dann taucht ein Problem auf - die Objekte auf den Bildern haben unterschiedliche Größen.  Der gleiche Fußball kann sich in der gesamten Höhe / Breite des Bildes befinden oder weit am Ziel liegen und nur 10 bis 20 Pixel von 1000 nehmen. Ich möchte den Brute-Force-Algorithmus schreiben: Wir durchlaufen einfach die Fenstergrößen.  Angenommen, wir haben 100x200 Pixel, dann gehen wir zu einem 2x2-, 2X3-, 3x2-, 2x4-, 4x2-, 3x3-Fenster ..., 3x4, 4x3 ... Ich denke, Sie verstehen, dass die Anzahl der möglichen Fenster 100 * 200 beträgt und jedes von uns durch das Bild geht Durchführen von Klassifizierungsoperationen (100-W_window) * (200-H_window), was viel Zeit in Anspruch nimmt.  Ich fürchte, wir werden nicht warten, bis ein solcher Algorithmus funktioniert. <br><br>  Sie können natürlich je nach Objekt die charakteristischsten Fenster auswählen, aber es funktioniert auch sehr lange, und wenn es schnell ist, ist es unwahrscheinlich, dass es genau ist - in realen Anwendungen gibt es wahnsinnig viele Variationen in der Größe von Objekten in den Bildern. <br></div></div><br>  Außerdem werde ich mich manchmal auf eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neue Überprüfung des Erkennungsbereichs ab Januar 2019 verlassen</a> (Bilder werden auch davon stammen).  Dies ist nur ein Muss, wenn Sie schnell einen möglichst umfassenden Einblick in DL bei der Erkennung erhalten möchten. <br><br>  Einer der ersten Artikel zur Erkennung und Lokalisierung mit CNN war <a href="">Overfeat</a> .  Die Autoren behaupten, dass sie zuerst ein neuronales Netzwerk zur Erkennung in ImageNet verwendet haben, um das Problem neu zu formulieren und den Verlust zu ändern.  Der Ansatz war übrigens fast durchgehend (siehe unten das Überfeature-Schema). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8o/5v/tv/8o5vtvhgukkn7frba0nx8yltfis.png" width="700"></div><br>  Die nächste wichtige Architektur war das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf</a> <b>Regionen</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">basierende Convolutional Neural Network</a> ( <b>RCNN</b> ), das 2014 von Forschern von <abbr>FAIR</abbr> erfunden wurde.  Das Wesentliche ist, dass es zunächst viele der sogenannten „Regionen von Interesse“ (RoIs) vorhersagt, in denen sich möglicherweise Objekte befinden können (mithilfe des Selective Search-Algorithmus), diese klassifiziert und die Koordinaten der Boxen mithilfe von CNN verfeinert. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2r/2p/kp/2r2pkpcoysglv4z_v-ll_y14mqw.png" width="700"></div><br>  Eine solche Pipeline hat zwar das gesamte System verlangsamt, da wir jede Region durch das neuronale Netzwerk geführt haben (wir haben tausende Male weitergeleitet).  Ein Jahr später rüstete derselbe FAIR Ross Girshick RCNN auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fast-RCNN auf</a> .  Hier bestand die Idee darin, die selektive Suche und die Netzwerkvorhersage auszutauschen: Zuerst leiten wir das gesamte Bild durch ein vorab trainiertes neuronales Netzwerk und dann prognostizieren wir interessierende Regionen über die vom Backbone-Netzwerk ausgegebene Feature-Map (z. B. unter Verwendung derselben selektiven Suche, aber es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">andere Algorithmen</a> ).  Es war immer noch ziemlich langsam, viel langsamer als in Echtzeit (im Moment gehen wir davon aus, dass Echtzeit weniger als 40 Millisekunden pro Bild beträgt). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tn/uc/d-/tnucd-y6i7tr4edgjeudtrsj16u.png" width="700"></div><br>  Die Geschwindigkeit wurde vor allem nicht von CNN, sondern vom Box-Generierungsalgorithmus selbst beeinflusst. Daher wurde beschlossen, ihn durch ein zweites neuronales Netzwerk zu ersetzen - das Region Proposal Network ( <b>RPN</b> ) -, das trainiert wird, um die interessierenden Regionen von Objekten vorherzusagen.  So <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erschien Faster-RCNN</a> (ja, sie haben offensichtlich lange nicht an den Namen gedacht).  Schema: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6v/h_/ye/6vh_yee2zrsflyhh8jdvtm_bbgy.png" width="700"></div><br>  Dann gab es eine weitere Verbesserung in Form von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">R-FCN</a> , wir werden nicht im Detail darüber sprechen, aber ich möchte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mask-RCNN erwähnen</a> .  Mask-RCNN ist ein einzigartiges Netzwerk, das erste neuronale Netzwerk, das gleichzeitig das <b>Problem der Erkennung und</b> Instanzsegmentierung löst. Es sagt die genauen Masken (Silhouetten) von Objekten in Begrenzungsrahmen voraus.  Ihre Idee ist eigentlich recht einfach: Es gibt zwei Zweige: zur Erkennung und zur Segmentierung, und Sie müssen das Netzwerk für beide Aufgaben gleichzeitig trainieren.  Die Hauptsache ist, Daten markiert zu haben.  Mask-RCNN selbst ist Faster-RCNN sehr ähnlich: Das Backbone ist das gleiche, aber am Ende gibt es zwei <b>„Köpfe“</b> (wie die <b>letzten Schichten des</b> neuronalen Netzwerks oft genannt werden) für zwei verschiedene Aufgaben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7w/ig/hq/7wighq6ox7tptik5f_7d7cez2hg.png" width="700"></div><br>  Dies waren die sogenannten <b>zweistufigen</b> (oder <b>regionalen</b> ) Ansätze.  Parallel dazu wurden Analoga für die DL-Detektion entwickelt - <b>einstufige</b> Ansätze.  Dazu gehören neuronale Netze wie: SSD (Single-Shot Detector), YOLO (You Only Look Once), DSOD (Deeply Supervised Object Detector), RFBNet (Receptive Field Block Network) und viele andere (siehe Karte unten) von <a href="">dieses Repository</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/lc/-y/uelc-yeav4_avjdkycglf9uwrmm.png" width="750"></div><br>  Einstufige Ansätze verwenden im Gegensatz zu zweistufigen Ansätzen keinen separaten Algorithmus zum Erzeugen von Boxen, sondern sagen einfach mehrere Boxkoordinaten für jede Feature-Map voraus, die von einem Faltungs-Neuronalen Netzwerk erzeugt wird.  YOLO verhält sich ähnlich, SSD ist etwas anders, aber es gibt nur eine Idee: Eine 1x1-Faltung sagt viele Zahlen aus den empfangenen Feature-Maps detailliert voraus, wir sind uns jedoch im Voraus einig, welche Zahl dies bedeutet. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qj/ml/w_/qjmlw_ympdcib6jkpfdjdvcirdy.png" width="600"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jr/i3/oy/jri3oymb48sv5vq9dwwdxbndszg.png" width="600"></div><br>  Zum Beispiel sagen wir aus einer Feature-Map voraus, dass die Größe von 13x13x256 eine Feature-Map mit 13x13x (4 * (5 + 80)) Zahlen ist, wobei wir in der Tiefe 85 Zahlen für 4 Boxen vorhersagen: Die ersten 4 Zahlen in der Sequenz sind immer die Koordinaten der Box, die 5 .. - Vertrauen in das Boxen und 80 Zahlen - die Wahrscheinlichkeiten jeder der Klassen (Klassifikation).  Dies ist notwendig, um dann die notwendigen Zahlen den notwendigen Verlusten zu unterwerfen und das neuronale Netz richtig zu trainieren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ao/xi/o2/aoxio2rty3hgpu2f8mduomvs9nu.png" width="800"></div><br>  Ich möchte darauf aufmerksam machen, dass die Qualität der Arbeit des Detektors von der Qualität des neuronalen Netzwerks abhängt, um Merkmale ( <b>dh ein neuronales Backbone-Netzwerk</b> ) zu extrahieren.  Normalerweise wird diese Rolle von einer der Architekturen gespielt, über die ich in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">früheren Artikel gesprochen habe</a> (ResNet, SENet usw.), aber manchmal entwickeln Autoren ihre eigenen optimaleren Architekturen (z. B. Darknet-53 in YOLOv3) oder Modifikationen (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Feature Pyramid Pooling)</a> (FPN)). <br><br>  Ich stelle erneut fest, dass wir das Netzwerk gleichzeitig auf Klassifizierung und Regression trainieren.  In der Community wird dies als Verlust mehrerer Aufgaben bezeichnet: Die Summe der Verluste für mehrere Aufgaben (mit einigen Koeffizienten) wird in einem Verlust angezeigt. <br><br><div class="spoiler">  <b class="spoiler_title">Nachrichten mit führendem Multitask-Verlust</b> <div class="spoiler_text">  Bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Machines Can See 2019 verwendete</a> einer der Redner den Multi-Task-Verlust für 7 Aufgaben gleichzeitig <s>, Carl</s> .  Es stellte sich heraus, dass einige Aufgaben ursprünglich als Gegengewicht zueinander gesetzt wurden und ein „Konflikt“ entstand, der das Netzwerk daran hinderte, besser zu lernen, als wenn es für jede Aufgabe einzeln trainiert würde.  Schlussfolgerung: Wenn Sie den Verlust mehrerer Aufgaben verwenden, stellen Sie sicher, dass dieselben Aufgaben nicht mit der Anweisung in Konflikt stehen (z. B. kann die Vorhersage der Grenzen von Objekten und ihrer internen Segmentierung sich gegenseitig stören, da diese Dinge auf unterschiedlichen Vorzeichen im Netzwerk beruhen können).  Der Autor hat dies umgangen, indem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">er für jede Aufgabe separate Squeeze-and-Excitation-Blöcke hinzugefügt hat</a> . <br></div></div><br>  Kürzlich erschienen Artikel aus dem Jahr 2019, in denen die Autoren ein noch besseres Geschwindigkeits- / Genauigkeitsverhältnis in der Erkennungsaufgabe unter Verwendung einer <b>punktbasierten Box-Vorhersage</b> deklarieren.  Ich spreche von den Artikeln <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">„Objekte als Punkte“</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">„CornerNet-Lite“</a> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ExtremeNet</a> ist eine Modifikation von CornerNet.  Es scheint, dass sie jetzt bei der Erkennung mithilfe neuronaler Netze als SOTA bezeichnet werden können (dies ist jedoch nicht genau). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wk/qf/mr/wkqfmrenwm3u5f6c6bzinjcffga.png" width="900"></div><br>  Wenn meine Erklärung der Detektoren plötzlich immer noch chaotisch und unverständlich schien, diskutiere ich sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserem Video</a> langsam.  Vielleicht solltest du es zuerst sehen. <br><br>  Im Folgenden habe ich Tabellen neuronaler Netze zur Erkennung mit Links zum Code und einer kurzen Beschreibung der Chips jedes Netzes angegeben.  Ich habe versucht, nur die Netzwerke zu sammeln, die wirklich wichtig sind (zumindest ihre Ideen), um heute eine gute Vorstellung von der Objekterkennung zu haben: <br><br><div class="spoiler">  <b class="spoiler_title">Neuronale Netzdetektoren (zweistufig)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schlüsselidee </th><th>  Code </th></tr><tr><td>  2013-2014 </td><td>  <a href="">RCNN</a> </td><td>  Erzeugung von Regionen von Interesse und Vorhersage des neuronalen Netzwerks von Klassen in ihnen </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fast-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Führen Sie das Bild zuerst durch das Netzwerk und generieren Sie dann interessierende Regionen</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schneller-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verwenden Sie RPN, um interessierende Regionen zu generieren</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">R-FCN</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vollständig faltungsorientierter Ansatz, anstatt interessierende Regionen zu erzeugen</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mask-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zwei „Köpfe“ zum gleichzeitigen Lösen von zwei Aufgaben, RoI-Align</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Keras, TF</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Argumentation-RCNN</a> </td><td>  Verbesserung der Qualität von RCNN durch Erstellung eines Diagramms semantischer Beziehungen von Objekten </td><td>  --- ---. </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Neuronale Netzdetektoren (einstufig)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schlüsselidee </th><th>  Code </th></tr><tr><td>  2013-2014 </td><td>  <a href="">Überfeature</a> </td><td>  einer der ersten Detektoren für neuronale Netze </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">C ++ (mit Wrappern für andere Sprachen)</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SSD</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sehr flexibler einstufiger Ansatz, der jetzt in vielen Anwendungen verwendet wird</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yolo</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine Idee ähnlich der von SSD entwickelt sich parallel und ist nicht weniger beliebt (es gibt neue Versionen).</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">C ++</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">YOLOv2 (auch bekannt als YOLO9000)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine Reihe von Verbesserungen für YOLO</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">YOLOv3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine Reihe von Verbesserungen für YOLOv2</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2017-2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DSOD</a> </td><td>  Deep Supervision Idea und DenseNet Ideas </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2017-2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RFBNet</a> </td><td>  Faltungsfilter werden basierend auf der Struktur des menschlichen visuellen Systems ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RF-</a> Block) genau ausgewählt. </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Neuronale Netzdetektoren (Sonstiges)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schlüsselidee </th><th>  Code </th></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RetinaNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">spezieller Fokusverlust zur Lösung des Problems des Klassenungleichgewichts</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Keras</a> </td></tr><tr><td>  2014-2015 </td><td>  <a href="">SPP</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Modul, mit dem Sie effektiv mit Bildern unterschiedlicher Größe arbeiten können</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Keras</a> </td></tr><tr><td>  2016-2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FPN</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verfügen über Pyramiden zur besseren Erkennung von Objekten unterschiedlicher Größe</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NAS-FPN</a> </td><td>  Finden der besten FPN mit der Suche nach neuronaler Architektur </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Daedalus</a> </td><td>  wie man den Detektor mit einem gegnerischen Angriff bricht </td><td>  --- ---. </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Neuronale Netzdetektoren (punktbasiert)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schlüsselidee </th><th>  Code </th></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Centernet</a> </td><td>  Ein neuer Ansatz zur Erkennung, mit dem das Problem der gleichzeitigen Suche nach Punkten, Feldern und 3D-Feldern schnell und effizient gelöst werden kann </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cornernet</a> </td><td>  Vorhersage von Kästchen basierend auf Paaren von Eckpunkten </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CornerNet-Lite</a> </td><td>  beschleunigtes Cornernet </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ExtremeNet</a> </td><td>  Vorhersage von „extremen“ Punkten von Objekten (geometrisch genaue Grenzen) </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Um zu verstehen, wie die Geschwindigkeit / Qualität jeder Architektur korreliert, können Sie sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Test</a> oder seine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">populärere Version</a> ansehen. <br><br>  Architektur ist in Ordnung, aber Erkennung ist in erster Linie eine praktische Aufgabe.  "Haben Sie nicht hundert Netzwerke, aber mindestens eines funktioniert" - das ist meine Botschaft.  Es gibt Links zum Code in der obigen Tabelle, aber ich persönlich stoße selten darauf, Detektoren direkt aus Repositorys zu starten (zumindest mit dem Ziel einer weiteren Bereitstellung in der Produktion).  Am häufigsten wird hierfür eine Bibliothek verwendet, beispielsweise die TensorFlow-Objekterkennungs-API (siehe den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">praktischen Teil meiner Lektion</a> ) oder eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bibliothek von Forschern von CUHK</a> .  Ich mache Sie auf einen weiteren Supertisch aufmerksam (Sie mögen sie, oder?): <br><br><div class="spoiler">  <b class="spoiler_title">Bibliotheken zum Ausführen von Erkennungsmodellen</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Titel </th><th>  Die Autoren </th><th>  Beschreibung </th><th>  Implementierte neuronale Netze </th><th>  Framework </th></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Detectron</a> </td><td>  Facebook AI Research </td><td>  Facebook-Repository mit verschiedenen Modellcodes zur Erkennung und Bewertung der Körperhaltung </td><td>  Alle regional </td><td>  Caffe2 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TF-Objekterkennungs-API</a> </td><td>  TensorFlow-Team </td><td>  Viele gebrauchsfertige Modelle (Gewichte sind angegeben) </td><td>  Alle regionalen und SSDs (mit unterschiedlichen Backbones) </td><td>  Tensorflow </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Darkflow</a> </td><td>  thtrieu </td><td>  Gebrauchsfertige YOLO- und YOLOv2-Implementierungen </td><td>  Alle YOLO-Typen (mit Änderungen) außer YOLOv3 </td><td>  Tensorflow </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mmdetektion</a> </td><td>  Öffnen Sie MMLab (CUHK) </td><td>  Eine große Anzahl von Detektoren auf PyTorch finden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sie in ihrem Artikel</a> </td><td>  Fast alle Modelle außer der YOLO-Familie </td><td>  Pytorch </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Darknet (modifiziert)</a> </td><td>  AlexAB </td><td>  Bequeme Implementierung von YOLOv3 mit vielen Verbesserungen am <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ursprünglichen Repository</a> </td><td>  YOLOv3 </td><td>  C ++ </td></tr></tbody></table></div><br></div></div><br>  Oft müssen Sie ein Objekt nur einer Klasse erkennen, das jedoch spezifisch und sehr variabel ist.  Zum Beispiel, um alle Gesichter auf dem Foto zu erkennen (zur weiteren Überprüfung / Zählung von Personen), um ganze Personen zu erkennen (zur erneuten Identifizierung / Zählung / Verfolgung) oder um Text in der Szene zu erkennen (zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OCR</a> / Übersetzung von Wörtern auf dem Foto).  Im Allgemeinen funktioniert der „normale“ Erkennungsansatz hier bis zu einem gewissen Grad, aber jede dieser Unteraufgaben hat ihre eigenen Tricks, um die Qualität zu verbessern. <br><br><a name="2"></a><h2>  Gesichtserkennung: Nicht gefangen - kein Dieb </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ic/ul/rp/iculrpbc7niyrdxg1yk_8r82nsw.jpeg" width="700"></div><br>  Hier tritt eine gewisse Spezifität auf, da Gesichter häufig einen relativ kleinen Teil des Bildes einnehmen.  Außerdem schauen die Leute nicht immer in die Kamera, oft ist das Gesicht nur von der Seite sichtbar.  Einer der ersten Ansätze zur Gesichtserkennung war der berühmte Viola-Jones-Detektor, der auf Haar-Kaskaden basiert und bereits 2001 erfunden wurde. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hc/tf/zn/hctfzn0xudbedi_aymhmlcwkamu.png" width="400"></div><br>  Neuronale Netze <s>waren damals nicht in Mode,</s> sie waren immer noch nicht so stark in der Vision, aber der gute alte handgefertigte Ansatz hat seine Aufgabe erfüllt.  Es wurden verschiedene Arten von speziellen Filtermasken aktiv verwendet, die dazu beitrugen, Gesichtsbereiche aus dem Bild und ihren Zeichen zu extrahieren, und diese Zeichen wurden dann dem AdaBoost-Klassifikator übermittelt.  Übrigens funktioniert diese Methode wirklich gut und jetzt ist sie schnell genug und startet sofort <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mit OpenCV</a> .  Der Nachteil dieses Detektors besteht darin, dass er nur Gesichter sieht, die frontal an der Kamera angebracht sind.  Man muss sich nur ein wenig umdrehen und die Stabilität der Erkennung wird verletzt. <br><br>  Für solch komplexere Fälle können Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dlib verwenden</a> .  Dies ist C ++ - eine Bibliothek, in der viele Bildverarbeitungsalgorithmen implementiert sind, auch zur Gesichtserkennung. <br><br>  Von den neuronalen Netzwerkansätzen bei der Gesichtserkennung ist das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kaskadierte Multitask-CNN (MTCNN)</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MatLab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TensorFlow</a> ) von besonderer Bedeutung.  Im Allgemeinen wird es jetzt aktiv verwendet (im selben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Facenet</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/80/wn/vu/80wnvuf59poodmodzswcgjlyjt4.jpeg" width="400"></div><br>  Die Idee von MTCNN besteht darin, drei neuronale Netze nacheinander (daher eine <b>„Kaskade“</b> ) zu verwenden, um die Position eines Gesichts und seiner singulären Punkte vorherzusagen.  In diesem Fall gibt es genau 5 spezielle Punkte im Gesicht: das linke Auge, das rechte Auge, den linken Rand der Lippen, den rechten Rand der Lippen und der Nase.  Das erste neuronale Netzwerk aus der Kaskade ( <abbr title="Proposal Net">P-Net</abbr> ) wird verwendet, um potenzielle Regionen des Gesichts zu erzeugen.  Das zweite ( <abbr title="Refine Net">R-Net</abbr> ) - um die Koordinaten der empfangenen Boxen zu verbessern.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das dritte ( </font></font><abbr title="Facial Landmarks Net"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O-Net</font></font></abbr><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) neuronale Netzwerk regressiert erneut die Koordinaten der Boxen und sagt zusätzlich 5 Schlüsselpunkte des Gesichts voraus. </font><font style="vertical-align: inherit;">Dieses Netzwerk ist eine Mehrfachaufgabe, da drei Aufgaben gelöst sind: Regression von Boxpunkten, Klassifizierung von Gesicht / Nicht-Gesicht für jede Box und Regression von Gesichtspunkten. </font><font style="vertical-align: inherit;">Darüber hinaus erledigt MTCNN alles in Echtzeit, dh es benötigt weniger als 40 ms pro Bild.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/iz/d5/5eizd5lwag9umfo1ccypep42eik.jpeg" width="800"></div><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie lesen Sie immer noch keine Artikel mit ArXiv selbst?</font></font></b> <div class="spoiler_text"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Fall empfehle ich, den </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Originalartikel über MTCNN</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zu lesen </font><font style="vertical-align: inherit;">, wenn Sie bereits Hintergrundinformationen zu Faltungsnetzwerken haben. </font><font style="vertical-align: inherit;">Dieser Artikel umfasst nur </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5 Seiten</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , enthält jedoch alle Informationen, die Sie zum Verständnis des Ansatzes benötigen. </font><font style="vertical-align: inherit;">Probieren Sie es aus, es wird sich verzögern :)</font></font><br></div></div><br>   State-of-the-Art   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dual Shot Face Detector (DSFD)</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FaceBoxes</a> . FaceBoxes      CPU (!),  DSFD    (   2019 ). DSFD  ,  MTCNN,          ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dilated convolutions</a> ),        . ,  dilated convolutions            .    DSFD (,   ?). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xv/or/it/xvoritceua0_ocjteyvm4xvisbq.jpeg"></div><br>     <b></b> ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> ,      . <br><br><a name="3"></a><h2>  :  ( )  </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qn/v9/wo/qnv9woeqru3dioeennrkvalkjqg.png" width="500"></div><br>     .  , ,   bounding box',    (   ),    .     ,   , ,       recognition-,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">    </a> . <br><br>       bounding box',        ,    ( ).     , , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">EAST-</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/j3/hd/bj/j3hdbj-xozs4voec8ekiz4k1eo0.png" width="500"></div><br><br>  EAST-  ,      ,    : <br><br><ol><li> Text Score Map' (     ) </li><li>     </li><li>        </li></ol><br>  ,      (  ),  .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv-</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dj/vt/uu/djvtuu36dzinoratlumncsgb5t8.png" width="700"></div><br><br>    (    )  ,    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TextBoxes++</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SegLinks</a> ,  EAST,   ,    . <br><br>         ,  <b></b>     .       —    .     ,      ,   ,          . , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MORAN</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  PyTorch</a> )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ASTER</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  TensorFlow</a> )     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vt/ot/nc/vtotncz1d1zhhsiytuzmarta9ta.png" width="700"></div><br><br>    - ,          : CNN  RNN.       ,     .    MORAN':     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hu/qx/_2/huqx_2jakjcggsgacvhj9a8vce4.png" width="300"></div><br>       EAST', -       ,           .  ,           ,     . <br><br>   <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a></b>   ,  / .      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spatial Transformet Network (STN)</a> ,              (,       ,    ).   / STN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6y/z1/p9/6yz1p942ensgepwirqrtzpcnb7q.jpeg" width="700"></div><br>   STN    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> (  ,  )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  PyTorch</a> . <br><br>  MORAN (     )    —      ,        <b> </b>  x   y,     ,      .    <i><abbr title="Korrektur, Korrektur">rectification</abbr></i> ,         ( <i>rectifier'</i> ).         : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/n4/az/du/n4azduih5p7wd9sx-tqhpgy-p20.png" width="300"></div><br>  Zusätzlich zu den Ansätzen zur „modularen“ Texterkennung (Erkennungsnetzwerk -&gt; Erkennungsnetzwerk) gibt es jedoch eine End-to-End-Architektur: Die Eingabe ist ein Bild, und die Ausgabe ist eine Erkennung, und der Text wird in ihnen erkannt.  Und das alles ist eine einzige Pipeline, die beide Aufgaben gleichzeitig lernt.  In dieser Richtung gibt es die beeindruckende Arbeit des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fast Oriented Text Spotting mit einem Unified Network ( <b>FOTS</b> ) ( <b>PyTorch-</b></a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> ), bei der die Autoren auch feststellen, dass der End-to-End-Ansatz doppelt so schnell ist wie „Erkennung + Erkennung“.  Unten sehen Sie das FOTS-Diagramm für neuronale Netze. Eine besondere Rolle spielt der RoiRotate-Block, aufgrund dessen es möglich ist, Gradienten aus dem Netzwerk zur Erkennung auf das neuronale Netzwerk zur Erkennung zu übertragen (dies ist wirklich komplizierter als es scheint). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/a5/xh/ch/a5xhchryrwf-zpfudielmab9pqu.png" width="800"></div><br>  Übrigens findet jedes Jahr die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICDAR-</a> Konferenz <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">statt</a> , an der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehrere Wettbewerbe</a> zur Erkennung von Text in verschiedenen Bildern teilnehmen. <br><br><h3>  Aktuelle Probleme bei der Erkennung </h3><br>  Meiner Meinung nach ist das Hauptproblem bei der Erkennung jetzt nicht die Qualität des Detektormodells, sondern die Daten: Die Markierung ist normalerweise lang und teuer, insbesondere wenn viele Klassen erkannt werden müssen (aber es gibt übrigens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein Beispiel für eine Lösung</a> für 500 Klassen).  Daher widmen sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viele Arbeiten</a> nun der Erzeugung der plausibelsten Daten „synthetisch“ und dem Erhalt von Markups „kostenlos“.  Unten ist ein Bild aus <s>meinem Diplom eines</s> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikels von Nvidia</a> , der sich speziell mit der Erzeugung synthetischer Daten befasst. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6k/mn/wh/6kmnwhvc1pgahzwtjloa4dy9sbm.png" width="800"></div><br>  Trotzdem ist es großartig, dass wir jetzt sicher sagen können, wo auf dem Bild was sein soll.  Und wenn wir zum Beispiel die Menge von etwas auf dem Rahmen berechnen wollen, dann reicht es aus, dies zu erkennen und die Anzahl der Kästchen herauszugeben.  Bei der Erkennung von Personen funktioniert das übliche YOLO auch gut, nur die Hauptsache ist, viele Daten einzureichen.  Der gleiche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Darkflow ist</a> geeignet, und die Klasse „Mensch“ ist in fast allen wichtigen Erkennungsdatensätzen enthalten.  Wenn wir also mit der Kamera die Anzahl der Personen zählen möchten, die beispielsweise an einem Tag vorbeigekommen sind, oder die Anzahl der Waren, die eine Person in einem Geschäft aufgenommen hat, erkennen wir einfach die Menge und geben sie aus ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/m2/ts/pl/m2tsplbgrnmoauvlghy8ivq2jdm.jpeg" width="700"></div><br>  Hör auf  Wenn wir jedoch Personen in jedem Bild von der Kamera erkennen möchten, können wir ihre Anzahl in einem Bild und in zwei Bildern berechnen - nicht mehr, da wir nicht sagen können, wo genau sich welche Person befindet.  Wir brauchen einen Algorithmus, mit dem wir genau eindeutige Personen im Videostream zählen können.  Es mag ein Algorithmus zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erneuten Identifizierung sein</a> , aber wenn es um Video und Erkennung geht, ist es eine Sünde, keine Tracking-Algorithmen zu verwenden. <br><br><a name="4"></a><h3>  Video und Tracking: in einem Stream </h3><br>  Bisher haben wir nur über Aufgaben in Bildern gesprochen, aber das Interessanteste passiert im Video.  Um die gleiche Erkennung von Handlungen zu lösen, müssen wir nicht nur die sogenannte <i>räumliche</i> Komponente verwenden, sondern auch die <i>zeitliche</i> , da Video eine <i>zeitliche</i> Folge von Bildern ist. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qm/da/pa/qmdapao0kcytnxhorj7htm6susy.jpeg" width="700"></div><br>  Tracking ist ein Analogon zur Bilderkennung, jedoch für Video.  Das heißt, wir möchten dem Netzwerk beibringen, nicht das Boxen im Bild vorherzusagen, sondern ein Tracklet in der Zeit (was im Wesentlichen eine Folge von Boxen ist).  Unten sehen Sie ein Beispiel für ein Bild, das die „Schwänze“ zeigt - die Spuren dieser Personen im Video. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xz/yv/c_/xzyvc_oumhrnf_-bmf8gz5l_hli.png" width="600"></div><br>  Lassen Sie uns darüber nachdenken, wie Sie das Tracking-Problem lösen können.  Es gebe ein Video und seine Bilder Nr. 1 und Nr. 2.  Betrachten wir bisher nur ein Objekt - wir verfolgen einen Ball.  Bei Bild 1 können wir einen Detektor verwenden, um es zu erkennen.  Beim zweiten können wir auch einen Ball erkennen, und wenn er alleine da ist, ist alles in Ordnung: Wir sagen, dass das Boxen auf dem vorherigen Frame das Boxen des gleichen Balls wie auf Frame # 2 ist.  Sie können auch mit den verbleibenden Frames unterhalb des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GIFs</a> des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pyimagesearch-</a> Vision- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurses fortfahren</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_w/et/ak/_wetakdpybemefhert6cxslqaju.gif" width="600"></div><br>  Übrigens, um Zeit zu sparen, können wir das neuronale Netzwerk nicht im zweiten Frame starten, sondern einfach die Box des Balls aus dem ersten Frame „herausschneiden“ und im zweiten Frame oder Pixel für Pixel nach genau der gleichen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Korrelation</a> suchen.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Korrelations-Tracker</a> verwenden diesen Ansatz. Sie gelten als einfach und mehr oder weniger zuverlässig, wenn es sich um einfache Fälle wie „Verfolgen eines Balls vor der Kamera in einem leeren Raum“ handelt.  Diese Aufgabe wird auch als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">visuelle Objektverfolgung bezeichnet</a> .  Unten sehen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein Beispiel für die Arbeit des</a> Korrelations-Trackers am Beispiel einer Person. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cv/fp/qd/cvfpqdd5ghelxfpucxrejxq6vpm.gif" width="600"></div><br>  Wenn es jedoch mehrere Erkennungen / Personen gibt, müssen Sie in der Lage sein, die Felder aus Bild 1 und Bild 2 abzugleichen.  Die erste Idee, die mir in den Sinn kommt, ist zu versuchen, die Box mit der Box <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">abzustimmen</a> , die den größten Schnittbereich ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">IoU</a> ) aufweist.  Richtig, bei mehreren überlappenden Erkennungen ist ein solcher Tracker instabil, sodass Sie noch mehr Informationen verwenden müssen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qx/i2/t1/qxi2t1ejpnir0h52ip-23zpf4ti.png" width="600"></div><br>  Der Ansatz mit IoU basiert nur auf den <i>„geometrischen“</i> Erkennungszeichen, dh es wird lediglich versucht, diese anhand der Nähe zu Frames zu vergleichen.  Wir verfügen jedoch über ein ganzes Bild (in diesem Fall sogar über zwei), und wir können die Tatsache nutzen, dass sich in diesen Erkennungen <i>„visuelle“ Zeichen befinden</i> .  Außerdem haben wir eine Erkennungshistorie für jede Person, die es uns ermöglicht, ihre nächste Position auf der Grundlage von Geschwindigkeit und Bewegungsrichtung genauer vorherzusagen. Dies kann bedingt als <i>„physische“ Zeichen bezeichnet werden</i> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mc/ay/se/mcaysee7xq25-j6hnvjalhfy5z0.gif" width="700"></div><br>  Einer der ersten Echtzeit-Tracker, der absolut zuverlässig und in der Lage war, schwierige Situationen zu bewältigen, wurde 2016 veröffentlicht. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Simple Online und Realtime Traker (SORT)</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-Code</a> ).  SORT verwendete keine visuellen Zeichen und neuronalen Netze, sondern schätzte nur eine Reihe von Parametern jeder Box in jedem Frame: die aktuelle Geschwindigkeit (x und y getrennt) und Größe (Höhe und Breite).  Das Seitenverhältnis einer Box wird immer von der ersten Erkennung dieser Box an übernommen.  Ferner werden die Geschwindigkeiten unter Verwendung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kalman-Filtern</a> vorhergesagt (sie sind in der Welt der Signalverarbeitung im Allgemeinen gut und leicht), die Schnittmatrix der Boxen durch IoU wird erstellt und die Erkennungen werden durch den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ungarischen Algorithmus</a> zugewiesen. <br><br>  Wenn es Ihnen so vorkommt, als sei die Mathematik schon ein bisschen viel geworden, dann wird in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Artikel</a> alles auf zugängliche Weise erklärt (das ist mittel :). <br><br>  Bereits 2017 wurde eine Modifikation von SORT in Form von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepSORT</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code für TensorFlow</a> ) veröffentlicht.  DeepSORT hat bereits begonnen, das neuronale Netzwerk zum Extrahieren visueller Zeichen zu verwenden, um Kollisionen aufzulösen.  Die Qualität des Trackings ist gewachsen - nicht umsonst gilt es heute als einer der besten Online-Tracker. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/it/ny/15itnyht32oes3n-keaqk36jnpq.gif" width="800"></div><br>  Das Gebiet der Verfolgung entwickelt sich tatsächlich aktiv: Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tracker mit siamesischen neuronalen Netzen</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tracker mit RNN</a> .  Halten Sie Ihren Finger am Puls der Zeit, denn an jedem Tag kann eine noch genauere und schnellere Architektur herauskommen (oder ist bereits herausgekommen).  Übrigens ist es sehr praktisch, solche Dinge auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PapersWithCode</a> zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verfolgen</a> . Es gibt immer Links zu Artikeln und Code für sie (falls vorhanden). <br><br><h3>  Nachwort </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jb/yh/bf/jbyhbf2ovu_ynurp_t_d9_hi4vg.jpeg" width="600"></div><br><br>  Wir haben wirklich viel erlebt und viel gelernt.  Aber Computer Vision ist ein extrem weites Gebiet, und ich bin eine extrem hartnäckige Person.  Aus diesem Grund werden wir Sie im dritten Artikel dieses Zyklus sehen (wird es der letzte sein? Wer weiß ...), in dem wir die Segmentierung, die Beurteilung der Körperhaltung, die Erkennung von Aktionen auf einem Video und die Erstellung einer Beschreibung aus einem Bild unter Verwendung neuronaler Netze ausführlicher diskutieren werden. <br><br>  PS Ich möchte Vadim Gorbatschow für seine wertvollen Ratschläge und Kommentare bei der Vorbereitung dieses und des vorherigen Artikels meinen besonderen Dank aussprechen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458190/">https://habr.com/ru/post/de458190/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458176/index.html">Die Exaflop-Barriere wird 2021 überwunden</a></li>
<li><a href="../de458180/index.html">Kea-basierter Failover-DHCP-Server</a></li>
<li><a href="../de458182/index.html">Wir lesen VKontakte über RSS</a></li>
<li><a href="../de458184/index.html">Haxe und PHP: statische Eingabe, Pfeilfunktionen, Metaprogrammierung und vieles mehr</a></li>
<li><a href="../de458186/index.html">WAL in PostgreSQL: 1. Puffercache</a></li>
<li><a href="../de458202/index.html">Schauen Sie sich einfach SObjectizer an, wenn Sie Actors oder CSP in Ihrem C ++ - Projekt verwenden möchten</a></li>
<li><a href="../de458204/index.html">So bewerten Sie die Speicherleistung unter Linux: Benchmarking mit offenen Tools</a></li>
<li><a href="../de458206/index.html">Erhabener Text 3 für das Site-Layout. Passen Sie das Erscheinungsbild an und installieren Sie Plugins. Anfängerleitfaden</a></li>
<li><a href="../de458208/index.html">Digitale Veranstaltungen in Moskau vom 01. bis 07. Juli</a></li>
<li><a href="../de458214/index.html">Pentest-Labor "Pentestit Test Lab 12" - volle Passage</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>