<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöÆ üõ∂ üì´ Redes neuronales y aprendizaje profundo, cap√≠tulo 1: uso de redes neuronales para reconocer n√∫meros escritos a mano üòª üê∑ üè∫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota 
 Aqu√≠ hay una traducci√≥n del libro en l√≠nea gratuito de Michael Nielsen, Neural Networks and Deep Learning, distribuido bajo la Licencia Creativ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neuronales y aprendizaje profundo, cap√≠tulo 1: uso de redes neuronales para reconocer n√∫meros escritos a mano</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456738/"><h3>  Nota </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/d1f/7ac/ee5/d1f7acee5600a381c43f05c7df9c439a.jpg" alt="Michael nielsen" align="left">  Aqu√≠ hay una traducci√≥n del libro en l√≠nea gratuito de Michael Nielsen, Neural Networks and Deep Learning, distribuido bajo la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Licencia Creative Commons Reconocimiento-No Comercial 3.0 Unported</a> .  La motivaci√≥n para su creaci√≥n fue la experiencia exitosa de traducir un libro de texto de programaci√≥n, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Expressive JavaScript</a> .  El libro sobre redes neuronales tambi√©n es bastante popular; los autores de art√≠culos en ingl√©s lo citan activamente.  No encontr√© sus traducciones, excepto la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">traducci√≥n del comienzo del primer cap√≠tulo con abreviaturas</a> . <br><br>  Aquellos que quieran agradecer al autor del libro pueden hacerlo en su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">p√°gina oficial</a> , mediante transferencia a trav√©s de PayPal o Bitcoin.  Para apoyar al traductor en Habr√© hay un formulario "para apoyar al autor". <br><br><div class="spoiler">  <b class="spoiler_title">Contenido</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 1: uso de redes neuronales para reconocer n√∫meros escritos a mano</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 2: c√≥mo funciona el algoritmo de retropropagaci√≥n</a> </li><li>  Cap√≠tulo 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: mejorar el m√©todo de entrenamiento de redes neuronales</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: ¬øPor qu√© la regularizaci√≥n ayuda a reducir el reciclaje?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 3: ¬øc√≥mo elegir hiperpar√°metros de red neuronal?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 4: prueba visual de que las redes neuronales son capaces de calcular cualquier funci√≥n</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 5: ¬øpor qu√© las redes neuronales profundas son tan dif√≠ciles de entrenar?</a> </li><li>  Cap√≠tulo 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: aprendizaje profundo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: progreso reciente en el reconocimiento de im√°genes</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ep√≠logo: ¬øexiste un algoritmo simple para crear inteligencia?</a> </li></ul></div></div><br><h2>  Introduccion </h2><br>  Este tutorial le informar√° en detalle sobre conceptos como: <br><br><ul><li>  Redes neuronales: un excelente paradigma de software, creado bajo la influencia de la biolog√≠a, y que permite que la computadora aprenda bas√°ndose en observaciones. </li><li>  El aprendizaje profundo es un poderoso conjunto de t√©cnicas de entrenamiento de redes neuronales. </li></ul><br>  Las redes neuronales (NS) y el aprendizaje profundo (GO) brindan hoy la mejor soluci√≥n a muchos problemas en las √°reas de reconocimiento de im√°genes, procesamiento de voz y lenguaje natural.  Este tutorial le ense√±ar√° muchos de los conceptos clave que sustentan NS y GO. <br><a name="habracut"></a><br><h2>  ¬øDe qu√© trata este libro? </h2><br>  NS es uno de los mejores paradigmas de software jam√°s inventado por el hombre.  Con un enfoque de programaci√≥n est√°ndar, le decimos a la computadora qu√© hacer, dividir las tareas grandes en muchas peque√±as y determinar con precisi√≥n las tareas que la computadora realizar√° f√°cilmente.  En el caso de la Asamblea Nacional, por el contrario, no le decimos a la computadora c√≥mo resolver el problema.  √âl mismo aprende esto sobre la base de "observaciones" de los datos, "inventando" su propia soluci√≥n al problema. <br><br>  El aprendizaje automatizado basado en datos suena prometedor.  Sin embargo, hasta 2006, no sab√≠amos c√≥mo capacitar a la Asamblea Nacional para que pudieran trascender los enfoques m√°s tradicionales, con la excepci√≥n de algunos casos especiales.  En 2006, t√©cnicas de entrenamiento de los llamados  redes neuronales profundas (GNS).  Ahora estas t√©cnicas se conocen como aprendizaje profundo (GO).  Continuaron desarroll√°ndose, y hoy GNS y GO han logrado resultados sorprendentes en muchas tareas importantes relacionadas con la visi√≥n por computadora, el reconocimiento del habla y el procesamiento del lenguaje natural.  A gran escala, est√°n siendo implementados por empresas como Google, Microsoft y Facebook. <br><br>  El prop√≥sito de este libro es ayudarlo a dominar los conceptos clave de las redes neuronales, incluidas las t√©cnicas modernas de GO.  Despu√©s de trabajar con el tutorial, escribir√° un c√≥digo que usa NS y GO para resolver problemas complejos de reconocimiento de patrones.  Tendr√° una base para usar NS y defensa civil en el enfoque para resolver sus propios problemas. <br><br><h3>  Enfoque basado en principios </h3><br>  Una de las creencias subyacentes en el libro es que es mejor adquirir una comprensi√≥n s√≥lida de los principios clave de la Asamblea Nacional y la Sociedad Civil que obtener conocimiento de una larga lista de ideas diferentes.  Si comprende bien las ideas clave, comprender√° r√°pidamente otro material nuevo.  En el lenguaje del programador, podemos decir que estudiaremos la sintaxis b√°sica, las bibliotecas y las estructuras de datos del nuevo lenguaje.  Es posible que reconozca solo una peque√±a fracci√≥n de todo el idioma (muchos idiomas tienen bibliotecas est√°ndar inmensas), sin embargo, puede comprender nuevas bibliotecas y estructuras de datos de forma r√°pida y sencilla. <br><br>  Entonces, este libro categ√≥ricamente no es material educativo sobre c√≥mo usar una biblioteca en particular para la Asamblea Nacional.  Si solo quieres aprender a trabajar con la biblioteca, ¬°no leas el libro!  Encuentre la biblioteca que necesita y trabaje con materiales de capacitaci√≥n y documentaci√≥n.  Pero tenga en cuenta: aunque este enfoque tiene la ventaja de resolver el problema instant√°neamente, si desea comprender exactamente qu√© est√° sucediendo dentro de la Asamblea Nacional, si desea dominar ideas que ser√°n relevantes en muchos a√±os, entonces no ser√° suficiente para que simplemente estudie alg√∫n tipo de biblioteca de moda  Debe comprender las ideas confiables y a largo plazo que subyacen en el trabajo de la Asamblea Nacional.  La tecnolog√≠a va y viene, y las ideas duran para siempre. <br><br><h3>  Enfoque pr√°ctico </h3><br>  Estudiaremos los principios b√°sicos con el ejemplo de una tarea espec√≠fica: ense√±ar a una computadora a reconocer n√∫meros escritos a mano.  Utilizando enfoques de programaci√≥n tradicionales, esta tarea es extremadamente dif√≠cil de resolver.  Sin embargo, podemos resolverlo bastante bien con un NS simple y varias docenas de l√≠neas de c√≥digo, sin bibliotecas especiales.  Adem√°s, mejoraremos gradualmente este programa, incluyendo constantemente m√°s y m√°s ideas clave sobre la Asamblea Nacional y la Defensa Civil. <br><br>  Este enfoque pr√°ctico significa que necesitar√° algo de experiencia en programaci√≥n.  Pero no tienes que ser un programador profesional.  Escrib√≠ el c√≥digo de Python (versi√≥n 2.7) que deber√≠a quedar claro incluso si no ha escrito programas de Python.  En el proceso de estudio, crearemos nuestra propia biblioteca para la Asamblea Nacional, que puede utilizar para experimentos y capacitaci√≥n adicional.  Todo el c√≥digo se puede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">descargar aqu√≠</a> .  Una vez terminado el libro, o en el proceso de lectura, puede elegir una de las bibliotecas m√°s completas para la Asamblea Nacional, adaptada para su uso en estos proyectos. <br><br>  Los requisitos matem√°ticos para comprender el material son bastante promedio.  La mayor√≠a de los cap√≠tulos tienen partes matem√°ticas, pero generalmente son √°lgebra elemental y gr√°ficos de funciones.  A veces uso matem√°ticas m√°s avanzadas, pero estructur√© el material para que puedas entenderlo, incluso si algunos detalles te eluden.  La mayor√≠a de las matem√°ticas se usan en el cap√≠tulo 2, que requiere un poco de matan√°lisis y √°lgebra lineal.  Para aquellos a quienes no est√°n familiarizados, comienzo el Cap√≠tulo 2 con una introducci√≥n a las matem√°ticas.  Si le resulta dif√≠cil, simplemente omita el cap√≠tulo hasta el informe.  En cualquier caso, no te preocupes por esto. <br><br>  Un libro rara vez se orienta al mismo tiempo hacia una comprensi√≥n de los principios y un enfoque pr√°ctico.  Pero creo que es mejor estudiar sobre la base de las ideas fundamentales de la Asamblea Nacional.  Escribiremos c√≥digo de trabajo, y no solo estudiaremos teor√≠a abstracta, y usted puede explorar y extender este c√≥digo.  De esta manera, comprender√° los conceptos b√°sicos, tanto la teor√≠a como la pr√°ctica, y podr√° aprender m√°s. <br><br><h3>  Ejercicios y tareas </h3><br>  Los autores de libros t√©cnicos a menudo advierten al lector que simplemente necesita completar todos los ejercicios y resolver todos los problemas.  Al leerme tales advertencias, siempre me parecen un poco extra√±as.  ¬øMe pasar√° algo malo si no realizo ejercicios y resuelvo problemas?  No por supuesto.  Ahorrar√© tiempo con una comprensi√≥n menos profunda.  A veces vale la pena.  A veces no. <br><br>  ¬øQu√© vale la pena hacer con este libro?  Le aconsejo que intente completar la mayor√≠a de los ejercicios, pero no intente resolver la mayor√≠a de las tareas. <br><br>  La mayor√≠a de los ejercicios deben completarse porque estos son controles b√°sicos para una comprensi√≥n adecuada del material.  Si no puede realizar el ejercicio con relativa facilidad, debe haberse perdido algo fundamental.  Por supuesto, si realmente est√° atascado en alg√∫n tipo de ejercicio, su√©ltelo, tal vez sea una especie de peque√±o malentendido, o tal vez haya formulado algo mal.  Pero si la mayor√≠a de los ejercicios le causan dificultades, lo m√°s probable es que necesite volver a leer el material anterior. <br><br>  Las tareas son otra cosa.  Son m√°s dif√≠ciles que los ejercicios, y con algunos tendr√°s dificultades.  Esto es molesto, pero, por supuesto, la paciencia frente a tal decepci√≥n es la √∫nica forma de comprender y absorber realmente el tema. <br><br>  Por lo tanto, no recomiendo resolver todos los problemas.  Mejor a√∫n: elige tu propio proyecto.  Es posible que desee utilizar NS para clasificar su colecci√≥n de m√∫sica.  O para predecir el valor de las acciones.  O algo mas.  Pero encuentra un proyecto interesante para ti.  Y luego puede ignorar las tareas del libro, o utilizarlas simplemente como inspiraci√≥n para trabajar en su proyecto.  Los problemas con su propio proyecto le ense√±ar√°n m√°s que trabajar con cualquier cantidad de tareas.  La participaci√≥n emocional es un factor clave en el logro del dominio. <br><br>  Por supuesto, si bien es posible que no tenga un proyecto de este tipo.  Esto es normal  Resuelve tareas para las que sientas una motivaci√≥n intr√≠nseca.  Use material del libro para ayudarlo a encontrar ideas para proyectos creativos personales. <br><br><h2>  Capitulo 1 </h2><br>  El sistema visual humano es una de las maravillas del mundo.  Considere la siguiente secuencia de n√∫meros escritos a mano: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><br>  La mayor√≠a de la gente los leer√° f√°cilmente, como 504192. Pero esta simplicidad es enga√±osa.  En cada hemisferio del cerebro, una persona tiene una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">corteza visual primaria</a> , tambi√©n conocida como V1, que contiene 140 millones de neuronas y decenas de miles de millones de conexiones entre ellas.  Al mismo tiempo, no solo V1 est√° involucrado en la visi√≥n humana, sino toda una secuencia de regiones cerebrales (V2, V3, V4 y V5) que se dedican al procesamiento de im√°genes cada vez m√°s complejo.  Llevamos en nuestras cabezas una supercomputadora sintonizada por la evoluci√≥n durante cientos de millones de a√±os, y perfectamente adaptada para comprender el mundo visible.  Reconocer n√∫meros escritos a mano no es tan f√°cil.  Es solo que nosotros, las personas, sorprendentemente, sorprendentemente bien, reconocemos lo que nuestros ojos nos muestran.  Pero casi todo este trabajo se lleva a cabo inconscientemente.  Y, por lo general, no le damos importancia a la dif√≠cil tarea que resuelven nuestros sistemas visuales. <br><br>  La dificultad de reconocer patrones visuales se hace evidente cuando intentas escribir un programa de computadora para reconocer n√∫meros como los anteriores.  Lo que parece f√°cil en nuestra ejecuci√≥n de repente resulta ser extremadamente complejo.  El simple concepto de c√≥mo reconocemos las formas ("el nueve tiene un bucle en la parte superior y la barra vertical en la parte inferior derecha") no es tan simple para una expresi√≥n algor√≠tmica.  Al tratar de articular estas reglas con claridad, r√°pidamente queda atrapado en un atolladero de excepciones, dificultades y ocasiones especiales.  La tarea parece desesperada. <br><br>  Enfoque NS para resolver el problema de una manera diferente.  La idea es tomar los muchos n√∫meros escritos a mano conocidos como ejemplos de ense√±anza, <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a67/2ff/15c/a672ff15c58d9672b9f5d2b427a20eb6.png"><br><br>  y desarrolle un sistema que pueda aprender de estos ejemplos.  En otras palabras, la Asamblea Nacional usa ejemplos para construir autom√°ticamente reglas de reconocimiento de d√≠gitos escritas a mano.  Adem√°s, al aumentar el n√∫mero de ejemplos de capacitaci√≥n, la red puede aprender m√°s sobre n√∫meros escritos a mano y mejorar su precisi√≥n.  Entonces, aunque he citado m√°s de 100 estudios de casos, quiz√°s podamos crear un mejor sistema de reconocimiento de escritura a mano utilizando miles o incluso millones y miles de millones de estudios de casos. <br><br>  En este cap√≠tulo, escribiremos un programa de computadora que implementa el aprendizaje NS para reconocer n√∫meros escritos a mano.  El programa tendr√° solo 74 l√≠neas y no utilizar√° bibliotecas especiales para la Asamblea Nacional.  Sin embargo, este breve programa podr√° reconocer n√∫meros escritos a mano con una precisi√≥n de m√°s del 96%, sin necesidad de intervenci√≥n humana.  Adem√°s, en futuros cap√≠tulos desarrollaremos ideas que pueden mejorar la precisi√≥n al 99% o m√°s.  De hecho, los mejores NS comerciales hacen un trabajo tan bueno que los bancos los utilizan para procesar cheques y el servicio postal para reconocer direcciones. <br><br>  Nos concentramos en el reconocimiento de escritura a mano, ya que este es un gran prototipo de una tarea para estudiar NS.  Tal prototipo es ideal para nosotros: es una tarea dif√≠cil (reconocer n√∫meros escritos a mano no es una tarea f√°cil), pero no tan complicada que requiere una soluci√≥n extremadamente compleja o una inmensa potencia inform√°tica.  Adem√°s, esta es una excelente manera de desarrollar t√©cnicas m√°s complejas, como GO.  Por lo tanto, en el libro volveremos constantemente a la tarea de reconocimiento de escritura a mano.  M√°s adelante discutiremos c√≥mo se pueden aplicar estas ideas a otras tareas de la visi√≥n por computadora, al reconocimiento de voz, el procesamiento del lenguaje natural y otras √°reas. <br><br>  Por supuesto, si el prop√≥sito de este cap√≠tulo fuera solo escribir un programa para reconocer n√∫meros escritos a mano, ¬°entonces el cap√≠tulo ser√≠a mucho m√°s corto!  Sin embargo, en el proceso desarrollaremos muchas ideas clave relacionadas con NS, incluidos dos tipos importantes de neuronas artificiales ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">perceptr√≥n</a> y neurona sigmoidea), y el algoritmo de aprendizaje est√°ndar de NS, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el descenso de gradiente estoc√°stico</a> .  En el texto, me concentro en explicar por qu√© todo se hace de esta manera, y en dar forma a su comprensi√≥n de la Asamblea Nacional.  Esto requiere una conversaci√≥n m√°s larga que si acabara de presentar la mec√°nica b√°sica de lo que est√° sucediendo, pero cuesta una comprensi√≥n m√°s profunda que tendr√°.  Entre otras ventajas: al final del cap√≠tulo comprender√° qu√© es una defensa civil y por qu√© es tan importante. <br><br><h3>  Perceptrones </h3><br>  ¬øQu√© es una red neuronal?  Para comenzar, hablar√© sobre un tipo de neurona artificial llamada perceptr√≥n.  Los perceptrones fueron inventados por el cient√≠fico <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Frank Rosenblatt</a> en los a√±os 50 y 60, inspirados en los primeros trabajos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Warren McCallock</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Walter Pitts</a> .  Hoy en d√≠a, otros modelos de neuronas artificiales se usan con mayor frecuencia: en este libro, y los trabajos m√°s modernos sobre NS utilizan principalmente el modelo sigmoide de la neurona.  La veremos pronto.  Pero para comprender por qu√© las neuronas sigmoideas se definen de esta manera, vale la pena pasar tiempo analizando el perceptr√≥n. <br><br>  Entonces, ¬øc√≥mo funcionan los perceptrones?  El perceptr√≥n recibe varios n√∫meros binarios x <sub>1</sub> , x <sub>2</sub> , ... y da un n√∫mero binario: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/293/1f3/f5c/2931f3f5c6125d14262954583868f959.png"><br><br>  En este ejemplo, el perceptr√≥n tiene tres n√∫meros de entrada, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  En general, puede haber m√°s o menos de ellos.  Rosenblatt propuso una regla simple para calcular el resultado.  Introdujo pesos, w <sub>1</sub> , w <sub>2</sub> , n√∫meros reales, expresando la importancia de los n√∫meros de entrada correspondientes para los resultados.  La salida de una neurona, 0 o 1, est√° determinada por si una suma ponderada es menor o mayor que cierto umbral [umbral] <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mtext" id="MJXp-Span-2">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-3"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-5"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-6" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-7" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-8"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-9" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-10" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-11"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-13" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.764ex" height="2.057ex" viewBox="0 -520.7 4634.5 885.9" role="img" focusable="false" style="vertical-align: -0.848ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(2562,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(3670,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="809" y="-213"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-1"> \ sum_j w_jx_j </script>  .  Al igual que los pesos, el umbral es un n√∫mero real, un par√°metro de una neurona.  En t√©rminos matem√°ticos: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-14"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-15"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-16"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-20"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mo" id="MJXp-Span-21" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25"><font style="vertical-align: inherit;">g </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27"><font style="vertical-align: inherit;">n </font></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29"><font style="vertical-align: inherit;">c </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30"><font style="vertical-align: inherit;">a </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31"><font style="vertical-align: inherit;">s </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32"><font style="vertical-align: inherit;">e </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33"><font style="vertical-align: inherit;">s</font></span></span><span class="MJXp-mn" id="MJXp-Span-34"><font style="vertical-align: inherit;"> 0 </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37"><font style="vertical-align: inherit;">f </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-40"><font style="vertical-align: inherit;">s </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41"><font style="vertical-align: inherit;">u </font></span><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">m </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-44" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">j </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">w </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-47" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">j </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-48"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-49" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">x </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-48"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-50" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">j</font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52"><font style="vertical-align: inherit;"> l </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54"><font style="vertical-align: inherit;">q </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55"><font style="vertical-align: inherit;">u </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56"><font style="vertical-align: inherit;">m </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58"><font style="vertical-align: inherit;">r </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59"><font style="vertical-align: inherit;">a </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60"><font style="vertical-align: inherit;">l</font></span></font><span class="MJXp-mtext" id="MJXp-Span-22">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mn" id="MJXp-Span-34"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-35">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-38">&nbsp;</span><span class="MJXp-mtext" id="MJXp-Span-39">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-40"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41"><font style="vertical-align: inherit;"></font></span><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-44" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-47" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-48"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-49" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-50" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mtext" id="MJXp-Span-51">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mspace" id="MJXp-Span-61" style="width: 0em; height: 0em;"></span><span class="MJXp-mn" id="MJXp-Span-62">1</span><span class="MJXp-mtext" id="MJXp-Span-63">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-64">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-65">f</span><span class="MJXp-mtext" id="MJXp-Span-66">&nbsp;</span><span class="MJXp-mtext" id="MJXp-Span-67">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-68">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-69">u</span><span class="MJXp-msubsup" id="MJXp-Span-70"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-71" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-72" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-73"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-74" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-75" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-76"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-77" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-78" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-79" style="margin-left: 0.333em; margin-right: 0.333em;">&gt;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-80">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-81">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-82">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-83">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-84">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-85">l</span><span class="MJXp-mtext" id="MJXp-Span-86">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-88">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-89">d</span><span class="MJXp-mrow" id="MJXp-Span-90"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-91">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-92">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-93">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-94">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-95">s</span></span><span class="MJXp-mtext" id="MJXp-Span-96">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-97">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-98">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-99">g</span><span class="MJXp-mrow" id="MJXp-Span-100"><span class="MJXp-mn" id="MJXp-Span-101">1</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="76.867ex" height="6.033ex" viewBox="0 -780.1 33095.6 2597.7" role="img" focusable="false" style="vertical-align: -4.222ex; max-width: 638px;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(6478,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="469" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6C" x="999" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-69" x="1297" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-64" x="1643" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="2166" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-3D" x="2973" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-62" x="4280" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-65" x="4709" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-67" x="5176" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-69" x="5656" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6E" x="6002" y="0"></use><g transform="translate(6602,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-63" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="433" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="963" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-65" x="1432" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="1899" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-30" x="8971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-69" x="9721" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-66" x="10067" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="11117" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="11587" y="0"></use><g transform="translate(12159,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(13429,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(14537,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6C" x="15752" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-65" x="16050" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-71" x="16517" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="16977" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="17550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-62" x="18428" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-72" x="18858" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="19309" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6C" x="19839" y="0"></use></g><g transform="translate(7860,-1432)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-31" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-69" x="750" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-66" x="1096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="2146" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="2616" y="0"></use><g transform="translate(3188,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(4458,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(5566,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-3E" x="6808" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="7865" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="8437" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-62" x="9316" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-72" x="9745" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="10197" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6C" x="10726" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-65" x="11275" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6E" x="11741" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-64" x="12342" y="0"></use><g transform="translate(12865,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-63" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="433" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="963" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6F" x="1432" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="1918" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-74" x="15503" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="15864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-67" x="16394" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-31" x="16874" y="0"></use></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> salida = \ begin {cases} 0 ~ if ~ \ sum_j w_jx_j \ leq umbral \\ 1 ~ if ~ \ sum_j w_jx_j> umbral \ end {casos} \ tag {1} </script></p><br><br>  ¬°Esa es toda la descripci√≥n del perceptr√≥n! <br><br>  Este es el modelo matem√°tico b√°sico.  Un perceptr√≥n puede considerarse como un tomador de decisiones al sopesar la evidencia.  D√©jame darte un ejemplo no muy realista, pero simple.  Digamos que se acerca el fin de semana, y escuchaste que se celebrar√° un festival de queso en tu ciudad.  Te gusta el queso y trata de decidir si ir al festival o no.  Puede tomar una decisi√≥n sopesando tres factores: <br><br><ol><li>  ¬øHace buen tiempo? </li><li>  ¬øTu pareja quiere ir contigo? </li><li>  ¬øEst√° el festival lejos del transporte p√∫blico?  (No tienes coche). </li></ol><br>  Estos tres factores pueden representarse como variables binarias x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Por ejemplo, x <sub>1</sub> = 1 si el clima es bueno y 0 si es malo.  x <sub>2</sub> = 1 si tu compa√±ero quiere ir, y 0 si no.  Lo mismo para x <sub>3</sub> . <br><br>  Ahora, digamos que eres tan fan√°tico del queso que est√°s listo para ir al festival, incluso si a tu pareja no le interesa y es dif√≠cil llegar a √©l.  Pero quiz√°s odies el mal tiempo y, en caso de mal tiempo, no ir√°s al festival.  Puede usar perceptrones para modelar tal proceso de toma de decisiones.  Una forma es elegir el peso w <sub>1</sub> = 6 para el clima, y ‚Äã‚Äãw <sub>2</sub> = 2, w <sub>3</sub> = 2 para otras condiciones.  Un valor mayor de w <sub>1</sub> significa que el clima le importa mucho m√°s que si su pareja se unir√° a usted o la proximidad del festival a una parada.  Finalmente, suponga que selecciona el umbral 5 para el perceptr√≥n. Con estas opciones, el perceptr√≥n implementa el modelo de decisi√≥n deseado, dando 1 cuando el clima es bueno y 0 cuando es malo.  El deseo del compa√±ero y la proximidad de la parada no afectan el valor de salida. <br><br>  Al cambiar los pesos y los umbrales, podemos obtener diferentes modelos de toma de decisiones.  Por ejemplo, supongamos que tomamos el umbral 3. Luego, el perceptr√≥n decide que debe ir al festival, ya sea cuando hace buen tiempo o cuando el festival est√° cerca de una parada de autob√∫s y su pareja acepta ir con usted.  En otras palabras, el modelo es diferente.  Bajar el umbral significa que quieres ir m√°s al festival. <br><br>  ¬°Obviamente, el perceptr√≥n no es un modelo humano completo de toma de decisiones!  Pero este ejemplo muestra c√≥mo un perceptr√≥n puede pesar diferentes tipos de evidencia para tomar decisiones.  Parece posible que una red compleja de perceptrones pueda tomar decisiones muy complejas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f1/853/0d8/5f18530d8e17d640b3146924f7666032.png"><br><br>  En esta red, la primera columna de perceptrones, lo que llamamos la primera capa de perceptrones, toma tres decisiones muy simples, sopesando la evidencia de entrada.  ¬øQu√© pasa con los perceptrones de la segunda capa?  Cada uno de ellos toma una decisi√≥n, sopesando los resultados de la primera capa de toma de decisiones.  De esta manera, el perceptr√≥n de la segunda capa puede tomar una decisi√≥n a un nivel m√°s complejo y abstracto en comparaci√≥n con el perceptr√≥n de la primera capa.  E incluso los perceptrones en la tercera capa pueden tomar decisiones a√∫n m√°s complejas.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De esta manera, una red multicapa de perceptrones puede manejar decisiones complejas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por cierto, cuando determin√© el perceptr√≥n, dije que solo tiene un valor de salida. Pero en la red en la parte superior, los perceptrones parecen tener varios valores de salida. De hecho, solo tienen una salida. Muchas flechas de salida son solo una forma conveniente de mostrar que la salida del perceptr√≥n se usa como entrada de varios otros perceptrones. Esto es menos engorroso que dibujar una √∫nica salida de ramificaci√≥n. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Simplifiquemos la descripci√≥n de los perceptrones. Condici√≥n</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-102"><span class="MJXp-msubsup" id="MJXp-Span-103"><span class="MJXp-mo" id="MJXp-Span-104" style="margin-left: 0.111em; margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œ£ </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-105" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-106"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-107" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-108" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-109"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-110" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-111" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-112" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> &gt; </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-113"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-114"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-115"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un e </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-116"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-117"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-118"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-119"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-120"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d</font></font></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> inc√≥moda, y podemos estar de acuerdo en dos cambios para la grabaci√≥n de su simplicidad. </font><font style="vertical-align: inherit;">Lo primero es grabar</font></font><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-3">\sum_j w_jx_j > treshold</script><font style="vertical-align: inherit;"></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-121"><span class="MJXp-msubsup" id="MJXp-Span-122"><span class="MJXp-mo" id="MJXp-Span-123" style="margin-left: 0.111em; margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œ£ </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-124" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-125"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-126" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-127" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-128"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-129" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-130" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> como el producto escalar,</font></font><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-4">\sum_j w_jx_j</script><font style="vertical-align: inherit;"></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-131"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-132"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-133" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ãÖ </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-134"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mo" id="MJXp-Span-135" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-136"><span class="MJXp-mo" id="MJXp-Span-137" style="margin-left: 0.111em; margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àë </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-138" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-139"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-140" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-141" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-142"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-143" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-144" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , donde w y x son vectores cuyos componentes son pesos y datos de entrada, respectivamente. </font><font style="vertical-align: inherit;">El segundo es transferir el umbral a otra parte de la desigualdad y reemplazarlo con un valor conocido como desplazamiento de perceptr√≥n [sesgo],</font></font><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-5">w \cdot x = \sum_j w_jx_j</script><font style="vertical-align: inherit;"></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-145"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-146"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b </font></font></span><span class="MJXp-mo" id="MJXp-Span-147" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚â° </font></font></span><span class="MJXp-mo" id="MJXp-Span-148" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-149"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-150"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-151"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-152"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-153"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-154"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-155"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-156"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-157"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d</font></font></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-6">b \equiv ‚àíthreshold</script>  .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Usando el desplazamiento en lugar de un umbral, podemos reescribir la regla del perceptr√≥n: </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-158"><span class="MJXp-mtable" id="MJXp-Span-159"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-160" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-161" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-162"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-163"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-164"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-165"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-166"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-167"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mo" id="MJXp-Span-168" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-mrow" id="MJXp-Span-169"><span class="MJXp-mo" id="MJXp-Span-170" style="margin-left: 0em; margin-right: 0em; vertical-align: -0.472em;"><span class="MJXp-right MJXp-scale5" style="font-size: 2.889em; margin-left: -0.22em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">{ </font></font></span></span><span class="MJXp-mtable" id="MJXp-Span-171"><span><span class="MJXp-mtr" id="MJXp-Span-172" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-173" style="text-align: left;"><span class="MJXp-mn" id="MJXp-Span-174"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0 </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-176"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-177"><font style="vertical-align: inherit;">f </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-179"><font style="vertical-align: inherit;">w </font></span><span class="MJXp-mo" id="MJXp-Span-180" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">‚ãÖ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-181"><font style="vertical-align: inherit;">x </font></span><span class="MJXp-mo" id="MJXp-Span-182" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">+ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-183"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mo" id="MJXp-Span-184" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;">‚â§ </font></span><span class="MJXp-mn" id="MJXp-Span-185"><font style="vertical-align: inherit;">0 </font></span></font><span class="MJXp-mtext" id="MJXp-Span-175">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-176"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-177"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-178">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-179"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-180" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-181"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-182" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-183"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-184" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mn" id="MJXp-Span-185"><font style="vertical-align: inherit;"></font></span></span></span><span class="MJXp-mtr" id="MJXp-Span-186" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-187" style="padding-top: 0.2em; text-align: left;"><span class="MJXp-mn" id="MJXp-Span-188"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1 </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-190"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-191"><font style="vertical-align: inherit;">f </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-193"><font style="vertical-align: inherit;">w </font></span><span class="MJXp-mo" id="MJXp-Span-194" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">‚ãÖ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-195"><font style="vertical-align: inherit;">x </font></span><span class="MJXp-mo" id="MJXp-Span-196" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">+ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mo" id="MJXp-Span-198" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;">&gt; </font></span><span class="MJXp-mn" id="MJXp-Span-199"><font style="vertical-align: inherit;">0</font></span></font><span class="MJXp-mtext" id="MJXp-Span-189">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-190"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-191"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-192">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-193"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-194" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-195"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-196" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-198" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mn" id="MJXp-Span-199"><font style="vertical-align: inherit;"></font></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-200" style="margin-left: 0em; margin-right: 0em;"></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-7"> output = \begin{cases} 0 ~ if ~ w \cdot x + b \leq 0 \\ 1 ~ if ~ w \cdot x + b > 0 \end{cases} \tag{2} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El desplazamiento se puede representar como una medida de lo f√°cil que es obtener un valor de 1 en la salida del perceptr√≥n. O, en t√©rminos biol√≥gicos, el desplazamiento es una medida de lo f√°cil que es activar el perceptr√≥n. Un perceptr√≥n con un sesgo muy grande es extremadamente f√°cil de dar 1. Pero con un sesgo negativo muy grande, esto es dif√≠cil de hacer. Obviamente, la introducci√≥n del sesgo es un peque√±o cambio en la descripci√≥n de los perceptrones, pero luego veremos que conduce a una mayor simplificaci√≥n de la grabaci√≥n. Por lo tanto, adem√°s no usaremos el umbral, sino que siempre usaremos el desplazamiento.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Describ√≠ los perceptrones en t√©rminos del m√©todo de sopesar la evidencia para la toma de decisiones. Otro m√©todo de uso es el c√°lculo de funciones l√≥gicas elementales, que generalmente consideramos los c√°lculos principales, como AND, OR y NAND. Supongamos, por ejemplo, que tenemos un perceptr√≥n con dos entradas, el peso de cada una de ellas es -2, y su desplazamiento es 3. Aqu√≠ est√°: la </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/530/d45/9a9/530d459a9262c475caf057106a500ddc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entrada 00 da la salida 1, porque (‚àí2) ‚àó 0 + (- 2 ) ‚àó 0 + 3 = 3 es mayor que cero. Los mismos c√°lculos dicen que las entradas 01 y 10 dan 1. Pero 11 en la entrada da 0 en la salida, ya que (‚àí2) ‚àó 1 + (- 2) ‚àó 1 + 3 = ‚àí1, menor que cero. ¬°Por lo tanto, nuestro perceptr√≥n implementa la funci√≥n NAND!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este ejemplo muestra que los perceptrones se pueden usar para calcular funciones l√≥gicas b√°sicas. De hecho, podemos usar redes perceptron para calcular cualquier funci√≥n l√≥gica en general. El hecho es que la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">compuerta l√≥gica</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> NAND </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">es</font></a><font style="vertical-align: inherit;"> universal para los c√°lculos: es posible construir cualquier c√°lculo sobre la base. Por ejemplo, puede usar puertas NAND para crear un circuito que agregue dos bits, x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Para hacer esto, calcule la suma de bits</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-201"><span class="MJXp-msubsup" id="MJXp-Span-202"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-203" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-204" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-205" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚äï </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-206"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-207" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-208" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></span></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , as√≠ como</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">la bandera de acarreo</font></a><font style="vertical-align: inherit;">, que es 1 cuando x</font><sub><font style="vertical-align: inherit;">1</font></sub><font style="vertical-align: inherit;">y x</font><sub><font style="vertical-align: inherit;">2</font></sub><font style="vertical-align: inherit;">son 1, es decir, la bandera de acarreo es simplemente el resultado de la multiplicaci√≥n por bits x</font><sub><font style="vertical-align: inherit;">1</font></sub><font style="vertical-align: inherit;">x</font><sub><font style="vertical-align: inherit;">2</font></sub><font style="vertical-align: inherit;">:</font><font style="vertical-align: inherit;">para obtener la red equivalente de los perceptrones, reemplazamos todos Las compuertas NAND son perceptrones con dos entradas, el peso de cada una de ellas es -2, y con un desplazamiento de 3. Aqu√≠ est√° la red resultante. Tenga en cuenta que mov√≠ el perceptr√≥n correspondiente a la v√°lvula inferior derecha, solo para que sea m√°s conveniente dibujar flechas:</font></font><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-8"> x_1 \oplus x_2 </script><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d25/af6/afe/d25af6afec499649a0b338f2ccc67f63.png"><br><br><font style="vertical-align: inherit;"></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/222/5b4/71b/2225b471b11b7357777ebc024bd287c2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un aspecto notable de esta red perceptr√≥n es que la salida de la izquierda se usa dos veces como entrada en la parte inferior. Al definir el modelo del perceptr√≥n, no mencion√© la admisibilidad de tal esquema de doble salida en el mismo lugar. De hecho, en realidad no importa. Si no queremos permitir esto, simplemente podemos combinar dos l√≠neas con pesos de -2 en una con un peso de -4. (Si esto no te parece obvio, detente y demu√©stralo a ti mismo). Despu√©s de este cambio, la red se ve de la siguiente manera, con todos los pesos no asignados iguales a -2, todos los desplazamientos iguales a 3, y se marca un peso -4: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/151/d84/dbd/151d84dbdf944cc9016ed656ba70e424.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tal registro de perceptrones que tienen una salida pero no entradas:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/41c/8e3/e46/41c8e3e46587bf536bab96b8427e9bcd.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es solo una abreviatura. Esto no significa que no tenga entradas. Para entender esto, supongamos que tenemos un perceptr√≥n sin entradas. Entonces la suma ponderada ‚àë </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> siempre ser√≠a cero, por lo que el perceptr√≥n dar√≠a 1 para b&gt; 0 y 0 para b ‚â§ 0. Es decir, el perceptr√≥n solo dar√≠a un valor fijo, y no lo que necesitamos (x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en el ejemplo arriba). Es mejor considerar los perceptrones de entrada no como perceptrones, sino como unidades especiales que simplemente se definen para producir los valores deseados x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ...</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El ejemplo del sumador demuestra c√≥mo se puede utilizar una red perceptr√≥n para simular un circuito que contiene muchas compuertas NAND. Y dado que estas puertas son universales para los c√°lculos, por lo tanto, los perceptrones son universales para los c√°lculos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La versatilidad computacional de los perceptrones es alentadora y decepcionante. Es alentador, asegurando que la red perceptron pueda ser tan poderosa como cualquier otro dispositivo inform√°tico. Decepcionante, dando la impresi√≥n de que los perceptrones son solo un nuevo tipo de puerta l√≥gica NAND. M√°s o menos descubrimiento!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, la situaci√≥n es realmente mejor. </font><font style="vertical-align: inherit;">Resulta que podemos desarrollar algoritmos de entrenamiento que pueden ajustar autom√°ticamente los pesos y los desplazamientos de la red de las neuronas artificiales. </font><font style="vertical-align: inherit;">Este ajuste tiene lugar en respuesta a est√≠mulos externos, sin la intervenci√≥n directa de un programador. </font><font style="vertical-align: inherit;">Estos algoritmos de aprendizaje nos permiten usar neuronas artificiales de una manera radicalmente diferente de las puertas l√≥gicas ordinarias. </font><font style="vertical-align: inherit;">En lugar de registrar expl√≠citamente un circuito desde puertas NAND y otras, nuestras redes neuronales simplemente pueden aprender a resolver problemas, a veces aquellos para los que ser√≠a extremadamente dif√≠cil dise√±ar directamente un circuito regular.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Neuronas sigmoideas </font></font></h3><br>  Los algoritmos de aprendizaje son geniales.  Sin embargo, ¬øc√≥mo desarrollar un algoritmo para una red neuronal?  Supongamos que tenemos una red de perceptrones que queremos usar para capacitarnos en la resoluci√≥n de un problema.  Suponga que la entrada a la red puede ser p√≠xeles de una imagen escaneada de un d√≠gito escrito a mano.  Y queremos que la red conozca los pesos y las compensaciones necesarias para clasificar correctamente los n√∫meros.  Para comprender c√≥mo puede funcionar dicha capacitaci√≥n, imaginemos que estamos cambiando ligeramente un cierto peso (o sesgo) en la red.  Queremos que este peque√±o cambio conduzca a un peque√±o cambio en la salida de la red.  Como veremos pronto, esta propiedad hace posible el aprendizaje.  Esquem√°ticamente, queremos lo siguiente (obviamente, ¬°tal red es demasiado simple para reconocer la escritura a mano!): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2cf/d97/76f/2cfd9776fdb27a7106bdf2a94d76eb46.png"><br><br>  Si un peque√±o cambio en el peso (o sesgo) llevara a un peque√±o cambio en el resultado de salida, podr√≠amos cambiar los pesos y los sesgos para que nuestra red se comporte un poco m√°s cerca de lo que queremos.  Por ejemplo, supongamos que la red asign√≥ incorrectamente la imagen a "8", aunque deber√≠a haber sido a "9".  Podr√≠amos descubrir c√≥mo hacer un peque√±o cambio en el peso y el desplazamiento para que la red se acerque un poco m√°s a clasificar la imagen como "9".  Y luego repetir√≠amos esto, cambiando pesos y turnos una y otra vez para obtener el mejor y el mejor resultado.  La red aprender√≠a. <br><br>  El problema es que si hay perceptrones en la red, esto no sucede.  Un peque√±o cambio en los pesos o el desplazamiento de cualquier perceptr√≥n a veces puede conducir a un cambio en su salida al opuesto, por ejemplo, de 0 a 1. Tal cambio puede cambiar el comportamiento del resto de la red de una manera muy complicada.  E incluso si ahora nuestro "9" se reconoce correctamente, el comportamiento de la red con todas las otras im√°genes probablemente ha cambiado por completo de una manera que es dif√≠cil de controlar.  Debido a esto, es dif√≠cil imaginar c√≥mo podemos ajustar gradualmente los pesos y las compensaciones para que la red se acerque gradualmente al comportamiento deseado.  Quiz√°s hay alguna forma inteligente de solucionar este problema.  Pero no existe una soluci√≥n simple al problema de aprender una red de perceptrones. <br><br>  Este problema se puede evitar introduciendo un nuevo tipo de neurona artificial llamada neurona sigmoidea.  Son similares a los perceptrones, pero modificados para que los peque√±os cambios en los pesos y las compensaciones den como resultado solo peque√±os cambios en la salida.  Este es un hecho b√°sico que permitir√° que la red de neuronas sigmoides aprenda. <br><br>  D√©jame describir una neurona sigmoidea.  Los dibujaremos de la misma manera que los perceptrones: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2cd/0b3/ccf/2cd0b3ccf36d0dabf96fd15649c29f90.png"><br><br>  Tiene la misma entrada x <sub>1</sub> , x <sub>2</sub> , ... Pero en lugar de ser igual a 0 o 1, estas entradas pueden tener cualquier valor en el rango de 0 a 1. Por ejemplo, un valor de 0.638 ser√° una entrada v√°lida para neurona sigmoidea (CH).  Al igual que el perceptr√≥n, SN tiene pesos para cada entrada, w <sub>1</sub> , w <sub>2</sub> , ... y el sesgo total b.  Pero su valor de salida no ser√° 0 o 1. Ser√° œÉ (w‚ãÖx + b), donde œÉ es el sigmoide. <br><br>  Por cierto, œÉ a veces se llama una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">funci√≥n log√≠stica</a> , y esta clase de neuronas se llama neuronas log√≠sticas.  Es √∫til recordar esta terminolog√≠a, ya que estos t√©rminos son utilizados por muchas personas que trabajan con redes neuronales.  Sin embargo, nos adheriremos a la terminolog√≠a sigmoidea. <br><br>  La funci√≥n se define de la siguiente manera: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-209"><span class="MJXp-mtext" id="MJXp-Span-210">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-211">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-212">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-213">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-214">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-215">a</span><span class="MJXp-mo" id="MJXp-Span-216" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-217">z</span><span class="MJXp-mo" id="MJXp-Span-218" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-219">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-220">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-221">q</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-222">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-223">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-224">v</span><span class="MJXp-mtext" id="MJXp-Span-225">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-226">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-227">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-228">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-229">c</span><span class="MJXp-mrow" id="MJXp-Span-230"><span class="MJXp-mn" id="MJXp-Span-231">1</span></span><span class="MJXp-mrow" id="MJXp-Span-232"><span class="MJXp-mn" id="MJXp-Span-233">1</span><span class="MJXp-mo" id="MJXp-Span-234" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-235"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-236" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-237" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-238">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-239">z</span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-240">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-241">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-242">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-243">g</span><span class="MJXp-mrow" id="MJXp-Span-244"><span class="MJXp-mn" id="MJXp-Span-245">3</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-9"> \ sigma (z) \ equiv \ frac {1} {1 + e ^ {- z}} \ tag {3} </script></p><br><br>  En nuestro caso, el valor de salida de la neurona sigmoidea con datos de entrada x <sub>1</sub> , x <sub>2</sub> , ... por los pesos w <sub>1</sub> , w <sub>2</sub> , ... y el desplazamiento b se considerar√° como: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-246"><span class="MJXp-mtext" id="MJXp-Span-247">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-248">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-249">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-250">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-251">c</span><span class="MJXp-mrow" id="MJXp-Span-252"><span class="MJXp-mn" id="MJXp-Span-253">1</span></span><span class="MJXp-mrow" id="MJXp-Span-254"><span class="MJXp-mn" id="MJXp-Span-255">1</span><span class="MJXp-mo" id="MJXp-Span-256" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-257">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-258">x</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-259">p</span><span class="MJXp-mo" id="MJXp-Span-260" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mo" id="MJXp-Span-261" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-262">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-263">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-264">u</span><span class="MJXp-msubsup" id="MJXp-Span-265"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-266" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-267" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-268"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-269" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-270" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-271"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-272" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-273" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-274" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-275">b</span><span class="MJXp-mo" id="MJXp-Span-276" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mtext" id="MJXp-Span-277">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-278">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-279">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-280">g</span><span class="MJXp-mrow" id="MJXp-Span-281"><span class="MJXp-mn" id="MJXp-Span-282">4</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-10"> \ frac {1} {1 + exp (- \ sum_j w_jx_j - b)} \ tag {4} </script></p><br><br>  A primera vista, CH parece completamente diferente a las neuronas.  El aspecto algebraico de un sigmoide puede parecer confuso y oscuro si no est√° familiarizado con √©l.  De hecho, hay muchas similitudes entre los perceptrones y el SN, y la forma algebraica de un sigmoide resulta ser m√°s un detalle t√©cnico que una barrera seria para la comprensi√≥n. <br><br>  Para comprender las similitudes con el modelo de perceptr√≥n, suponga que z ‚â° w ‚ãÖ x + b es un n√∫mero positivo grande.  Entonces e - z ‚âà 0, por lo tanto, œÉ (z) ‚âà 1. En otras palabras, cuando z = w ‚ãÖ x + b es grande y positivo, el rendimiento de SN es aproximadamente igual a 1, como para el perceptr√≥n.  Suponga que z = w ‚ãÖ x + b es grande con un signo menos.  Entonces e - z ‚Üí ‚àû, y œÉ (z) ‚âà 0. Entonces, para z grande con un signo menos, el comportamiento del SN tambi√©n se acerca al perceptr√≥n.  Y solo cuando w ‚ãÖ x + b tiene un tama√±o promedio, se observan serias desviaciones del modelo de perceptr√≥n. <br><br>  ¬øQu√© pasa con la forma algebraica de œÉ?  ¬øC√≥mo lo entendemos?  De hecho, la forma exacta de œÉ no es tan importante: la forma de la funci√≥n en el gr√°fico es importante.  Aqu√≠ esta: <br><br><img src="https://habrastorage.org/webt/sm/u4/jv/smu4jvbwuriryrfojrpt-ukdr6w.png"><br><br>  Esta es una versi√≥n fluida de la funci√≥n de paso: <br><br><img src="https://habrastorage.org/webt/2i/p0/a-/2ip0a-3cmpstyiwrglx_michl3m.png"><br><br>  Si œÉ fuera paso a paso, entonces el SN ser√≠a un perceptr√≥n, ya que tendr√≠a 0 o 1 salida dependiendo del signo w ‚ãÖ x + b (bueno, de hecho, en z = 0, el perceptr√≥n da 0, y la funci√≥n de paso 1 , entonces en ese punto, la funci√≥n tendr√≠a que ser cambiada). <br><br>  Usando la funci√≥n real œÉ, obtenemos un perceptr√≥n suavizado.  Y lo principal aqu√≠ es la suavidad de la funci√≥n, no su forma exacta.  La suavidad significa que peque√±os cambios Œîw <sub>j</sub> pesos y Œ¥b compensaciones dar√°n peque√±os cambios Œî salida de la salida.  El √°lgebra nos dice que la salida Œî se aproxima bien de la siguiente manera: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-283"><span class="MJXp-mtext" id="MJXp-Span-284">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-285">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-286">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-287">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-289">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-290">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-291">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-292">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-293">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-294">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-295">t</span><span class="MJXp-mtext" id="MJXp-Span-296">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-297">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-298">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-299">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-300">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-301">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-302">x</span><span class="MJXp-mtext" id="MJXp-Span-303">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-304">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-305">u</span><span class="MJXp-msubsup" id="MJXp-Span-306"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-307" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-308" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mtext" id="MJXp-Span-309">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-310">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-311">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-312">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-313">c</span><span class="MJXp-mrow" id="MJXp-Span-314"><span class="MJXp-mtext" id="MJXp-Span-315">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-316">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-317">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-318">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-319">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-320">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-321">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-322">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-323">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-324">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-325">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-326">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-327">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-328">t</span></span><span class="MJXp-mrow" id="MJXp-Span-329"><span class="MJXp-mtext" id="MJXp-Span-330">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-331">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-332">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-333">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-334">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-335">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-336">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-337">l</span><span class="MJXp-msubsup" id="MJXp-Span-338"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-339" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-340" style="vertical-align: -0.4em;">j</span></span></span><span class="MJXp-mtext" id="MJXp-Span-341">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-342">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-343">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-344">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-345">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-346">a</span><span class="MJXp-msubsup" id="MJXp-Span-347"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-348" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-349" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-350" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-351">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-352">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-353">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-354">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-355">c</span><span class="MJXp-mrow" id="MJXp-Span-356"><span class="MJXp-mtext" id="MJXp-Span-357">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-358">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-359">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-360"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-361"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-362"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-363"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-364"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-365"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-366"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-367"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-368"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-369"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-370"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span></span><font style="vertical-align: inherit;"><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-373"><font style="vertical-align: inherit;">p </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-374"><font style="vertical-align: inherit;">a </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-375"><font style="vertical-align: inherit;">r </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-376"><font style="vertical-align: inherit;">t </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-377"><font style="vertical-align: inherit;">i </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-378"><font style="vertical-align: inherit;">a </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379"><font style="vertical-align: inherit;">l </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-380"><font style="vertical-align: inherit;">b</font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-382"><font style="vertical-align: inherit;"> D </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-383"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-384"><font style="vertical-align: inherit;">l </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-385"><font style="vertical-align: inherit;">t </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-386"><font style="vertical-align: inherit;">a </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-389"><font style="vertical-align: inherit;">t </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-390"><font style="vertical-align: inherit;">a </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-391"><font style="vertical-align: inherit;">g </font></span><span class="MJXp-mrow" id="MJXp-Span-392"><span class="MJXp-mn" id="MJXp-Span-393"><font style="vertical-align: inherit;">5</font></span></span></font><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mtext" id="MJXp-Span-372">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-373"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-374"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-375"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-376"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-377"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-378"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-380"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mtext" id="MJXp-Span-381">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-382"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-383"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-384"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-385"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-386"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-388">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-389"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-390"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-391"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mrow" id="MJXp-Span-392"><span class="MJXp-mn" id="MJXp-Span-393"><font style="vertical-align: inherit;"></font></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-11"> \ Delta output \ approx \ sum_j \ frac {\ partial output} {\ partial w_j} \ Delta w_j + \ frac {\ partial output} {\ partial b} \ Delta b \ tag {5} </script></p><br><br>  Donde la suma es sobre todos los pesos w <sub>j</sub> , y ‚àÇoutput / ‚àÇw <sub>j</sub> y ‚àÇoutput / ‚àÇb denotan derivadas parciales de la salida con respecto a w <sub>j</sub> y b, respectivamente.  ¬°No se asuste si se siente inseguro en compa√±√≠a de derivados privados!  Aunque la f√≥rmula parece complicada, con todas estas derivadas parciales, en realidad dice algo bastante simple (y √∫til): la salida Œî es una funci√≥n lineal que depende de los pesos y sesgos Œîw <sub>j</sub> y Œîb.  Su linealidad facilita la selecci√≥n de peque√±os cambios en pesos y compensaciones para lograr cualquier sesgo de salida peque√±o deseado.  Por lo tanto, aunque los SN son similares a los perceptrones en el comportamiento cualitativo, facilitan la comprensi√≥n de c√≥mo se puede cambiar la producci√≥n cambiando los pesos y los desplazamientos. <br><br>  Si la forma general œÉ es importante para nosotros, y no su forma exacta, ¬øpor qu√© usamos tal f√≥rmula (3)?  De hecho, m√°s adelante a veces consideraremos neuronas cuya salida es f (w ‚ãÖ x + b), donde f () es alguna otra funci√≥n de activaci√≥n.  Lo principal que cambia cuando cambia la funci√≥n es el valor de las derivadas parciales en la ecuaci√≥n (5).  Resulta que cuando calculamos estas derivadas parciales, el uso de œÉ simplifica enormemente el √°lgebra, ya que los exponentes tienen propiedades muy agradables al diferenciar.  En cualquier caso, œÉ se usa a menudo para trabajar con redes neuronales, y con mayor frecuencia en este libro usaremos dicha funci√≥n de activaci√≥n. <br><br>  ¬øC√≥mo interpretar el resultado del trabajo de CH?  Obviamente, la principal diferencia entre los perceptrones y el CH es que el CH no da solo 0 o 1. Su salida puede ser cualquier n√∫mero real de 0 a 1, por lo que valores como 0.173 o 0.689 son v√°lidos.  Esto puede ser √∫til, por ejemplo, si desea que el valor de salida indique, por ejemplo, el brillo promedio de los p√≠xeles de la imagen recibida en la entrada del NS.  Pero a veces puede ser inconveniente.  Supongamos que queremos que la salida de la red diga que "se ingres√≥ la imagen 9" o "imagen de entrada no 9".  Obviamente, ser√≠a m√°s f√°cil si los valores de salida fueran 0 o 1, como un perceptr√≥n.  Pero en la pr√°ctica, podemos estar de acuerdo en que cualquier valor de salida de al menos 0.5 significar√≠a "9" en la entrada, y cualquier valor inferior a 0.5 significar√≠a que es "no 9".  Siempre indicar√© expl√≠citamente la existencia de tales acuerdos. <br><br>  Ejercicios <br><br><ul><li>  Perceptrones simuladores de CH, parte 1 </li></ul><br>  Supongamos que tomamos todos los pesos y desplazamientos de una red de perceptrones, y los multiplicamos por una constante positiva c&gt; 0.  Muestre que el comportamiento de la red no cambia. <br><br><ul><li>  Perceptrones simuladores de CH, parte 2 </li></ul><br>  Supongamos que tenemos la misma situaci√≥n que en el problema anterior: una red de perceptrones.  Supongamos tambi√©n que se seleccionan los datos de entrada para la red.  No necesitamos un valor espec√≠fico, lo principal es que es fijo.  Suponga que los pesos y los desplazamientos son tales que w‚ãÖx + b ‚â† 0, donde x es el valor de entrada de cualquier perceptr√≥n de la red.  Ahora reemplazamos todos los perceptrones en la red con SN, y multiplicamos los pesos y desplazamientos por la constante positiva c&gt; 0.  Muestre que en el l√≠mite c ‚Üí ‚àû el comportamiento de la red desde el SN ser√° exactamente el mismo que el de las redes de perceptrones.  ¬øC√≥mo se violar√° esta afirmaci√≥n si para uno de los perceptrones w‚ãÖx + b = 0? <br><br><h3>  Arquitectura de red neuronal </h3><br>  En la siguiente secci√≥n, presentar√© una red neuronal capaz de una buena clasificaci√≥n de n√∫meros escritos a mano.  Antes de eso, es √∫til explicar la terminolog√≠a que nos permite se√±alar diferentes partes de la red.  Digamos que tenemos la siguiente red: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/472/521/00b/47252100b91b8e1a527796217a6ed0fa.png"><br><br>  Como mencion√©, la capa m√°s a la izquierda de la red se llama capa de entrada, y sus neuronas se llaman neuronas de entrada.  La capa de salida m√°s a la derecha contiene neuronas de salida o, como en nuestro caso, una neurona de salida.  La capa intermedia se llama oculta, porque sus neuronas no son ni de entrada ni de salida.  El t√©rmino "oculto" puede sonar un poco misterioso; cuando lo escuch√© por primera vez, decid√≠ que deber√≠a tener una importancia filos√≥fica o matem√°tica profunda, sin embargo, solo significa "no entrar y no salir".  La red de arriba solo tiene una capa oculta, pero algunas redes tienen varias capas ocultas.  Por ejemplo, en la siguiente red de cuatro capas hay dos capas ocultas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/26e/be9/c5f/26ebe9c5f4c7f61413cfa43e67151734.png"><br><br>  Esto puede ser confuso, pero por razones hist√≥ricas, tales redes de m√∫ltiples capas a veces se denominan perceptrones multicapa, MLP, a pesar de que consisten en neuronas sigmoideas en lugar de perceptrones.  No voy a usar esa terminolog√≠a porque es confusa, pero debo advertir sobre su existencia. <br><br>  Dise√±ar capas de entrada y salida a veces es una tarea simple.  Por ejemplo, supongamos que estamos tratando de determinar si el n√∫mero escrito a mano significa "9" o no.  Un circuito de red natural codificar√° el brillo de los p√≠xeles de la imagen en las neuronas de entrada.  Si la imagen es en blanco y negro, 64x64 p√≠xeles de tama√±o, entonces tendremos 64x64 = 4096 neuronas de entrada, con un brillo en el rango de 0 a 1. La capa de salida contendr√° solo una neurona, cuyo valor menor que 0.5 significar√° que "en la entrada no fue 9 ", pero los valores m√°s significar√°n que" la entrada fue 9 ". <br><br>  Y aunque dise√±ar capas de entrada y salida es a menudo una tarea simple, dise√±ar capas ocultas puede ser un arte dif√≠cil.  En particular, no es posible describir el proceso de desarrollo de capas ocultas con unas simples reglas generales.  Los investigadores de la Asamblea Nacional han desarrollado muchas reglas heur√≠sticas para el dise√±o de capas ocultas que ayudan a obtener el comportamiento deseado de las redes neuronales.  Por ejemplo, dicha heur√≠stica se puede utilizar para comprender c√≥mo lograr un compromiso entre el n√∫mero de capas ocultas y el tiempo disponible para capacitar a la red.  M√°s adelante nos encontraremos con algunas de estas reglas. <br><br>  Hasta ahora, hemos estado discutiendo las NS en las que la salida de una capa se usa como entrada para la siguiente.  Dichas redes se denominan redes neuronales de distribuci√≥n directa.  Esto significa que no hay bucles en la red: la informaci√≥n siempre avanza y nunca retroalimenta.  Si tuvi√©ramos bucles, encontrar√≠amos situaciones en las que el sigmoide de entrada depender√≠a de la salida.  Ser√≠a dif√≠cil de comprender, y no permitimos tales bucles. <br><br>  Sin embargo, hay otros modelos de NS artificiales en los que es posible usar bucles de retroalimentaci√≥n.  Estos modelos se denominan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">redes neuronales recurrentes</a> (RNS).  La idea de estas redes es que sus neuronas se activan por per√≠odos limitados de tiempo.  Esta activaci√≥n puede estimular otros neutrones, que pueden activarse un poco m√°s tarde, tambi√©n por un tiempo limitado.  Esto conduce a la activaci√≥n de las siguientes neuronas, y con el tiempo obtenemos una cascada de neuronas activadas.  Los bucles en tales modelos no presentan problemas, ya que la salida de una neurona afecta su entrada en un momento posterior, y no de inmediato. <br><br>  Los RNS no fueron tan influyentes como los NS de distribuci√≥n directa, en particular porque los algoritmos de entrenamiento para los RNS hasta ahora tienen menos potencial.  Sin embargo, el RNS sigue siendo extremadamente interesante.  En el esp√≠ritu de trabajo, est√°n mucho m√°s cerca del cerebro que NS de distribuci√≥n directa.  Es posible que el RNS pueda resolver problemas importantes que, con la ayuda de la distribuci√≥n directa NS, pueden resolverse con grandes dificultades.  Sin embargo, para limitar el alcance de nuestro estudio, nos concentraremos en el NS de distribuci√≥n directa m√°s utilizado. <br><br><h3>  Red simple de clasificaci√≥n de tinta </h3><br>  Una vez definidas las redes neuronales, volveremos al reconocimiento de escritura a mano.  La tarea de reconocer n√∫meros escritos a mano se puede dividir en dos subtareas.  Primero, queremos encontrar una manera de dividir una imagen que contenga muchos d√≠gitos en una secuencia de im√°genes individuales, cada una de las cuales contiene un d√≠gito.  Por ejemplo, nos gustar√≠a dividir la imagen <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><br>  en seis separados <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b36/135/6ae/b361356aec440b1dbf77c8dedbc6f9b6.png"><br><br>  Los humanos podemos resolver f√°cilmente este problema de segmentaci√≥n, pero es dif√≠cil para un programa de computadora dividir correctamente la imagen.  Despu√©s de la segmentaci√≥n, el programa necesita clasificar cada d√≠gito individual.  Entonces, por ejemplo, queremos que nuestro programa reconozca que el primer d√≠gito <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e5a/c2a/808/e5ac2a808e18ac02ba0f09b2052fff4f.png"><br><br>  son las 5. <br><br>  Nos concentraremos en crear un programa para resolver el segundo problema, la clasificaci√≥n de n√∫meros individuales.  Resulta que el problema de la segmentaci√≥n no es tan dif√≠cil de resolver tan pronto como encontramos una buena manera de clasificar los d√≠gitos individuales.  Existen muchos enfoques para resolver el problema de segmentaci√≥n.  Una de ellas es probar muchas formas diferentes de segmentaci√≥n de im√°genes utilizando el clasificador de d√≠gitos individuales, evaluando cada intento.  La segmentaci√≥n de prueba es muy apreciada si el clasificador de d√≠gitos individuales conf√≠a en la clasificaci√≥n de todos los segmentos, y baja si tiene problemas en uno o m√°s segmentos.  La idea es que si el clasificador tiene problemas en alg√∫n lugar, lo m√°s probable es que la segmentaci√≥n sea incorrecta.  Esta idea y otras opciones se pueden utilizar para una buena soluci√≥n al problema de segmentaci√≥n.  Entonces, en lugar de preocuparnos por la segmentaci√≥n, nos concentraremos en desarrollar un NS capaz de resolver una tarea m√°s interesante y compleja, a saber, reconocer n√∫meros escritos a mano individuales. <br><br>  Para reconocer d√≠gitos individuales, usaremos NS de tres capas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/350/3dc/e6b3503dcc1e2848980d17ad8c193018.png"><br><br>  La capa de red de entrada contiene neuronas que codifican varios valores de los p√≠xeles de entrada.  Como se indicar√° en la siguiente secci√≥n, nuestros datos de entrenamiento consistir√°n en muchas im√°genes de d√≠gitos escritos a mano escaneados de 28x28 p√≠xeles de tama√±o, por lo que la capa de entrada contiene 28x28 = 784 neuronas.  Por simplicidad, no indiqu√© la mayor√≠a de las 784 neuronas en el diagrama.  Los p√≠xeles entrantes son en blanco y negro, con un valor de 0.0 que indica blanco, 1.0 que indica negro y valores intermedios que indican sombras de gris cada vez m√°s oscuras. <br><br>  La segunda capa de la red est√° oculta.  Denotamos el n√∫mero de neuronas en esta capa n, y experimentaremos con diferentes valores de n.  El ejemplo anterior muestra una peque√±a capa oculta que contiene solo n = 15 neuronas. <br><br>  Hay 10 neuronas en la capa de salida de la red.  Si la primera neurona est√° activada, es decir, su valor de salida es ‚âà 1, esto indica que la red cree que la entrada fue 0. Si la segunda neurona est√° activada, la red cree que la entrada fue 1. Y as√≠ sucesivamente.  Estrictamente hablando, numeramos las neuronas de salida de 0 a 9 y observamos cu√°l de ellas ten√≠a el valor de activaci√≥n m√°ximo.  Si esto es, digamos, la neurona No. 6, entonces nuestra red cree que la entrada fue el n√∫mero 6. Y as√≠ sucesivamente. <br><br>  Quiz√°s se pregunte por qu√© necesitamos usar diez neuronas.  Despu√©s de todo, queremos saber qu√© d√≠gito del 0 al 9 corresponde a la imagen de entrada.  Ser√≠a natural usar solo 4 neuronas de salida, cada una de las cuales tomar√≠a un valor binario, dependiendo de si su valor de salida est√° m√°s cerca de 0 o 1. Cuatro neuronas ser√≠an suficientes, ya que 2 <sup>4</sup> = 16, m√°s de 10 valores posibles.  ¬øPor qu√© nuestra red deber√≠a usar 10 neuronas?  ¬øEs esto ineficaz?  La base para esto es emp√≠rica;  podemos probar ambas variantes de la red, y resulta que para esta tarea, una red con 10 neuronas de salida est√° mejor entrenada para reconocer n√∫meros que una red con 4.  Sin embargo, la pregunta sigue siendo, ¬øpor qu√© son mejores las 10 neuronas de salida?  ¬øHay alguna heur√≠stica que nos diga de antemano que se deben usar 10 neuronas de salida en lugar de 4? <br><br>  Para entender por qu√©, es √∫til pensar en lo que hace una red neuronal.  Primero, considere la opci√≥n con 10 neuronas de salida.  Nos centramos en la primera neurona de salida, que est√° tratando de decidir si la imagen entrante es cero.  Lo hace sopesando la evidencia obtenida de una capa oculta.  ¬øQu√© hacen las neuronas ocultas?  Supongamos que la primera neurona en la capa oculta determina si hay algo como esto en la imagen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/650/c35/402/650c3540204f98b406e25078ddc8742a.png"><br><br>  Puede hacer esto asignando pesos grandes a p√≠xeles que coincidan con esta imagen, y pesos peque√±os al resto.  Del mismo modo, suponga que las neuronas segunda, tercera y cuarta en la capa oculta est√°n buscando si hay fragmentos similares en la imagen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d94/4c3/cea/d944c3cea54d91106d2ee2198d04c801.png"><br><br>  Como habr√°s adivinado, todos estos cuatro fragmentos juntos dan la imagen 0, que vimos anteriormente: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f99/a20/1e5/f99a201e557bacdbe39190a6f913b49e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces, si las cuatro neuronas ocultas est√°n activadas, podemos concluir que el n√∫mero es 0. Por supuesto, esta no es la √∫nica evidencia de que 0 se mostr√≥ all√≠; podemos obtener 0 de muchas otras maneras (cambiando ligeramente estas im√°genes o distorsion√°ndolas ligeramente). </font><font style="vertical-align: inherit;">Sin embargo, podemos decir con certeza que, al menos en este caso, podemos concluir que hab√≠a 0 en la entrada.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si suponemos que la red funciona as√≠, podemos dar una explicaci√≥n plausible de por qu√© es mejor usar 10 neuronas de salida en lugar de 4. Si tuvi√©ramos 4 neuronas de salida, la primera neurona intentar√≠a decidir cu√°l es el bit m√°s significativo del d√≠gito entrante. Y no hay una manera f√°cil de asociar el bit m√°s significativo con las formas simples dadas anteriormente. Es dif√≠cil imaginar razones hist√≥ricas por las cuales las partes de la forma de un d√≠gito estar√≠an de alguna manera relacionadas con el bit m√°s significativo de la salida.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, todo lo anterior solo es compatible con la heur√≠stica. </font><font style="vertical-align: inherit;">Nada habla a favor del hecho de que una red de tres capas deber√≠a funcionar como dije, y las neuronas ocultas deber√≠an encontrar componentes simples de las formas. </font><font style="vertical-align: inherit;">Quiz√°s el complicado algoritmo de aprendizaje encuentre algunos pesos que nos permitir√°n usar solo 4 neuronas de salida. </font><font style="vertical-align: inherit;">Sin embargo, como m√©todo heur√≠stico, mi m√©todo funciona bien y puede ahorrarle un tiempo considerable en el desarrollo de una buena arquitectura NS.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ejercicios </font></font></h3><br><ul><li>      ,      .          ,     .         . ,   3   ,       (  )     0,99,       0,01. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/f5b/4af/2e9/f5b4af2e9acf846ab8cc5c60dac20c03.png"><br><br><h3>     </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces, tenemos el esquema de NA: ¬øc√≥mo aprender a reconocer los n√∫meros? Lo primero que necesitamos son datos de entrenamiento, los llamados conjunto de datos de entrenamiento. Utilizaremos el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kit MNIST que</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> contiene decenas de miles de im√°genes escaneadas de n√∫meros escritos a mano y su clasificaci√≥n correcta. El nombre que recibi√≥ MNIST debido a que es un subconjunto modificado de los dos conjuntos de datos recopilados por </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NIST</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , el Instituto Nacional de Est√°ndares y Tecnolog√≠a de EE. UU. Aqu√≠ hay algunas im√°genes de MNIST: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b36/135/6ae/b361356aec440b1dbf77c8dedbc6f9b6.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estos son los mismos n√∫meros que se dieron al comienzo del cap√≠tulo como tarea de reconocimiento. ¬°Por supuesto, cuando revise el NS, le pediremos que reconozca las im√°genes incorrectas que ya estaban en el conjunto de entrenamiento!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los datos de MNIST constan de dos partes. El primero contiene 60,000 im√°genes destinadas a la capacitaci√≥n. Estos son manuscritos escaneados de 250 personas, la mitad de los cuales eran empleados de la Oficina del Censo de los Estados Unidos, y la otra mitad eran estudiantes de secundaria. Las im√°genes son en blanco y negro, midiendo 28x28 p√≠xeles. La segunda parte del conjunto de datos MNIST es de 10.000 im√°genes para probar la red. Esta tambi√©n es una imagen en blanco y negro de 28x28 p√≠xeles. Utilizaremos estos datos para evaluar qu√© tan bien la red ha aprendido a reconocer los n√∫meros. Para mejorar la calidad de la evaluaci√≥n, estas cifras fueron tomadas de otras 250 personas que no participaron en la grabaci√≥n del conjunto de capacitaci√≥n (aunque tambi√©n eran empleados de la Oficina y estudiantes de secundaria). Esto nos ayuda a asegurarnos de que nuestro sistema pueda reconocer la escritura a mano de personas que no conoci√≥ durante la capacitaci√≥n.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La entrada de entrenamiento ser√° denotada por x. Ser√° conveniente tratar cada imagen de entrada x como un vector con 28x28 = 784 mediciones. Cada valor dentro del vector indica el brillo de un p√≠xel en la imagen. Denotaremos el valor de salida como y = y (x), donde y es un vector de diez dimensiones. Por ejemplo, si una determinada imagen de entrenamiento x contiene 6, entonces y (x) = (0,0,0,0,0,0,1,0,0,0) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ser√° el vector que necesitamos. T es una operaci√≥n de transposici√≥n que convierte un vector de fila en un vector de columna. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Queremos encontrar un algoritmo que nos permita buscar dichos pesos y compensaciones para que la salida de la red se acerque a y (x) para todas las entradas de entrenamiento x. Para cuantificar la aproximaci√≥n de este objetivo, definimos una funci√≥n de costo (a veces llamada </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">funci√≥n de p√©rdida</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">; en el libro usaremos la funci√≥n de costo, pero tenga en cuenta otro nombre):</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-394"><span class="MJXp-mtable" id="MJXp-Span-395"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-396" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-397" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-398">C</span><span class="MJXp-mo" id="MJXp-Span-399" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-400">w</span><span class="MJXp-mo" id="MJXp-Span-401" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-402">b</span><span class="MJXp-mo" id="MJXp-Span-403" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-404" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mfrac" id="MJXp-Span-405" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-406">1</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-407">2</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-408">n</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-409"><span class=""><span class="MJXp-mo" id="MJXp-Span-410" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">‚àë</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-411" style="margin-left: 0px;">x</span></span></span><span class="MJXp-mrow" id="MJXp-Span-412"><span class="MJXp-mo" id="MJXp-Span-413" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mrow" id="MJXp-Span-414"><span class="MJXp-mo" id="MJXp-Span-415" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-416">y</span><span class="MJXp-mo" id="MJXp-Span-417" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-418">x</span><span class="MJXp-mo" id="MJXp-Span-419" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-420" style="margin-left: 0em; margin-right: 0.222em;">‚Äì</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-421">a</span><span class="MJXp-mrow" id="MJXp-Span-422"><span class="MJXp-mo" id="MJXp-Span-423" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-msubsup" id="MJXp-Span-424"><span class="MJXp-mrow" id="MJXp-Span-425" style="margin-right: 0.05em;"><span class="MJXp-mo" id="MJXp-Span-426" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-427" style="vertical-align: 0.5em;">2</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-12"> C(w, b) = \frac{1}{2n} \sum_x || y(x) ‚Äì a ||^2 \tag{6} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aqu√≠ w denota un conjunto de pesos de red, b es un conjunto de desplazamientos, n es el n√∫mero de datos de entrada de entrenamiento, a es el vector de datos de salida cuando x son datos de entrada, y la suma pasa a trav√©s de toda la entrada de entrenamiento x. La salida, por supuesto, depende de x, w y b, pero por simplicidad no design√© esta dependencia. La notaci√≥n || v || significa la longitud del vector v. Llamaremos a C una funci√≥n de costo cuadr√°tico; a veces tambi√©n se denomina error est√°ndar o MSE. Si observa detenidamente C, puede ver que no es negativa, ya que todos los miembros de la suma no son negativos. Adem√°s, el costo de C (w, b) se vuelve peque√±o, es decir, C (w, b) ‚âà 0, precisamente cuando y (x) es aproximadamente igual al vector de salida a para todos los datos de entrada de entrenamiento x. Entonces, nuestro algoritmo funcion√≥ bien si logramos encontrar pesos y compensaciones tales que C (w, b) ‚âà 0. Y viceversa, funcion√≥ mal cuando C (w,b) grande: esto significa que y (x) no coincide con la salida para una gran cantidad de entrada. Resulta que el objetivo del algoritmo de entrenamiento es minimizar el costo de C (w, b) en funci√≥n de los pesos y las compensaciones. En otras palabras, necesitamos encontrar un conjunto de pesos y compensaciones que minimicen el valor del costo. Haremos esto usando un algoritmo llamado descenso de gradiente.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬øPor qu√© necesitamos un valor cuadr√°tico? ¬øNo nos interesa principalmente la cantidad de im√°genes que la red reconoce correctamente? ¬øEs posible simplemente maximizar este n√∫mero directamente y no minimizar el valor intermedio del valor cuadr√°tico? El problema es que el n√∫mero de im√°genes correctamente reconocidas no es una funci√≥n uniforme de los pesos y las compensaciones de la red. En su mayor parte, los peque√±os cambios en los pesos y las compensaciones no cambiar√°n el n√∫mero de im√°genes correctamente reconocidas. Debido a esto, es dif√≠cil entender c√≥mo cambiar los pesos y los prejuicios para mejorar la eficiencia. Si utilizamos una funci√≥n de costo uniforme, nos ser√° f√°cil entender c√≥mo hacer peque√±os cambios en los pesos y las compensaciones para mejorar el costo. Por lo tanto, primero nos centraremos en el valor cuadr√°tico y luego estudiaremos la precisi√≥n de la clasificaci√≥n.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Incluso teniendo en cuenta que queremos utilizar una funci√≥n de costo uniforme, ¬øpuede que le interese saber por qu√© elegimos la funci√≥n cuadr√°tica para la ecuaci√≥n (6)? ¬øNo es posible elegirlo arbitrariamente? ¬øQuiz√°s si elegimos una funci√≥n diferente, obtendr√≠amos un conjunto completamente diferente de minimizaci√≥n de pesos y compensaciones? Una pregunta razonable, y luego examinaremos nuevamente la funci√≥n de costo y haremos algunas correcciones. Sin embargo, la funci√≥n de costo cuadr√°tico funciona muy bien para comprender las cosas b√°sicas en el aprendizaje de NS, por lo que por ahora nos mantendremos firmes.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para resumir: nuestro objetivo en el entrenamiento de NS es encontrar pesos y compensaciones que minimicen la funci√≥n de costo cuadr√°tico C (w, b). La tarea est√° bien planteada, pero hasta ahora tiene muchas estructuras de distracci√≥n: la interpretaci√≥n de w y b como pesos y compensaciones, la funci√≥n œÉ oculta en el fondo, la elecci√≥n de la arquitectura de red, MNIST, etc. Resulta que podemos entender mucho, ignorando la mayor parte de esta estructura y concentr√°ndonos solo en el aspecto de la minimizaci√≥n. Entonces, por ahora, nos olvidaremos de la forma especial de la funci√≥n de costos, la comunicaci√≥n con la Asamblea Nacional, etc. En cambio, vamos a imaginar que solo tenemos una funci√≥n con muchas variables y queremos minimizarla. Desarrollaremos una tecnolog√≠a llamada descenso de gradiente, que puede usarse para resolver tales problemas. Y luego volvemos a una cierta funci√≥n,que queremos minimizar para la Asamblea Nacional.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bueno, digamos que estamos tratando de minimizar alguna funci√≥n C (v). Puede ser cualquier funci√≥n con valores reales de muchas variables v = v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ... Tenga en cuenta que reemplac√© la notaci√≥n w y b por v para mostrar que puede ser cualquier funci√≥n: ya no estamos obsesionados con HC. Es √∫til imaginar que una funci√≥n C tiene solo dos variables: v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/fff/752/1ad/fff7521ad0e339cb68eceace0f200697.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nos gustar√≠a encontrar d√≥nde C alcanza un m√≠nimo global. Por supuesto, con la funci√≥n dibujada arriba, podemos estudiar el gr√°fico y encontrar el m√≠nimo. En este sentido, ¬°puedo haberte dado una funci√≥n demasiado simple! En el caso general, C puede ser una funci√≥n compleja de muchas variables, y generalmente es imposible simplemente mirar el gr√°fico y encontrar el m√≠nimo.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Una forma de resolver el problema es usar √°lgebra para encontrar el m√≠nimo anal√≠ticamente. Podemos calcular las derivadas e intentar usarlas para encontrar el extremo. Si tenemos suerte, esto funcionar√° cuando C sea una funci√≥n de una o dos variables. Pero con una gran cantidad de variables, esto se convierte en una pesadilla. Y para las NS, a menudo necesitamos muchas m√°s variables: para las NS m√°s grandes, las funciones de costo de una manera compleja dependen de miles de millones de pesos y desplazamientos. ¬°Usar √°lgebra para minimizar estas funciones fallar√°!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Habiendo declarado que ser√≠a m√°s conveniente para nosotros considerar C como una funci√≥n de dos variables, dije dos veces en dos p√°rrafos "s√≠, pero ¬øy si es una funci√≥n de un n√∫mero mucho mayor de variables?" Pido disculpas. Cr√©enos que realmente ser√° √∫til representar C como una funci√≥n dos variables, es solo que a veces esta imagen se desmorona, por eso se necesitaban los dos p√°rrafos anteriores. Para el razonamiento matem√°tico, a menudo es necesario hacer malabarismos con varias representaciones intuitivas, aprendiendo al mismo tiempo cu√°ndo se puede usar la representaci√≥n y cu√°ndo no zya).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bien, eso significa que el √°lgebra no funcionar√°. Afortunadamente, existe una gran analog√≠a que ofrece un algoritmo que funciona bien. Imaginamos nuestra funci√≥n como algo as√≠ como un valle. Con el √∫ltimo horario, no ser√° tan dif√≠cil de hacer. E imaginamos una pelota rodando por la ladera del valle. Nuestra experiencia nos dice que la pelota finalmente se deslizar√° hasta el fondo. ¬øQuiz√°s podemos usar esta idea para encontrar el m√≠nimo de una funci√≥n? Seleccionamos al azar el punto de partida para una pelota imaginaria y luego simulamos el movimiento de la pelota, como si estuviera rodando hacia el fondo del valle. Podemos usar esta simulaci√≥n simplemente contando las derivadas (y, posiblemente, las segundas derivadas) de C: nos contar√°n todo sobre la forma local del valle y, por lo tanto, sobre c√≥mo rodar√° nuestra bola.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seg√∫n lo que escribiste, podr√≠as pensar que escribiremos las ecuaciones de movimiento de Newton para la pelota, consideraremos los efectos de la fricci√≥n y la gravedad, y as√≠ sucesivamente. De hecho, no estaremos tan cerca de seguir esta analog√≠a con la pelota: ¬°estamos desarrollando un algoritmo para minimizar C y no una simulaci√≥n exacta de las leyes de la f√≠sica! Esta analog√≠a deber√≠a estimular nuestra imaginaci√≥n y no limitar nuestro pensamiento. Entonces, en lugar de sumergirnos en los detalles complejos de la f√≠sica, hagamos la pregunta: si fu√©ramos nombrados dios por un d√≠a, y cre√°ramos nuestras propias leyes de la f√≠sica, dici√©ndole a la pelota c√≥mo rodar qu√© ley o las leyes de movimiento elegir√≠amos, para que la pelota siempre ruede sobre fondo del valle? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para aclarar el problema, pensaremos en lo que sucede si movemos la pelota una peque√±a distancia Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en la direcci√≥n de v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, y una peque√±a distancia Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en la direcci√≥n de v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">El √°lgebra nos dice que C cambia de la siguiente manera:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-428"><span class="MJXp-mtable" id="MJXp-Span-429"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-430" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-431" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-432">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-433">C</span><span class="MJXp-mo" id="MJXp-Span-434" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mfrac" id="MJXp-Span-435" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-436">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-437">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-438">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-439"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-440" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-441" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-442">Œî</span><span class="MJXp-msubsup" id="MJXp-Span-443"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-444" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-445" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-446" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mfrac" id="MJXp-Span-447" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-448">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-449">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-450">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-451"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-452" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-453" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-454">Œî</span><span class="MJXp-msubsup" id="MJXp-Span-455"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-456" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-457" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-13"> \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2 \tag{7} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Encontraremos una manera de elegir tales Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para que ŒîC sea menor que cero; </font><font style="vertical-align: inherit;">es decir, los seleccionaremos para que la bola ruede hacia abajo. </font><font style="vertical-align: inherit;">Para comprender c√≥mo hacer esto, es √∫til definir Œîv como el vector de cambios, es decir, Œîv ‚â° (Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , donde T es la operaci√≥n de transposici√≥n que convierte los vectores de fila en vectores de columna. </font><font style="vertical-align: inherit;">Tambi√©n definimos el vector gradiente de C como derivados parciales (‚àÇS / ‚àÇV </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ‚àÇS / ‚àÇV </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Denotamos el vector gradiente por ‚àá:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-458"><span class="MJXp-mtable" id="MJXp-Span-459"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-460" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-461" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-462">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-463">C</span><span class="MJXp-mo" id="MJXp-Span-464" style="margin-left: 0.333em; margin-right: 0.333em;">‚â°</span><span class="MJXp-mo" id="MJXp-Span-465" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mfrac" id="MJXp-Span-466" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-467">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-468">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-469">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-470"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-471" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-472" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-473" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mfrac" id="MJXp-Span-474" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-475">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-476">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-477">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-478"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-479" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-480" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-481"><span class="MJXp-mo" id="MJXp-Span-482" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-483" style="vertical-align: 0.5em;">T</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-14"> \nabla C \equiv (\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2})^T \tag{8} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pronto reescribiremos el cambio en ŒîC a trav√©s de Œîv y el gradiente ‚àáC. Mientras tanto, quiero aclarar algo, por lo que las personas a menudo cuelgan del gradiente. Cuando se encontraron por primera vez con ‚àáC, las personas a veces no entienden c√≥mo deber√≠an percibir el s√≠mbolo ‚àá. ¬øQu√© significa espec√≠ficamente? De hecho, puede considerar con seguridad ‚àáC un √∫nico objeto matem√°tico, un vector previamente definido, que simplemente se escribe con dos caracteres. Desde este punto de vista, ‚àá es como agitar una bandera que informa que "‚àáC es un vector de gradiente". Existen puntos de vista m√°s avanzados desde los cuales ‚àá puede considerarse como una entidad matem√°tica independiente (por ejemplo, como un operador de diferenciaci√≥n), pero no los necesitamos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Con tales definiciones, la expresi√≥n (7) puede reescribirse como:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-484"><span class="MJXp-mtable" id="MJXp-Span-485"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-486" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-487" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-488">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-489">C</span><span class="MJXp-mo" id="MJXp-Span-490" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mi" id="MJXp-Span-491">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-492">C</span><span class="MJXp-mo" id="MJXp-Span-493" style="margin-left: 0.267em; margin-right: 0.267em;">‚ãÖ</span><span class="MJXp-mi" id="MJXp-Span-494">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-495">v</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-15"> \Delta C \approx \nabla C \cdot \Delta v \tag{9} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esta ecuaci√≥n ayuda a explicar por qu√© ‚àáC se llama un vector de gradiente: conecta los cambios en v con los cambios en C, tal como se espera de una entidad llamada gradiente. [eng. gradiente - desviaci√≥n / aprox. transl.] Sin embargo, es m√°s interesante que esta ecuaci√≥n nos permita ver c√≥mo elegir Œîv para que ŒîC sea negativo. Digamos que elegimos</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-496"><span class="MJXp-mtable" id="MJXp-Span-497"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-498" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-499" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-500">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-501">v</span><span class="MJXp-mo" id="MJXp-Span-502" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-503" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-504">Œ∑</span><span class="MJXp-mi" id="MJXp-Span-505">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-506">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-16"> \Delta v = - \eta \nabla C \tag{10} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde Œ∑ es un peque√±o par√°metro positivo (velocidad de aprendizaje). Entonces la ecuaci√≥n (9) nos dice que ŒîC ‚âà - Œ∑ ‚àáC ‚ãÖ ‚àáC = - Œ∑ || ‚àáC || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Desde || ‚àáC || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚â• 0, esto asegura que ŒîC ‚â§ 0, es decir, C disminuir√° todo el tiempo si cambiamos v, como se prescribe en (10) (por supuesto, como parte de la aproximaci√≥n de la ecuaci√≥n (9)). ¬°Y esto es exactamente lo que necesitamos! Por lo tanto, tomamos la ecuaci√≥n (10) para determinar la "ley de movimiento" de la pelota en nuestro algoritmo de descenso de gradiente. Es decir, utilizaremos la ecuaci√≥n (10) para calcular el valor de Œîv, y luego moveremos la pelota a este valor:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-507"><span class="MJXp-mtable" id="MJXp-Span-508"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-509" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-510" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-511">v</span><span class="MJXp-mo" id="MJXp-Span-512" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msup" id="MJXp-Span-513"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-514" style="margin-right: 0.05em;">v</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-515" style="vertical-align: 0.5em;">‚Ä≤</span></span><span class="MJXp-mo" id="MJXp-Span-516" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-517">v</span><span class="MJXp-mo" id="MJXp-Span-518" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-519">Œ∑</span><span class="MJXp-mi" id="MJXp-Span-520">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-521">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-17"> v \rightarrow v' = v - \eta \nabla C \tag{11} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Luego, nuevamente aplicamos esta regla para el pr√≥ximo movimiento. Continuando con la repetici√≥n, bajaremos C hasta que, con suerte, alcancemos un m√≠nimo global. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En resumen, el descenso del gradiente funciona a trav√©s del c√°lculo secuencial del gradiente ‚àá C, y el desplazamiento posterior en la direcci√≥n opuesta, lo que conduce a una "ca√≠da" a lo largo de la pendiente del valle. Esto se puede visualizar de la siguiente manera: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f7/495/966/5f749596634bc20923f5f8a3e49a3b9f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tenga en cuenta que con esta regla, el descenso de gradiente no reproduce el movimiento f√≠sico real. En la vida real, la pelota tiene un impulso que puede permitirle rodar por la pendiente, o incluso rodar por alg√∫n tiempo. Solo despu√©s del trabajo de la fuerza de fricci√≥n se garantiza que la bola ruede por el valle. Nuestra regla de selecci√≥n Œîv solo dice "baja". ¬°Una regla bastante buena para encontrar el m√≠nimo!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para que el descenso de gradiente funcione correctamente, debemos elegir un valor suficientemente peque√±o de la velocidad de aprendizaje Œ∑ para que la ecuaci√≥n (9) sea una buena aproximaci√≥n. De lo contrario, puede resultar que ŒîC&gt; 0 - ¬°nada bueno! Al mismo tiempo, no es necesario que Œ∑ sea demasiado peque√±o, ya que los cambios en Œîv ser√°n peque√±os y el algoritmo funcionar√° muy lentamente. En la pr√°ctica, Œ∑ cambia de modo que la ecuaci√≥n (9) da una buena aproximaci√≥n y el algoritmo no funciona muy lentamente. M√°s adelante veremos c√≥mo funciona. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le expliqu√© el descenso del gradiente cuando la funci√≥n C depend√≠a solo de dos variables. Pero todo funciona de la misma manera si C es una funci√≥n de muchas variables. Supongamos que tiene m variables, v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ..., v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Entonces, el cambio en ŒîC causado por un peque√±o cambio en Œîv = (Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ..., Œîv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ser√°</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-522"><span class="MJXp-mtable" id="MJXp-Span-523"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-524" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-525" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-526">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-527">C</span><span class="MJXp-mo" id="MJXp-Span-528" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mi" id="MJXp-Span-529">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-530">C</span><span class="MJXp-mo" id="MJXp-Span-531" style="margin-left: 0.267em; margin-right: 0.267em;">‚ãÖ</span><span class="MJXp-mi" id="MJXp-Span-532">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-533">v</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-18"> \Delta C \approx \nabla C \cdot \Delta v \tag{12} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> donde el gradiente ‚àáC es el vector </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-534"><span class="MJXp-mtable" id="MJXp-Span-535"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-536" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-537" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-538">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-539">C</span><span class="MJXp-mo" id="MJXp-Span-540" style="margin-left: 0.333em; margin-right: 0.333em;">‚â°</span><span class="MJXp-mo" id="MJXp-Span-541" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mfrac" id="MJXp-Span-542" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-543">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-544">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-545">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-546"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-547" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-548" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-549" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-550" style="margin-left: 0em; margin-right: 0em;">‚Ä¶</span><span class="MJXp-mo" id="MJXp-Span-551" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mfrac" id="MJXp-Span-552" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-553">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-554">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-555">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-556"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-557" style="margin-right: 0.05em;">v</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-558" style="vertical-align: -0.4em;">m</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-559"><span class="MJXp-mo" id="MJXp-Span-560" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-561" style="vertical-align: 0.5em;">T</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-19"> \nabla C \equiv (\frac{\partial C}{\partial v_1},‚Ä¶, \frac{\partial C}{\partial v_m})^T \tag{13} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Al igual que con dos variables, podemos elegir </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-562"><span class="MJXp-mtable" id="MJXp-Span-563"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-564" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-565" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-566">Œî</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-567">v</span><span class="MJXp-mo" id="MJXp-Span-568" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-569" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-570">Œ∑</span><span class="MJXp-mi" id="MJXp-Span-571">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-572">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-20"> \Delta v = - \eta \nabla C \tag{14} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y aseg√∫rese de que nuestra expresi√≥n aproximada (12) para ŒîC sea negativa. </font><font style="vertical-align: inherit;">Esto nos da una forma de ir al m√≠nimo a lo largo del gradiente, incluso cuando C es una funci√≥n de muchas variables, aplicando la regla de actualizaci√≥n una y otra vez.</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-573"><span class="MJXp-mtable" id="MJXp-Span-574"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-575" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-576" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-577">v</span><span class="MJXp-mo" id="MJXp-Span-578" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msup" id="MJXp-Span-579"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-580" style="margin-right: 0.05em;">v</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-581" style="vertical-align: 0.5em;">‚Ä≤</span></span><span class="MJXp-mo" id="MJXp-Span-582" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-583">v</span><span class="MJXp-mo" id="MJXp-Span-584" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-585">Œ∑</span><span class="MJXp-mi" id="MJXp-Span-586">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-587">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-21-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-21"> v \rightarrow v' = v - \eta \nabla C \tag{15} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esta regla de actualizaci√≥n puede considerarse el algoritmo definitorio de descenso de gradiente. Nos da un m√©todo para cambiar repetidamente la posici√≥n de v en busca del m√≠nimo de la funci√≥n C. Esta regla no siempre funciona: varias cosas pueden salir mal, evitando que el descenso del gradiente encuentre el m√≠nimo global de C - volveremos a este punto en los siguientes cap√≠tulos. Pero en la pr√°ctica, el descenso de gradiente a menudo funciona muy bien, y veremos que en la Asamblea Nacional esta es una forma efectiva de minimizar la funci√≥n de costos y, por lo tanto, capacitar a la red.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En cierto sentido, el descenso de gradiente puede considerarse la estrategia de b√∫squeda m√≠nima √≥ptima. </font><font style="vertical-align: inherit;">Supongamos que estamos tratando de mover Œîv a una posici√≥n para minimizar C. Esto es equivalente a minimizar ŒîC ‚âà ‚àáC ‚ãÖ Œîv. </font><font style="vertical-align: inherit;">Limitaremos el tama√±o del paso para que || Œîv || </font><font style="vertical-align: inherit;">= Œµ para alguna peque√±a constante Œµ&gt; 0. En otras palabras, queremos mover una peque√±a distancia de un tama√±o fijo, y tratar de encontrar la direcci√≥n del movimiento que disminuya C tanto como sea posible. </font><font style="vertical-align: inherit;">Se puede demostrar que la elecci√≥n de Œîv minimizando ‚àáC ‚ãÖ Œîv es Œîv = -Œ∑‚àáC, donde Œ∑ = Œµ / || ‚àáC || est√° determinada por la restricci√≥n || Œîv || </font><font style="vertical-align: inherit;">= Œµ. </font><font style="vertical-align: inherit;">Por lo tanto, el descenso de gradiente puede considerarse una forma de dar peque√±os pasos en la direcci√≥n que disminuye m√°s C.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ejercicios </font></font></h3><br><ul><li>     . :      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  ‚Äî </a> , ,  ,      . </li><li>     ,      ,       .  ,       ?            ? </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Las personas han estudiado muchas opciones para el descenso en gradiente, incluidas aquellas que reproducen con mayor precisi√≥n una pelota f√≠sica real. Dichas opciones tienen sus ventajas, pero tambi√©n un gran inconveniente: la necesidad de calcular las segundas derivadas parciales de C, que pueden consumir muchos recursos. Para comprender esto, supongamos que necesitamos calcular todas las segundas derivadas parciales ‚àÇ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Si las variables v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j son</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> millones, entonces necesitamos calcular aproximadamente un bill√≥n (un mill√≥n al cuadrado) de las segundas derivadas parciales (en realidad, medio bill√≥n, ya que ‚àÇ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = ‚àÇ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àÇv </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Pero capturaste la esencia). Esto requerir√° muchos recursos inform√°ticos. Hay trucos para evitar esto, y la b√∫squeda de alternativas al descenso de gradiente es un √°rea de investigaci√≥n activa. Sin embargo, en este libro usaremos el descenso de gradiente y sus variantes como el enfoque principal para aprender NS. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬øC√≥mo aplicamos el gradiente descendente al aprendizaje de NA? Necesitamos usarlo para buscar pesos w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y compensaciones b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> que minimicen la ecuaci√≥n de costos (6). Reescribamos la regla de actualizaci√≥n de descenso de gradiente reemplazando las variables </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vj</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> con </font><font style="vertical-align: inherit;">pesos y compensaciones. En otras palabras, ahora nuestra "posici√≥n" tiene los componentes w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y el vector gradiente ‚àáC tiene los componentes correspondientes ‚àÇC / ‚àÇw</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y ‚àÇC / ‚àÇb </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Habiendo escrito nuestra regla de actualizaci√≥n con nuevos componentes, obtenemos:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-588"><span class="MJXp-mtable" id="MJXp-Span-589"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-590" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-591" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-592"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-593" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-594" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-595" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msubsup" id="MJXp-Span-596"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-597" style="margin-right: 0.05em;">w</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-599">‚Ä≤</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-598">k</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-600" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-601"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-602" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-603" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-604" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-605">Œ∑</span><span class="MJXp-mfrac" id="MJXp-Span-606" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-607">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-608">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-609">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-610"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-611" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-612" style="vertical-align: -0.4em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-22-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-22"> w_k \rightarrow w'_k = w_k - \eta \frac{\partial C}{\partial w_k} \tag{16} </script></p><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-613"><span class="MJXp-mtable" id="MJXp-Span-614"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-615" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-616" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-617"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-618" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-619" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-620" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msubsup" id="MJXp-Span-621"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-622" style="margin-right: 0.05em;">b</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-624">‚Ä≤</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-623">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-625" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-626"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-627" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-628" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-629" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-630">Œ∑</span><span class="MJXp-mfrac" id="MJXp-Span-631" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-632">‚àÇ</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-633">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-634">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-635"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-636" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-637" style="vertical-align: -0.4em;">l</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-23-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-23"> b_l \rightarrow b'_l = b_l - \eta \frac{\partial C}{\partial b_l} \tag{17} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al volver a aplicar esta regla de actualizaci√≥n, podemos "rodar cuesta abajo" y, con un poco de suerte, encontrar la funci√≥n de costo m√≠nimo. En otras palabras, esta regla puede usarse para entrenar a la Asamblea Nacional. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Existen varios obst√°culos para aplicar la regla de descenso de gradiente. Los estudiaremos con m√°s detalle en los siguientes cap√≠tulos. Pero por ahora, quiero mencionar solo un problema. Para entenderlo, volvamos al valor cuadr√°tico en la ecuaci√≥n (6). Tenga en cuenta que esta funci√≥n de costo se parece a C = 1 / n ‚àë </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , es decir, es el costo promedio C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚â° (|| y (x) ‚àía || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) / 2 para ejemplos de capacitaci√≥n individual. En la pr√°ctica, para calcular el gradiente ‚àáC necesitamos calcular los gradientes ‚àáC </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">por separado para cada entrada de entrenamiento x, y luego promediarlos, ‚àáC = 1 / n ‚àë </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àáC </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Desafortunadamente, cuando la cantidad de entrada ser√° muy grande, tomar√° mucho tiempo y dicha capacitaci√≥n ser√° lenta. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para acelerar el aprendizaje, puede usar el descenso de gradiente estoc√°stico. La idea es calcular aproximadamente el gradiente de ‚àáC calculando ‚àáC </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para una peque√±a muestra aleatoria de entrada de entrenamiento. Al calcular su promedio, podemos obtener r√°pidamente una buena estimaci√≥n del gradiente verdadero ‚àáC, y esto ayuda a acelerar el descenso del gradiente y, por lo tanto, al entrenamiento.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al formular con mayor precisi√≥n, el descenso de gradiente estoc√°stico funciona mediante un muestreo aleatorio de un peque√±o n√∫mero de m datos de entrada de entrenamiento. </font><font style="vertical-align: inherit;">Llamaremos a estos datos aleatorios X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , .., X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y lo llamaremos un mini paquete. </font><font style="vertical-align: inherit;">Si el tama√±o de la muestra m es lo suficientemente grande, el valor promedio de ‚àáC </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ser√° lo suficientemente cercano al promedio de todos los ‚àáCx, es decir</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-638"><span class="MJXp-mtable" id="MJXp-Span-639"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-640" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-641" style="text-align: center;"><span class="MJXp-mfrac" id="MJXp-Span-642" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-643"><span class="MJXp-mo" id="MJXp-Span-644" style="margin-left: 0.111em; margin-right: 0.05em;">‚àë</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-649">m</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-645"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-646">j</span><span class="MJXp-mo" id="MJXp-Span-647">=</span><span class="MJXp-mn" id="MJXp-Span-648">1</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-650">‚àá</span><span class="MJXp-msubsup" id="MJXp-Span-651"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-652" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-653" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-654"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-655" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-656" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-657">m</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-658" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mfrac" id="MJXp-Span-659" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-660"><span class="MJXp-mo" id="MJXp-Span-661" style="margin-left: 0.111em; margin-right: 0.05em;">‚àë</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-662" style="vertical-align: -0.4em;">x</span></span><span class="MJXp-mi" id="MJXp-Span-663">‚àá</span><span class="MJXp-msubsup" id="MJXp-Span-664"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-665" style="margin-right: 0.05em;">C</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-666" style="vertical-align: -0.4em;">x</span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-667">n</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-668" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi" id="MJXp-Span-669">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-670">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-24-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-24"> \frac{\sum^m_{j=1} \nabla C_{X_j}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C \tag{18} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde la segunda cantidad va sobre todo el conjunto de datos de entrenamiento. </font><font style="vertical-align: inherit;">Al intercambiar partes, obtenemos</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-671"><span class="MJXp-mtable" id="MJXp-Span-672"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-673" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-674" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-675">‚àá</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-676">C</span><span class="MJXp-mo" id="MJXp-Span-677" style="margin-left: 0.333em; margin-right: 0.333em;">‚âà</span><span class="MJXp-mfrac" id="MJXp-Span-678" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-679">1</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-680">m</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-681"><span><span class="MJXp-over"><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-687" style="margin-right: 0px; margin-left: 0px;">m</span></span><span class=""><span class="MJXp-mo" id="MJXp-Span-682" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">‚àë</span></span></span></span></span><span class=" MJXp-script"><span class="MJXp-mrow" id="MJXp-Span-683" style="margin-left: 0px;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-684">j</span><span class="MJXp-mo" id="MJXp-Span-685">=</span><span class="MJXp-mn" id="MJXp-Span-686">1</span></span></span></span><span class="MJXp-mi" id="MJXp-Span-688">‚àá</span><span class="MJXp-msubsup" id="MJXp-Span-689"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-690" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-691" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-692"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-693" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-694" style="vertical-align: -0.4em;">j</span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-25-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-25"> \nabla C \approx \frac{1}{m} \sum^m_{j=1} \nabla C_{X_j} \tag{19} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lo que confirma que podemos estimar el gradiente general calculando los gradientes para un minipack seleccionado al azar. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para relacionar esto directamente con el entrenamiento de NS, supongamos que w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> denotan los pesos y desplazamientos de nuestro NS. </font><font style="vertical-align: inherit;">Luego, el descenso de gradiente estoc√°stico selecciona un mini paquete aleatorio de datos de entrada y aprende de ellos</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-695"><span class="MJXp-mtable" id="MJXp-Span-696"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-697" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-698" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-699"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-700" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-701" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-702" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msubsup" id="MJXp-Span-703"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-704" style="margin-right: 0.05em;">w</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-706">‚Ä≤</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-705">k</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-707" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-708"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-709" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-710" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-711" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mfrac" id="MJXp-Span-712" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-713">Œ∑</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-714">m</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-715"><span class=""><span class="MJXp-mo" id="MJXp-Span-716" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">‚àë</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-717" style="margin-left: 0px;">j</span></span></span><span class="MJXp-mfrac" id="MJXp-Span-718" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-719">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-720"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-721" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-722" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-723"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-724" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-725" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-726">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-727"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-728" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-729" style="vertical-align: -0.4em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-26-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-26"> w_k \rightarrow w'_k = w_k - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20} </script></p><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-730"><span class="MJXp-mtable" id="MJXp-Span-731"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-732" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-733" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-734"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-735" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-736" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-737" style="margin-left: 0.333em; margin-right: 0.333em;">‚Üí</span><span class="MJXp-msubsup" id="MJXp-Span-738"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-739" style="margin-right: 0.05em;">b</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-741">‚Ä≤</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-740">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-742" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-743"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-744" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-745" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-746" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mfrac" id="MJXp-Span-747" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-748">Œ∑</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-749">m</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-750"><span class=""><span class="MJXp-mo" id="MJXp-Span-751" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">‚àë</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-752" style="margin-left: 0px;">j</span></span></span><span class="MJXp-mfrac" id="MJXp-Span-753" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-754">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-755"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-756" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-757" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-758"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-759" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-760" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-761">‚àÇ</span><span class="MJXp-msubsup" id="MJXp-Span-762"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-763" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-764" style="vertical-align: -0.4em;">l</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-27-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-27"> b_l \rightarrow b'_l = b_l - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l} \tag{21} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde es la suma de todos los ejemplos de entrenamiento X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en el minipaquete actual. </font><font style="vertical-align: inherit;">Luego seleccionamos otro mini-paquete al azar y estudiamos en √©l. </font><font style="vertical-align: inherit;">Y as√≠ sucesivamente, hasta agotar todos los datos de entrenamiento, lo que se llama el final de la era de entrenamiento. </font><font style="vertical-align: inherit;">En este momento, estamos comenzando de nuevo una nueva era de aprendizaje.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por cierto, vale la pena se√±alar que los acuerdos sobre el escalado de la funci√≥n de costos y la actualizaci√≥n de los pesos y las compensaciones difieren en un mini paquete. En la ecuaci√≥n (6), escalamos la funci√≥n de costo 1 / n veces. A veces las personas omiten 1 / n al sumar los costos de los ejemplos de capacitaci√≥n individual, en lugar de calcular el promedio. Esto es √∫til cuando el n√∫mero total de ejemplos de entrenamiento no se conoce de antemano. Esto puede suceder, por ejemplo, cuando aparecen datos adicionales en tiempo real. Del mismo modo, las reglas de actualizaci√≥n de mini-paquete (20) y (21) a veces omiten el miembro de 1 / m delante de la suma. Conceptualmente, esto no afecta nada, ya que es equivalente a un cambio en la velocidad de aprendizaje Œ∑. Sin embargo, vale la pena prestar atenci√≥n al comparar varios trabajos.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un descenso de gradiente estoc√°stico puede considerarse como un voto pol√≠tico: es mucho m√°s f√°cil tomar una muestra en forma de un mini paquete que aplicar un descenso de gradiente a una muestra completa, al igual que una encuesta a la salida de un sitio es m√°s f√°cil que realizar una elecci√≥n completa. Por ejemplo, si nuestro conjunto de entrenamiento tiene un tama√±o de n = 60,000, como MNIST, y hacemos una muestra de un mini paquete de tama√±o m = 10, ¬°entonces aceleraremos la estimaci√≥n del gradiente 6000 veces! Por supuesto, la estimaci√≥n no ser√° ideal, habr√° fluctuaciones estad√≠sticas, pero no es necesario que sea ideal: solo necesitamos movernos en la direcci√≥n que disminuye C, lo que significa que no necesitamos calcular el gradiente con precisi√≥n. En la pr√°ctica, el descenso gradiente estoc√°stico es una t√©cnica de ense√±anza com√∫n y poderosa para la Asamblea Nacional, y la base de la mayor√≠a de las tecnolog√≠as de ense√±anza que desarrollaremos como parte del libro.</font></font><br><br><h3>  </h3><br><ul><li>       -  1.  ,    x         w <sub>k</sub> ‚Üí w‚Ä≤ <sub>k</sub> = w <sub>k</sub> ‚àí Œ∑ ‚àÇC <sub>x</sub> / ‚àÇw <sub>k</sub>  b <sub>l</sub> ‚Üí b‚Ä≤ <sub>l</sub> = b <sub>l</sub> ‚àí Œ∑ ‚àÇC <sub>x</sub> / ‚àÇb <sub>l</sub> .              .  Y as√≠ sucesivamente.   ,  -,   .  -            ( ).       -         -  20. </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perm√≠tanme terminar esta secci√≥n con una discusi√≥n sobre un tema que a veces molesta a las personas que primero han encontrado un descenso de gradiente. En NS, el valor de C es una funci√≥n de muchas variables (todos los pesos y compensaciones) y, en cierto sentido, determina la superficie en un espacio muy multidimensional. La gente comienza a pensar: "Tendr√© que visualizar todas estas dimensiones adicionales". Y comienzan a preocuparse: "No puedo navegar en cuatro dimensiones, sin mencionar cinco (o cinco millones)". ¬øTienen alguna cualidad especial que tienen las supermatem√°ticas "reales"? Por supuesto que no. Incluso los matem√°ticos profesionales no pueden visualizar el espacio de cuatro dimensiones bastante bien, si es que lo hacen. Van a trucos, desarrollando otras formas de representar lo que est√° sucediendo. Esto es exactamente lo que hicimos:Utilizamos la representaci√≥n algebraica (en lugar de visual) de ŒîC para comprender c√≥mo moverse para que C disminuya. Las personas que hacen un buen trabajo con una gran cantidad de dimensiones tienen en mente una gran biblioteca de t√©cnicas similares; Nuestro truco algebraico es solo un ejemplo. Puede que estas t√©cnicas no sean tan simples como estamos acostumbrados al visualizar tres dimensiones, pero cuando crea una biblioteca de t√©cnicas similares, comienza a pensar bien en dimensiones superiores. No entrar√© en detalles, pero si est√° interesado, puede que le gustea qu√© estamos acostumbrados cuando visualizamos tres dimensiones, pero cuando creas una biblioteca de t√©cnicas similares, comienzas a pensar bien en dimensiones superiores. No entrar√© en detalles, pero si est√° interesado, puede que le gustea qu√© estamos acostumbrados cuando visualizamos tres dimensiones, pero cuando creas una biblioteca de t√©cnicas similares, comienzas a pensar bien en dimensiones superiores. No entrar√© en detalles, pero si est√° interesado, puede que le guste</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Una discusi√≥n de algunas de estas t√©cnicas por</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> matem√°ticos profesionales que est√°n acostumbrados a pensar en dimensiones superiores. </font><font style="vertical-align: inherit;">Aunque algunas de las t√©cnicas discutidas son bastante complejas, la mayor√≠a de las mejores respuestas son intuitivas y accesibles para todos.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Implementaci√≥n de una red para clasificar n√∫meros </font></font></h3><br>  Bien, ahora escribamos un programa que aprenda a reconocer los d√≠gitos escritos a mano utilizando el descenso de gradiente estoc√°stico y los datos de entrenamiento de MNIST.  ¬°Haremos esto con un programa corto en Python 2.7 que consta de solo 74 l√≠neas!  Lo primero que necesitamos es descargar los datos de MNIST.  Si usa git, puede obtenerlos clonando el repositorio de este libro: <br><br> <code>git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git</code> <br> <br>  Si no, descargue el c√≥digo <a href="">desde el enlace</a> . <br><br>  Por cierto, cuando mencion√© los datos de MNIST anteriormente, dije que est√°n divididos en 60,000 im√°genes de entrenamiento y 10,000 im√°genes de prueba.  Esta es la descripci√≥n oficial de MNIST.  Romperemos los datos de manera un poco diferente.  Dejaremos las im√°genes de verificaci√≥n sin cambios, pero dividiremos el conjunto de capacitaci√≥n en dos partes: 50,000 im√°genes, que usaremos para capacitar a la Asamblea Nacional, e 10,000 im√°genes individuales para confirmaci√≥n adicional.  Si bien no los utilizaremos, m√°s adelante nos ser√°n √∫tiles cuando comprendamos la configuraci√≥n de algunos hiperpar√°metros del NS, la velocidad de aprendizaje, etc., que nuestro algoritmo no selecciona directamente.  Aunque los datos que corroboran no son parte de la especificaci√≥n MNIST original, muchos usan MNIST de esta manera, y en el campo de HC el uso de datos corroborantes es com√∫n.  Ahora, hablando de los "datos de entrenamiento MNIST", me referir√© a nuestros 50,000 karitnoks, no los 60,000 originales. <br><br>  Adem√°s de los datos MNIST, tambi√©n necesitamos una biblioteca de Python llamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Numpy</a> para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√°lculos</a> r√°pidos de √°lgebra lineal.  Si no lo tiene, puede tomarlo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">desde el enlace</a> . <br><br>  Antes de darle todo el programa, perm√≠tame explicarle las caracter√≠sticas principales del c√≥digo para NS.  El lugar central est√° ocupado por la clase Red, que usamos para representar a la Asamblea Nacional.  Aqu√≠ est√° el c√≥digo de inicializaci√≥n para el objeto de red: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, sizes)</span></span></span><span class="hljs-function">:</span></span> self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  La matriz de tama√±os contiene el n√∫mero de neuronas en las capas correspondientes.  Entonces, si queremos crear un objeto de red con dos neuronas en la primera capa, tres neuronas en la segunda capa y una neurona en la tercera, entonces lo escribiremos as√≠: <br><br><pre> <code class="python hljs">net = Network([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre> <br>  Las compensaciones y los pesos en el objeto Red se inicializan aleatoriamente utilizando la funci√≥n numpy np.random.randn, que genera una distribuci√≥n gaussiana con una expectativa matem√°tica de 0 y una desviaci√≥n est√°ndar de 1. Esta inicializaci√≥n aleatoria le da a nuestro algoritmo de descenso de gradiente estoc√°stico un punto de partida.  En los siguientes cap√≠tulos encontraremos las mejores formas de inicializar pesos y compensaciones, pero por ahora esto es suficiente.  Tenga en cuenta que el c√≥digo de inicializaci√≥n de la red supone que se ingresar√° la primera capa de neuronas, y no les asigna un sesgo, ya que solo se usan para calcular la salida. <br><br>  Tambi√©n tenga en cuenta que las compensaciones y los pesos se almacenan como una matriz de matrices Numpy.  Por ejemplo, net.weights [1] es una matriz de Numpy que almacena los pesos que conectan las capas segunda y tercera de las neuronas (esta no es la primera y la segunda capa, ya que en Python la numeraci√≥n de los elementos de la matriz proviene de cero).  Como tomar√° demasiado tiempo escribir net.weights [1], denotamos esta matriz como w.  Esta es una matriz tal que w <sub>jk</sub> es el peso de la conexi√≥n entre la kth neurona en la segunda capa y la jth neurona en la tercera.  Tal orden de √≠ndices j y k puede parecer extra√±o, ¬øno ser√≠a m√°s l√≥gico intercambiarlos?  Pero la gran ventaja de tal registro es que se obtiene el vector de activaci√≥n de la tercera capa de neuronas: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-765"><span class="MJXp-msup" id="MJXp-Span-766"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-767" style="margin-right: 0.05em;">a</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-768" style="vertical-align: 0.5em;">‚Ä≤</span></span><span class="MJXp-mo" id="MJXp-Span-769" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-770">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-771">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-772">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-773">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-774">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-775">a</span><span class="MJXp-mo" id="MJXp-Span-776" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-777">w</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-778">a</span><span class="MJXp-mo" id="MJXp-Span-779" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-780">b</span><span class="MJXp-mo" id="MJXp-Span-781" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-782">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-783">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-784">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-785">g</span><span class="MJXp-mrow" id="MJXp-Span-786"><span class="MJXp-mn" id="MJXp-Span-787">22</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-28-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-28"> a '= \ sigma (wa + b) \ tag {22} </script></p><br><br>  Veamos esta ecuaci√≥n bastante rica.  a es el vector de activaci√≥n de la segunda capa de neuronas.  Para obtener a ', multiplicamos a por la matriz de peso w, y sumamos el vector de desplazamiento b.  Luego aplicamos el elemento sigmoide œÉ elemento por elemento a cada elemento del vector wa + b (esto se llama vectorizaci√≥n de la funci√≥n œÉ).  Es f√°cil verificar que la ecuaci√≥n (22) da el mismo resultado que la regla (4) para calcular una neurona sigmoidea. <br><br><h3>  Ejercicio </h3><br><ul><li>  Escriba la ecuaci√≥n (22) en forma de componente y aseg√∫rese de que da el mismo resultado que la regla (4) para calcular una neurona sigmoidea. </li></ul><br>  Con todo esto en mente, es f√°cil escribir c√≥digo que calcule la salida de un objeto de red.  Comenzamos definiendo un sigmoide: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sigmoid</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0</span></span>/(<span class="hljs-number"><span class="hljs-number">1.0</span></span>+np.exp(-z))</code> </pre> <br>  Tenga en cuenta que cuando el par√°metro z es un vector o matriz de Numpy, Numpy aplicar√° autom√°ticamente el elemento sigmoide en cuanto a elementos, es decir, en forma de vector. <br><br>  Agregue un m√©todo de propagaci√≥n directa a la clase de red, que toma una entrada de la red como entrada y devuelve la salida correspondiente.  Se supone que el par√°metro a es (n, 1) Numpy ndarray, no un vector (n,).  Aqu√≠ n es el n√∫mero de neuronas de entrada.  Si intenta usar el vector (n,), obtendr√° resultados extra√±os. <br><br>  El m√©todo simplemente aplica la ecuaci√≥n (22) a cada capa: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">feedforward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, a)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""       "a"""</span></span><span class="hljs-string"><span class="hljs-string">" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a</span></span></code> </pre> <br>  Por supuesto, b√°sicamente los objetos de la red los necesitamos para aprender.  Para hacer esto, les daremos el m√©todo SGD, que implementa el descenso de gradiente estoc√°stico.  Aqu√≠ est√° su c√≥digo.  En algunos lugares es bastante misterioso, pero a continuaci√≥n lo analizaremos con m√°s detalle. <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SGD</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, training_data, epochs, mini_batch_size, eta, test_data=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    -    . training_data ‚Äì   "(x, y)",       .       .  test_data ,          ,     .     ,    . """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> test_data: n_test = len(test_data) n = len(training_data) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">0</span></span>, n, mini_batch_size)] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> mini_batch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mini_batches: self.update_mini_batch(mini_batch, eta) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> test_data: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Epoch {0}: {1} / {2}"</span></span>.format( j, self.evaluate(test_data), n_test) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Epoch {0} complete"</span></span>.format(j)</code> </pre> <br>  training_data es una lista de tuplas "(x, y)" que representan la entrada de entrenamiento y la salida deseada.  Las variables epochs y mini_batch_size son el n√∫mero de epochs para aprender y el tama√±o de los mini-paquetes para usar.  eta - velocidad de aprendizaje, Œ∑.  Si test_data est√° configurado, la red se evaluar√° con los datos de verificaci√≥n despu√©s de cada era, y se mostrar√° el progreso actual.  Esto es √∫til para seguir el progreso, pero ralentiza significativamente el trabajo. <br><br>  El c√≥digo funciona as√≠.  En cada era, comienza mezclando accidentalmente datos de entrenamiento y luego los divide en mini paquetes del tama√±o correcto.  Esta es una manera f√°cil de crear una muestra de datos de entrenamiento.  Luego, para cada mini_batch aplicamos un paso de descenso de gradiente.  Esto se realiza mediante el c√≥digo self.update_mini_batch (mini_batch, eta), que actualiza los pesos y las compensaciones de la red de acuerdo con una iteraci√≥n del descenso del gradiente utilizando solo datos de entrenamiento en mini_batch.  Aqu√≠ est√° el c√≥digo para el m√©todo update_mini_batch: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update_mini_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, mini_batch, eta)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    ,          -. mini_batch ‚Äì    (x, y),  eta ‚Äì  ."""</span></span> nabla_b = [np.zeros(b.shape) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.biases] nabla_w = [np.zeros(w.shape) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.weights] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> nb, dnb <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> nw, dnw <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w, nw <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b, nb <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.biases, nabla_b)]</code> </pre> <br>  La mayor parte del trabajo se realiza por l√≠nea. <br><br><pre> <code class="python hljs"> delta_nabla_b, delta_nabla_w = self.backprop(x, y)</code> </pre> <br>  Invoca un algoritmo de retropropagaci√≥n, una forma r√°pida de calcular el gradiente de una funci√≥n de costo.  As√≠ que update_mini_batch simplemente calcula estos gradientes para cada ejemplo de entrenamiento de mini_batch, y luego actualiza self.weights y self.biases. <br><br>  Hasta ahora, no demostrar√© c√≥digo para self.backprop.  Aprenderemos sobre la retropropagaci√≥n en el pr√≥ximo cap√≠tulo, y habr√° un c√≥digo self.backprop.  Por ahora, suponga que se comporta como se indica, devolviendo un gradiente apropiado para el costo asociado con el ejemplo de capacitaci√≥n x. <br><br>  Veamos todo el programa, incluidos los comentarios explicativos.  Con la excepci√≥n de la funci√≥n self.backprop, el programa habla por s√≠ mismo: el trabajo principal lo realizan self.SGD y self.update_mini_batch.  El m√©todo self.backprop utiliza varias funciones adicionales para calcular el gradiente, a saber, sigmoid_prime, que calcula la derivada del sigmoid, y self.cost_derivative, que no describir√© aqu√≠.  Puede hacerse una idea de ellos mirando el c√≥digo y los comentarios.  En el pr√≥ximo cap√≠tulo los consideraremos con m√°s detalle.  Tenga en cuenta que si bien el programa parece largo, la mayor parte del c√≥digo son comentarios que hacen que sea m√°s f√°cil de entender.  De hecho, el programa en s√≠ consta de solo 74 l√≠neas de c√≥digo que no est√°n vac√≠as ni comentarios.  Todo el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo est√° disponible en GitHub</a> . <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" network.py ~~~~~~~~~~           .      .     ,    .   ,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #   import random #   import numpy as np class Network(object): def __init__(self, sizes): """  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].               0    1. ,      ,       ,        . """ self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] def feedforward(self, a): """   ,  ``a`` -  .""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """    -    . training_data ‚Äì   "(x, y)",       .       .  test_data ,          ,     .     ,    . """ if test_data: n_test = len(test_data) n = len(training_data) for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoch {0}: {1} / {2}".format( j, self.evaluate(test_data), n_test) else: print "Epoch {0} complete".format(j) def update_mini_batch(self, mini_batch, eta): """    ,          -. mini_batch ‚Äì    (x, y),  eta ‚Äì  .""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """  ``(nabla_b, nabla_w)``,      C_x. ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #   activation = x activations = [x] #      zs = [] #     z- for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) #   delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) """ l      ,      . l = 1    , l = 2 ‚Äì ,   .    ,   python      .""" for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): """    ,      .    ‚Äì          .""" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) def cost_derivative(self, output_activations, y): """    ( C_x /  a)   .""" return (output_activations-y) ####   def sigmoid(z): """.""" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): """ .""" return sigmoid(z)*(1-sigmoid(z))</span></span></code> </pre> <br>  ¬øQu√© tan bien reconoce el programa los n√∫meros escritos a mano?  Comencemos cargando los datos MNIST.  Haremos esto usando el peque√±o programa auxiliar mnist_loader.py, que describir√© a continuaci√≥n.  Ejecute los siguientes comandos en el shell de Python: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper()</code> </pre> <br>  Esto, por supuesto, se puede hacer en un programa separado, pero si trabaja en paralelo con un libro, ser√° m√°s f√°cil. <br><br>  Despu√©s de descargar los datos de MNIST, configure una red de 30 neuronas ocultas.  Haremos esto despu√©s de importar el programa descrito anteriormente, que se llama red: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network &gt;&gt;&gt; net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])</code> </pre> <br>  Finalmente, utilizamos el descenso de gradiente estoc√°stico para el entrenamiento en datos de entrenamiento para 30 eras, con un tama√±o de mini paquete de 10 y una velocidad de aprendizaje de Œ∑ = 3.0: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">3.0</span></span>, test_data=test_data)</code> </pre> <br>  Si est√° ejecutando c√≥digo en paralelo con la lectura de un libro, tenga en cuenta que la ejecuci√≥n tardar√° varios minutos.  Le sugiero que comience todo, contin√∫e leyendo y verifique peri√≥dicamente lo que produce el programa.  Si tiene prisa, puede reducir la cantidad de eras reduciendo la cantidad de neuronas ocultas o utilizando solo una parte de los datos de entrenamiento.  El c√≥digo de trabajo final funcionar√° m√°s r√°pido: ¬°estos scripts de Python est√°n dise√±ados para hacerle entender c√≥mo funciona la red y no son de alto rendimiento!  Y, por supuesto, despu√©s de la capacitaci√≥n, la red puede funcionar muy r√°pidamente en casi cualquier plataforma inform√°tica.  Por ejemplo, cuando ense√±amos a la red una buena selecci√≥n de pesos y compensaciones, se puede portar f√°cilmente para trabajar en JavaScript en un navegador web o como una aplicaci√≥n nativa en un dispositivo m√≥vil.  En cualquier caso, el programa que entrena la red neuronal hace aproximadamente la misma conclusi√≥n.  Ella escribe el n√∫mero de im√°genes de prueba correctamente reconocidas despu√©s de cada era de entrenamiento.  Como puede ver, incluso despu√©s de una era, alcanza una precisi√≥n de 9,129 de 10,000, y este n√∫mero contin√∫a creciendo: <br><br> <code>Epoch 0: 9129 / 10000 <br> Epoch 1: 9295 / 10000 <br> Epoch 2: 9348 / 10000 <br> ... <br> Epoch 27: 9528 / 10000 <br> Epoch 28: 9542 / 10000 <br> Epoch 29: 9534 / 10000</code> <br> <br>  ¬°Resulta que la red entrenada da un porcentaje de clasificaci√≥n correcta de aproximadamente 95 - 95.42% como m√°ximo!  Un primer intento bastante prometedor.  Le advierto que su c√≥digo no necesariamente producir√° exactamente los mismos resultados, ya que inicializamos la red con ponderaciones y compensaciones aleatorias.  Para este cap√≠tulo, he elegido el mejor de tres intentos. <br><br>  Reiniciemos el experimento cambiando el n√∫mero de neuronas ocultas a 100. Como antes, si ejecuta el c√≥digo al mismo tiempo que lee, tenga en cuenta que lleva bastante tiempo (en mi m√°quina, cada era toma varias decenas de segundos), por lo que es mejor leer en paralelo con ejecuci√≥n de c√≥digo. <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">3.0</span></span>, test_data=test_data)</code> </pre> <br>  Naturalmente, esto mejora el resultado al 96.59%.  Al menos en este caso, usar m√°s neuronas ocultas ayuda a obtener mejores resultados. <br><br>  Los comentarios de los lectores sugieren que los resultados de este experimento var√≠an mucho y que algunos resultados de aprendizaje son mucho peores.  Usando t√©cnicas del Cap√≠tulo 3 para reducir seriamente la diversidad de la eficiencia del trabajo de una carrera a otra. <br><br>  Por supuesto, para lograr tal precisi√≥n, tuve que elegir un cierto n√∫mero de eras para aprender, el tama√±o del mini paquete y la velocidad de aprendizaje Œ∑.  Como mencion√© anteriormente, se denominan hiperpar√°metros de nuestra Asamblea Nacional, para distinguirlos de los par√°metros simples (pesos y compensaciones) que el algoritmo ajusta durante el entrenamiento.  Si elegimos mal los hiperpar√°metros, obtendremos malos resultados.  Supongamos, por ejemplo, que hemos elegido la tasa de aprendizaje Œ∑ = 0.001: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, test_data=test_data)</code> </pre> <br>  Los resultados son mucho menos impresionantes: <br><br> <code>Epoch 0: 1139 / 10000 <br> Epoch 1: 1136 / 10000 <br> Epoch 2: 1135 / 10000 <br> ... <br> Epoch 27: 2101 / 10000 <br> Epoch 28: 2123 / 10000 <br> Epoch 29: 2142 / 10000</code> <br> <br>  Sin embargo, puede ver que la eficiencia de la red crece lentamente con el tiempo.  Esto sugiere que puede intentar aumentar la velocidad de aprendizaje, por ejemplo, a 0.01.  En este caso, los resultados ser√°n mejores, lo que indica la necesidad de aumentar a√∫n m√°s la velocidad (si el cambio mejora la situaci√≥n, ¬°cambie m√°s!).  Si hace esto varias veces, finalmente llegaremos a Œ∑ = 1.0 (y a veces incluso 3.0), que est√° cerca de nuestros experimentos anteriores.  Entonces, aunque inicialmente seleccionamos mal los hiperpar√°metros, al menos reunimos suficiente informaci√≥n para poder mejorar nuestra elecci√≥n de par√°metros. <br><br>  En general, la depuraci√≥n de NA es un asunto complicado.  Esto es especialmente cierto cuando la elecci√≥n de hiperpar√°metros iniciales produce resultados que no exceden el ruido aleatorio.  Supongamos que intentamos usar una arquitectura exitosa de 30 neuronas, pero cambiamos la velocidad de aprendizaje a 100.0: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">100.0</span></span>, test_data=test_data)</code> </pre> <br>  Al final, resulta que fuimos demasiado lejos y tomamos demasiada velocidad: <br><br> <code>Epoch 0: 1009 / 10000 <br> Epoch 1: 1009 / 10000 <br> Epoch 2: 1009 / 10000 <br> Epoch 3: 1009 / 10000 <br> ... <br> Epoch 27: 982 / 10000 <br> Epoch 28: 982 / 10000 <br> Epoch 29: 982 / 10000</code> <br> <br>  Ahora imagine que nos estamos acercando a esta tarea por primera vez.  Por supuesto, sabemos por los primeros experimentos que ser√≠a correcto reducir la velocidad de aprendizaje.  Pero si tuvi√©ramos que abordar esta tarea por primera vez, no tendr√≠amos resultados que pudieran llevarnos a la soluci√≥n correcta.  ¬øPodr√≠amos comenzar a pensar que quiz√°s hemos elegido los par√°metros iniciales incorrectos para los pesos y las compensaciones, y que es dif√≠cil para la red aprender?  ¬øO tal vez no tenemos suficientes datos de entrenamiento para obtener un resultado significativo?  ¬øQuiz√°s no esperamos suficientes eras?  ¬øQuiz√°s una red neuronal con tal arquitectura simplemente no puede aprender a reconocer n√∫meros escritos a mano?  ¬øQuiz√°s la velocidad de aprendizaje es demasiado lenta?  Cuando abordas la tarea por primera vez, nunca tienes confianza. <br><br>  De esto vale la pena aprender una lecci√≥n de que depurar NS no es una tarea trivial, y esto, como la programaci√≥n regular, es parte del arte.  Debe aprender este arte de depuraci√≥n para obtener buenos resultados de NS.  En general, necesitamos desarrollar una heur√≠stica para seleccionar buenos hiperpar√°metros y buena arquitectura.  Discutiremos esto en detalle en el libro, incluyendo c√≥mo seleccion√© los hiperpar√°metros anteriores. <br><br><h3>  Ejercicio </h3><br><ul><li>  Intente crear una red de solo dos capas, entrada y salida, sin ocultaci√≥n, con 784 y 10 neuronas, respectivamente.  Entrena la red con descenso de gradiente estoc√°stico.  ¬øQu√© precisi√≥n de clasificaci√≥n obtienes? </li></ul><br>  Anteriormente omit√≠ los detalles de cargar datos de MNIST.  Sucede de manera bastante simple.  Aqu√≠ est√° el c√≥digo para completar la imagen.  Las estructuras de datos se describen en los comentarios: todo es simple, tuplas y matrices de objetos Numpy ndarray (si no est√° familiarizado con dichos objetos, imag√≠nelos como vectores). <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" mnist_loader ~~~~~~~~~~~~      MNIST.       ``load_data``  ``load_data_wrapper``.  , ``load_data_wrapper`` -  ,     . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import cPickle import gzip #  import numpy as np def load_data(): """  MNIST   ,  ,    . ``training_data``      .    .  numpy ndarray  50 000 .   ‚Äì     numpy ndarray  784 ,  28 * 28 = 784    MNIST.  ‚Äì  numpy ndarray  50 000 .   ‚Äì   0  9   ,    . ``validation_data``  ``test_data`` ,    10 000 .    ,           ``training_data``.    - ``load_data_wrapper()``. """ f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data) def load_data_wrapper(): """ ,  ``(training_data, validation_data, test_data)``.   ``load_data``,         .  , ``training_data`` -    50 000    , ``(x, y)``. ``x`` -  784- numpy.ndarray,   . ``y`` -  10- numpy.ndarray,   ,     ``x``. ``validation_data``  ``test_data`` -  ,   10 000    , ``(x, y)``. ``x`` -  784- numpy.ndarray,   ,  ``y`` -   ,  ,   ( ),  ``x``. ,  ,           .         .""" tr_d, va_d, te_d = load_data() training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data) def vectorized_result(j): """ 10-    1.0   j     .      (0..9)     .""" e = np.zeros((10, 1)) e[j] = 1.0 return e</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dije que nuestro programa est√° logrando resultados bastante buenos. ¬øQu√© significa esto? ¬øBueno comparado con qu√©? Es √∫til tener los resultados de algunas pruebas simples y b√°sicas con las que podr√≠a hacer una comparaci√≥n para comprender qu√© significan "buenos resultados". El nivel base m√°s simple, por supuesto, ser√≠a una suposici√≥n aleatoria. Esto se puede hacer en aproximadamente el 10% de los casos. ¬°Y mostramos un resultado mucho mejor! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬øQu√© pasa con un nivel base menos trivial? Veamos qu√© tan oscura es la imagen. Por ejemplo, la imagen 2 generalmente ser√° m√°s oscura que la imagen 1, simplemente porque tiene m√°s p√≠xeles oscuros, como se ve en los ejemplos a continuaci√≥n:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f59/0de/5b7/f590de5b7bc6581b9854b2013e5013de.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De ello se deduce que podemos calcular la oscuridad promedio para cada d√≠gito de 0 a 9. Cuando obtenemos una nueva imagen, calculamos su oscuridad y suponemos que muestra una figura con la oscuridad promedio m√°s cercana. Este es un procedimiento simple que es f√°cil de programar, por lo que no escribir√© c√≥digo; si est√° interesado, se </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">encuentra en GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Pero esta es una mejora seria en comparaci√≥n con las suposiciones aleatorias: el c√≥digo reconoce correctamente 2,225 de 10,000 im√°genes, es decir, proporciona una precisi√≥n del 22.25%.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No es dif√≠cil encontrar otras ideas que logren precisi√≥n en el rango de 20 a 50%. Despu√©s de trabajar un poco m√°s, puede superar el 50%. Pero para lograr una precisi√≥n mucho mayor, es mejor usar algoritmos MO autoritativos. Probemos con uno de los algoritmos m√°s famosos, el m√©todo de vector de soporte o SVM. Si no est√° familiarizado con SVM, no se preocupe, no necesitamos comprender estos detalles. Solo usamos una biblioteca de Python llamada scikit-learn, que proporciona una interfaz simple a la biblioteca r√°pida de C para SVM, conocida como </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LIBSVM</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si ejecutamos el clasificador SVM scikit-learn con la configuraci√≥n predeterminada, obtenemos la clasificaci√≥n correcta de 9,435 de 10,000 (el c√≥digo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est√° disponible en el enlace</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) Esto ya es una gran mejora sobre el enfoque ingenuo de clasificar im√°genes por oscuridad. Esto significa que SVM funciona tan bien como nuestro NS, solo que un poco peor. En los siguientes cap√≠tulos nos familiarizaremos con nuevas t√©cnicas que nos permitir√°n mejorar nuestro NS para que superen en gran medida a SVM.</font></font><br><br>  Pero eso no es todo.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El resultado 9 435 de 10 000 de scikit-learn se especifica para la configuraci√≥n predeterminada. SVM tiene muchos par√°metros que se pueden ajustar, y puede buscar par√°metros que mejoren este resultado. No entrar√© en detalles; se pueden leer en el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">art√≠culo de</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andreas Muller. Mostr√≥ que al hacer un trabajo para optimizar los par√°metros, es posible lograr una precisi√≥n de al menos 98.5%. En otras palabras, un SVM bien sintonizado genera solo un d√≠gito de 70 errores. ¬°Un buen resultado! ¬øPuede NA lograr m√°s? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resulta que pueden. Hoy, un NS bien ajustado supera cualquier otra tecnolog√≠a conocida en la soluci√≥n MNIST, incluida SVM. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R√©cord para 2013</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">clasific√≥ correctamente 9,979 de 10,000 im√°genes. Y veremos la mayor√≠a de las tecnolog√≠as utilizadas para esto en este libro. Este nivel de precisi√≥n es cercano al humano, y tal vez incluso lo supera, ya que varias im√°genes de MNIST son dif√≠ciles de descifrar incluso para humanos, por ejemplo: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/def/4b3/b37/def4b3b37d8d510615d60a372a06ad47.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Creo que estar√° de acuerdo en que es dif√≠cil clasificarlas! Con tales im√°genes en el conjunto de datos MNIST, es sorprendente que el NS pueda reconocer correctamente todas las 10,000 im√°genes, excepto 21. Por lo general, los programadores creen que resolver una tarea tan compleja requiere un algoritmo complejo. Pero incluso el NS est√° en el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trabajo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">el titular del registro utiliza algoritmos bastante simples, que son peque√±as variaciones de los que examinamos en este cap√≠tulo. </font><font style="vertical-align: inherit;">Toda la complejidad aparece autom√°ticamente durante el entrenamiento en funci√≥n de los datos de entrenamiento. </font><font style="vertical-align: inherit;">En cierto sentido, la moraleja de nuestros resultados y los contenidos en trabajos m√°s complejos es que para algunas tareas</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> algoritmo complejo ‚â§ algoritmo de entrenamiento simple + buenos datos de entrenamiento </font></font></blockquote><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Al aprendizaje profundo </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aunque nuestra red muestra un rendimiento impresionante, se logra de una manera misteriosa. </font><font style="vertical-align: inherit;">Los pesos y las redes de mezcla se detectan autom√°ticamente. </font><font style="vertical-align: inherit;">Por lo tanto, no tenemos una explicaci√≥n lista de c√≥mo la red hace lo que hace. </font><font style="vertical-align: inherit;">¬øHay alguna manera de entender los principios b√°sicos de clasificaci√≥n por una red de n√∫meros escritos a mano? </font><font style="vertical-align: inherit;">¬øY es posible, dados ellos, mejorar el resultado?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reformulamos estas preguntas de manera m√°s estricta: supongamos que en unas pocas d√©cadas el NS se convertir√° en inteligencia artificial (IA). ¬øEntenderemos c√≥mo funciona esta IA? Quiz√°s las redes seguir√°n siendo incomprensibles para nosotros, con sus pesos y compensaciones, ya que se asignan autom√°ticamente. En los primeros a√±os de la investigaci√≥n de IA, la gente esperaba que intentar crear IA tambi√©n nos ayudar√≠a a comprender los principios subyacentes de la inteligencia, y tal vez incluso el trabajo del cerebro humano. Sin embargo, al final puede resultar que no entenderemos ni el cerebro ni c√≥mo funciona la IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para abordar estos problemas, recordemos la interpretaci√≥n de las neuronas artificiales que di al comienzo del cap√≠tulo: que esta es una forma de sopesar la evidencia. Supongamos que queremos determinar si la cara de una persona est√° en una imagen:</font></font><br><br><img src="https://habrastorage.org/webt/up/2e/af/up2eafnwrrpph5xdai4oapmgw8m.jpeg"><br><br><img src="https://habrastorage.org/webt/tu/8i/an/tu8ianduufnfebbjnqaibtzskyy.jpeg"><br><br><img src="https://habrastorage.org/webt/wf/tj/4j/wftj4j86hzytgwyypbw_eqk0jte.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este problema puede abordarse de la misma manera que el reconocimiento de escritura a mano: el uso de p√≠xeles de imagen como entrada para el NS, y la salida del NS ser√° una neurona que dir√°: "S√≠, esta es una cara" o "No, esto no es una cara ". </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Supongamos que hacemos esto, pero sin usar un algoritmo de aprendizaje. Intentaremos crear manualmente una red, eligiendo los pesos y las compensaciones apropiadas. ¬øC√≥mo podemos abordar esto? Por un momento, olvidando la Asamblea Nacional, podr√≠amos dividir la tarea en subtareas: ¬øla imagen de los ojos tiene en la esquina superior izquierda? ¬øHay un ojo en la esquina superior derecha? ¬øHay una nariz media? ¬øHay una boca en el medio? ¬øHay pelo en la parte superior?</font></font> Y as√≠ sucesivamente. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si las respuestas a varias de estas preguntas son positivas, o incluso "probablemente s√≠", entonces concluimos que la imagen puede tener una cara. Por el contrario, si las respuestas son no, entonces probablemente no hay persona.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esto, por supuesto, es una heur√≠stica aproximada y tiene muchas deficiencias. Quiz√°s sea un hombre calvo y no tenga cabello. Quiz√°s podamos ver solo una parte de la cara, o la cara en √°ngulo, de modo que algunas partes de la cara est√©n cerradas. Sin embargo, la heur√≠stica sugiere que si podemos resolver subproblemas con la ayuda de redes neuronales, entonces quiz√°s podamos crear NS para reconocimiento facial combinando redes para subtareas. La siguiente es una posible arquitectura de dicha red en la que las subredes se indican mediante rect√°ngulos. Este no es un enfoque completamente realista para resolver el problema del reconocimiento facial: es necesario para ayudarnos a comprender intuitivamente el trabajo de las redes neuronales. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/61b/dd0/ef6/61bdd0ef651ebc30afff87ed56204bd0.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En los rect√°ngulos hay subtareas: ¬øtiene la imagen de los ojos en la esquina superior izquierda? ¬øHay un ojo en la esquina superior derecha? ¬øHay una nariz media? ¬øHay una boca en el medio? ¬øHay pelo en la parte superior? Y as√≠ sucesivamente.</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es posible que las subredes tambi√©n se puedan desmontar en componentes. Tome la cuesti√≥n de tener un ojo en la esquina superior izquierda. Se puede distinguir en preguntas como: "¬øHay una ceja?", "¬øHay pesta√±as?", "¬øHay una pupila?" Y as√≠ sucesivamente. Por supuesto, las preguntas deben contener informaci√≥n sobre la ubicaci√≥n: "¬øEst√° la ceja ubicada en la esquina superior izquierda, arriba de la pupila?", Y as√≠ sucesivamente, pero simplifiquemosla por ahora. Por lo tanto, la red que responde a la pregunta sobre la presencia del ojo se puede desmontar en los componentes: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d87/871/6ff/d878716ff0ecad83cfa5b0e9c54a866e.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"¬øHay una ceja?", "¬øHay pesta√±as?", "¬øHay una pupila?"</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estas preguntas se pueden dividir en peque√±as, en pasos a trav√©s de muchas capas. Como resultado, trabajaremos con subredes que respondan preguntas tan simples que puedan desmontarse f√°cilmente a nivel de p√≠xeles. Estas preguntas pueden referirse, por ejemplo, a la presencia o ausencia de formas simples en ciertos lugares de la imagen. Las neuronas individuales asociadas con los p√≠xeles podr√°n responder a ellas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El resultado es una red que desglosa preguntas muy complejas, ya sea que una persona est√© en la imagen, en preguntas muy simples que pueden responderse a nivel de p√≠xel individual. Lo har√° a trav√©s de una secuencia de muchas capas, en la cual las primeras responden preguntas muy simples y espec√≠ficas sobre la imagen, y las √∫ltimas crean una jerarqu√≠a de conceptos m√°s complejos y abstractos. Las redes con una estructura multicapa de este tipo, dos o m√°s capas ocultas, se denominan redes neuronales profundas (GNS).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por supuesto, no habl√© sobre c√≥mo hacer esta subred recursiva. Definitivamente no ser√° pr√°ctico seleccionar manualmente pesos y compensaciones. Nos gustar√≠a utilizar algoritmos de entrenamiento para que la red aprenda autom√°ticamente los pesos y las compensaciones, y a trav√©s de ellos las jerarqu√≠as de conceptos, basadas en datos de entrenamiento. Los investigadores en las d√©cadas de 1980 y 1990 intentaron utilizar el descenso de gradiente estoc√°stico y la propagaci√≥n hacia atr√°s para entrenar GNS. Desafortunadamente, con la excepci√≥n de algunas arquitecturas especiales, no tuvieron √©xito. Las redes entrenaron, pero muy lentamente, y en la pr√°ctica fue demasiado lento para que se pueda utilizar de alguna manera.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Desde 2006, se han desarrollado varias tecnolog√≠as para entrenar STS. Se basan en el descenso de gradiente estoc√°stico y la propagaci√≥n hacia atr√°s, pero tambi√©n contienen nuevas ideas. Permitieron entrenar redes mucho m√°s profundas; hoy en d√≠a, las personas entrenan silenciosamente redes con 5-10 capas. Y resulta que resuelven muchos problemas mucho mejor que los NS poco profundos, es decir, las redes con una capa oculta. La raz√≥n, por supuesto, es que STS puede crear una compleja jerarqu√≠a de conceptos. Esto es similar a c√≥mo los lenguajes de programaci√≥n usan esquemas modulares e ideas de abstracci√≥n para que puedan crear programas inform√°ticos complejos. Comparar un NS profundo con un NS superficial es aproximadamente c√≥mo comparar un lenguaje de programaci√≥n que puede realizar llamadas a funciones con un lenguaje que no lo hace. La abstracci√≥n en el NS no se parece a los lenguajes de programaci√≥n,Pero tiene la misma importancia.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/456738/">https://habr.com/ru/post/456738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456722/index.html">Recetas PostgreSQL: Programador de tareas as√≠ncrono</a></li>
<li><a href="../456724/index.html">5 maneras extremadamente simples de acelerar significativamente su aplicaci√≥n VueJS</a></li>
<li><a href="../456730/index.html">Libro "{No sabes JS} Tipos y construcciones gramaticales"</a></li>
<li><a href="../456732/index.html">Ser un mentor</a></li>
<li><a href="../456736/index.html">Recetas PostgreSQL: cURL: obtener, publicar y ... correo electr√≥nico</a></li>
<li><a href="../456740/index.html">Inmersi√≥n en redes neuronales convolucionales. Parte 5/1 - 9</a></li>
<li><a href="../456744/index.html">10 problemas que resolv√≠ con recordatorios en mi tel√©fono inteligente</a></li>
<li><a href="../456746/index.html">Big data: gran responsabilidad, gran estr√©s y mucho dinero</a></li>
<li><a href="../456748/index.html">Impresora t√©rmica 2003 de un mercado de pulgas: ¬øqu√© puede hacer en 2019?</a></li>
<li><a href="../456754/index.html">GitOps: comparaci√≥n de los m√©todos Pull y Push</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>