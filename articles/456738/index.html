<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>游뛅 游띴 游닒 Redes neuronales y aprendizaje profundo, cap칤tulo 1: uso de redes neuronales para reconocer n칰meros escritos a mano 游떁 游냥 游낔</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota 
 Aqu칤 hay una traducci칩n del libro en l칤nea gratuito de Michael Nielsen, Neural Networks and Deep Learning, distribuido bajo la Licencia Creativ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neuronales y aprendizaje profundo, cap칤tulo 1: uso de redes neuronales para reconocer n칰meros escritos a mano</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456738/"><h3>  Nota </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/d1f/7ac/ee5/d1f7acee5600a381c43f05c7df9c439a.jpg" alt="Michael nielsen" align="left">  Aqu칤 hay una traducci칩n del libro en l칤nea gratuito de Michael Nielsen, Neural Networks and Deep Learning, distribuido bajo la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Licencia Creative Commons Reconocimiento-No Comercial 3.0 Unported</a> .  La motivaci칩n para su creaci칩n fue la experiencia exitosa de traducir un libro de texto de programaci칩n, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Expressive JavaScript</a> .  El libro sobre redes neuronales tambi칠n es bastante popular; los autores de art칤culos en ingl칠s lo citan activamente.  No encontr칠 sus traducciones, excepto la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">traducci칩n del comienzo del primer cap칤tulo con abreviaturas</a> . <br><br>  Aquellos que quieran agradecer al autor del libro pueden hacerlo en su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">p치gina oficial</a> , mediante transferencia a trav칠s de PayPal o Bitcoin.  Para apoyar al traductor en Habr칠 hay un formulario "para apoyar al autor". <br><br><div class="spoiler">  <b class="spoiler_title">Contenido</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap칤tulo 1: uso de redes neuronales para reconocer n칰meros escritos a mano</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap칤tulo 2: c칩mo funciona el algoritmo de retropropagaci칩n</a> </li><li>  Cap칤tulo 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: mejorar el m칠todo de entrenamiento de redes neuronales</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: 쯇or qu칠 la regularizaci칩n ayuda a reducir el reciclaje?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 3: 쯖칩mo elegir hiperpar치metros de red neuronal?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap칤tulo 4: prueba visual de que las redes neuronales son capaces de calcular cualquier funci칩n</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap칤tulo 5: 쯣or qu칠 las redes neuronales profundas son tan dif칤ciles de entrenar?</a> </li><li>  Cap칤tulo 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: aprendizaje profundo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: progreso reciente en el reconocimiento de im치genes</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ep칤logo: 쯘xiste un algoritmo simple para crear inteligencia?</a> </li></ul></div></div><br><h2>  Introduccion </h2><br>  Este tutorial le informar치 en detalle sobre conceptos como: <br><br><ul><li>  Redes neuronales: un excelente paradigma de software, creado bajo la influencia de la biolog칤a, y que permite que la computadora aprenda bas치ndose en observaciones. </li><li>  El aprendizaje profundo es un poderoso conjunto de t칠cnicas de entrenamiento de redes neuronales. </li></ul><br>  Las redes neuronales (NS) y el aprendizaje profundo (GO) brindan hoy la mejor soluci칩n a muchos problemas en las 치reas de reconocimiento de im치genes, procesamiento de voz y lenguaje natural.  Este tutorial le ense침ar치 muchos de los conceptos clave que sustentan NS y GO. <br><a name="habracut"></a><br><h2>  쮻e qu칠 trata este libro? </h2><br>  NS es uno de los mejores paradigmas de software jam치s inventado por el hombre.  Con un enfoque de programaci칩n est치ndar, le decimos a la computadora qu칠 hacer, dividir las tareas grandes en muchas peque침as y determinar con precisi칩n las tareas que la computadora realizar치 f치cilmente.  En el caso de la Asamblea Nacional, por el contrario, no le decimos a la computadora c칩mo resolver el problema.  칄l mismo aprende esto sobre la base de "observaciones" de los datos, "inventando" su propia soluci칩n al problema. <br><br>  El aprendizaje automatizado basado en datos suena prometedor.  Sin embargo, hasta 2006, no sab칤amos c칩mo capacitar a la Asamblea Nacional para que pudieran trascender los enfoques m치s tradicionales, con la excepci칩n de algunos casos especiales.  En 2006, t칠cnicas de entrenamiento de los llamados  redes neuronales profundas (GNS).  Ahora estas t칠cnicas se conocen como aprendizaje profundo (GO).  Continuaron desarroll치ndose, y hoy GNS y GO han logrado resultados sorprendentes en muchas tareas importantes relacionadas con la visi칩n por computadora, el reconocimiento del habla y el procesamiento del lenguaje natural.  A gran escala, est치n siendo implementados por empresas como Google, Microsoft y Facebook. <br><br>  El prop칩sito de este libro es ayudarlo a dominar los conceptos clave de las redes neuronales, incluidas las t칠cnicas modernas de GO.  Despu칠s de trabajar con el tutorial, escribir치 un c칩digo que usa NS y GO para resolver problemas complejos de reconocimiento de patrones.  Tendr치 una base para usar NS y defensa civil en el enfoque para resolver sus propios problemas. <br><br><h3>  Enfoque basado en principios </h3><br>  Una de las creencias subyacentes en el libro es que es mejor adquirir una comprensi칩n s칩lida de los principios clave de la Asamblea Nacional y la Sociedad Civil que obtener conocimiento de una larga lista de ideas diferentes.  Si comprende bien las ideas clave, comprender치 r치pidamente otro material nuevo.  En el lenguaje del programador, podemos decir que estudiaremos la sintaxis b치sica, las bibliotecas y las estructuras de datos del nuevo lenguaje.  Es posible que reconozca solo una peque침a fracci칩n de todo el idioma (muchos idiomas tienen bibliotecas est치ndar inmensas), sin embargo, puede comprender nuevas bibliotecas y estructuras de datos de forma r치pida y sencilla. <br><br>  Entonces, este libro categ칩ricamente no es material educativo sobre c칩mo usar una biblioteca en particular para la Asamblea Nacional.  Si solo quieres aprender a trabajar con la biblioteca, 춰no leas el libro!  Encuentre la biblioteca que necesita y trabaje con materiales de capacitaci칩n y documentaci칩n.  Pero tenga en cuenta: aunque este enfoque tiene la ventaja de resolver el problema instant치neamente, si desea comprender exactamente qu칠 est치 sucediendo dentro de la Asamblea Nacional, si desea dominar ideas que ser치n relevantes en muchos a침os, entonces no ser치 suficiente para que simplemente estudie alg칰n tipo de biblioteca de moda  Debe comprender las ideas confiables y a largo plazo que subyacen en el trabajo de la Asamblea Nacional.  La tecnolog칤a va y viene, y las ideas duran para siempre. <br><br><h3>  Enfoque pr치ctico </h3><br>  Estudiaremos los principios b치sicos con el ejemplo de una tarea espec칤fica: ense침ar a una computadora a reconocer n칰meros escritos a mano.  Utilizando enfoques de programaci칩n tradicionales, esta tarea es extremadamente dif칤cil de resolver.  Sin embargo, podemos resolverlo bastante bien con un NS simple y varias docenas de l칤neas de c칩digo, sin bibliotecas especiales.  Adem치s, mejoraremos gradualmente este programa, incluyendo constantemente m치s y m치s ideas clave sobre la Asamblea Nacional y la Defensa Civil. <br><br>  Este enfoque pr치ctico significa que necesitar치 algo de experiencia en programaci칩n.  Pero no tienes que ser un programador profesional.  Escrib칤 el c칩digo de Python (versi칩n 2.7) que deber칤a quedar claro incluso si no ha escrito programas de Python.  En el proceso de estudio, crearemos nuestra propia biblioteca para la Asamblea Nacional, que puede utilizar para experimentos y capacitaci칩n adicional.  Todo el c칩digo se puede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">descargar aqu칤</a> .  Una vez terminado el libro, o en el proceso de lectura, puede elegir una de las bibliotecas m치s completas para la Asamblea Nacional, adaptada para su uso en estos proyectos. <br><br>  Los requisitos matem치ticos para comprender el material son bastante promedio.  La mayor칤a de los cap칤tulos tienen partes matem치ticas, pero generalmente son 치lgebra elemental y gr치ficos de funciones.  A veces uso matem치ticas m치s avanzadas, pero estructur칠 el material para que puedas entenderlo, incluso si algunos detalles te eluden.  La mayor칤a de las matem치ticas se usan en el cap칤tulo 2, que requiere un poco de matan치lisis y 치lgebra lineal.  Para aquellos a quienes no est치n familiarizados, comienzo el Cap칤tulo 2 con una introducci칩n a las matem치ticas.  Si le resulta dif칤cil, simplemente omita el cap칤tulo hasta el informe.  En cualquier caso, no te preocupes por esto. <br><br>  Un libro rara vez se orienta al mismo tiempo hacia una comprensi칩n de los principios y un enfoque pr치ctico.  Pero creo que es mejor estudiar sobre la base de las ideas fundamentales de la Asamblea Nacional.  Escribiremos c칩digo de trabajo, y no solo estudiaremos teor칤a abstracta, y usted puede explorar y extender este c칩digo.  De esta manera, comprender치 los conceptos b치sicos, tanto la teor칤a como la pr치ctica, y podr치 aprender m치s. <br><br><h3>  Ejercicios y tareas </h3><br>  Los autores de libros t칠cnicos a menudo advierten al lector que simplemente necesita completar todos los ejercicios y resolver todos los problemas.  Al leerme tales advertencias, siempre me parecen un poco extra침as.  쯄e pasar치 algo malo si no realizo ejercicios y resuelvo problemas?  No por supuesto.  Ahorrar칠 tiempo con una comprensi칩n menos profunda.  A veces vale la pena.  A veces no. <br><br>  쯈u칠 vale la pena hacer con este libro?  Le aconsejo que intente completar la mayor칤a de los ejercicios, pero no intente resolver la mayor칤a de las tareas. <br><br>  La mayor칤a de los ejercicios deben completarse porque estos son controles b치sicos para una comprensi칩n adecuada del material.  Si no puede realizar el ejercicio con relativa facilidad, debe haberse perdido algo fundamental.  Por supuesto, si realmente est치 atascado en alg칰n tipo de ejercicio, su칠ltelo, tal vez sea una especie de peque침o malentendido, o tal vez haya formulado algo mal.  Pero si la mayor칤a de los ejercicios le causan dificultades, lo m치s probable es que necesite volver a leer el material anterior. <br><br>  Las tareas son otra cosa.  Son m치s dif칤ciles que los ejercicios, y con algunos tendr치s dificultades.  Esto es molesto, pero, por supuesto, la paciencia frente a tal decepci칩n es la 칰nica forma de comprender y absorber realmente el tema. <br><br>  Por lo tanto, no recomiendo resolver todos los problemas.  Mejor a칰n: elige tu propio proyecto.  Es posible que desee utilizar NS para clasificar su colecci칩n de m칰sica.  O para predecir el valor de las acciones.  O algo mas.  Pero encuentra un proyecto interesante para ti.  Y luego puede ignorar las tareas del libro, o utilizarlas simplemente como inspiraci칩n para trabajar en su proyecto.  Los problemas con su propio proyecto le ense침ar치n m치s que trabajar con cualquier cantidad de tareas.  La participaci칩n emocional es un factor clave en el logro del dominio. <br><br>  Por supuesto, si bien es posible que no tenga un proyecto de este tipo.  Esto es normal  Resuelve tareas para las que sientas una motivaci칩n intr칤nseca.  Use material del libro para ayudarlo a encontrar ideas para proyectos creativos personales. <br><br><h2>  Capitulo 1 </h2><br>  El sistema visual humano es una de las maravillas del mundo.  Considere la siguiente secuencia de n칰meros escritos a mano: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><br>  La mayor칤a de la gente los leer치 f치cilmente, como 504192. Pero esta simplicidad es enga침osa.  En cada hemisferio del cerebro, una persona tiene una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">corteza visual primaria</a> , tambi칠n conocida como V1, que contiene 140 millones de neuronas y decenas de miles de millones de conexiones entre ellas.  Al mismo tiempo, no solo V1 est치 involucrado en la visi칩n humana, sino toda una secuencia de regiones cerebrales (V2, V3, V4 y V5) que se dedican al procesamiento de im치genes cada vez m치s complejo.  Llevamos en nuestras cabezas una supercomputadora sintonizada por la evoluci칩n durante cientos de millones de a침os, y perfectamente adaptada para comprender el mundo visible.  Reconocer n칰meros escritos a mano no es tan f치cil.  Es solo que nosotros, las personas, sorprendentemente, sorprendentemente bien, reconocemos lo que nuestros ojos nos muestran.  Pero casi todo este trabajo se lleva a cabo inconscientemente.  Y, por lo general, no le damos importancia a la dif칤cil tarea que resuelven nuestros sistemas visuales. <br><br>  La dificultad de reconocer patrones visuales se hace evidente cuando intentas escribir un programa de computadora para reconocer n칰meros como los anteriores.  Lo que parece f치cil en nuestra ejecuci칩n de repente resulta ser extremadamente complejo.  El simple concepto de c칩mo reconocemos las formas ("el nueve tiene un bucle en la parte superior y la barra vertical en la parte inferior derecha") no es tan simple para una expresi칩n algor칤tmica.  Al tratar de articular estas reglas con claridad, r치pidamente queda atrapado en un atolladero de excepciones, dificultades y ocasiones especiales.  La tarea parece desesperada. <br><br>  Enfoque NS para resolver el problema de una manera diferente.  La idea es tomar los muchos n칰meros escritos a mano conocidos como ejemplos de ense침anza, <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a67/2ff/15c/a672ff15c58d9672b9f5d2b427a20eb6.png"><br><br>  y desarrolle un sistema que pueda aprender de estos ejemplos.  En otras palabras, la Asamblea Nacional usa ejemplos para construir autom치ticamente reglas de reconocimiento de d칤gitos escritas a mano.  Adem치s, al aumentar el n칰mero de ejemplos de capacitaci칩n, la red puede aprender m치s sobre n칰meros escritos a mano y mejorar su precisi칩n.  Entonces, aunque he citado m치s de 100 estudios de casos, quiz치s podamos crear un mejor sistema de reconocimiento de escritura a mano utilizando miles o incluso millones y miles de millones de estudios de casos. <br><br>  En este cap칤tulo, escribiremos un programa de computadora que implementa el aprendizaje NS para reconocer n칰meros escritos a mano.  El programa tendr치 solo 74 l칤neas y no utilizar치 bibliotecas especiales para la Asamblea Nacional.  Sin embargo, este breve programa podr치 reconocer n칰meros escritos a mano con una precisi칩n de m치s del 96%, sin necesidad de intervenci칩n humana.  Adem치s, en futuros cap칤tulos desarrollaremos ideas que pueden mejorar la precisi칩n al 99% o m치s.  De hecho, los mejores NS comerciales hacen un trabajo tan bueno que los bancos los utilizan para procesar cheques y el servicio postal para reconocer direcciones. <br><br>  Nos concentramos en el reconocimiento de escritura a mano, ya que este es un gran prototipo de una tarea para estudiar NS.  Tal prototipo es ideal para nosotros: es una tarea dif칤cil (reconocer n칰meros escritos a mano no es una tarea f치cil), pero no tan complicada que requiere una soluci칩n extremadamente compleja o una inmensa potencia inform치tica.  Adem치s, esta es una excelente manera de desarrollar t칠cnicas m치s complejas, como GO.  Por lo tanto, en el libro volveremos constantemente a la tarea de reconocimiento de escritura a mano.  M치s adelante discutiremos c칩mo se pueden aplicar estas ideas a otras tareas de la visi칩n por computadora, al reconocimiento de voz, el procesamiento del lenguaje natural y otras 치reas. <br><br>  Por supuesto, si el prop칩sito de este cap칤tulo fuera solo escribir un programa para reconocer n칰meros escritos a mano, 춰entonces el cap칤tulo ser칤a mucho m치s corto!  Sin embargo, en el proceso desarrollaremos muchas ideas clave relacionadas con NS, incluidos dos tipos importantes de neuronas artificiales ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">perceptr칩n</a> y neurona sigmoidea), y el algoritmo de aprendizaje est치ndar de NS, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el descenso de gradiente estoc치stico</a> .  En el texto, me concentro en explicar por qu칠 todo se hace de esta manera, y en dar forma a su comprensi칩n de la Asamblea Nacional.  Esto requiere una conversaci칩n m치s larga que si acabara de presentar la mec치nica b치sica de lo que est치 sucediendo, pero cuesta una comprensi칩n m치s profunda que tendr치.  Entre otras ventajas: al final del cap칤tulo comprender치 qu칠 es una defensa civil y por qu칠 es tan importante. <br><br><h3>  Perceptrones </h3><br>  쯈u칠 es una red neuronal?  Para comenzar, hablar칠 sobre un tipo de neurona artificial llamada perceptr칩n.  Los perceptrones fueron inventados por el cient칤fico <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Frank Rosenblatt</a> en los a침os 50 y 60, inspirados en los primeros trabajos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Warren McCallock</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Walter Pitts</a> .  Hoy en d칤a, otros modelos de neuronas artificiales se usan con mayor frecuencia: en este libro, y los trabajos m치s modernos sobre NS utilizan principalmente el modelo sigmoide de la neurona.  La veremos pronto.  Pero para comprender por qu칠 las neuronas sigmoideas se definen de esta manera, vale la pena pasar tiempo analizando el perceptr칩n. <br><br>  Entonces, 쯖칩mo funcionan los perceptrones?  El perceptr칩n recibe varios n칰meros binarios x <sub>1</sub> , x <sub>2</sub> , ... y da un n칰mero binario: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/293/1f3/f5c/2931f3f5c6125d14262954583868f959.png"><br><br>  En este ejemplo, el perceptr칩n tiene tres n칰meros de entrada, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  En general, puede haber m치s o menos de ellos.  Rosenblatt propuso una regla simple para calcular el resultado.  Introdujo pesos, w <sub>1</sub> , w <sub>2</sub> , n칰meros reales, expresando la importancia de los n칰meros de entrada correspondientes para los resultados.  La salida de una neurona, 0 o 1, est치 determinada por si una suma ponderada es menor o mayor que cierto umbral [umbral] <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mtext" id="MJXp-Span-2">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-3"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-5"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-6" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-7" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-8"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-9" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-10" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-11"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-13" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.764ex" height="2.057ex" viewBox="0 -520.7 4634.5 885.9" role="img" focusable="false" style="vertical-align: -0.848ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(2562,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(3670,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="809" y="-213"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-1"> \ sum_j w_jx_j </script>  .  Al igual que los pesos, el umbral es un n칰mero real, un par치metro de una neurona.  En t칠rminos matem치ticos: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-14"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-15"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-16"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-20"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mo" id="MJXp-Span-21" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25"><font style="vertical-align: inherit;">g </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27"><font style="vertical-align: inherit;">n </font></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29"><font style="vertical-align: inherit;">c </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30"><font style="vertical-align: inherit;">a </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31"><font style="vertical-align: inherit;">s </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32"><font style="vertical-align: inherit;">e </font></span></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33"><font style="vertical-align: inherit;">s</font></span></span><span class="MJXp-mn" id="MJXp-Span-34"><font style="vertical-align: inherit;"> 0 </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37"><font style="vertical-align: inherit;">f </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-40"><font style="vertical-align: inherit;">s </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41"><font style="vertical-align: inherit;">u </font></span><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">m </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-44" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">j </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">w </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-47" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">j </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-48"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-49" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">x </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-48"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-50" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">j</font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52"><font style="vertical-align: inherit;"> l </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54"><font style="vertical-align: inherit;">q </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55"><font style="vertical-align: inherit;">u </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56"><font style="vertical-align: inherit;">m </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58"><font style="vertical-align: inherit;">r </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59"><font style="vertical-align: inherit;">a </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60"><font style="vertical-align: inherit;">l</font></span></font><span class="MJXp-mtext" id="MJXp-Span-22">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mrow" id="MJXp-Span-28"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mn" id="MJXp-Span-34"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-35">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-38">&nbsp;</span><span class="MJXp-mtext" id="MJXp-Span-39">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-40"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41"><font style="vertical-align: inherit;"></font></span><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-44" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-47" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-48"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-49" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-50" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mtext" id="MJXp-Span-51">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mspace" id="MJXp-Span-61" style="width: 0em; height: 0em;"></span><span class="MJXp-mn" id="MJXp-Span-62">1</span><span class="MJXp-mtext" id="MJXp-Span-63">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-64">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-65">f</span><span class="MJXp-mtext" id="MJXp-Span-66">&nbsp;</span><span class="MJXp-mtext" id="MJXp-Span-67">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-68">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-69">u</span><span class="MJXp-msubsup" id="MJXp-Span-70"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-71" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-72" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-73"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-74" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-75" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-76"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-77" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-78" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-79" style="margin-left: 0.333em; margin-right: 0.333em;">&gt;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-80">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-81">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-82">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-83">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-84">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-85">l</span><span class="MJXp-mtext" id="MJXp-Span-86">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-88">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-89">d</span><span class="MJXp-mrow" id="MJXp-Span-90"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-91">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-92">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-93">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-94">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-95">s</span></span><span class="MJXp-mtext" id="MJXp-Span-96">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-97">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-98">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-99">g</span><span class="MJXp-mrow" id="MJXp-Span-100"><span class="MJXp-mn" id="MJXp-Span-101">1</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="76.867ex" height="6.033ex" viewBox="0 -780.1 33095.6 2597.7" role="img" focusable="false" style="vertical-align: -4.222ex; max-width: 638px;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(6478,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="469" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6C" x="999" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-69" x="1297" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-64" x="1643" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="2166" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-3D" x="2973" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-62" x="4280" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-65" x="4709" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-67" x="5176" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-69" x="5656" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6E" x="6002" y="0"></use><g transform="translate(6602,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-63" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="433" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="963" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-65" x="1432" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="1899" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-30" x="8971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-69" x="9721" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-66" x="10067" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="11117" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="11587" y="0"></use><g transform="translate(12159,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(13429,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(14537,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6C" x="15752" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-65" x="16050" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-71" x="16517" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="16977" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="17550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-62" x="18428" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-72" x="18858" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="19309" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6C" x="19839" y="0"></use></g><g transform="translate(7860,-1432)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-31" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-69" x="750" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-66" x="1096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="2146" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="2616" y="0"></use><g transform="translate(3188,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1242" y="-213"></use></g><g transform="translate(4458,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="1013" y="-213"></use></g><g transform="translate(5566,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6A" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-3E" x="6808" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-75" x="7865" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6D" x="8437" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-62" x="9316" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-72" x="9745" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="10197" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6C" x="10726" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-65" x="11275" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6E" x="11741" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-64" x="12342" y="0"></use><g transform="translate(12865,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-63" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="433" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="963" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-6F" x="1432" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-73" x="1918" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-74" x="15503" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-61" x="15864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMATHI-67" x="16394" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/456738/&amp;usg=ALkJrhjxCmNtIPxt73mpsatKfeek255iTg#MJMAIN-31" x="16874" y="0"></use></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> salida = \ begin {cases} 0 ~ if ~ \ sum_j w_jx_j \ leq umbral \\ 1 ~ if ~ \ sum_j w_jx_j> umbral \ end {casos} \ tag {1} </script></p><br><br>  춰Esa es toda la descripci칩n del perceptr칩n! <br><br>  Este es el modelo matem치tico b치sico.  Un perceptr칩n puede considerarse como un tomador de decisiones al sopesar la evidencia.  D칠jame darte un ejemplo no muy realista, pero simple.  Digamos que se acerca el fin de semana, y escuchaste que se celebrar치 un festival de queso en tu ciudad.  Te gusta el queso y trata de decidir si ir al festival o no.  Puede tomar una decisi칩n sopesando tres factores: <br><br><ol><li>  쮿ace buen tiempo? </li><li>  쯊u pareja quiere ir contigo? </li><li>  쮼st치 el festival lejos del transporte p칰blico?  (No tienes coche). </li></ol><br>  Estos tres factores pueden representarse como variables binarias x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Por ejemplo, x <sub>1</sub> = 1 si el clima es bueno y 0 si es malo.  x <sub>2</sub> = 1 si tu compa침ero quiere ir, y 0 si no.  Lo mismo para x <sub>3</sub> . <br><br>  Ahora, digamos que eres tan fan치tico del queso que est치s listo para ir al festival, incluso si a tu pareja no le interesa y es dif칤cil llegar a 칠l.  Pero quiz치s odies el mal tiempo y, en caso de mal tiempo, no ir치s al festival.  Puede usar perceptrones para modelar tal proceso de toma de decisiones.  Una forma es elegir el peso w <sub>1</sub> = 6 para el clima, y 긍 <sub>2</sub> = 2, w <sub>3</sub> = 2 para otras condiciones.  Un valor mayor de w <sub>1</sub> significa que el clima le importa mucho m치s que si su pareja se unir치 a usted o la proximidad del festival a una parada.  Finalmente, suponga que selecciona el umbral 5 para el perceptr칩n. Con estas opciones, el perceptr칩n implementa el modelo de decisi칩n deseado, dando 1 cuando el clima es bueno y 0 cuando es malo.  El deseo del compa침ero y la proximidad de la parada no afectan el valor de salida. <br><br>  Al cambiar los pesos y los umbrales, podemos obtener diferentes modelos de toma de decisiones.  Por ejemplo, supongamos que tomamos el umbral 3. Luego, el perceptr칩n decide que debe ir al festival, ya sea cuando hace buen tiempo o cuando el festival est치 cerca de una parada de autob칰s y su pareja acepta ir con usted.  En otras palabras, el modelo es diferente.  Bajar el umbral significa que quieres ir m치s al festival. <br><br>  춰Obviamente, el perceptr칩n no es un modelo humano completo de toma de decisiones!  Pero este ejemplo muestra c칩mo un perceptr칩n puede pesar diferentes tipos de evidencia para tomar decisiones.  Parece posible que una red compleja de perceptrones pueda tomar decisiones muy complejas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f1/853/0d8/5f18530d8e17d640b3146924f7666032.png"><br><br>  En esta red, la primera columna de perceptrones, lo que llamamos la primera capa de perceptrones, toma tres decisiones muy simples, sopesando la evidencia de entrada.  쯈u칠 pasa con los perceptrones de la segunda capa?  Cada uno de ellos toma una decisi칩n, sopesando los resultados de la primera capa de toma de decisiones.  De esta manera, el perceptr칩n de la segunda capa puede tomar una decisi칩n a un nivel m치s complejo y abstracto en comparaci칩n con el perceptr칩n de la primera capa.  E incluso los perceptrones en la tercera capa pueden tomar decisiones a칰n m치s complejas.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De esta manera, una red multicapa de perceptrones puede manejar decisiones complejas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por cierto, cuando determin칠 el perceptr칩n, dije que solo tiene un valor de salida. Pero en la red en la parte superior, los perceptrones parecen tener varios valores de salida. De hecho, solo tienen una salida. Muchas flechas de salida son solo una forma conveniente de mostrar que la salida del perceptr칩n se usa como entrada de varios otros perceptrones. Esto es menos engorroso que dibujar una 칰nica salida de ramificaci칩n. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Simplifiquemos la descripci칩n de los perceptrones. Condici칩n</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-102"><span class="MJXp-msubsup" id="MJXp-Span-103"><span class="MJXp-mo" id="MJXp-Span-104" style="margin-left: 0.111em; margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">풖 </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-105" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-106"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-107" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-108" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-109"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-110" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-111" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-112" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> &gt; </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-113"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-114"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-115"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un e </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-116"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-117"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-118"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-119"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-120"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d</font></font></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> inc칩moda, y podemos estar de acuerdo en dos cambios para la grabaci칩n de su simplicidad. </font><font style="vertical-align: inherit;">Lo primero es grabar</font></font><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-3">\sum_j w_jx_j > treshold</script><font style="vertical-align: inherit;"></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-121"><span class="MJXp-msubsup" id="MJXp-Span-122"><span class="MJXp-mo" id="MJXp-Span-123" style="margin-left: 0.111em; margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">풖 </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-124" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-125"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-126" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">de w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-127" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-128"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-129" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-130" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> como el producto escalar,</font></font><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-4">\sum_j w_jx_j</script><font style="vertical-align: inherit;"></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-131"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-132"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-133" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">긘 </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-134"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mo" id="MJXp-Span-135" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-136"><span class="MJXp-mo" id="MJXp-Span-137" style="margin-left: 0.111em; margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">갬 </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-138" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-139"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-140" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-141" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-142"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-143" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-144" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , donde w y x son vectores cuyos componentes son pesos y datos de entrada, respectivamente. </font><font style="vertical-align: inherit;">El segundo es transferir el umbral a otra parte de la desigualdad y reemplazarlo con un valor conocido como desplazamiento de perceptr칩n [sesgo],</font></font><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-5">w \cdot x = \sum_j w_jx_j</script><font style="vertical-align: inherit;"></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-145"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-146"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b </font></font></span><span class="MJXp-mo" id="MJXp-Span-147" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">고 </font></font></span><span class="MJXp-mo" id="MJXp-Span-148" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-149"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-150"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-151"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-152"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-153"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-154"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">h </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-155"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-156"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-157"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">d</font></font></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-6">b \equiv 뇺hreshold</script>  .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Usando el desplazamiento en lugar de un umbral, podemos reescribir la regla del perceptr칩n: </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-158"><span class="MJXp-mtable" id="MJXp-Span-159"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-160" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-161" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-162"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-163"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-164"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-165"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-166"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-167"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mo" id="MJXp-Span-168" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-mrow" id="MJXp-Span-169"><span class="MJXp-mo" id="MJXp-Span-170" style="margin-left: 0em; margin-right: 0em; vertical-align: -0.472em;"><span class="MJXp-right MJXp-scale5" style="font-size: 2.889em; margin-left: -0.22em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">{ </font></font></span></span><span class="MJXp-mtable" id="MJXp-Span-171"><span><span class="MJXp-mtr" id="MJXp-Span-172" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-173" style="text-align: left;"><span class="MJXp-mn" id="MJXp-Span-174"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0 </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-176"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-177"><font style="vertical-align: inherit;">f </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-179"><font style="vertical-align: inherit;">w </font></span><span class="MJXp-mo" id="MJXp-Span-180" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">긘 </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-181"><font style="vertical-align: inherit;">x </font></span><span class="MJXp-mo" id="MJXp-Span-182" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">+ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-183"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mo" id="MJXp-Span-184" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;">곣 </font></span><span class="MJXp-mn" id="MJXp-Span-185"><font style="vertical-align: inherit;">0 </font></span></font><span class="MJXp-mtext" id="MJXp-Span-175">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-176"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-177"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-178">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-179"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-180" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-181"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-182" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-183"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-184" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mn" id="MJXp-Span-185"><font style="vertical-align: inherit;"></font></span></span></span><span class="MJXp-mtr" id="MJXp-Span-186" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-187" style="padding-top: 0.2em; text-align: left;"><span class="MJXp-mn" id="MJXp-Span-188"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1 </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-190"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-191"><font style="vertical-align: inherit;">f </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-193"><font style="vertical-align: inherit;">w </font></span><span class="MJXp-mo" id="MJXp-Span-194" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">긘 </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-195"><font style="vertical-align: inherit;">x </font></span><span class="MJXp-mo" id="MJXp-Span-196" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">+ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mo" id="MJXp-Span-198" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;">&gt; </font></span><span class="MJXp-mn" id="MJXp-Span-199"><font style="vertical-align: inherit;">0</font></span></font><span class="MJXp-mtext" id="MJXp-Span-189">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-190"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-191"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-192">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-193"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-194" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-195"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-196" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-198" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mn" id="MJXp-Span-199"><font style="vertical-align: inherit;"></font></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-200" style="margin-left: 0em; margin-right: 0em;"></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-7"> output = \begin{cases} 0 ~ if ~ w \cdot x + b \leq 0 \\ 1 ~ if ~ w \cdot x + b > 0 \end{cases} \tag{2} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El desplazamiento se puede representar como una medida de lo f치cil que es obtener un valor de 1 en la salida del perceptr칩n. O, en t칠rminos biol칩gicos, el desplazamiento es una medida de lo f치cil que es activar el perceptr칩n. Un perceptr칩n con un sesgo muy grande es extremadamente f치cil de dar 1. Pero con un sesgo negativo muy grande, esto es dif칤cil de hacer. Obviamente, la introducci칩n del sesgo es un peque침o cambio en la descripci칩n de los perceptrones, pero luego veremos que conduce a una mayor simplificaci칩n de la grabaci칩n. Por lo tanto, adem치s no usaremos el umbral, sino que siempre usaremos el desplazamiento.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Describ칤 los perceptrones en t칠rminos del m칠todo de sopesar la evidencia para la toma de decisiones. Otro m칠todo de uso es el c치lculo de funciones l칩gicas elementales, que generalmente consideramos los c치lculos principales, como AND, OR y NAND. Supongamos, por ejemplo, que tenemos un perceptr칩n con dos entradas, el peso de cada una de ellas es -2, y su desplazamiento es 3. Aqu칤 est치: la </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/530/d45/9a9/530d459a9262c475caf057106a500ddc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entrada 00 da la salida 1, porque (2) 갱 0 + (- 2 ) 갱 0 + 3 = 3 es mayor que cero. Los mismos c치lculos dicen que las entradas 01 y 10 dan 1. Pero 11 en la entrada da 0 en la salida, ya que (2) 갱 1 + (- 2) 갱 1 + 3 = 1, menor que cero. 춰Por lo tanto, nuestro perceptr칩n implementa la funci칩n NAND!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este ejemplo muestra que los perceptrones se pueden usar para calcular funciones l칩gicas b치sicas. De hecho, podemos usar redes perceptron para calcular cualquier funci칩n l칩gica en general. El hecho es que la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">compuerta l칩gica</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> NAND </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">es</font></a><font style="vertical-align: inherit;"> universal para los c치lculos: es posible construir cualquier c치lculo sobre la base. Por ejemplo, puede usar puertas NAND para crear un circuito que agregue dos bits, x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Para hacer esto, calcule la suma de bits</font></font><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-201"><span class="MJXp-msubsup" id="MJXp-Span-202"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-203" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-204" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-205" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 굿 </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-206"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-207" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-208" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></span></span></span></span><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , as칤 como</font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">la bandera de acarreo</font></a><font style="vertical-align: inherit;">, que es 1 cuando x</font><sub><font style="vertical-align: inherit;">1</font></sub><font style="vertical-align: inherit;">y x</font><sub><font style="vertical-align: inherit;">2</font></sub><font style="vertical-align: inherit;">son 1, es decir, la bandera de acarreo es simplemente el resultado de la multiplicaci칩n por bits x</font><sub><font style="vertical-align: inherit;">1</font></sub><font style="vertical-align: inherit;">x</font><sub><font style="vertical-align: inherit;">2</font></sub><font style="vertical-align: inherit;">:</font><font style="vertical-align: inherit;">para obtener la red equivalente de los perceptrones, reemplazamos todos Las compuertas NAND son perceptrones con dos entradas, el peso de cada una de ellas es -2, y con un desplazamiento de 3. Aqu칤 est치 la red resultante. Tenga en cuenta que mov칤 el perceptr칩n correspondiente a la v치lvula inferior derecha, solo para que sea m치s conveniente dibujar flechas:</font></font><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-8"> x_1 \oplus x_2 </script><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><sub><font style="vertical-align: inherit;"></font></sub><font style="vertical-align: inherit;"></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d25/af6/afe/d25af6afec499649a0b338f2ccc67f63.png"><br><br><font style="vertical-align: inherit;"></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/222/5b4/71b/2225b471b11b7357777ebc024bd287c2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un aspecto notable de esta red perceptr칩n es que la salida de la izquierda se usa dos veces como entrada en la parte inferior. Al definir el modelo del perceptr칩n, no mencion칠 la admisibilidad de tal esquema de doble salida en el mismo lugar. De hecho, en realidad no importa. Si no queremos permitir esto, simplemente podemos combinar dos l칤neas con pesos de -2 en una con un peso de -4. (Si esto no te parece obvio, detente y demu칠stralo a ti mismo). Despu칠s de este cambio, la red se ve de la siguiente manera, con todos los pesos no asignados iguales a -2, todos los desplazamientos iguales a 3, y se marca un peso -4: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/151/d84/dbd/151d84dbdf944cc9016ed656ba70e424.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tal registro de perceptrones que tienen una salida pero no entradas:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/41c/8e3/e46/41c8e3e46587bf536bab96b8427e9bcd.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es solo una abreviatura. Esto no significa que no tenga entradas. Para entender esto, supongamos que tenemos un perceptr칩n sin entradas. Entonces la suma ponderada 갬 </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> siempre ser칤a cero, por lo que el perceptr칩n dar칤a 1 para b&gt; 0 y 0 para b 곣 0. Es decir, el perceptr칩n solo dar칤a un valor fijo, y no lo que necesitamos (x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en el ejemplo arriba). Es mejor considerar los perceptrones de entrada no como perceptrones, sino como unidades especiales que simplemente se definen para producir los valores deseados x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ...</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El ejemplo del sumador demuestra c칩mo se puede utilizar una red perceptr칩n para simular un circuito que contiene muchas compuertas NAND. Y dado que estas puertas son universales para los c치lculos, por lo tanto, los perceptrones son universales para los c치lculos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La versatilidad computacional de los perceptrones es alentadora y decepcionante. Es alentador, asegurando que la red perceptron pueda ser tan poderosa como cualquier otro dispositivo inform치tico. Decepcionante, dando la impresi칩n de que los perceptrones son solo un nuevo tipo de puerta l칩gica NAND. M치s o menos descubrimiento!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, la situaci칩n es realmente mejor. </font><font style="vertical-align: inherit;">Resulta que podemos desarrollar algoritmos de entrenamiento que pueden ajustar autom치ticamente los pesos y los desplazamientos de la red de las neuronas artificiales. </font><font style="vertical-align: inherit;">Este ajuste tiene lugar en respuesta a est칤mulos externos, sin la intervenci칩n directa de un programador. </font><font style="vertical-align: inherit;">Estos algoritmos de aprendizaje nos permiten usar neuronas artificiales de una manera radicalmente diferente de las puertas l칩gicas ordinarias. </font><font style="vertical-align: inherit;">En lugar de registrar expl칤citamente un circuito desde puertas NAND y otras, nuestras redes neuronales simplemente pueden aprender a resolver problemas, a veces aquellos para los que ser칤a extremadamente dif칤cil dise침ar directamente un circuito regular.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Neuronas sigmoideas </font></font></h3><br>  Los algoritmos de aprendizaje son geniales.  Sin embargo, 쯖칩mo desarrollar un algoritmo para una red neuronal?  Supongamos que tenemos una red de perceptrones que queremos usar para capacitarnos en la resoluci칩n de un problema.  Suponga que la entrada a la red puede ser p칤xeles de una imagen escaneada de un d칤gito escrito a mano.  Y queremos que la red conozca los pesos y las compensaciones necesarias para clasificar correctamente los n칰meros.  Para comprender c칩mo puede funcionar dicha capacitaci칩n, imaginemos que estamos cambiando ligeramente un cierto peso (o sesgo) en la red.  Queremos que este peque침o cambio conduzca a un peque침o cambio en la salida de la red.  Como veremos pronto, esta propiedad hace posible el aprendizaje.  Esquem치ticamente, queremos lo siguiente (obviamente, 춰tal red es demasiado simple para reconocer la escritura a mano!): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2cf/d97/76f/2cfd9776fdb27a7106bdf2a94d76eb46.png"><br><br>  Si un peque침o cambio en el peso (o sesgo) llevara a un peque침o cambio en el resultado de salida, podr칤amos cambiar los pesos y los sesgos para que nuestra red se comporte un poco m치s cerca de lo que queremos.  Por ejemplo, supongamos que la red asign칩 incorrectamente la imagen a "8", aunque deber칤a haber sido a "9".  Podr칤amos descubrir c칩mo hacer un peque침o cambio en el peso y el desplazamiento para que la red se acerque un poco m치s a clasificar la imagen como "9".  Y luego repetir칤amos esto, cambiando pesos y turnos una y otra vez para obtener el mejor y el mejor resultado.  La red aprender칤a. <br><br>  El problema es que si hay perceptrones en la red, esto no sucede.  Un peque침o cambio en los pesos o el desplazamiento de cualquier perceptr칩n a veces puede conducir a un cambio en su salida al opuesto, por ejemplo, de 0 a 1. Tal cambio puede cambiar el comportamiento del resto de la red de una manera muy complicada.  E incluso si ahora nuestro "9" se reconoce correctamente, el comportamiento de la red con todas las otras im치genes probablemente ha cambiado por completo de una manera que es dif칤cil de controlar.  Debido a esto, es dif칤cil imaginar c칩mo podemos ajustar gradualmente los pesos y las compensaciones para que la red se acerque gradualmente al comportamiento deseado.  Quiz치s hay alguna forma inteligente de solucionar este problema.  Pero no existe una soluci칩n simple al problema de aprender una red de perceptrones. <br><br>  Este problema se puede evitar introduciendo un nuevo tipo de neurona artificial llamada neurona sigmoidea.  Son similares a los perceptrones, pero modificados para que los peque침os cambios en los pesos y las compensaciones den como resultado solo peque침os cambios en la salida.  Este es un hecho b치sico que permitir치 que la red de neuronas sigmoides aprenda. <br><br>  D칠jame describir una neurona sigmoidea.  Los dibujaremos de la misma manera que los perceptrones: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2cd/0b3/ccf/2cd0b3ccf36d0dabf96fd15649c29f90.png"><br><br>  Tiene la misma entrada x <sub>1</sub> , x <sub>2</sub> , ... Pero en lugar de ser igual a 0 o 1, estas entradas pueden tener cualquier valor en el rango de 0 a 1. Por ejemplo, un valor de 0.638 ser치 una entrada v치lida para neurona sigmoidea (CH).  Al igual que el perceptr칩n, SN tiene pesos para cada entrada, w <sub>1</sub> , w <sub>2</sub> , ... y el sesgo total b.  Pero su valor de salida no ser치 0 o 1. Ser치 픢 (w긘x + b), donde 픢 es el sigmoide. <br><br>  Por cierto, 픢 a veces se llama una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">funci칩n log칤stica</a> , y esta clase de neuronas se llama neuronas log칤sticas.  Es 칰til recordar esta terminolog칤a, ya que estos t칠rminos son utilizados por muchas personas que trabajan con redes neuronales.  Sin embargo, nos adheriremos a la terminolog칤a sigmoidea. <br><br>  La funci칩n se define de la siguiente manera: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-209"><span class="MJXp-mtext" id="MJXp-Span-210">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-211">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-212">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-213">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-214">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-215">a</span><span class="MJXp-mo" id="MJXp-Span-216" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-217">z</span><span class="MJXp-mo" id="MJXp-Span-218" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-219">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-220">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-221">q</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-222">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-223">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-224">v</span><span class="MJXp-mtext" id="MJXp-Span-225">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-226">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-227">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-228">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-229">c</span><span class="MJXp-mrow" id="MJXp-Span-230"><span class="MJXp-mn" id="MJXp-Span-231">1</span></span><span class="MJXp-mrow" id="MJXp-Span-232"><span class="MJXp-mn" id="MJXp-Span-233">1</span><span class="MJXp-mo" id="MJXp-Span-234" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-235"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-236" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-237" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-238"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-239">z</span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-240">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-241">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-242">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-243">g</span><span class="MJXp-mrow" id="MJXp-Span-244"><span class="MJXp-mn" id="MJXp-Span-245">3</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-9"> \ sigma (z) \ equiv \ frac {1} {1 + e ^ {- z}} \ tag {3} </script></p><br><br>  En nuestro caso, el valor de salida de la neurona sigmoidea con datos de entrada x <sub>1</sub> , x <sub>2</sub> , ... por los pesos w <sub>1</sub> , w <sub>2</sub> , ... y el desplazamiento b se considerar치 como: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-246"><span class="MJXp-mtext" id="MJXp-Span-247">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-248">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-249">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-250">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-251">c</span><span class="MJXp-mrow" id="MJXp-Span-252"><span class="MJXp-mn" id="MJXp-Span-253">1</span></span><span class="MJXp-mrow" id="MJXp-Span-254"><span class="MJXp-mn" id="MJXp-Span-255">1</span><span class="MJXp-mo" id="MJXp-Span-256" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-257">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-258">x</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-259">p</span><span class="MJXp-mo" id="MJXp-Span-260" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mo" id="MJXp-Span-261" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mtext" id="MJXp-Span-262">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-263">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-264">u</span><span class="MJXp-msubsup" id="MJXp-Span-265"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-266" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-267" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-268"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-269" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-270" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-msubsup" id="MJXp-Span-271"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-272" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-273" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-274" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-275">b</span><span class="MJXp-mo" id="MJXp-Span-276" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mtext" id="MJXp-Span-277">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-278">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-279">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-280">g</span><span class="MJXp-mrow" id="MJXp-Span-281"><span class="MJXp-mn" id="MJXp-Span-282">4</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-10"> \ frac {1} {1 + exp (- \ sum_j w_jx_j - b)} \ tag {4} </script></p><br><br>  A primera vista, CH parece completamente diferente a las neuronas.  El aspecto algebraico de un sigmoide puede parecer confuso y oscuro si no est치 familiarizado con 칠l.  De hecho, hay muchas similitudes entre los perceptrones y el SN, y la forma algebraica de un sigmoide resulta ser m치s un detalle t칠cnico que una barrera seria para la comprensi칩n. <br><br>  Para comprender las similitudes con el modelo de perceptr칩n, suponga que z 고 w 긘 x + b es un n칰mero positivo grande.  Entonces e - z 곋 0, por lo tanto, 픢 (z) 곋 1. En otras palabras, cuando z = w 긘 x + b es grande y positivo, el rendimiento de SN es aproximadamente igual a 1, como para el perceptr칩n.  Suponga que z = w 긘 x + b es grande con un signo menos.  Entonces e - z  , y 픢 (z) 곋 0. Entonces, para z grande con un signo menos, el comportamiento del SN tambi칠n se acerca al perceptr칩n.  Y solo cuando w 긘 x + b tiene un tama침o promedio, se observan serias desviaciones del modelo de perceptr칩n. <br><br>  쯈u칠 pasa con la forma algebraica de 픢?  쮺칩mo lo entendemos?  De hecho, la forma exacta de 픢 no es tan importante: la forma de la funci칩n en el gr치fico es importante.  Aqu칤 esta: <br><br><img src="https://habrastorage.org/webt/sm/u4/jv/smu4jvbwuriryrfojrpt-ukdr6w.png"><br><br>  Esta es una versi칩n fluida de la funci칩n de paso: <br><br><img src="https://habrastorage.org/webt/2i/p0/a-/2ip0a-3cmpstyiwrglx_michl3m.png"><br><br>  Si 픢 fuera paso a paso, entonces el SN ser칤a un perceptr칩n, ya que tendr칤a 0 o 1 salida dependiendo del signo w 긘 x + b (bueno, de hecho, en z = 0, el perceptr칩n da 0, y la funci칩n de paso 1 , entonces en ese punto, la funci칩n tendr칤a que ser cambiada). <br><br>  Usando la funci칩n real 픢, obtenemos un perceptr칩n suavizado.  Y lo principal aqu칤 es la suavidad de la funci칩n, no su forma exacta.  La suavidad significa que peque침os cambios 풊w <sub>j</sub> pesos y 풦b compensaciones dar치n peque침os cambios 풊 salida de la salida.  El 치lgebra nos dice que la salida 풊 se aproxima bien de la siguiente manera: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-283"><span class="MJXp-mtext" id="MJXp-Span-284">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-285">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-286">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-287">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-289">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-290">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-291">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-292">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-293">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-294">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-295">t</span><span class="MJXp-mtext" id="MJXp-Span-296">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-297">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-298">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-299">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-300">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-301">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-302">x</span><span class="MJXp-mtext" id="MJXp-Span-303">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-304">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-305">u</span><span class="MJXp-msubsup" id="MJXp-Span-306"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-307" style="margin-right: 0.05em;">m</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-308" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mtext" id="MJXp-Span-309">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-310">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-311">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-312">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-313">c</span><span class="MJXp-mrow" id="MJXp-Span-314"><span class="MJXp-mtext" id="MJXp-Span-315">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-316">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-317">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-318">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-319">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-320">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-321">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-322">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-323">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-324">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-325">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-326">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-327">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-328">t</span></span><span class="MJXp-mrow" id="MJXp-Span-329"><span class="MJXp-mtext" id="MJXp-Span-330">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-331">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-332">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-333">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-334">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-335">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-336">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-337">l</span><span class="MJXp-msubsup" id="MJXp-Span-338"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-339" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-340" style="vertical-align: -0.4em;">j</span></span></span><span class="MJXp-mtext" id="MJXp-Span-341">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-342">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-343">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-344">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-345">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-346">a</span><span class="MJXp-msubsup" id="MJXp-Span-347"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-348" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-349" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-350" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-351">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-352">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-353">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-354">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-355">c</span><span class="MJXp-mrow" id="MJXp-Span-356"><span class="MJXp-mtext" id="MJXp-Span-357">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-358">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-359">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-360"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-361"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-362"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-363"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-364"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-365"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-366"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-367"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-368"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">p </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-369"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-370"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t </font></font></span></span><font style="vertical-align: inherit;"><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-373"><font style="vertical-align: inherit;">p </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-374"><font style="vertical-align: inherit;">a </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-375"><font style="vertical-align: inherit;">r </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-376"><font style="vertical-align: inherit;">t </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-377"><font style="vertical-align: inherit;">i </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-378"><font style="vertical-align: inherit;">a </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379"><font style="vertical-align: inherit;">l </font></span></span><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-380"><font style="vertical-align: inherit;">b</font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-382"><font style="vertical-align: inherit;"> D </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-383"><font style="vertical-align: inherit;">e </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-384"><font style="vertical-align: inherit;">l </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-385"><font style="vertical-align: inherit;">t </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-386"><font style="vertical-align: inherit;">a </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-389"><font style="vertical-align: inherit;">t </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-390"><font style="vertical-align: inherit;">a </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-391"><font style="vertical-align: inherit;">g </font></span><span class="MJXp-mrow" id="MJXp-Span-392"><span class="MJXp-mn" id="MJXp-Span-393"><font style="vertical-align: inherit;">5</font></span></span></font><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mtext" id="MJXp-Span-372">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-373"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-374"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-375"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-376"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-377"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-378"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-380"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mtext" id="MJXp-Span-381">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-382"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-383"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-384"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-385"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-386"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-388">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-389"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-390"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-391"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mrow" id="MJXp-Span-392"><span class="MJXp-mn" id="MJXp-Span-393"><font style="vertical-align: inherit;"></font></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-11"> \ Delta output \ approx \ sum_j \ frac {\ partial output} {\ partial w_j} \ Delta w_j + \ frac {\ partial output} {\ partial b} \ Delta b \ tag {5} </script></p><br><br>  Donde la suma es sobre todos los pesos w <sub>j</sub> , y 객output / 객w <sub>j</sub> y 객output / 객b denotan derivadas parciales de la salida con respecto a w <sub>j</sub> y b, respectivamente.  춰No se asuste si se siente inseguro en compa침칤a de derivados privados!  Aunque la f칩rmula parece complicada, con todas estas derivadas parciales, en realidad dice algo bastante simple (y 칰til): la salida 풊 es una funci칩n lineal que depende de los pesos y sesgos 풊w <sub>j</sub> y 풊b.  Su linealidad facilita la selecci칩n de peque침os cambios en pesos y compensaciones para lograr cualquier sesgo de salida peque침o deseado.  Por lo tanto, aunque los SN son similares a los perceptrones en el comportamiento cualitativo, facilitan la comprensi칩n de c칩mo se puede cambiar la producci칩n cambiando los pesos y los desplazamientos. <br><br>  Si la forma general 픢 es importante para nosotros, y no su forma exacta, 쯣or qu칠 usamos tal f칩rmula (3)?  De hecho, m치s adelante a veces consideraremos neuronas cuya salida es f (w 긘 x + b), donde f () es alguna otra funci칩n de activaci칩n.  Lo principal que cambia cuando cambia la funci칩n es el valor de las derivadas parciales en la ecuaci칩n (5).  Resulta que cuando calculamos estas derivadas parciales, el uso de 픢 simplifica enormemente el 치lgebra, ya que los exponentes tienen propiedades muy agradables al diferenciar.  En cualquier caso, 픢 se usa a menudo para trabajar con redes neuronales, y con mayor frecuencia en este libro usaremos dicha funci칩n de activaci칩n. <br><br>  쮺칩mo interpretar el resultado del trabajo de CH?  Obviamente, la principal diferencia entre los perceptrones y el CH es que el CH no da solo 0 o 1. Su salida puede ser cualquier n칰mero real de 0 a 1, por lo que valores como 0.173 o 0.689 son v치lidos.  Esto puede ser 칰til, por ejemplo, si desea que el valor de salida indique, por ejemplo, el brillo promedio de los p칤xeles de la imagen recibida en la entrada del NS.  Pero a veces puede ser inconveniente.  Supongamos que queremos que la salida de la red diga que "se ingres칩 la imagen 9" o "imagen de entrada no 9".  Obviamente, ser칤a m치s f치cil si los valores de salida fueran 0 o 1, como un perceptr칩n.  Pero en la pr치ctica, podemos estar de acuerdo en que cualquier valor de salida de al menos 0.5 significar칤a "9" en la entrada, y cualquier valor inferior a 0.5 significar칤a que es "no 9".  Siempre indicar칠 expl칤citamente la existencia de tales acuerdos. <br><br>  Ejercicios <br><br><ul><li>  Perceptrones simuladores de CH, parte 1 </li></ul><br>  Supongamos que tomamos todos los pesos y desplazamientos de una red de perceptrones, y los multiplicamos por una constante positiva c&gt; 0.  Muestre que el comportamiento de la red no cambia. <br><br><ul><li>  Perceptrones simuladores de CH, parte 2 </li></ul><br>  Supongamos que tenemos la misma situaci칩n que en el problema anterior: una red de perceptrones.  Supongamos tambi칠n que se seleccionan los datos de entrada para la red.  No necesitamos un valor espec칤fico, lo principal es que es fijo.  Suponga que los pesos y los desplazamientos son tales que w긘x + b  0, donde x es el valor de entrada de cualquier perceptr칩n de la red.  Ahora reemplazamos todos los perceptrones en la red con SN, y multiplicamos los pesos y desplazamientos por la constante positiva c&gt; 0.  Muestre que en el l칤mite c   el comportamiento de la red desde el SN ser치 exactamente el mismo que el de las redes de perceptrones.  쮺칩mo se violar치 esta afirmaci칩n si para uno de los perceptrones w긘x + b = 0? <br><br><h3>  Arquitectura de red neuronal </h3><br>  En la siguiente secci칩n, presentar칠 una red neuronal capaz de una buena clasificaci칩n de n칰meros escritos a mano.  Antes de eso, es 칰til explicar la terminolog칤a que nos permite se침alar diferentes partes de la red.  Digamos que tenemos la siguiente red: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/472/521/00b/47252100b91b8e1a527796217a6ed0fa.png"><br><br>  Como mencion칠, la capa m치s a la izquierda de la red se llama capa de entrada, y sus neuronas se llaman neuronas de entrada.  La capa de salida m치s a la derecha contiene neuronas de salida o, como en nuestro caso, una neurona de salida.  La capa intermedia se llama oculta, porque sus neuronas no son ni de entrada ni de salida.  El t칠rmino "oculto" puede sonar un poco misterioso; cuando lo escuch칠 por primera vez, decid칤 que deber칤a tener una importancia filos칩fica o matem치tica profunda, sin embargo, solo significa "no entrar y no salir".  La red de arriba solo tiene una capa oculta, pero algunas redes tienen varias capas ocultas.  Por ejemplo, en la siguiente red de cuatro capas hay dos capas ocultas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/26e/be9/c5f/26ebe9c5f4c7f61413cfa43e67151734.png"><br><br>  Esto puede ser confuso, pero por razones hist칩ricas, tales redes de m칰ltiples capas a veces se denominan perceptrones multicapa, MLP, a pesar de que consisten en neuronas sigmoideas en lugar de perceptrones.  No voy a usar esa terminolog칤a porque es confusa, pero debo advertir sobre su existencia. <br><br>  Dise침ar capas de entrada y salida a veces es una tarea simple.  Por ejemplo, supongamos que estamos tratando de determinar si el n칰mero escrito a mano significa "9" o no.  Un circuito de red natural codificar치 el brillo de los p칤xeles de la imagen en las neuronas de entrada.  Si la imagen es en blanco y negro, 64x64 p칤xeles de tama침o, entonces tendremos 64x64 = 4096 neuronas de entrada, con un brillo en el rango de 0 a 1. La capa de salida contendr치 solo una neurona, cuyo valor menor que 0.5 significar치 que "en la entrada no fue 9 ", pero los valores m치s significar치n que" la entrada fue 9 ". <br><br>  Y aunque dise침ar capas de entrada y salida es a menudo una tarea simple, dise침ar capas ocultas puede ser un arte dif칤cil.  En particular, no es posible describir el proceso de desarrollo de capas ocultas con unas simples reglas generales.  Los investigadores de la Asamblea Nacional han desarrollado muchas reglas heur칤sticas para el dise침o de capas ocultas que ayudan a obtener el comportamiento deseado de las redes neuronales.  Por ejemplo, dicha heur칤stica se puede utilizar para comprender c칩mo lograr un compromiso entre el n칰mero de capas ocultas y el tiempo disponible para capacitar a la red.  M치s adelante nos encontraremos con algunas de estas reglas. <br><br>  Hasta ahora, hemos estado discutiendo las NS en las que la salida de una capa se usa como entrada para la siguiente.  Dichas redes se denominan redes neuronales de distribuci칩n directa.  Esto significa que no hay bucles en la red: la informaci칩n siempre avanza y nunca retroalimenta.  Si tuvi칠ramos bucles, encontrar칤amos situaciones en las que el sigmoide de entrada depender칤a de la salida.  Ser칤a dif칤cil de comprender, y no permitimos tales bucles. <br><br>  Sin embargo, hay otros modelos de NS artificiales en los que es posible usar bucles de retroalimentaci칩n.  Estos modelos se denominan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">redes neuronales recurrentes</a> (RNS).  La idea de estas redes es que sus neuronas se activan por per칤odos limitados de tiempo.  Esta activaci칩n puede estimular otros neutrones, que pueden activarse un poco m치s tarde, tambi칠n por un tiempo limitado.  Esto conduce a la activaci칩n de las siguientes neuronas, y con el tiempo obtenemos una cascada de neuronas activadas.  Los bucles en tales modelos no presentan problemas, ya que la salida de una neurona afecta su entrada en un momento posterior, y no de inmediato. <br><br>  Los RNS no fueron tan influyentes como los NS de distribuci칩n directa, en particular porque los algoritmos de entrenamiento para los RNS hasta ahora tienen menos potencial.  Sin embargo, el RNS sigue siendo extremadamente interesante.  En el esp칤ritu de trabajo, est치n mucho m치s cerca del cerebro que NS de distribuci칩n directa.  Es posible que el RNS pueda resolver problemas importantes que, con la ayuda de la distribuci칩n directa NS, pueden resolverse con grandes dificultades.  Sin embargo, para limitar el alcance de nuestro estudio, nos concentraremos en el NS de distribuci칩n directa m치s utilizado. <br><br><h3>  Red simple de clasificaci칩n de tinta </h3><br>  Una vez definidas las redes neuronales, volveremos al reconocimiento de escritura a mano.  La tarea de reconocer n칰meros escritos a mano se puede dividir en dos subtareas.  Primero, queremos encontrar una manera de dividir una imagen que contenga muchos d칤gitos en una secuencia de im치genes individuales, cada una de las cuales contiene un d칤gito.  Por ejemplo, nos gustar칤a dividir la imagen <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><br>  en seis separados <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b36/135/6ae/b361356aec440b1dbf77c8dedbc6f9b6.png"><br><br>  Los humanos podemos resolver f치cilmente este problema de segmentaci칩n, pero es dif칤cil para un programa de computadora dividir correctamente la imagen.  Despu칠s de la segmentaci칩n, el programa necesita clasificar cada d칤gito individual.  Entonces, por ejemplo, queremos que nuestro programa reconozca que el primer d칤gito <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e5a/c2a/808/e5ac2a808e18ac02ba0f09b2052fff4f.png"><br><br>  son las 5. <br><br>  Nos concentraremos en crear un programa para resolver el segundo problema, la clasificaci칩n de n칰meros individuales.  Resulta que el problema de la segmentaci칩n no es tan dif칤cil de resolver tan pronto como encontramos una buena manera de clasificar los d칤gitos individuales.  Existen muchos enfoques para resolver el problema de segmentaci칩n.  Una de ellas es probar muchas formas diferentes de segmentaci칩n de im치genes utilizando el clasificador de d칤gitos individuales, evaluando cada intento.  La segmentaci칩n de prueba es muy apreciada si el clasificador de d칤gitos individuales conf칤a en la clasificaci칩n de todos los segmentos, y baja si tiene problemas en uno o m치s segmentos.  La idea es que si el clasificador tiene problemas en alg칰n lugar, lo m치s probable es que la segmentaci칩n sea incorrecta.  Esta idea y otras opciones se pueden utilizar para una buena soluci칩n al problema de segmentaci칩n.  Entonces, en lugar de preocuparnos por la segmentaci칩n, nos concentraremos en desarrollar un NS capaz de resolver una tarea m치s interesante y compleja, a saber, reconocer n칰meros escritos a mano individuales. <br><br>  Para reconocer d칤gitos individuales, usaremos NS de tres capas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/350/3dc/e6b3503dcc1e2848980d17ad8c193018.png"><br><br>  La capa de red de entrada contiene neuronas que codifican varios valores de los p칤xeles de entrada.  Como se indicar치 en la siguiente secci칩n, nuestros datos de entrenamiento consistir치n en muchas im치genes de d칤gitos escritos a mano escaneados de 28x28 p칤xeles de tama침o, por lo que la capa de entrada contiene 28x28 = 784 neuronas.  Por simplicidad, no indiqu칠 la mayor칤a de las 784 neuronas en el diagrama.  Los p칤xeles entrantes son en blanco y negro, con un valor de 0.0 que indica blanco, 1.0 que indica negro y valores intermedios que indican sombras de gris cada vez m치s oscuras. <br><br>  La segunda capa de la red est치 oculta.  Denotamos el n칰mero de neuronas en esta capa n, y experimentaremos con diferentes valores de n.  El ejemplo anterior muestra una peque침a capa oculta que contiene solo n = 15 neuronas. <br><br>  Hay 10 neuronas en la capa de salida de la red.  Si la primera neurona est치 activada, es decir, su valor de salida es 곋 1, esto indica que la red cree que la entrada fue 0. Si la segunda neurona est치 activada, la red cree que la entrada fue 1. Y as칤 sucesivamente.  Estrictamente hablando, numeramos las neuronas de salida de 0 a 9 y observamos cu치l de ellas ten칤a el valor de activaci칩n m치ximo.  Si esto es, digamos, la neurona No. 6, entonces nuestra red cree que la entrada fue el n칰mero 6. Y as칤 sucesivamente. <br><br>  Quiz치s se pregunte por qu칠 necesitamos usar diez neuronas.  Despu칠s de todo, queremos saber qu칠 d칤gito del 0 al 9 corresponde a la imagen de entrada.  Ser칤a natural usar solo 4 neuronas de salida, cada una de las cuales tomar칤a un valor binario, dependiendo de si su valor de salida est치 m치s cerca de 0 o 1. Cuatro neuronas ser칤an suficientes, ya que 2 <sup>4</sup> = 16, m치s de 10 valores posibles.  쯇or qu칠 nuestra red deber칤a usar 10 neuronas?  쮼s esto ineficaz?  La base para esto es emp칤rica;  podemos probar ambas variantes de la red, y resulta que para esta tarea, una red con 10 neuronas de salida est치 mejor entrenada para reconocer n칰meros que una red con 4.  Sin embargo, la pregunta sigue siendo, 쯣or qu칠 son mejores las 10 neuronas de salida?  쮿ay alguna heur칤stica que nos diga de antemano que se deben usar 10 neuronas de salida en lugar de 4? <br><br>  Para entender por qu칠, es 칰til pensar en lo que hace una red neuronal.  Primero, considere la opci칩n con 10 neuronas de salida.  Nos centramos en la primera neurona de salida, que est치 tratando de decidir si la imagen entrante es cero.  Lo hace sopesando la evidencia obtenida de una capa oculta.  쯈u칠 hacen las neuronas ocultas?  Supongamos que la primera neurona en la capa oculta determina si hay algo como esto en la imagen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/650/c35/402/650c3540204f98b406e25078ddc8742a.png"><br><br>  Puede hacer esto asignando pesos grandes a p칤xeles que coincidan con esta imagen, y pesos peque침os al resto.  Del mismo modo, suponga que las neuronas segunda, tercera y cuarta en la capa oculta est치n buscando si hay fragmentos similares en la imagen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d94/4c3/cea/d944c3cea54d91106d2ee2198d04c801.png"><br><br>  Como habr치s adivinado, todos estos cuatro fragmentos juntos dan la imagen 0, que vimos anteriormente: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f99/a20/1e5/f99a201e557bacdbe39190a6f913b49e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces, si las cuatro neuronas ocultas est치n activadas, podemos concluir que el n칰mero es 0. Por supuesto, esta no es la 칰nica evidencia de que 0 se mostr칩 all칤; podemos obtener 0 de muchas otras maneras (cambiando ligeramente estas im치genes o distorsion치ndolas ligeramente). </font><font style="vertical-align: inherit;">Sin embargo, podemos decir con certeza que, al menos en este caso, podemos concluir que hab칤a 0 en la entrada.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si suponemos que la red funciona as칤, podemos dar una explicaci칩n plausible de por qu칠 es mejor usar 10 neuronas de salida en lugar de 4. Si tuvi칠ramos 4 neuronas de salida, la primera neurona intentar칤a decidir cu치l es el bit m치s significativo del d칤gito entrante. Y no hay una manera f치cil de asociar el bit m치s significativo con las formas simples dadas anteriormente. Es dif칤cil imaginar razones hist칩ricas por las cuales las partes de la forma de un d칤gito estar칤an de alguna manera relacionadas con el bit m치s significativo de la salida.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, todo lo anterior solo es compatible con la heur칤stica. </font><font style="vertical-align: inherit;">Nada habla a favor del hecho de que una red de tres capas deber칤a funcionar como dije, y las neuronas ocultas deber칤an encontrar componentes simples de las formas. </font><font style="vertical-align: inherit;">Quiz치s el complicado algoritmo de aprendizaje encuentre algunos pesos que nos permitir치n usar solo 4 neuronas de salida. </font><font style="vertical-align: inherit;">Sin embargo, como m칠todo heur칤stico, mi m칠todo funciona bien y puede ahorrarle un tiempo considerable en el desarrollo de una buena arquitectura NS.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ejercicios </font></font></h3><br><ul><li>      ,      .          ,     .         . ,   3   ,       (  )     0,99,       0,01. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/f5b/4af/2e9/f5b4af2e9acf846ab8cc5c60dac20c03.png"><br><br><h3>     </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces, tenemos el esquema de NA: 쯖칩mo aprender a reconocer los n칰meros? Lo primero que necesitamos son datos de entrenamiento, los llamados conjunto de datos de entrenamiento. Utilizaremos el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kit MNIST que</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> contiene decenas de miles de im치genes escaneadas de n칰meros escritos a mano y su clasificaci칩n correcta. El nombre que recibi칩 MNIST debido a que es un subconjunto modificado de los dos conjuntos de datos recopilados por </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NIST</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , el Instituto Nacional de Est치ndares y Tecnolog칤a de EE. UU. Aqu칤 hay algunas im치genes de MNIST: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b36/135/6ae/b361356aec440b1dbf77c8dedbc6f9b6.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estos son los mismos n칰meros que se dieron al comienzo del cap칤tulo como tarea de reconocimiento. 춰Por supuesto, cuando revise el NS, le pediremos que reconozca las im치genes incorrectas que ya estaban en el conjunto de entrenamiento!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los datos de MNIST constan de dos partes. El primero contiene 60,000 im치genes destinadas a la capacitaci칩n. Estos son manuscritos escaneados de 250 personas, la mitad de los cuales eran empleados de la Oficina del Censo de los Estados Unidos, y la otra mitad eran estudiantes de secundaria. Las im치genes son en blanco y negro, midiendo 28x28 p칤xeles. La segunda parte del conjunto de datos MNIST es de 10.000 im치genes para probar la red. Esta tambi칠n es una imagen en blanco y negro de 28x28 p칤xeles. Utilizaremos estos datos para evaluar qu칠 tan bien la red ha aprendido a reconocer los n칰meros. Para mejorar la calidad de la evaluaci칩n, estas cifras fueron tomadas de otras 250 personas que no participaron en la grabaci칩n del conjunto de capacitaci칩n (aunque tambi칠n eran empleados de la Oficina y estudiantes de secundaria). Esto nos ayuda a asegurarnos de que nuestro sistema pueda reconocer la escritura a mano de personas que no conoci칩 durante la capacitaci칩n.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La entrada de entrenamiento ser치 denotada por x. Ser치 conveniente tratar cada imagen de entrada x como un vector con 28x28 = 784 mediciones. Cada valor dentro del vector indica el brillo de un p칤xel en la imagen. Denotaremos el valor de salida como y = y (x), donde y es un vector de diez dimensiones. Por ejemplo, si una determinada imagen de entrenamiento x contiene 6, entonces y (x) = (0,0,0,0,0,0,1,0,0,0) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ser치 el vector que necesitamos. T es una operaci칩n de transposici칩n que convierte un vector de fila en un vector de columna. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Queremos encontrar un algoritmo que nos permita buscar dichos pesos y compensaciones para que la salida de la red se acerque a y (x) para todas las entradas de entrenamiento x. Para cuantificar la aproximaci칩n de este objetivo, definimos una funci칩n de costo (a veces llamada </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">funci칩n de p칠rdida</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">; en el libro usaremos la funci칩n de costo, pero tenga en cuenta otro nombre):</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-394"><span class="MJXp-mtable" id="MJXp-Span-395"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-396" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-397" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-398">C</span><span class="MJXp-mo" id="MJXp-Span-399" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-400">w</span><span class="MJXp-mo" id="MJXp-Span-401" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-402">b</span><span class="MJXp-mo" id="MJXp-Span-403" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-404" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mfrac" id="MJXp-Span-405" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-406">1</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-407">2</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-408">n</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-409"><span class=""><span class="MJXp-mo" id="MJXp-Span-410" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">갬</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-411" style="margin-left: 0px;">x</span></span></span><span class="MJXp-mrow" id="MJXp-Span-412"><span class="MJXp-mo" id="MJXp-Span-413" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mrow" id="MJXp-Span-414"><span class="MJXp-mo" id="MJXp-Span-415" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-416">y</span><span class="MJXp-mo" id="MJXp-Span-417" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-418">x</span><span class="MJXp-mo" id="MJXp-Span-419" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-420" style="margin-left: 0em; margin-right: 0.222em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-421">a</span><span class="MJXp-mrow" id="MJXp-Span-422"><span class="MJXp-mo" id="MJXp-Span-423" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-msubsup" id="MJXp-Span-424"><span class="MJXp-mrow" id="MJXp-Span-425" style="margin-right: 0.05em;"><span class="MJXp-mo" id="MJXp-Span-426" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-427" style="vertical-align: 0.5em;">2</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-12"> C(w, b) = \frac{1}{2n} \sum_x || y(x)  a ||^2 \tag{6} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aqu칤 w denota un conjunto de pesos de red, b es un conjunto de desplazamientos, n es el n칰mero de datos de entrada de entrenamiento, a es el vector de datos de salida cuando x son datos de entrada, y la suma pasa a trav칠s de toda la entrada de entrenamiento x. La salida, por supuesto, depende de x, w y b, pero por simplicidad no design칠 esta dependencia. La notaci칩n || v || significa la longitud del vector v. Llamaremos a C una funci칩n de costo cuadr치tico; a veces tambi칠n se denomina error est치ndar o MSE. Si observa detenidamente C, puede ver que no es negativa, ya que todos los miembros de la suma no son negativos. Adem치s, el costo de C (w, b) se vuelve peque침o, es decir, C (w, b) 곋 0, precisamente cuando y (x) es aproximadamente igual al vector de salida a para todos los datos de entrada de entrenamiento x. Entonces, nuestro algoritmo funcion칩 bien si logramos encontrar pesos y compensaciones tales que C (w, b) 곋 0. Y viceversa, funcion칩 mal cuando C (w,b) grande: esto significa que y (x) no coincide con la salida para una gran cantidad de entrada. Resulta que el objetivo del algoritmo de entrenamiento es minimizar el costo de C (w, b) en funci칩n de los pesos y las compensaciones. En otras palabras, necesitamos encontrar un conjunto de pesos y compensaciones que minimicen el valor del costo. Haremos esto usando un algoritmo llamado descenso de gradiente.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">쯇or qu칠 necesitamos un valor cuadr치tico? 쯅o nos interesa principalmente la cantidad de im치genes que la red reconoce correctamente? 쮼s posible simplemente maximizar este n칰mero directamente y no minimizar el valor intermedio del valor cuadr치tico? El problema es que el n칰mero de im치genes correctamente reconocidas no es una funci칩n uniforme de los pesos y las compensaciones de la red. En su mayor parte, los peque침os cambios en los pesos y las compensaciones no cambiar치n el n칰mero de im치genes correctamente reconocidas. Debido a esto, es dif칤cil entender c칩mo cambiar los pesos y los prejuicios para mejorar la eficiencia. Si utilizamos una funci칩n de costo uniforme, nos ser치 f치cil entender c칩mo hacer peque침os cambios en los pesos y las compensaciones para mejorar el costo. Por lo tanto, primero nos centraremos en el valor cuadr치tico y luego estudiaremos la precisi칩n de la clasificaci칩n.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Incluso teniendo en cuenta que queremos utilizar una funci칩n de costo uniforme, 쯣uede que le interese saber por qu칠 elegimos la funci칩n cuadr치tica para la ecuaci칩n (6)? 쯅o es posible elegirlo arbitrariamente? 쯈uiz치s si elegimos una funci칩n diferente, obtendr칤amos un conjunto completamente diferente de minimizaci칩n de pesos y compensaciones? Una pregunta razonable, y luego examinaremos nuevamente la funci칩n de costo y haremos algunas correcciones. Sin embargo, la funci칩n de costo cuadr치tico funciona muy bien para comprender las cosas b치sicas en el aprendizaje de NS, por lo que por ahora nos mantendremos firmes.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para resumir: nuestro objetivo en el entrenamiento de NS es encontrar pesos y compensaciones que minimicen la funci칩n de costo cuadr치tico C (w, b). La tarea est치 bien planteada, pero hasta ahora tiene muchas estructuras de distracci칩n: la interpretaci칩n de w y b como pesos y compensaciones, la funci칩n 픢 oculta en el fondo, la elecci칩n de la arquitectura de red, MNIST, etc. Resulta que podemos entender mucho, ignorando la mayor parte de esta estructura y concentr치ndonos solo en el aspecto de la minimizaci칩n. Entonces, por ahora, nos olvidaremos de la forma especial de la funci칩n de costos, la comunicaci칩n con la Asamblea Nacional, etc. En cambio, vamos a imaginar que solo tenemos una funci칩n con muchas variables y queremos minimizarla. Desarrollaremos una tecnolog칤a llamada descenso de gradiente, que puede usarse para resolver tales problemas. Y luego volvemos a una cierta funci칩n,que queremos minimizar para la Asamblea Nacional.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bueno, digamos que estamos tratando de minimizar alguna funci칩n C (v). Puede ser cualquier funci칩n con valores reales de muchas variables v = v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ... Tenga en cuenta que reemplac칠 la notaci칩n w y b por v para mostrar que puede ser cualquier funci칩n: ya no estamos obsesionados con HC. Es 칰til imaginar que una funci칩n C tiene solo dos variables: v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/fff/752/1ad/fff7521ad0e339cb68eceace0f200697.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nos gustar칤a encontrar d칩nde C alcanza un m칤nimo global. Por supuesto, con la funci칩n dibujada arriba, podemos estudiar el gr치fico y encontrar el m칤nimo. En este sentido, 춰puedo haberte dado una funci칩n demasiado simple! En el caso general, C puede ser una funci칩n compleja de muchas variables, y generalmente es imposible simplemente mirar el gr치fico y encontrar el m칤nimo.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Una forma de resolver el problema es usar 치lgebra para encontrar el m칤nimo anal칤ticamente. Podemos calcular las derivadas e intentar usarlas para encontrar el extremo. Si tenemos suerte, esto funcionar치 cuando C sea una funci칩n de una o dos variables. Pero con una gran cantidad de variables, esto se convierte en una pesadilla. Y para las NS, a menudo necesitamos muchas m치s variables: para las NS m치s grandes, las funciones de costo de una manera compleja dependen de miles de millones de pesos y desplazamientos. 춰Usar 치lgebra para minimizar estas funciones fallar치!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Habiendo declarado que ser칤a m치s conveniente para nosotros considerar C como una funci칩n de dos variables, dije dos veces en dos p치rrafos "s칤, pero 쯫 si es una funci칩n de un n칰mero mucho mayor de variables?" Pido disculpas. Cr칠enos que realmente ser치 칰til representar C como una funci칩n dos variables, es solo que a veces esta imagen se desmorona, por eso se necesitaban los dos p치rrafos anteriores. Para el razonamiento matem치tico, a menudo es necesario hacer malabarismos con varias representaciones intuitivas, aprendiendo al mismo tiempo cu치ndo se puede usar la representaci칩n y cu치ndo no zya).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bien, eso significa que el 치lgebra no funcionar치. Afortunadamente, existe una gran analog칤a que ofrece un algoritmo que funciona bien. Imaginamos nuestra funci칩n como algo as칤 como un valle. Con el 칰ltimo horario, no ser치 tan dif칤cil de hacer. E imaginamos una pelota rodando por la ladera del valle. Nuestra experiencia nos dice que la pelota finalmente se deslizar치 hasta el fondo. 쯈uiz치s podemos usar esta idea para encontrar el m칤nimo de una funci칩n? Seleccionamos al azar el punto de partida para una pelota imaginaria y luego simulamos el movimiento de la pelota, como si estuviera rodando hacia el fondo del valle. Podemos usar esta simulaci칩n simplemente contando las derivadas (y, posiblemente, las segundas derivadas) de C: nos contar치n todo sobre la forma local del valle y, por lo tanto, sobre c칩mo rodar치 nuestra bola.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seg칰n lo que escribiste, podr칤as pensar que escribiremos las ecuaciones de movimiento de Newton para la pelota, consideraremos los efectos de la fricci칩n y la gravedad, y as칤 sucesivamente. De hecho, no estaremos tan cerca de seguir esta analog칤a con la pelota: 춰estamos desarrollando un algoritmo para minimizar C y no una simulaci칩n exacta de las leyes de la f칤sica! Esta analog칤a deber칤a estimular nuestra imaginaci칩n y no limitar nuestro pensamiento. Entonces, en lugar de sumergirnos en los detalles complejos de la f칤sica, hagamos la pregunta: si fu칠ramos nombrados dios por un d칤a, y cre치ramos nuestras propias leyes de la f칤sica, dici칠ndole a la pelota c칩mo rodar qu칠 ley o las leyes de movimiento elegir칤amos, para que la pelota siempre ruede sobre fondo del valle? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para aclarar el problema, pensaremos en lo que sucede si movemos la pelota una peque침a distancia 풊v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en la direcci칩n de v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, y una peque침a distancia 풊v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en la direcci칩n de v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">El 치lgebra nos dice que C cambia de la siguiente manera:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-428"><span class="MJXp-mtable" id="MJXp-Span-429"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-430" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-431" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-432">풊</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-433">C</span><span class="MJXp-mo" id="MJXp-Span-434" style="margin-left: 0.333em; margin-right: 0.333em;">곋</span><span class="MJXp-mfrac" id="MJXp-Span-435" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-436">객</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-437">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-438">객</span><span class="MJXp-msubsup" id="MJXp-Span-439"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-440" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-441" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-442">풊</span><span class="MJXp-msubsup" id="MJXp-Span-443"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-444" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-445" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-446" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mfrac" id="MJXp-Span-447" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-448">객</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-449">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-450">객</span><span class="MJXp-msubsup" id="MJXp-Span-451"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-452" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-453" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-454">풊</span><span class="MJXp-msubsup" id="MJXp-Span-455"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-456" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-457" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-13"> \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2 \tag{7} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Encontraremos una manera de elegir tales 풊v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 풊v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para que 풊C sea menor que cero; </font><font style="vertical-align: inherit;">es decir, los seleccionaremos para que la bola ruede hacia abajo. </font><font style="vertical-align: inherit;">Para comprender c칩mo hacer esto, es 칰til definir 풊v como el vector de cambios, es decir, 풊v 고 (풊v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , 풊v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , donde T es la operaci칩n de transposici칩n que convierte los vectores de fila en vectores de columna. </font><font style="vertical-align: inherit;">Tambi칠n definimos el vector gradiente de C como derivados parciales (객S / 객V </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , 객S / 객V </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Denotamos el vector gradiente por 갢:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-458"><span class="MJXp-mtable" id="MJXp-Span-459"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-460" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-461" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-462">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-463">C</span><span class="MJXp-mo" id="MJXp-Span-464" style="margin-left: 0.333em; margin-right: 0.333em;">고</span><span class="MJXp-mo" id="MJXp-Span-465" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mfrac" id="MJXp-Span-466" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-467">객</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-468">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-469">객</span><span class="MJXp-msubsup" id="MJXp-Span-470"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-471" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-472" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-473" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mfrac" id="MJXp-Span-474" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-475">객</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-476">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-477">객</span><span class="MJXp-msubsup" id="MJXp-Span-478"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-479" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-480" style="vertical-align: -0.4em;">2</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-481"><span class="MJXp-mo" id="MJXp-Span-482" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-483" style="vertical-align: 0.5em;">T</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-14"> \nabla C \equiv (\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2})^T \tag{8} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pronto reescribiremos el cambio en 풊C a trav칠s de 풊v y el gradiente 갢C. Mientras tanto, quiero aclarar algo, por lo que las personas a menudo cuelgan del gradiente. Cuando se encontraron por primera vez con 갢C, las personas a veces no entienden c칩mo deber칤an percibir el s칤mbolo 갢. 쯈u칠 significa espec칤ficamente? De hecho, puede considerar con seguridad 갢C un 칰nico objeto matem치tico, un vector previamente definido, que simplemente se escribe con dos caracteres. Desde este punto de vista, 갢 es como agitar una bandera que informa que "갢C es un vector de gradiente". Existen puntos de vista m치s avanzados desde los cuales 갢 puede considerarse como una entidad matem치tica independiente (por ejemplo, como un operador de diferenciaci칩n), pero no los necesitamos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Con tales definiciones, la expresi칩n (7) puede reescribirse como:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-484"><span class="MJXp-mtable" id="MJXp-Span-485"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-486" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-487" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-488">풊</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-489">C</span><span class="MJXp-mo" id="MJXp-Span-490" style="margin-left: 0.333em; margin-right: 0.333em;">곋</span><span class="MJXp-mi" id="MJXp-Span-491">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-492">C</span><span class="MJXp-mo" id="MJXp-Span-493" style="margin-left: 0.267em; margin-right: 0.267em;">긘</span><span class="MJXp-mi" id="MJXp-Span-494">풊</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-495">v</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-15"> \Delta C \approx \nabla C \cdot \Delta v \tag{9} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esta ecuaci칩n ayuda a explicar por qu칠 갢C se llama un vector de gradiente: conecta los cambios en v con los cambios en C, tal como se espera de una entidad llamada gradiente. [eng. gradiente - desviaci칩n / aprox. transl.] Sin embargo, es m치s interesante que esta ecuaci칩n nos permita ver c칩mo elegir 풊v para que 풊C sea negativo. Digamos que elegimos</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-496"><span class="MJXp-mtable" id="MJXp-Span-497"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-498" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-499" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-500">풊</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-501">v</span><span class="MJXp-mo" id="MJXp-Span-502" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-503" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-504">풩</span><span class="MJXp-mi" id="MJXp-Span-505">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-506">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-16"> \Delta v = - \eta \nabla C \tag{10} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde 풩 es un peque침o par치metro positivo (velocidad de aprendizaje). Entonces la ecuaci칩n (9) nos dice que 풊C 곋 - 풩 갢C 긘 갢C = - 풩 || 갢C || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Desde || 갢C || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 곤 0, esto asegura que 풊C 곣 0, es decir, C disminuir치 todo el tiempo si cambiamos v, como se prescribe en (10) (por supuesto, como parte de la aproximaci칩n de la ecuaci칩n (9)). 춰Y esto es exactamente lo que necesitamos! Por lo tanto, tomamos la ecuaci칩n (10) para determinar la "ley de movimiento" de la pelota en nuestro algoritmo de descenso de gradiente. Es decir, utilizaremos la ecuaci칩n (10) para calcular el valor de 풊v, y luego moveremos la pelota a este valor:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-507"><span class="MJXp-mtable" id="MJXp-Span-508"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-509" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-510" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-511">v</span><span class="MJXp-mo" id="MJXp-Span-512" style="margin-left: 0.333em; margin-right: 0.333em;"></span><span class="MJXp-msup" id="MJXp-Span-513"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-514" style="margin-right: 0.05em;">v</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-515" style="vertical-align: 0.5em;"></span></span><span class="MJXp-mo" id="MJXp-Span-516" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-517">v</span><span class="MJXp-mo" id="MJXp-Span-518" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-519">풩</span><span class="MJXp-mi" id="MJXp-Span-520">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-521">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-17"> v \rightarrow v' = v - \eta \nabla C \tag{11} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Luego, nuevamente aplicamos esta regla para el pr칩ximo movimiento. Continuando con la repetici칩n, bajaremos C hasta que, con suerte, alcancemos un m칤nimo global. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En resumen, el descenso del gradiente funciona a trav칠s del c치lculo secuencial del gradiente 갢 C, y el desplazamiento posterior en la direcci칩n opuesta, lo que conduce a una "ca칤da" a lo largo de la pendiente del valle. Esto se puede visualizar de la siguiente manera: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f7/495/966/5f749596634bc20923f5f8a3e49a3b9f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tenga en cuenta que con esta regla, el descenso de gradiente no reproduce el movimiento f칤sico real. En la vida real, la pelota tiene un impulso que puede permitirle rodar por la pendiente, o incluso rodar por alg칰n tiempo. Solo despu칠s del trabajo de la fuerza de fricci칩n se garantiza que la bola ruede por el valle. Nuestra regla de selecci칩n 풊v solo dice "baja". 춰Una regla bastante buena para encontrar el m칤nimo!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para que el descenso de gradiente funcione correctamente, debemos elegir un valor suficientemente peque침o de la velocidad de aprendizaje 풩 para que la ecuaci칩n (9) sea una buena aproximaci칩n. De lo contrario, puede resultar que 풊C&gt; 0 - 춰nada bueno! Al mismo tiempo, no es necesario que 풩 sea demasiado peque침o, ya que los cambios en 풊v ser치n peque침os y el algoritmo funcionar치 muy lentamente. En la pr치ctica, 풩 cambia de modo que la ecuaci칩n (9) da una buena aproximaci칩n y el algoritmo no funciona muy lentamente. M치s adelante veremos c칩mo funciona. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le expliqu칠 el descenso del gradiente cuando la funci칩n C depend칤a solo de dos variables. Pero todo funciona de la misma manera si C es una funci칩n de muchas variables. Supongamos que tiene m variables, v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ..., v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Entonces, el cambio en 풊C causado por un peque침o cambio en 풊v = (풊v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , ..., 풊v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ser치</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-522"><span class="MJXp-mtable" id="MJXp-Span-523"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-524" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-525" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-526">풊</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-527">C</span><span class="MJXp-mo" id="MJXp-Span-528" style="margin-left: 0.333em; margin-right: 0.333em;">곋</span><span class="MJXp-mi" id="MJXp-Span-529">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-530">C</span><span class="MJXp-mo" id="MJXp-Span-531" style="margin-left: 0.267em; margin-right: 0.267em;">긘</span><span class="MJXp-mi" id="MJXp-Span-532">풊</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-533">v</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-18"> \Delta C \approx \nabla C \cdot \Delta v \tag{12} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> donde el gradiente 갢C es el vector </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-534"><span class="MJXp-mtable" id="MJXp-Span-535"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-536" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-537" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-538">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-539">C</span><span class="MJXp-mo" id="MJXp-Span-540" style="margin-left: 0.333em; margin-right: 0.333em;">고</span><span class="MJXp-mo" id="MJXp-Span-541" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mfrac" id="MJXp-Span-542" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-543">객</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-544">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-545">객</span><span class="MJXp-msubsup" id="MJXp-Span-546"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-547" style="margin-right: 0.05em;">v</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-548" style="vertical-align: -0.4em;">1</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-549" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-550" style="margin-left: 0em; margin-right: 0em;"></span><span class="MJXp-mo" id="MJXp-Span-551" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mfrac" id="MJXp-Span-552" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-553">객</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-554">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-555">객</span><span class="MJXp-msubsup" id="MJXp-Span-556"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-557" style="margin-right: 0.05em;">v</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-558" style="vertical-align: -0.4em;">m</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-559"><span class="MJXp-mo" id="MJXp-Span-560" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-561" style="vertical-align: 0.5em;">T</span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-19"> \nabla C \equiv (\frac{\partial C}{\partial v_1},, \frac{\partial C}{\partial v_m})^T \tag{13} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Al igual que con dos variables, podemos elegir </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-562"><span class="MJXp-mtable" id="MJXp-Span-563"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-564" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-565" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-566">풊</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-567">v</span><span class="MJXp-mo" id="MJXp-Span-568" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-569" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-570">풩</span><span class="MJXp-mi" id="MJXp-Span-571">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-572">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-20"> \Delta v = - \eta \nabla C \tag{14} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y aseg칰rese de que nuestra expresi칩n aproximada (12) para 풊C sea negativa. </font><font style="vertical-align: inherit;">Esto nos da una forma de ir al m칤nimo a lo largo del gradiente, incluso cuando C es una funci칩n de muchas variables, aplicando la regla de actualizaci칩n una y otra vez.</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-573"><span class="MJXp-mtable" id="MJXp-Span-574"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-575" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-576" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-577">v</span><span class="MJXp-mo" id="MJXp-Span-578" style="margin-left: 0.333em; margin-right: 0.333em;"></span><span class="MJXp-msup" id="MJXp-Span-579"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-580" style="margin-right: 0.05em;">v</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-581" style="vertical-align: 0.5em;"></span></span><span class="MJXp-mo" id="MJXp-Span-582" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-583">v</span><span class="MJXp-mo" id="MJXp-Span-584" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-585">풩</span><span class="MJXp-mi" id="MJXp-Span-586">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-587">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-21-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-21"> v \rightarrow v' = v - \eta \nabla C \tag{15} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esta regla de actualizaci칩n puede considerarse el algoritmo definitorio de descenso de gradiente. Nos da un m칠todo para cambiar repetidamente la posici칩n de v en busca del m칤nimo de la funci칩n C. Esta regla no siempre funciona: varias cosas pueden salir mal, evitando que el descenso del gradiente encuentre el m칤nimo global de C - volveremos a este punto en los siguientes cap칤tulos. Pero en la pr치ctica, el descenso de gradiente a menudo funciona muy bien, y veremos que en la Asamblea Nacional esta es una forma efectiva de minimizar la funci칩n de costos y, por lo tanto, capacitar a la red.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En cierto sentido, el descenso de gradiente puede considerarse la estrategia de b칰squeda m칤nima 칩ptima. </font><font style="vertical-align: inherit;">Supongamos que estamos tratando de mover 풊v a una posici칩n para minimizar C. Esto es equivalente a minimizar 풊C 곋 갢C 긘 풊v. </font><font style="vertical-align: inherit;">Limitaremos el tama침o del paso para que || 풊v || </font><font style="vertical-align: inherit;">= 풧 para alguna peque침a constante 풧&gt; 0. En otras palabras, queremos mover una peque침a distancia de un tama침o fijo, y tratar de encontrar la direcci칩n del movimiento que disminuya C tanto como sea posible. </font><font style="vertical-align: inherit;">Se puede demostrar que la elecci칩n de 풊v minimizando 갢C 긘 풊v es 풊v = -풩갢C, donde 풩 = 풧 / || 갢C || est치 determinada por la restricci칩n || 풊v || </font><font style="vertical-align: inherit;">= 풧. </font><font style="vertical-align: inherit;">Por lo tanto, el descenso de gradiente puede considerarse una forma de dar peque침os pasos en la direcci칩n que disminuye m치s C.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ejercicios </font></font></h3><br><ul><li>     . :      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">   </a> , ,  ,      . </li><li>     ,      ,       .  ,       ?            ? </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Las personas han estudiado muchas opciones para el descenso en gradiente, incluidas aquellas que reproducen con mayor precisi칩n una pelota f칤sica real. Dichas opciones tienen sus ventajas, pero tambi칠n un gran inconveniente: la necesidad de calcular las segundas derivadas parciales de C, que pueden consumir muchos recursos. Para comprender esto, supongamos que necesitamos calcular todas las segundas derivadas parciales 객 </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / 객v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 객v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Si las variables v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j son</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> millones, entonces necesitamos calcular aproximadamente un bill칩n (un mill칩n al cuadrado) de las segundas derivadas parciales (en realidad, medio bill칩n, ya que 객 </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / 객v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 객v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 객 </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / 객v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 객v </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Pero capturaste la esencia). Esto requerir치 muchos recursos inform치ticos. Hay trucos para evitar esto, y la b칰squeda de alternativas al descenso de gradiente es un 치rea de investigaci칩n activa. Sin embargo, en este libro usaremos el descenso de gradiente y sus variantes como el enfoque principal para aprender NS. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">쮺칩mo aplicamos el gradiente descendente al aprendizaje de NA? Necesitamos usarlo para buscar pesos w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y compensaciones b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> que minimicen la ecuaci칩n de costos (6). Reescribamos la regla de actualizaci칩n de descenso de gradiente reemplazando las variables </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vj</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> con </font><font style="vertical-align: inherit;">pesos y compensaciones. En otras palabras, ahora nuestra "posici칩n" tiene los componentes w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y el vector gradiente 갢C tiene los componentes correspondientes 객C / 객w</font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y 객C / 객b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Habiendo escrito nuestra regla de actualizaci칩n con nuevos componentes, obtenemos:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-588"><span class="MJXp-mtable" id="MJXp-Span-589"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-590" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-591" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-592"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-593" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-594" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-595" style="margin-left: 0.333em; margin-right: 0.333em;"></span><span class="MJXp-msubsup" id="MJXp-Span-596"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-597" style="margin-right: 0.05em;">w</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-599"></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-598">k</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-600" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-601"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-602" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-603" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-604" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-605">풩</span><span class="MJXp-mfrac" id="MJXp-Span-606" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-607">객</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-608">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-609">객</span><span class="MJXp-msubsup" id="MJXp-Span-610"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-611" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-612" style="vertical-align: -0.4em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-22-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-22"> w_k \rightarrow w'_k = w_k - \eta \frac{\partial C}{\partial w_k} \tag{16} </script></p><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-613"><span class="MJXp-mtable" id="MJXp-Span-614"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-615" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-616" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-617"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-618" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-619" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-620" style="margin-left: 0.333em; margin-right: 0.333em;"></span><span class="MJXp-msubsup" id="MJXp-Span-621"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-622" style="margin-right: 0.05em;">b</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-624"></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-623">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-625" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-626"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-627" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-628" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-629" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-630">풩</span><span class="MJXp-mfrac" id="MJXp-Span-631" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-632">객</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-633">C</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-634">객</span><span class="MJXp-msubsup" id="MJXp-Span-635"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-636" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-637" style="vertical-align: -0.4em;">l</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-23-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-23"> b_l \rightarrow b'_l = b_l - \eta \frac{\partial C}{\partial b_l} \tag{17} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al volver a aplicar esta regla de actualizaci칩n, podemos "rodar cuesta abajo" y, con un poco de suerte, encontrar la funci칩n de costo m칤nimo. En otras palabras, esta regla puede usarse para entrenar a la Asamblea Nacional. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Existen varios obst치culos para aplicar la regla de descenso de gradiente. Los estudiaremos con m치s detalle en los siguientes cap칤tulos. Pero por ahora, quiero mencionar solo un problema. Para entenderlo, volvamos al valor cuadr치tico en la ecuaci칩n (6). Tenga en cuenta que esta funci칩n de costo se parece a C = 1 / n 갬 </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , es decir, es el costo promedio C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 고 (|| y (x) 뇨 || </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) / 2 para ejemplos de capacitaci칩n individual. En la pr치ctica, para calcular el gradiente 갢C necesitamos calcular los gradientes 갢C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">por separado para cada entrada de entrenamiento x, y luego promediarlos, 갢C = 1 / n 갬 </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 갢C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Desafortunadamente, cuando la cantidad de entrada ser치 muy grande, tomar치 mucho tiempo y dicha capacitaci칩n ser치 lenta. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para acelerar el aprendizaje, puede usar el descenso de gradiente estoc치stico. La idea es calcular aproximadamente el gradiente de 갢C calculando 갢C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para una peque침a muestra aleatoria de entrada de entrenamiento. Al calcular su promedio, podemos obtener r치pidamente una buena estimaci칩n del gradiente verdadero 갢C, y esto ayuda a acelerar el descenso del gradiente y, por lo tanto, al entrenamiento.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al formular con mayor precisi칩n, el descenso de gradiente estoc치stico funciona mediante un muestreo aleatorio de un peque침o n칰mero de m datos de entrada de entrenamiento. </font><font style="vertical-align: inherit;">Llamaremos a estos datos aleatorios X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , .., X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y lo llamaremos un mini paquete. </font><font style="vertical-align: inherit;">Si el tama침o de la muestra m es lo suficientemente grande, el valor promedio de 갢C </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ser치 lo suficientemente cercano al promedio de todos los 갢Cx, es decir</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-638"><span class="MJXp-mtable" id="MJXp-Span-639"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-640" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-641" style="text-align: center;"><span class="MJXp-mfrac" id="MJXp-Span-642" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-643"><span class="MJXp-mo" id="MJXp-Span-644" style="margin-left: 0.111em; margin-right: 0.05em;">갬</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-649">m</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-645"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-646">j</span><span class="MJXp-mo" id="MJXp-Span-647">=</span><span class="MJXp-mn" id="MJXp-Span-648">1</span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-650">갢</span><span class="MJXp-msubsup" id="MJXp-Span-651"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-652" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-653" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-654"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-655" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-656" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-657">m</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-658" style="margin-left: 0.333em; margin-right: 0.333em;">곋</span><span class="MJXp-mfrac" id="MJXp-Span-659" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-660"><span class="MJXp-mo" id="MJXp-Span-661" style="margin-left: 0.111em; margin-right: 0.05em;">갬</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-662" style="vertical-align: -0.4em;">x</span></span><span class="MJXp-mi" id="MJXp-Span-663">갢</span><span class="MJXp-msubsup" id="MJXp-Span-664"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-665" style="margin-right: 0.05em;">C</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-666" style="vertical-align: -0.4em;">x</span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-667">n</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-668" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi" id="MJXp-Span-669">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-670">C</span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-24-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-24"> \frac{\sum^m_{j=1} \nabla C_{X_j}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C \tag{18} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde la segunda cantidad va sobre todo el conjunto de datos de entrenamiento. </font><font style="vertical-align: inherit;">Al intercambiar partes, obtenemos</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-671"><span class="MJXp-mtable" id="MJXp-Span-672"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-673" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-674" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-675">갢</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-676">C</span><span class="MJXp-mo" id="MJXp-Span-677" style="margin-left: 0.333em; margin-right: 0.333em;">곋</span><span class="MJXp-mfrac" id="MJXp-Span-678" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-679">1</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-680">m</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-681"><span><span class="MJXp-over"><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-687" style="margin-right: 0px; margin-left: 0px;">m</span></span><span class=""><span class="MJXp-mo" id="MJXp-Span-682" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">갬</span></span></span></span></span><span class=" MJXp-script"><span class="MJXp-mrow" id="MJXp-Span-683" style="margin-left: 0px;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-684">j</span><span class="MJXp-mo" id="MJXp-Span-685">=</span><span class="MJXp-mn" id="MJXp-Span-686">1</span></span></span></span><span class="MJXp-mi" id="MJXp-Span-688">갢</span><span class="MJXp-msubsup" id="MJXp-Span-689"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-690" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-691" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-692"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-693" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-694" style="vertical-align: -0.4em;">j</span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-25-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-25"> \nabla C \approx \frac{1}{m} \sum^m_{j=1} \nabla C_{X_j} \tag{19} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lo que confirma que podemos estimar el gradiente general calculando los gradientes para un minipack seleccionado al azar. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para relacionar esto directamente con el entrenamiento de NS, supongamos que w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y b </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> denotan los pesos y desplazamientos de nuestro NS. </font><font style="vertical-align: inherit;">Luego, el descenso de gradiente estoc치stico selecciona un mini paquete aleatorio de datos de entrada y aprende de ellos</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-695"><span class="MJXp-mtable" id="MJXp-Span-696"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-697" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-698" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-699"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-700" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-701" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-702" style="margin-left: 0.333em; margin-right: 0.333em;"></span><span class="MJXp-msubsup" id="MJXp-Span-703"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-704" style="margin-right: 0.05em;">w</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-706"></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-705">k</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-707" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-708"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-709" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-710" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-711" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mfrac" id="MJXp-Span-712" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-713">풩</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-714">m</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-715"><span class=""><span class="MJXp-mo" id="MJXp-Span-716" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">갬</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-717" style="margin-left: 0px;">j</span></span></span><span class="MJXp-mfrac" id="MJXp-Span-718" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-719">객</span><span class="MJXp-msubsup" id="MJXp-Span-720"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-721" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-722" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-723"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-724" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-725" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-726">객</span><span class="MJXp-msubsup" id="MJXp-Span-727"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-728" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-729" style="vertical-align: -0.4em;">k</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-26-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-26"> w_k \rightarrow w'_k = w_k - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20} </script></p><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-730"><span class="MJXp-mtable" id="MJXp-Span-731"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-732" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-733" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-734"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-735" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-736" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-737" style="margin-left: 0.333em; margin-right: 0.333em;"></span><span class="MJXp-msubsup" id="MJXp-Span-738"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-739" style="margin-right: 0.05em;">b</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-741"></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-740">l</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-742" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-743"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-744" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-745" style="vertical-align: -0.4em;">l</span></span><span class="MJXp-mo" id="MJXp-Span-746" style="margin-left: 0.267em; margin-right: 0.267em;"></span><span class="MJXp-mfrac" id="MJXp-Span-747" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-748">풩</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-749">m</span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-750"><span class=""><span class="MJXp-mo" id="MJXp-Span-751" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">갬</span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-752" style="margin-left: 0px;">j</span></span></span><span class="MJXp-mfrac" id="MJXp-Span-753" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-754">객</span><span class="MJXp-msubsup" id="MJXp-Span-755"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-756" style="margin-right: 0.05em;">C</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-757" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-758"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-759" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-760" style="vertical-align: -0.4em;">j</span></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-761">객</span><span class="MJXp-msubsup" id="MJXp-Span-762"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-763" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-764" style="vertical-align: -0.4em;">l</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-27-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-27"> b_l \rightarrow b'_l = b_l - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l} \tag{21} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde es la suma de todos los ejemplos de entrenamiento X </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en el minipaquete actual. </font><font style="vertical-align: inherit;">Luego seleccionamos otro mini-paquete al azar y estudiamos en 칠l. </font><font style="vertical-align: inherit;">Y as칤 sucesivamente, hasta agotar todos los datos de entrenamiento, lo que se llama el final de la era de entrenamiento. </font><font style="vertical-align: inherit;">En este momento, estamos comenzando de nuevo una nueva era de aprendizaje.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por cierto, vale la pena se침alar que los acuerdos sobre el escalado de la funci칩n de costos y la actualizaci칩n de los pesos y las compensaciones difieren en un mini paquete. En la ecuaci칩n (6), escalamos la funci칩n de costo 1 / n veces. A veces las personas omiten 1 / n al sumar los costos de los ejemplos de capacitaci칩n individual, en lugar de calcular el promedio. Esto es 칰til cuando el n칰mero total de ejemplos de entrenamiento no se conoce de antemano. Esto puede suceder, por ejemplo, cuando aparecen datos adicionales en tiempo real. Del mismo modo, las reglas de actualizaci칩n de mini-paquete (20) y (21) a veces omiten el miembro de 1 / m delante de la suma. Conceptualmente, esto no afecta nada, ya que es equivalente a un cambio en la velocidad de aprendizaje 풩. Sin embargo, vale la pena prestar atenci칩n al comparar varios trabajos.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un descenso de gradiente estoc치stico puede considerarse como un voto pol칤tico: es mucho m치s f치cil tomar una muestra en forma de un mini paquete que aplicar un descenso de gradiente a una muestra completa, al igual que una encuesta a la salida de un sitio es m치s f치cil que realizar una elecci칩n completa. Por ejemplo, si nuestro conjunto de entrenamiento tiene un tama침o de n = 60,000, como MNIST, y hacemos una muestra de un mini paquete de tama침o m = 10, 춰entonces aceleraremos la estimaci칩n del gradiente 6000 veces! Por supuesto, la estimaci칩n no ser치 ideal, habr치 fluctuaciones estad칤sticas, pero no es necesario que sea ideal: solo necesitamos movernos en la direcci칩n que disminuye C, lo que significa que no necesitamos calcular el gradiente con precisi칩n. En la pr치ctica, el descenso gradiente estoc치stico es una t칠cnica de ense침anza com칰n y poderosa para la Asamblea Nacional, y la base de la mayor칤a de las tecnolog칤as de ense침anza que desarrollaremos como parte del libro.</font></font><br><br><h3>  </h3><br><ul><li>       -  1.  ,    x         w <sub>k</sub>  w <sub>k</sub> = w <sub>k</sub>  풩 객C <sub>x</sub> / 객w <sub>k</sub>  b <sub>l</sub>  b <sub>l</sub> = b <sub>l</sub>  풩 객C <sub>x</sub> / 객b <sub>l</sub> .              .  Y as칤 sucesivamente.   ,  -,   .  -            ( ).       -         -  20. </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perm칤tanme terminar esta secci칩n con una discusi칩n sobre un tema que a veces molesta a las personas que primero han encontrado un descenso de gradiente. En NS, el valor de C es una funci칩n de muchas variables (todos los pesos y compensaciones) y, en cierto sentido, determina la superficie en un espacio muy multidimensional. La gente comienza a pensar: "Tendr칠 que visualizar todas estas dimensiones adicionales". Y comienzan a preocuparse: "No puedo navegar en cuatro dimensiones, sin mencionar cinco (o cinco millones)". 쯊ienen alguna cualidad especial que tienen las supermatem치ticas "reales"? Por supuesto que no. Incluso los matem치ticos profesionales no pueden visualizar el espacio de cuatro dimensiones bastante bien, si es que lo hacen. Van a trucos, desarrollando otras formas de representar lo que est치 sucediendo. Esto es exactamente lo que hicimos:Utilizamos la representaci칩n algebraica (en lugar de visual) de 풊C para comprender c칩mo moverse para que C disminuya. Las personas que hacen un buen trabajo con una gran cantidad de dimensiones tienen en mente una gran biblioteca de t칠cnicas similares; Nuestro truco algebraico es solo un ejemplo. Puede que estas t칠cnicas no sean tan simples como estamos acostumbrados al visualizar tres dimensiones, pero cuando crea una biblioteca de t칠cnicas similares, comienza a pensar bien en dimensiones superiores. No entrar칠 en detalles, pero si est치 interesado, puede que le gustea qu칠 estamos acostumbrados cuando visualizamos tres dimensiones, pero cuando creas una biblioteca de t칠cnicas similares, comienzas a pensar bien en dimensiones superiores. No entrar칠 en detalles, pero si est치 interesado, puede que le gustea qu칠 estamos acostumbrados cuando visualizamos tres dimensiones, pero cuando creas una biblioteca de t칠cnicas similares, comienzas a pensar bien en dimensiones superiores. No entrar칠 en detalles, pero si est치 interesado, puede que le guste</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Una discusi칩n de algunas de estas t칠cnicas por</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> matem치ticos profesionales que est치n acostumbrados a pensar en dimensiones superiores. </font><font style="vertical-align: inherit;">Aunque algunas de las t칠cnicas discutidas son bastante complejas, la mayor칤a de las mejores respuestas son intuitivas y accesibles para todos.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Implementaci칩n de una red para clasificar n칰meros </font></font></h3><br>  Bien, ahora escribamos un programa que aprenda a reconocer los d칤gitos escritos a mano utilizando el descenso de gradiente estoc치stico y los datos de entrenamiento de MNIST.  춰Haremos esto con un programa corto en Python 2.7 que consta de solo 74 l칤neas!  Lo primero que necesitamos es descargar los datos de MNIST.  Si usa git, puede obtenerlos clonando el repositorio de este libro: <br><br> <code>git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git</code> <br> <br>  Si no, descargue el c칩digo <a href="">desde el enlace</a> . <br><br>  Por cierto, cuando mencion칠 los datos de MNIST anteriormente, dije que est치n divididos en 60,000 im치genes de entrenamiento y 10,000 im치genes de prueba.  Esta es la descripci칩n oficial de MNIST.  Romperemos los datos de manera un poco diferente.  Dejaremos las im치genes de verificaci칩n sin cambios, pero dividiremos el conjunto de capacitaci칩n en dos partes: 50,000 im치genes, que usaremos para capacitar a la Asamblea Nacional, e 10,000 im치genes individuales para confirmaci칩n adicional.  Si bien no los utilizaremos, m치s adelante nos ser치n 칰tiles cuando comprendamos la configuraci칩n de algunos hiperpar치metros del NS, la velocidad de aprendizaje, etc., que nuestro algoritmo no selecciona directamente.  Aunque los datos que corroboran no son parte de la especificaci칩n MNIST original, muchos usan MNIST de esta manera, y en el campo de HC el uso de datos corroborantes es com칰n.  Ahora, hablando de los "datos de entrenamiento MNIST", me referir칠 a nuestros 50,000 karitnoks, no los 60,000 originales. <br><br>  Adem치s de los datos MNIST, tambi칠n necesitamos una biblioteca de Python llamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Numpy</a> para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c치lculos</a> r치pidos de 치lgebra lineal.  Si no lo tiene, puede tomarlo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">desde el enlace</a> . <br><br>  Antes de darle todo el programa, perm칤tame explicarle las caracter칤sticas principales del c칩digo para NS.  El lugar central est치 ocupado por la clase Red, que usamos para representar a la Asamblea Nacional.  Aqu칤 est치 el c칩digo de inicializaci칩n para el objeto de red: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, sizes)</span></span></span><span class="hljs-function">:</span></span> self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  La matriz de tama침os contiene el n칰mero de neuronas en las capas correspondientes.  Entonces, si queremos crear un objeto de red con dos neuronas en la primera capa, tres neuronas en la segunda capa y una neurona en la tercera, entonces lo escribiremos as칤: <br><br><pre> <code class="python hljs">net = Network([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre> <br>  Las compensaciones y los pesos en el objeto Red se inicializan aleatoriamente utilizando la funci칩n numpy np.random.randn, que genera una distribuci칩n gaussiana con una expectativa matem치tica de 0 y una desviaci칩n est치ndar de 1. Esta inicializaci칩n aleatoria le da a nuestro algoritmo de descenso de gradiente estoc치stico un punto de partida.  En los siguientes cap칤tulos encontraremos las mejores formas de inicializar pesos y compensaciones, pero por ahora esto es suficiente.  Tenga en cuenta que el c칩digo de inicializaci칩n de la red supone que se ingresar치 la primera capa de neuronas, y no les asigna un sesgo, ya que solo se usan para calcular la salida. <br><br>  Tambi칠n tenga en cuenta que las compensaciones y los pesos se almacenan como una matriz de matrices Numpy.  Por ejemplo, net.weights [1] es una matriz de Numpy que almacena los pesos que conectan las capas segunda y tercera de las neuronas (esta no es la primera y la segunda capa, ya que en Python la numeraci칩n de los elementos de la matriz proviene de cero).  Como tomar치 demasiado tiempo escribir net.weights [1], denotamos esta matriz como w.  Esta es una matriz tal que w <sub>jk</sub> es el peso de la conexi칩n entre la kth neurona en la segunda capa y la jth neurona en la tercera.  Tal orden de 칤ndices j y k puede parecer extra침o, 쯡o ser칤a m치s l칩gico intercambiarlos?  Pero la gran ventaja de tal registro es que se obtiene el vector de activaci칩n de la tercera capa de neuronas: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-765"><span class="MJXp-msup" id="MJXp-Span-766"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-767" style="margin-right: 0.05em;">a</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-768" style="vertical-align: 0.5em;"></span></span><span class="MJXp-mo" id="MJXp-Span-769" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-770">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-771">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-772">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-773">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-774">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-775">a</span><span class="MJXp-mo" id="MJXp-Span-776" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-777">w</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-778">a</span><span class="MJXp-mo" id="MJXp-Span-779" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-780">b</span><span class="MJXp-mo" id="MJXp-Span-781" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-782">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-783">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-784">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-785">g</span><span class="MJXp-mrow" id="MJXp-Span-786"><span class="MJXp-mn" id="MJXp-Span-787">22</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-28-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-28"> a '= \ sigma (wa + b) \ tag {22} </script></p><br><br>  Veamos esta ecuaci칩n bastante rica.  a es el vector de activaci칩n de la segunda capa de neuronas.  Para obtener a ', multiplicamos a por la matriz de peso w, y sumamos el vector de desplazamiento b.  Luego aplicamos el elemento sigmoide 픢 elemento por elemento a cada elemento del vector wa + b (esto se llama vectorizaci칩n de la funci칩n 픢).  Es f치cil verificar que la ecuaci칩n (22) da el mismo resultado que la regla (4) para calcular una neurona sigmoidea. <br><br><h3>  Ejercicio </h3><br><ul><li>  Escriba la ecuaci칩n (22) en forma de componente y aseg칰rese de que da el mismo resultado que la regla (4) para calcular una neurona sigmoidea. </li></ul><br>  Con todo esto en mente, es f치cil escribir c칩digo que calcule la salida de un objeto de red.  Comenzamos definiendo un sigmoide: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sigmoid</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0</span></span>/(<span class="hljs-number"><span class="hljs-number">1.0</span></span>+np.exp(-z))</code> </pre> <br>  Tenga en cuenta que cuando el par치metro z es un vector o matriz de Numpy, Numpy aplicar치 autom치ticamente el elemento sigmoide en cuanto a elementos, es decir, en forma de vector. <br><br>  Agregue un m칠todo de propagaci칩n directa a la clase de red, que toma una entrada de la red como entrada y devuelve la salida correspondiente.  Se supone que el par치metro a es (n, 1) Numpy ndarray, no un vector (n,).  Aqu칤 n es el n칰mero de neuronas de entrada.  Si intenta usar el vector (n,), obtendr치 resultados extra침os. <br><br>  El m칠todo simplemente aplica la ecuaci칩n (22) a cada capa: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">feedforward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, a)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""       "a"""</span></span><span class="hljs-string"><span class="hljs-string">" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a</span></span></code> </pre> <br>  Por supuesto, b치sicamente los objetos de la red los necesitamos para aprender.  Para hacer esto, les daremos el m칠todo SGD, que implementa el descenso de gradiente estoc치stico.  Aqu칤 est치 su c칩digo.  En algunos lugares es bastante misterioso, pero a continuaci칩n lo analizaremos con m치s detalle. <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SGD</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, training_data, epochs, mini_batch_size, eta, test_data=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    -    . training_data    "(x, y)",       .       .  test_data ,          ,     .     ,    . """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> test_data: n_test = len(test_data) n = len(training_data) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">0</span></span>, n, mini_batch_size)] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> mini_batch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mini_batches: self.update_mini_batch(mini_batch, eta) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> test_data: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Epoch {0}: {1} / {2}"</span></span>.format( j, self.evaluate(test_data), n_test) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Epoch {0} complete"</span></span>.format(j)</code> </pre> <br>  training_data es una lista de tuplas "(x, y)" que representan la entrada de entrenamiento y la salida deseada.  Las variables epochs y mini_batch_size son el n칰mero de epochs para aprender y el tama침o de los mini-paquetes para usar.  eta - velocidad de aprendizaje, 풩.  Si test_data est치 configurado, la red se evaluar치 con los datos de verificaci칩n despu칠s de cada era, y se mostrar치 el progreso actual.  Esto es 칰til para seguir el progreso, pero ralentiza significativamente el trabajo. <br><br>  El c칩digo funciona as칤.  En cada era, comienza mezclando accidentalmente datos de entrenamiento y luego los divide en mini paquetes del tama침o correcto.  Esta es una manera f치cil de crear una muestra de datos de entrenamiento.  Luego, para cada mini_batch aplicamos un paso de descenso de gradiente.  Esto se realiza mediante el c칩digo self.update_mini_batch (mini_batch, eta), que actualiza los pesos y las compensaciones de la red de acuerdo con una iteraci칩n del descenso del gradiente utilizando solo datos de entrenamiento en mini_batch.  Aqu칤 est치 el c칩digo para el m칠todo update_mini_batch: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update_mini_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, mini_batch, eta)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    ,          -. mini_batch     (x, y),  eta   ."""</span></span> nabla_b = [np.zeros(b.shape) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.biases] nabla_w = [np.zeros(w.shape) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.weights] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> nb, dnb <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> nw, dnw <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w, nw <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b, nb <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.biases, nabla_b)]</code> </pre> <br>  La mayor parte del trabajo se realiza por l칤nea. <br><br><pre> <code class="python hljs"> delta_nabla_b, delta_nabla_w = self.backprop(x, y)</code> </pre> <br>  Invoca un algoritmo de retropropagaci칩n, una forma r치pida de calcular el gradiente de una funci칩n de costo.  As칤 que update_mini_batch simplemente calcula estos gradientes para cada ejemplo de entrenamiento de mini_batch, y luego actualiza self.weights y self.biases. <br><br>  Hasta ahora, no demostrar칠 c칩digo para self.backprop.  Aprenderemos sobre la retropropagaci칩n en el pr칩ximo cap칤tulo, y habr치 un c칩digo self.backprop.  Por ahora, suponga que se comporta como se indica, devolviendo un gradiente apropiado para el costo asociado con el ejemplo de capacitaci칩n x. <br><br>  Veamos todo el programa, incluidos los comentarios explicativos.  Con la excepci칩n de la funci칩n self.backprop, el programa habla por s칤 mismo: el trabajo principal lo realizan self.SGD y self.update_mini_batch.  El m칠todo self.backprop utiliza varias funciones adicionales para calcular el gradiente, a saber, sigmoid_prime, que calcula la derivada del sigmoid, y self.cost_derivative, que no describir칠 aqu칤.  Puede hacerse una idea de ellos mirando el c칩digo y los comentarios.  En el pr칩ximo cap칤tulo los consideraremos con m치s detalle.  Tenga en cuenta que si bien el programa parece largo, la mayor parte del c칩digo son comentarios que hacen que sea m치s f치cil de entender.  De hecho, el programa en s칤 consta de solo 74 l칤neas de c칩digo que no est치n vac칤as ni comentarios.  Todo el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c칩digo est치 disponible en GitHub</a> . <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" network.py ~~~~~~~~~~           .      .     ,    .   ,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #   import random #   import numpy as np class Network(object): def __init__(self, sizes): """  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].               0    1. ,      ,       ,        . """ self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] def feedforward(self, a): """   ,  ``a`` -  .""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """    -    . training_data    "(x, y)",       .       .  test_data ,          ,     .     ,    . """ if test_data: n_test = len(test_data) n = len(training_data) for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoch {0}: {1} / {2}".format( j, self.evaluate(test_data), n_test) else: print "Epoch {0} complete".format(j) def update_mini_batch(self, mini_batch, eta): """    ,          -. mini_batch     (x, y),  eta   .""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """  ``(nabla_b, nabla_w)``,      C_x. ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #   activation = x activations = [x] #      zs = [] #     z- for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) #   delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) """ l      ,      . l = 1    , l = 2  ,   .    ,   python      .""" for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): """    ,      .              .""" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) def cost_derivative(self, output_activations, y): """    ( C_x /  a)   .""" return (output_activations-y) ####   def sigmoid(z): """.""" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): """ .""" return sigmoid(z)*(1-sigmoid(z))</span></span></code> </pre> <br>  쯈u칠 tan bien reconoce el programa los n칰meros escritos a mano?  Comencemos cargando los datos MNIST.  Haremos esto usando el peque침o programa auxiliar mnist_loader.py, que describir칠 a continuaci칩n.  Ejecute los siguientes comandos en el shell de Python: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper()</code> </pre> <br>  Esto, por supuesto, se puede hacer en un programa separado, pero si trabaja en paralelo con un libro, ser치 m치s f치cil. <br><br>  Despu칠s de descargar los datos de MNIST, configure una red de 30 neuronas ocultas.  Haremos esto despu칠s de importar el programa descrito anteriormente, que se llama red: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network &gt;&gt;&gt; net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])</code> </pre> <br>  Finalmente, utilizamos el descenso de gradiente estoc치stico para el entrenamiento en datos de entrenamiento para 30 eras, con un tama침o de mini paquete de 10 y una velocidad de aprendizaje de 풩 = 3.0: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">3.0</span></span>, test_data=test_data)</code> </pre> <br>  Si est치 ejecutando c칩digo en paralelo con la lectura de un libro, tenga en cuenta que la ejecuci칩n tardar치 varios minutos.  Le sugiero que comience todo, contin칰e leyendo y verifique peri칩dicamente lo que produce el programa.  Si tiene prisa, puede reducir la cantidad de eras reduciendo la cantidad de neuronas ocultas o utilizando solo una parte de los datos de entrenamiento.  El c칩digo de trabajo final funcionar치 m치s r치pido: 춰estos scripts de Python est치n dise침ados para hacerle entender c칩mo funciona la red y no son de alto rendimiento!  Y, por supuesto, despu칠s de la capacitaci칩n, la red puede funcionar muy r치pidamente en casi cualquier plataforma inform치tica.  Por ejemplo, cuando ense침amos a la red una buena selecci칩n de pesos y compensaciones, se puede portar f치cilmente para trabajar en JavaScript en un navegador web o como una aplicaci칩n nativa en un dispositivo m칩vil.  En cualquier caso, el programa que entrena la red neuronal hace aproximadamente la misma conclusi칩n.  Ella escribe el n칰mero de im치genes de prueba correctamente reconocidas despu칠s de cada era de entrenamiento.  Como puede ver, incluso despu칠s de una era, alcanza una precisi칩n de 9,129 de 10,000, y este n칰mero contin칰a creciendo: <br><br> <code>Epoch 0: 9129 / 10000 <br> Epoch 1: 9295 / 10000 <br> Epoch 2: 9348 / 10000 <br> ... <br> Epoch 27: 9528 / 10000 <br> Epoch 28: 9542 / 10000 <br> Epoch 29: 9534 / 10000</code> <br> <br>  춰Resulta que la red entrenada da un porcentaje de clasificaci칩n correcta de aproximadamente 95 - 95.42% como m치ximo!  Un primer intento bastante prometedor.  Le advierto que su c칩digo no necesariamente producir치 exactamente los mismos resultados, ya que inicializamos la red con ponderaciones y compensaciones aleatorias.  Para este cap칤tulo, he elegido el mejor de tres intentos. <br><br>  Reiniciemos el experimento cambiando el n칰mero de neuronas ocultas a 100. Como antes, si ejecuta el c칩digo al mismo tiempo que lee, tenga en cuenta que lleva bastante tiempo (en mi m치quina, cada era toma varias decenas de segundos), por lo que es mejor leer en paralelo con ejecuci칩n de c칩digo. <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">3.0</span></span>, test_data=test_data)</code> </pre> <br>  Naturalmente, esto mejora el resultado al 96.59%.  Al menos en este caso, usar m치s neuronas ocultas ayuda a obtener mejores resultados. <br><br>  Los comentarios de los lectores sugieren que los resultados de este experimento var칤an mucho y que algunos resultados de aprendizaje son mucho peores.  Usando t칠cnicas del Cap칤tulo 3 para reducir seriamente la diversidad de la eficiencia del trabajo de una carrera a otra. <br><br>  Por supuesto, para lograr tal precisi칩n, tuve que elegir un cierto n칰mero de eras para aprender, el tama침o del mini paquete y la velocidad de aprendizaje 풩.  Como mencion칠 anteriormente, se denominan hiperpar치metros de nuestra Asamblea Nacional, para distinguirlos de los par치metros simples (pesos y compensaciones) que el algoritmo ajusta durante el entrenamiento.  Si elegimos mal los hiperpar치metros, obtendremos malos resultados.  Supongamos, por ejemplo, que hemos elegido la tasa de aprendizaje 풩 = 0.001: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, test_data=test_data)</code> </pre> <br>  Los resultados son mucho menos impresionantes: <br><br> <code>Epoch 0: 1139 / 10000 <br> Epoch 1: 1136 / 10000 <br> Epoch 2: 1135 / 10000 <br> ... <br> Epoch 27: 2101 / 10000 <br> Epoch 28: 2123 / 10000 <br> Epoch 29: 2142 / 10000</code> <br> <br>  Sin embargo, puede ver que la eficiencia de la red crece lentamente con el tiempo.  Esto sugiere que puede intentar aumentar la velocidad de aprendizaje, por ejemplo, a 0.01.  En este caso, los resultados ser치n mejores, lo que indica la necesidad de aumentar a칰n m치s la velocidad (si el cambio mejora la situaci칩n, 춰cambie m치s!).  Si hace esto varias veces, finalmente llegaremos a 풩 = 1.0 (y a veces incluso 3.0), que est치 cerca de nuestros experimentos anteriores.  Entonces, aunque inicialmente seleccionamos mal los hiperpar치metros, al menos reunimos suficiente informaci칩n para poder mejorar nuestra elecci칩n de par치metros. <br><br>  En general, la depuraci칩n de NA es un asunto complicado.  Esto es especialmente cierto cuando la elecci칩n de hiperpar치metros iniciales produce resultados que no exceden el ruido aleatorio.  Supongamos que intentamos usar una arquitectura exitosa de 30 neuronas, pero cambiamos la velocidad de aprendizaje a 100.0: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">100.0</span></span>, test_data=test_data)</code> </pre> <br>  Al final, resulta que fuimos demasiado lejos y tomamos demasiada velocidad: <br><br> <code>Epoch 0: 1009 / 10000 <br> Epoch 1: 1009 / 10000 <br> Epoch 2: 1009 / 10000 <br> Epoch 3: 1009 / 10000 <br> ... <br> Epoch 27: 982 / 10000 <br> Epoch 28: 982 / 10000 <br> Epoch 29: 982 / 10000</code> <br> <br>  Ahora imagine que nos estamos acercando a esta tarea por primera vez.  Por supuesto, sabemos por los primeros experimentos que ser칤a correcto reducir la velocidad de aprendizaje.  Pero si tuvi칠ramos que abordar esta tarea por primera vez, no tendr칤amos resultados que pudieran llevarnos a la soluci칩n correcta.  쯇odr칤amos comenzar a pensar que quiz치s hemos elegido los par치metros iniciales incorrectos para los pesos y las compensaciones, y que es dif칤cil para la red aprender?  쯆 tal vez no tenemos suficientes datos de entrenamiento para obtener un resultado significativo?  쯈uiz치s no esperamos suficientes eras?  쯈uiz치s una red neuronal con tal arquitectura simplemente no puede aprender a reconocer n칰meros escritos a mano?  쯈uiz치s la velocidad de aprendizaje es demasiado lenta?  Cuando abordas la tarea por primera vez, nunca tienes confianza. <br><br>  De esto vale la pena aprender una lecci칩n de que depurar NS no es una tarea trivial, y esto, como la programaci칩n regular, es parte del arte.  Debe aprender este arte de depuraci칩n para obtener buenos resultados de NS.  En general, necesitamos desarrollar una heur칤stica para seleccionar buenos hiperpar치metros y buena arquitectura.  Discutiremos esto en detalle en el libro, incluyendo c칩mo seleccion칠 los hiperpar치metros anteriores. <br><br><h3>  Ejercicio </h3><br><ul><li>  Intente crear una red de solo dos capas, entrada y salida, sin ocultaci칩n, con 784 y 10 neuronas, respectivamente.  Entrena la red con descenso de gradiente estoc치stico.  쯈u칠 precisi칩n de clasificaci칩n obtienes? </li></ul><br>  Anteriormente omit칤 los detalles de cargar datos de MNIST.  Sucede de manera bastante simple.  Aqu칤 est치 el c칩digo para completar la imagen.  Las estructuras de datos se describen en los comentarios: todo es simple, tuplas y matrices de objetos Numpy ndarray (si no est치 familiarizado con dichos objetos, imag칤nelos como vectores). <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" mnist_loader ~~~~~~~~~~~~      MNIST.       ``load_data``  ``load_data_wrapper``.  , ``load_data_wrapper`` -  ,     . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import cPickle import gzip #  import numpy as np def load_data(): """  MNIST   ,  ,    . ``training_data``      .    .  numpy ndarray  50 000 .        numpy ndarray  784 ,  28 * 28 = 784    MNIST.    numpy ndarray  50 000 .      0  9   ,    . ``validation_data``  ``test_data`` ,    10 000 .    ,           ``training_data``.    - ``load_data_wrapper()``. """ f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data) def load_data_wrapper(): """ ,  ``(training_data, validation_data, test_data)``.   ``load_data``,         .  , ``training_data`` -    50 000    , ``(x, y)``. ``x`` -  784- numpy.ndarray,   . ``y`` -  10- numpy.ndarray,   ,     ``x``. ``validation_data``  ``test_data`` -  ,   10 000    , ``(x, y)``. ``x`` -  784- numpy.ndarray,   ,  ``y`` -   ,  ,   ( ),  ``x``. ,  ,           .         .""" tr_d, va_d, te_d = load_data() training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data) def vectorized_result(j): """ 10-    1.0   j     .      (0..9)     .""" e = np.zeros((10, 1)) e[j] = 1.0 return e</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dije que nuestro programa est치 logrando resultados bastante buenos. 쯈u칠 significa esto? 쮹ueno comparado con qu칠? Es 칰til tener los resultados de algunas pruebas simples y b치sicas con las que podr칤a hacer una comparaci칩n para comprender qu칠 significan "buenos resultados". El nivel base m치s simple, por supuesto, ser칤a una suposici칩n aleatoria. Esto se puede hacer en aproximadamente el 10% de los casos. 춰Y mostramos un resultado mucho mejor! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">쯈u칠 pasa con un nivel base menos trivial? Veamos qu칠 tan oscura es la imagen. Por ejemplo, la imagen 2 generalmente ser치 m치s oscura que la imagen 1, simplemente porque tiene m치s p칤xeles oscuros, como se ve en los ejemplos a continuaci칩n:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f59/0de/5b7/f590de5b7bc6581b9854b2013e5013de.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De ello se deduce que podemos calcular la oscuridad promedio para cada d칤gito de 0 a 9. Cuando obtenemos una nueva imagen, calculamos su oscuridad y suponemos que muestra una figura con la oscuridad promedio m치s cercana. Este es un procedimiento simple que es f치cil de programar, por lo que no escribir칠 c칩digo; si est치 interesado, se </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">encuentra en GitHub</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Pero esta es una mejora seria en comparaci칩n con las suposiciones aleatorias: el c칩digo reconoce correctamente 2,225 de 10,000 im치genes, es decir, proporciona una precisi칩n del 22.25%.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No es dif칤cil encontrar otras ideas que logren precisi칩n en el rango de 20 a 50%. Despu칠s de trabajar un poco m치s, puede superar el 50%. Pero para lograr una precisi칩n mucho mayor, es mejor usar algoritmos MO autoritativos. Probemos con uno de los algoritmos m치s famosos, el m칠todo de vector de soporte o SVM. Si no est치 familiarizado con SVM, no se preocupe, no necesitamos comprender estos detalles. Solo usamos una biblioteca de Python llamada scikit-learn, que proporciona una interfaz simple a la biblioteca r치pida de C para SVM, conocida como </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LIBSVM</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si ejecutamos el clasificador SVM scikit-learn con la configuraci칩n predeterminada, obtenemos la clasificaci칩n correcta de 9,435 de 10,000 (el c칩digo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est치 disponible en el enlace</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) Esto ya es una gran mejora sobre el enfoque ingenuo de clasificar im치genes por oscuridad. Esto significa que SVM funciona tan bien como nuestro NS, solo que un poco peor. En los siguientes cap칤tulos nos familiarizaremos con nuevas t칠cnicas que nos permitir치n mejorar nuestro NS para que superen en gran medida a SVM.</font></font><br><br>  Pero eso no es todo.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El resultado 9 435 de 10 000 de scikit-learn se especifica para la configuraci칩n predeterminada. SVM tiene muchos par치metros que se pueden ajustar, y puede buscar par치metros que mejoren este resultado. No entrar칠 en detalles; se pueden leer en el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">art칤culo de</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andreas Muller. Mostr칩 que al hacer un trabajo para optimizar los par치metros, es posible lograr una precisi칩n de al menos 98.5%. En otras palabras, un SVM bien sintonizado genera solo un d칤gito de 70 errores. 춰Un buen resultado! 쯇uede NA lograr m치s? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resulta que pueden. Hoy, un NS bien ajustado supera cualquier otra tecnolog칤a conocida en la soluci칩n MNIST, incluida SVM. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R칠cord para 2013</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">clasific칩 correctamente 9,979 de 10,000 im치genes. Y veremos la mayor칤a de las tecnolog칤as utilizadas para esto en este libro. Este nivel de precisi칩n es cercano al humano, y tal vez incluso lo supera, ya que varias im치genes de MNIST son dif칤ciles de descifrar incluso para humanos, por ejemplo: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/def/4b3/b37/def4b3b37d8d510615d60a372a06ad47.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">춰Creo que estar치 de acuerdo en que es dif칤cil clasificarlas! Con tales im치genes en el conjunto de datos MNIST, es sorprendente que el NS pueda reconocer correctamente todas las 10,000 im치genes, excepto 21. Por lo general, los programadores creen que resolver una tarea tan compleja requiere un algoritmo complejo. Pero incluso el NS est치 en el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trabajo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">el titular del registro utiliza algoritmos bastante simples, que son peque침as variaciones de los que examinamos en este cap칤tulo. </font><font style="vertical-align: inherit;">Toda la complejidad aparece autom치ticamente durante el entrenamiento en funci칩n de los datos de entrenamiento. </font><font style="vertical-align: inherit;">En cierto sentido, la moraleja de nuestros resultados y los contenidos en trabajos m치s complejos es que para algunas tareas</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> algoritmo complejo 곣 algoritmo de entrenamiento simple + buenos datos de entrenamiento </font></font></blockquote><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Al aprendizaje profundo </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aunque nuestra red muestra un rendimiento impresionante, se logra de una manera misteriosa. </font><font style="vertical-align: inherit;">Los pesos y las redes de mezcla se detectan autom치ticamente. </font><font style="vertical-align: inherit;">Por lo tanto, no tenemos una explicaci칩n lista de c칩mo la red hace lo que hace. </font><font style="vertical-align: inherit;">쮿ay alguna manera de entender los principios b치sicos de clasificaci칩n por una red de n칰meros escritos a mano? </font><font style="vertical-align: inherit;">쯏 es posible, dados ellos, mejorar el resultado?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reformulamos estas preguntas de manera m치s estricta: supongamos que en unas pocas d칠cadas el NS se convertir치 en inteligencia artificial (IA). 쮼ntenderemos c칩mo funciona esta IA? Quiz치s las redes seguir치n siendo incomprensibles para nosotros, con sus pesos y compensaciones, ya que se asignan autom치ticamente. En los primeros a침os de la investigaci칩n de IA, la gente esperaba que intentar crear IA tambi칠n nos ayudar칤a a comprender los principios subyacentes de la inteligencia, y tal vez incluso el trabajo del cerebro humano. Sin embargo, al final puede resultar que no entenderemos ni el cerebro ni c칩mo funciona la IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para abordar estos problemas, recordemos la interpretaci칩n de las neuronas artificiales que di al comienzo del cap칤tulo: que esta es una forma de sopesar la evidencia. Supongamos que queremos determinar si la cara de una persona est치 en una imagen:</font></font><br><br><img src="https://habrastorage.org/webt/up/2e/af/up2eafnwrrpph5xdai4oapmgw8m.jpeg"><br><br><img src="https://habrastorage.org/webt/tu/8i/an/tu8ianduufnfebbjnqaibtzskyy.jpeg"><br><br><img src="https://habrastorage.org/webt/wf/tj/4j/wftj4j86hzytgwyypbw_eqk0jte.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este problema puede abordarse de la misma manera que el reconocimiento de escritura a mano: el uso de p칤xeles de imagen como entrada para el NS, y la salida del NS ser치 una neurona que dir치: "S칤, esta es una cara" o "No, esto no es una cara ". </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Supongamos que hacemos esto, pero sin usar un algoritmo de aprendizaje. Intentaremos crear manualmente una red, eligiendo los pesos y las compensaciones apropiadas. 쮺칩mo podemos abordar esto? Por un momento, olvidando la Asamblea Nacional, podr칤amos dividir la tarea en subtareas: 쯟a imagen de los ojos tiene en la esquina superior izquierda? 쮿ay un ojo en la esquina superior derecha? 쮿ay una nariz media? 쮿ay una boca en el medio? 쮿ay pelo en la parte superior?</font></font> Y as칤 sucesivamente. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si las respuestas a varias de estas preguntas son positivas, o incluso "probablemente s칤", entonces concluimos que la imagen puede tener una cara. Por el contrario, si las respuestas son no, entonces probablemente no hay persona.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esto, por supuesto, es una heur칤stica aproximada y tiene muchas deficiencias. Quiz치s sea un hombre calvo y no tenga cabello. Quiz치s podamos ver solo una parte de la cara, o la cara en 치ngulo, de modo que algunas partes de la cara est칠n cerradas. Sin embargo, la heur칤stica sugiere que si podemos resolver subproblemas con la ayuda de redes neuronales, entonces quiz치s podamos crear NS para reconocimiento facial combinando redes para subtareas. La siguiente es una posible arquitectura de dicha red en la que las subredes se indican mediante rect치ngulos. Este no es un enfoque completamente realista para resolver el problema del reconocimiento facial: es necesario para ayudarnos a comprender intuitivamente el trabajo de las redes neuronales. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/61b/dd0/ef6/61bdd0ef651ebc30afff87ed56204bd0.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En los rect치ngulos hay subtareas: 쯦iene la imagen de los ojos en la esquina superior izquierda? 쮿ay un ojo en la esquina superior derecha? 쮿ay una nariz media? 쮿ay una boca en el medio? 쮿ay pelo en la parte superior? Y as칤 sucesivamente.</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es posible que las subredes tambi칠n se puedan desmontar en componentes. Tome la cuesti칩n de tener un ojo en la esquina superior izquierda. Se puede distinguir en preguntas como: "쮿ay una ceja?", "쮿ay pesta침as?", "쮿ay una pupila?" Y as칤 sucesivamente. Por supuesto, las preguntas deben contener informaci칩n sobre la ubicaci칩n: "쮼st치 la ceja ubicada en la esquina superior izquierda, arriba de la pupila?", Y as칤 sucesivamente, pero simplifiquemosla por ahora. Por lo tanto, la red que responde a la pregunta sobre la presencia del ojo se puede desmontar en los componentes: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d87/871/6ff/d878716ff0ecad83cfa5b0e9c54a866e.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"쮿ay una ceja?", "쮿ay pesta침as?", "쮿ay una pupila?"</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estas preguntas se pueden dividir en peque침as, en pasos a trav칠s de muchas capas. Como resultado, trabajaremos con subredes que respondan preguntas tan simples que puedan desmontarse f치cilmente a nivel de p칤xeles. Estas preguntas pueden referirse, por ejemplo, a la presencia o ausencia de formas simples en ciertos lugares de la imagen. Las neuronas individuales asociadas con los p칤xeles podr치n responder a ellas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El resultado es una red que desglosa preguntas muy complejas, ya sea que una persona est칠 en la imagen, en preguntas muy simples que pueden responderse a nivel de p칤xel individual. Lo har치 a trav칠s de una secuencia de muchas capas, en la cual las primeras responden preguntas muy simples y espec칤ficas sobre la imagen, y las 칰ltimas crean una jerarqu칤a de conceptos m치s complejos y abstractos. Las redes con una estructura multicapa de este tipo, dos o m치s capas ocultas, se denominan redes neuronales profundas (GNS).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por supuesto, no habl칠 sobre c칩mo hacer esta subred recursiva. Definitivamente no ser치 pr치ctico seleccionar manualmente pesos y compensaciones. Nos gustar칤a utilizar algoritmos de entrenamiento para que la red aprenda autom치ticamente los pesos y las compensaciones, y a trav칠s de ellos las jerarqu칤as de conceptos, basadas en datos de entrenamiento. Los investigadores en las d칠cadas de 1980 y 1990 intentaron utilizar el descenso de gradiente estoc치stico y la propagaci칩n hacia atr치s para entrenar GNS. Desafortunadamente, con la excepci칩n de algunas arquitecturas especiales, no tuvieron 칠xito. Las redes entrenaron, pero muy lentamente, y en la pr치ctica fue demasiado lento para que se pueda utilizar de alguna manera.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Desde 2006, se han desarrollado varias tecnolog칤as para entrenar STS. Se basan en el descenso de gradiente estoc치stico y la propagaci칩n hacia atr치s, pero tambi칠n contienen nuevas ideas. Permitieron entrenar redes mucho m치s profundas; hoy en d칤a, las personas entrenan silenciosamente redes con 5-10 capas. Y resulta que resuelven muchos problemas mucho mejor que los NS poco profundos, es decir, las redes con una capa oculta. La raz칩n, por supuesto, es que STS puede crear una compleja jerarqu칤a de conceptos. Esto es similar a c칩mo los lenguajes de programaci칩n usan esquemas modulares e ideas de abstracci칩n para que puedan crear programas inform치ticos complejos. Comparar un NS profundo con un NS superficial es aproximadamente c칩mo comparar un lenguaje de programaci칩n que puede realizar llamadas a funciones con un lenguaje que no lo hace. La abstracci칩n en el NS no se parece a los lenguajes de programaci칩n,Pero tiene la misma importancia.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/456738/">https://habr.com/ru/post/456738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456722/index.html">Recetas PostgreSQL: Programador de tareas as칤ncrono</a></li>
<li><a href="../456724/index.html">5 maneras extremadamente simples de acelerar significativamente su aplicaci칩n VueJS</a></li>
<li><a href="../456730/index.html">Libro "{No sabes JS} Tipos y construcciones gramaticales"</a></li>
<li><a href="../456732/index.html">Ser un mentor</a></li>
<li><a href="../456736/index.html">Recetas PostgreSQL: cURL: obtener, publicar y ... correo electr칩nico</a></li>
<li><a href="../456740/index.html">Inmersi칩n en redes neuronales convolucionales. Parte 5/1 - 9</a></li>
<li><a href="../456744/index.html">10 problemas que resolv칤 con recordatorios en mi tel칠fono inteligente</a></li>
<li><a href="../456746/index.html">Big data: gran responsabilidad, gran estr칠s y mucho dinero</a></li>
<li><a href="../456748/index.html">Impresora t칠rmica 2003 de un mercado de pulgas: 쯤u칠 puede hacer en 2019?</a></li>
<li><a href="../456754/index.html">GitOps: comparaci칩n de los m칠todos Pull y Push</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>