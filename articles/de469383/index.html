<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ôªÔ∏è üêõ üòñ Hyperkonvergente L√∂sung AERODISK vAIR. Basis - ARDFS-Dateisystem üë®üèº‚Äçüöí üöò üíÜ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Leser von Habr. Mit diesem Artikel er√∂ffnen wir einen Zyklus, der sich mit dem von uns entwickelten hyperkonvergenten System AERODISK vAIR befas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Hyperkonvergente L√∂sung AERODISK vAIR. Basis - ARDFS-Dateisystem</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/469383/"><p><img src="https://habrastorage.org/webt/sk/n3/zh/skn3zhr0fgbcuqfar5ozlg0v2tk.jpeg"></p><br><p>  Hallo Leser von Habr.  Mit diesem Artikel er√∂ffnen wir einen Zyklus, der sich mit dem von uns entwickelten hyperkonvergenten System AERODISK vAIR befasst.  Anfangs wollten wir, dass der erste Artikel alles √ºber alles erz√§hlt, aber das System ist ziemlich komplex, so dass wir einen Elefanten in Teilen essen werden. </p><br><p>  Beginnen wir die Geschichte mit der Geschichte des Systems, gehen wir tiefer in das ARDFS-Dateisystem ein, das die Grundlage von vAIR bildet, und sprechen wir auch ein wenig √ºber die Positionierung dieser L√∂sung auf dem russischen Markt. </p><br><p>  In zuk√ºnftigen Artikeln werden wir mehr √ºber verschiedene Architekturkomponenten (Cluster, Hypervisor, Load Balancer, √úberwachungssystem usw.), den Konfigurationsprozess sprechen, Lizenzierungsprobleme ansprechen, Crashtests separat anzeigen und nat√ºrlich √ºber Lasttests und schreiben Dimensionierung.  Wir werden auch einen separaten Artikel der Community-Version von vAIR widmen. </p><a name="habracut"></a><br><h2 id="aerodisk---eto-vrode-istoriya-pro-shd-ili-zachem-my-voobsche-nachali-zanimatsya-giperkonvergentom">  Ist eine Airdisc eine Geschichte √ºber Lagerung?  Oder warum haben wir √ºberhaupt angefangen, uns zu konvergieren? </h2><br><p>  Die Idee, ein eigenes Hyperkonvergenz zu schaffen, kam uns etwa im Jahr 2010.  Dann gab es keine Aerodisk und √§hnliche L√∂sungen (kommerzielle hyperkonvergente Boxed-Systeme) auf dem Markt.  Unsere Aufgabe war wie folgt: Von einer Reihe von Servern mit lokalen Festplatten, die √ºber eine Ethernet-Verbindung verbunden sind, mussten wir erweiterten Speicher erstellen und virtuelle Maschinen und ein Software-Netzwerk an derselben Stelle ausf√ºhren.  All dies musste ohne Speichersysteme implementiert werden (da es einfach kein Geld f√ºr die Speicherung und deren Bindung gab, wir aber noch kein eigenes Speichersystem erfunden hatten). </p><br><p>  Wir haben viele Open-Source-L√∂sungen ausprobiert und dieses Problem trotzdem gel√∂st, aber die L√∂sung war sehr kompliziert und schwer zu wiederholen.  Dar√ºber hinaus fiel diese Entscheidung aus der Kategorie ‚ÄûWerke?  Nicht anfassen! "  Nachdem wir dieses Problem gel√∂st hatten, entwickelten wir die Idee, das Ergebnis unserer Arbeit in ein vollwertiges Produkt umzuwandeln, nicht weiter. </p><br><p> Nach diesem Vorfall haben wir uns von dieser Idee entfernt, aber wir hatten immer noch das Gef√ºhl, dass diese Aufgabe vollst√§ndig l√∂sbar war und die Vorteile einer solchen L√∂sung mehr als offensichtlich waren.  In der Folge best√§tigten die freigegebenen HCI-Produkte ausl√§ndischer Unternehmen dieses Gef√ºhl nur. </p><br><p>  Daher sind wir Mitte 2016 im Rahmen der Entwicklung eines vollwertigen Produkts zu dieser Aufgabe zur√ºckgekehrt.  Da wir noch keine Beziehungen zu Investoren hatten, mussten wir f√ºr unser nicht sehr gro√ües Geld einen Entwicklungsstand kaufen.  Nachdem wir auf Avito BU-shyh Servern und Switches getippt hatten, machten wir uns an die Arbeit. </p><br><p><img src="https://habrastorage.org/webt/wy/ir/jr/wyirjr50guvzpnolcvdvniyn2mo.jpeg"></p><br><p>  Die Hauptaufgabe bestand darin, ein eigenes, wenn auch einfaches, aber eigenes Dateisystem zu erstellen, mit dem Daten in Form von virtuellen Bl√∂cken auf der n-ten Anzahl von Clusterknoten, die √ºber Ethernet miteinander verbunden sind, automatisch und gleichm√§√üig verteilt werden k√∂nnen.  In diesem Fall sollte der FS gut und leicht skalierbar und unabh√§ngig von benachbarten Systemen sein, d.h.  von vAIR in Form von "gerechter Lagerung" entfremdet sein. </p><br><p><img src="https://habrastorage.org/webt/il/vk/zs/ilvkzsyjkyr6pkcgoqs_ibumyus.jpeg"></p><br><p>  VAIR Erstes Konzept </p><br><p><img src="https://habrastorage.org/webt/h1/e0/pd/h1e0pda3j1ebbxls_cn_gmzm5ug.jpeg"></p><br><p>  Wir haben uns absichtlich geweigert, vorgefertigte Open-Source-L√∂sungen f√ºr die Organisation von erweitertem Speicher (Ceph, Gluster, Lustre und dergleichen) zugunsten unserer Entwicklung zu verwenden, da wir bereits viel Projekterfahrung mit ihnen hatten.  Nat√ºrlich sind diese L√∂sungen selbst wunderbar, und bevor wir an Aerodisk gearbeitet haben, haben wir mehr als ein Integrationsprojekt mit ihnen implementiert.  Es ist jedoch eine Sache, die spezifische Aufgabe eines Kunden zu realisieren, Mitarbeiter zu schulen und m√∂glicherweise Support f√ºr einen gro√üen Anbieter zu kaufen, und es ist eine ganz andere Sache, ein einfach zu replizierendes Produkt zu erstellen, das f√ºr verschiedene Aufgaben verwendet wird, die wir als Anbieter m√∂glicherweise selbst kennen wir werden nicht.  F√ºr den zweiten Zweck passten die vorhandenen Open Source-Produkte nicht zu uns, daher entschieden wir uns, das verteilte Dateisystem selbst zu sehen. <br>  Zwei Jahre sp√§ter erzielten mehrere Entwickler (die die Arbeit an vAIR mit der Arbeit an der klassischen Storage Engine kombinierten) ein bestimmtes Ergebnis. </p><br><p>  Bis zum Jahr 2018 hatten wir das einfachste Dateisystem geschrieben und es mit der notwendigen Bindung erg√§nzt.  Das System integrierte physische (lokale) Festplatten von verschiedenen Servern √ºber eine interne Verbindung in einen flachen Pool und ‚Äûschnitt‚Äú sie in virtuelle Bl√∂cke. Anschlie√üend wurden Blockger√§te mit unterschiedlichem Grad an Fehlertoleranz aus virtuellen Bl√∂cken erstellt, auf denen virtuelle KVM-Hypervisoren erstellt und ausgef√ºhrt wurden Autos. </p><br><p>  Wir haben uns nicht mit dem Namen des Dateisystems besch√§ftigt und es kurz ARDFS genannt (raten Sie mal, wie es entschl√ºsselt). </p><br><p>  Dieser Prototyp sah gut aus (nicht visuell, nat√ºrlich gab es damals kein visuelles Design) und zeigte gute Ergebnisse in Bezug auf Leistung und Skalierung.  Nach dem ersten echten Ergebnis haben wir die Weichen f√ºr dieses Projekt gestellt, nachdem wir eine vollwertige Entwicklungsumgebung und ein separates Team organisiert hatten, das nur mit vAIR besch√§ftigt war. </p><br><p>  Zu diesem Zeitpunkt war die allgemeine Architektur der L√∂sung gereift, die bis jetzt keine wesentlichen √Ñnderungen erfahren hatte. </p><br><h2 id="pogruzhaemsya-v-faylovuyu-sistemu-ardfs">  Eintauchen in das ARDFS-Dateisystem </h2><br><p>  ARDFS ist die Grundlage von vAIR, das verteilten Failover-Speicher f√ºr den gesamten Cluster bereitstellt.  Ein (aber nicht das einzige) Unterscheidungsmerkmal von ARDFS ist, dass keine zus√§tzlichen dedizierten Server f√ºr Meta und Verwaltung verwendet werden.  Dies sollte urspr√ºnglich die Konfiguration der L√∂sung und ihre Zuverl√§ssigkeit vereinfachen. </p><br><h3 id="struktura-hraneniya">  Speicherstruktur </h3><br><p>  Innerhalb aller Clusterknoten organisiert ARDFS einen logischen Pool aus dem gesamten verf√ºgbaren Speicherplatz.  Es ist wichtig zu verstehen, dass ein Pool noch keine Daten und kein formatierter Speicherplatz ist, sondern einfach ein Markup, d. H.  Alle Knoten, auf denen vAIR installiert ist, wenn sie zum Cluster hinzugef√ºgt werden, werden automatisch zum freigegebenen ARDFS-Pool hinzugef√ºgt, und die Festplattenressourcen werden automatisch f√ºr den gesamten Cluster freigegeben (und stehen f√ºr die zuk√ºnftige Datenspeicherung zur Verf√ºgung).  Mit diesem Ansatz k√∂nnen Sie Knoten im laufenden Betrieb hinzuf√ºgen und entfernen, ohne dass dies ernsthafte Auswirkungen auf ein bereits ausgef√ºhrtes System hat.  Das hei√üt,  Das System l√§sst sich sehr einfach mit ‚ÄûBausteinen‚Äú skalieren und bei Bedarf Knoten im Cluster hinzuf√ºgen oder entfernen. </p><br><p>  √úber dem ARDFS-Pool werden virtuelle Festplatten (Speicherobjekte f√ºr virtuelle Maschinen) hinzugef√ºgt, die aus virtuellen Bl√∂cken mit einer Gr√∂√üe von 4 Megabyte bestehen.  Virtuelle Festplatten speichern Daten direkt.  Auf der Ebene der virtuellen Festplatte wird auch ein Fehlertoleranzschema definiert. </p><br><p>  Wie Sie vielleicht vermutet haben, verwenden wir f√ºr die Fehlertoleranz des Festplattensubsystems nicht das Konzept von RAID (Redundantes Array unabh√§ngiger Festplatten), sondern RAIN (Redundantes Array unabh√§ngiger Knoten).  Das hei√üt,  Die Fehlertoleranz wird basierend auf Knoten und nicht auf Festplatten gemessen, automatisiert und verwaltet.  Festplatten sind nat√ºrlich auch ein Speicherobjekt. Sie werden wie alles andere √ºberwacht. Sie k√∂nnen alle Standardvorg√§nge mit ihnen ausf√ºhren, einschlie√ülich der Erstellung lokaler Hardware-RAIDs, aber der Cluster arbeitet mit Knoten. </p><br><p>  In einer Situation, in der Sie wirklich RAID ben√∂tigen (z. B. ein Szenario, das mehrere Fehler in kleinen Clustern unterst√ºtzt), hindert Sie nichts daran, lokale RAID-Controller zu verwenden und dar√ºber hinaus Stretched Storage und eine RAIN-Architektur auszuf√ºhren.  Dieses Szenario ist recht lebhaft und wird von uns unterst√ºtzt. Daher werden wir in einem Artikel √ºber typische Szenarien f√ºr die Verwendung von vAIR dar√ºber sprechen. </p><br><h3 id="shemy-otkazoustoychivosti-hranilischa">  Speicher-Failover-Schemata </h3><br><p>  M√∂glicherweise gibt es zwei Ausfallsicherheitsschemata f√ºr virtuelle vAIR-Festplatten: </p><br><p>  1) Replikationsfaktor oder nur Replikation - diese Methode der Fehlertoleranz ist einfach ‚Äûwie ein Stock und ein Seil‚Äú.  Eine synchrone Replikation zwischen Knoten mit einem Faktor von 2 (2 Kopien pro Cluster) oder 3 (3 Kopien) wird durchgef√ºhrt.  Mit RF-2 kann die virtuelle Festplatte dem Ausfall eines Knotens im Cluster standhalten, ‚Äûfrisst‚Äú jedoch die H√§lfte des verwendbaren Volumes, und RF-3 h√§lt dem Ausfall von 2 Knoten im Cluster stand, reserviert jedoch 2/3 des n√ºtzlichen Volumes f√ºr seine Anforderungen.  Dieses Schema ist RAID-1 sehr √§hnlich, dh eine in RF-2 konfigurierte virtuelle Festplatte ist resistent gegen den Ausfall eines Knotens des Clusters.  In diesem Fall sind die Daten in Ordnung und selbst die E / A werden nicht gestoppt.  Wenn ein heruntergefallener Knoten wieder in Betrieb genommen wird, beginnt die automatische Datenwiederherstellung / -synchronisation. </p><br><p>  Das Folgende sind Beispiele f√ºr die Verteilung von RF-2- und RF-3-Daten im normalen Modus und in einer Fehlersituation. </p><br><p>  Wir haben eine virtuelle Maschine mit einer Kapazit√§t von 8 MB eindeutiger (n√ºtzlicher) Daten, die auf 4 vAIR-Knoten ausgef√ºhrt wird.  Es ist klar, dass es in der Realit√§t unwahrscheinlich ist, dass es eine so geringe Menge geben wird, aber f√ºr ein Schema, das die Logik von ARDFS widerspiegelt, ist dieses Beispiel am verst√§ndlichsten.  AB sind virtuelle 4-MB-Bl√∂cke, die eindeutige Daten der virtuellen Maschine enthalten.  Mit RF-2 werden zwei Kopien dieser Bl√∂cke A1 + A2 bzw. B1 + B2 erstellt.  Diese Bl√∂cke werden von Knoten ‚Äûangelegt‚Äú, wobei der Schnittpunkt derselben Daten auf demselben Knoten vermieden wird, dh Kopie A1 befindet sich nicht auf derselben Note wie Kopie A2.  Bei B1 und B2 ist es √§hnlich. </p><br><p><img src="https://habrastorage.org/webt/ho/xm/oh/hoxmohhemj_whmr38pvgfyfousm.png"></p><br><p>  Bei einem Ausfall eines der Knoten (z. B. Knoten 3, der eine Kopie von B1 enth√§lt) wird diese Kopie automatisch auf dem Knoten aktiviert, auf dem keine Kopie seiner Kopie vorhanden ist (dh Kopie B2). </p><br><p><img src="https://habrastorage.org/webt/ex/xl/o4/exxlo4frqr3crhwbh_orvlcwl9g.png"></p><br><p>  Somit √ºberlebt die virtuelle Festplatte (bzw. die VMs) leicht den Ausfall eines Knotens im RF-2-Schema. </p><br><p>  Eine Schaltung mit Replikation leidet aufgrund ihrer Einfachheit und Zuverl√§ssigkeit unter denselben Schmerzen wie RAID1 - es gibt wenig nutzbaren Speicherplatz. </p><br><p>  2) L√∂schcodierung oder L√∂schcodierung (auch als "redundante Codierung", "L√∂schcodierung" oder "Redundanzcode" bekannt) existiert nur, um das obige Problem zu l√∂sen.  EC ist ein Redundanzschema, das im Vergleich zur Replikation eine hohe Datenverf√ºgbarkeit bei geringerem Festplatten-Overhead bietet.  Das Funktionsprinzip dieses Mechanismus √§hnelt RAID 5, 6, 6P. </p><br><p>  Bei der Codierung unterteilt der EC-Prozess den virtuellen Block (standardm√§√üig 4 MB) in Abh√§ngigkeit vom EC-Schema in mehrere kleinere ‚ÄûDatenelemente‚Äú (z. B. teilt ein 2 + 1-Schema jeden 4-MB-Block in 2 Teile von 2 MB).  Ferner erzeugt dieser Prozess "Parit√§tsbl√∂cke" f√ºr "Datenst√ºcke" von nicht mehr als einem der zuvor getrennten Teile.  Beim Decodieren generiert die EC die fehlenden Teile und liest die "√ºberlebenden" Daten im gesamten Cluster. </p><br><p>  Beispielsweise kann eine virtuelle Festplatte mit einem EC-Schema 2 + 1, die auf 4 Knoten eines Clusters implementiert ist, einem Ausfall eines einzelnen Knotens in einem Cluster auf dieselbe Weise wie RF-2 problemlos standhalten.  Gleichzeitig werden die Gemeinkosten niedriger sein, insbesondere betr√§gt der Kapazit√§tsfaktor bei RF-2 2 und bei EC 2 + 1 1,5. </p><br><p>  Wenn es einfacher zu beschreiben ist, ist die Quintessenz, dass der virtuelle Block in 2-8 (warum von 2 bis 8 siehe unten) "Teile" unterteilt ist, und f√ºr diese Teile werden die "Teile" der Parit√§t desselben Volumens berechnet. </p><br><p>  Infolgedessen werden Daten und Parit√§t gleichm√§√üig auf alle Knoten des Clusters verteilt.  Gleichzeitig verteilt ARDFS wie bei der Replikation Daten automatisch auf Knoten, so dass die Speicherung derselben Daten (Kopien von Daten und deren Parit√§t) auf einem Knoten verhindert wird, um die M√∂glichkeit eines Datenverlusts aufgrund der Tatsache, dass die Daten und ihre Daten ausgeschlossen sind, auszuschlie√üen Die Parit√§t endet pl√∂tzlich auf demselben Speicherknoten, was fehlschl√§gt. </p><br><p>  Unten sehen Sie ein Beispiel mit derselben virtuellen Maschine mit 8 MB und 4 Knoten, jedoch bereits mit dem EC 2 + 1-Schema. </p><br><p>  Die Bl√∂cke A und B sind in zwei Teile von jeweils 2 MB unterteilt (zwei, weil 2 + 1), dh A1 + A2 und B1 + B2.  Im Gegensatz zum Replikat ist A1 keine Kopie von A2, sondern ein virtueller Block A, der in zwei Teile unterteilt ist, ebenfalls mit Block B. Insgesamt erhalten wir zwei S√§tze von 4 MB, von denen jeder zwei Zwei-Megabyte-Teile enth√§lt.  Ferner erhalten wir f√ºr jeden dieser S√§tze Parit√§t mit einem Volumen von nicht mehr als einem St√ºck (d. H. 2 MB) zus√§tzliche + 2 Parit√§tsst√ºcke (AP und BP).  Insgesamt haben wir 4x2 Daten + 2x2 Parit√§t. </p><br><p>  Als n√§chstes werden die Teile von Knoten ‚Äûangeordnet‚Äú, damit sich die Daten nicht mit ihrer Parit√§t √ºberschneiden.  Das hei√üt,  A1 und A2 liegen nicht mit AP auf demselben Knoten. </p><br><p><img src="https://habrastorage.org/webt/s9/xi/om/s9xiombqm4ep-bos8lntf8kjq4s.png"></p><br><p>  Im Falle eines Ausfalls eines Knotens (zum Beispiel auch des dritten) wird der heruntergefallene Block B1 automatisch aus der Parit√§t BP wiederhergestellt, die auf Knoten Nr. 2 gespeichert ist, und wird auf dem Knoten aktiviert, auf dem keine B-Parit√§t vorliegt, d. H.  St√ºcke von BP.  In diesem Beispiel ist dies Knoten 1 </p><br><p><img src="https://habrastorage.org/webt/wu/gk/dr/wugkdrhgfund89fswagb7wc4iba.png"></p><br><p>  Ich bin sicher, der Leser hat eine Frage: </p><br><blockquote>  "Alles, was Sie beschrieben haben, wurde seit langem sowohl von Wettbewerbern als auch von Open Source-L√∂sungen implementiert. Was ist der Unterschied zwischen Ihrer Implementierung von EC in ARDFS?" </blockquote><p>  Und dann wird es interessante Funktionen des ARDFS geben. </p><br><h3 id="erasure-coding-s-uporom-na-gibkost">  L√∂schcodierung mit Schwerpunkt auf Flexibilit√§t </h3><br><p>  Anfangs haben wir ein ziemlich flexibles EC X + Y-Schema bereitgestellt, wobei X gleich einer Zahl von 2 bis 8 und Y gleich einer Zahl von 1 bis 8 ist, aber immer kleiner oder gleich X. Ein solches Schema ist aus Gr√ºnden der Flexibilit√§t vorgesehen.  Durch Erh√∂hen der Anzahl der Daten (X), in die die virtuelle Einheit unterteilt ist, kann der Overhead reduziert, dh der nutzbare Speicherplatz erh√∂ht werden. <br>  Eine Erh√∂hung der Anzahl der Parit√§tsbl√∂cke (Y) erh√∂ht die Zuverl√§ssigkeit der virtuellen Festplatte.  Je gr√∂√üer der Y-Wert ist, desto mehr Knoten im Cluster k√∂nnen ausfallen.  Das Erh√∂hen der Parit√§tsmenge verringert nat√ºrlich die Menge der nutzbaren Kapazit√§t, aber dies ist eine Geb√ºhr f√ºr die Zuverl√§ssigkeit. </p><br><p>  Die Abh√§ngigkeit der Leistung von EC-Schaltkreisen ist nahezu direkt: Je mehr ‚ÄûTeile‚Äú vorhanden sind, desto geringer ist die Leistung. Hier ben√∂tigen Sie nat√ºrlich ein ausgewogenes Erscheinungsbild. </p><br><p>  Mit diesem Ansatz k√∂nnen Administratoren den erweiterten Speicher am flexibelsten konfigurieren.  Innerhalb des ARDFS-Pools k√∂nnen Sie beliebige Fehlertoleranzschemata und deren Kombinationen verwenden, was unserer Meinung nach ebenfalls sehr n√ºtzlich ist. </p><br><p>  In der folgenden Tabelle werden mehrere (nicht alle m√∂glichen) HF- und EC-Schaltkreise verglichen. </p><br><p><img src="https://habrastorage.org/webt/g0/vj/oj/g0vjojekzkzjv2xbxbdhyw1fjy0.png"></p><br><p>  Die Tabelle zeigt, dass selbst die meiste ‚ÄûFrottee‚Äú -Kombination von EC 8 + 7, die den Verlust von bis zu 7 Knoten gleichzeitig in einem Cluster erm√∂glicht, weniger nutzbaren Speicherplatz (1.875 gegen√ºber 2) als die Standardreplikation ‚Äûverbraucht‚Äú und siebenmal besser sch√ºtzt. Dies macht diesen Schutzmechanismus zwar komplexer, aber in Situationen, in denen Sie unter Bedingungen mangelnden Speicherplatzes maximale Zuverl√§ssigkeit gew√§hrleisten m√ºssen, viel attraktiver.  Gleichzeitig m√ºssen Sie verstehen, dass jedes ‚ÄûPlus‚Äú f√ºr X oder Y einen zus√§tzlichen Overhead f√ºr die Produktivit√§t darstellt. Daher m√ºssen Sie im Dreieck zwischen Zuverl√§ssigkeit, Wirtschaftlichkeit und Leistung sehr sorgf√§ltig ausw√§hlen.  Aus diesem Grund widmen wir der Codierung der Gr√∂√üenl√∂schung einen separaten Artikel. </p><br><p><img src="https://habrastorage.org/webt/5v/2o/-j/5v2o-jpchgy8rqib7bugk35vmj4.png"></p><br><h3 id="nadezhnost-i-avtonomnost-faylovoy-sistemy">  Zuverl√§ssigkeit und Autonomie des Dateisystems </h3><br><p>  ARDFS wird lokal auf allen Knoten des Clusters ausgef√ºhrt und synchronisiert sie auf eigene Weise √ºber dedizierte Ethernet-Schnittstellen.  Ein wichtiger Punkt ist, dass ARDFS nicht nur Daten, sondern auch speicherbezogene Metadaten unabh√§ngig voneinander synchronisiert.  W√§hrend der Arbeit an ARDFS haben wir gleichzeitig eine Reihe vorhandener L√∂sungen untersucht und festgestellt, dass viele Dateisystem-Metasynchronisierungen mithilfe eines externen verteilten DBMS durchf√ºhren, das wir auch zum Synchronisieren verwenden, jedoch nur Konfigurationen, keine FS-Metadaten (zu diesem und anderen verwandten Subsystemen) im n√§chsten Artikel). </p><br><p>  Die Synchronisation von FS-Metadaten mit einem externen DBMS ist nat√ºrlich eine funktionierende L√∂sung, aber dann w√ºrde die Konsistenz der in ARDFS gespeicherten Daten vom externen DBMS und seinem Verhalten abh√§ngen (und sie ist ehrlich gesagt eine launische Frau), was unserer Meinung nach schlecht ist.  Warum?  Wenn die FS-Metadaten besch√§digt sind, k√∂nnen die FS-Daten selbst auch als ‚ÄûAuf Wiedersehen‚Äú bezeichnet werden. Daher haben wir uns f√ºr einen komplizierteren, aber zuverl√§ssigeren Weg entschieden. </p><br><p>  Wir haben das Metadatensynchronisations-Subsystem f√ºr ARDFS unabh√§ngig gemacht und es lebt v√∂llig unabh√§ngig von den benachbarten Subsystemen.  Das hei√üt,  Kein anderes Subsystem kann ARDFS-Daten besch√§digen.  Unserer Meinung nach ist dies der zuverl√§ssigste und korrekteste Weg, und ist es wirklich so - die Zeit wird es zeigen.  Dar√ºber hinaus ergibt sich bei diesem Ansatz ein zus√§tzlicher Vorteil.  ARDFS kann unabh√§ngig von vAIR verwendet werden, genau wie erweiterter Speicher, den wir sicherlich in zuk√ºnftigen Produkten verwenden werden. </p><br><p>  Als Ergebnis der Entwicklung von ARDFS haben wir ein flexibles und zuverl√§ssiges Dateisystem erhalten, mit dem Sie die Wahl haben, Kapazit√§t zu sparen oder alles an Leistung zu verlieren oder Speicher f√ºr eine moderate Geb√ºhr hochzuverl√§ssig zu machen, aber die Leistungsanforderungen zu reduzieren. </p><br><p>  Zusammen mit einer einfachen Lizenzierungsrichtlinie und einem flexiblen Bereitstellungsmodell (in Zukunft wird es von vAIR von Knoten lizenziert und entweder von Software oder als PAC bereitgestellt) k√∂nnen Sie die L√∂sung sehr genau auf die unterschiedlichsten Anforderungen der Kunden zuschneiden, und in Zukunft ist es einfach, dieses Gleichgewicht aufrechtzuerhalten. </p><br><h2 id="komu-eto-chudo-nuzhno">  Wer braucht dieses Wunder? </h2><br><p>  Einerseits k√∂nnen wir sagen, dass es bereits Akteure auf dem Markt gibt, die ernsthafte Entscheidungen im Bereich der Hyperkonvergenz treffen und wohin wir tats√§chlich gehen.  Diese Aussage scheint wahr zu sein, ABER ... </p><br><p>  Auf der anderen Seite sehen wir und unsere Partner, wenn wir auf die Felder gehen und mit Kunden kommunizieren, dass dies √ºberhaupt nicht der Fall ist.  Es gibt viele Probleme f√ºr die Hyperkonvergenz, irgendwo wussten die Leute einfach nicht, dass es solche L√∂sungen gibt, irgendwo schien es teuer, irgendwo gab es erfolglose Tests alternativer L√∂sungen, aber irgendwo verbieten sie den Kauf aufgrund von Sanktionen im Allgemeinen.  Im Allgemeinen wurde das Feld nicht gepfl√ºgt, also gingen wir, um das jungfr√§uliche Land zu erheben))). </p><br><h3 id="kogda-shd-luchshe-chem-gks">  Wann ist Speicher besser als GCS? </h3><br><p>  W√§hrend der Arbeit mit dem Markt werden wir oft gefragt, wann es besser ist, das klassische Schema mit Speichersystemen zu verwenden, und wann es hyperkonvergent ist.  Viele Unternehmen - Hersteller von GCS (insbesondere solche, die keinen Speicher in ihrem Portfolio haben) sagen: "Speicher ist √ºberlebt, nur hyperkonvergent!"  Dies ist eine k√ºhne Aussage, die jedoch die Realit√§t nicht ganz widerspiegelt. </p><br><p>  In Wahrheit schwimmt der Speichermarkt zwar in Richtung hyperkonvergenter und √§hnlicher L√∂sungen, aber es gibt immer ein ‚ÄûAber‚Äú. </p><br><p>  Erstens k√∂nnen Rechenzentren und IT-Infrastrukturen, die nach dem klassischen Schema mit Speichersystemen gebaut wurden, nicht einfach so wieder aufgebaut werden, weshalb die Modernisierung und Fertigstellung solcher Infrastrukturen immer noch ein 5-7-j√§hriges Erbe ist. </p><br><p>  Zweitens werden die Infrastrukturen, die derzeit gr√∂√ütenteils gebaut werden (dh die Russische F√∂deration), nach dem klassischen Schema unter Verwendung von Speichersystemen gebaut, und zwar nicht, weil die Menschen nichts √ºber Hyperkonvergenz wissen, sondern weil der Markt f√ºr Hyperkonvergenz neu ist, L√∂sungen und Standards noch nicht festgelegt wurden , IT-Mitarbeiter wurden noch nicht geschult, es gibt wenig Erfahrung und wir m√ºssen hier und jetzt Rechenzentren bauen.  Und dieser Trend h√§lt noch 3-5 Jahre an (und dann ein weiteres Erbe, siehe Absatz 1). </p><br><p>  Drittens eine rein technische Einschr√§nkung bei zus√§tzlichen kleinen Verz√∂gerungen von 2 Millisekunden pro Schreibvorgang (nat√ºrlich ohne den lokalen Cache), die Geb√ºhren f√ºr verteilten Speicher sind. </p><br><p>  Vergessen wir nicht, gro√üe physische Server zu verwenden, die die vertikale Skalierung des Festplattensubsystems lieben. </p><br><p>  Es gibt viele notwendige und beliebte Aufgaben, bei denen sich das Speichersystem besser verh√§lt als das GCS.  Hier sind nat√ºrlich diejenigen Hersteller nicht einverstanden, die keine Speichersysteme in ihrem Produktportfolio haben, aber wir sind bereit, vern√ºnftig zu argumentieren.  Nat√ºrlich werden wir als Entwickler beider Produkte in einer der zuk√ºnftigen Ver√∂ffentlichungen definitiv einen Vergleich von Speichersystemen und GCS anstellen, wo wir klar zeigen werden, was unter welchen Bedingungen besser ist. </p><br><h3 id="a-gde-giperkonvergentnye-resheniya-budut-rabotat-luchshe-shd">  Und wo funktionieren hyperkonvergente L√∂sungen besser als Speichersysteme? </h3><br><p>  Basierend auf den obigen Thesen gibt es drei offensichtliche Schlussfolgerungen: </p><br><ol><li>  Wenn zus√§tzliche 2 Millisekunden Aufzeichnungsverz√∂gerungen, die in einem Produkt stabil auftreten (jetzt sprechen wir nicht mehr √ºber Kunststoffe, Sie k√∂nnen Nanosekunden auf Kunststoffen anzeigen), nicht kritisch sind, reicht eine hyperkonvergente Funktion aus. </li><li>  Wenn die Last von gro√üen physischen Servern in viele kleine virtuelle Server umgewandelt und von Knoten verteilt werden kann, funktioniert der Hyperkonvergent auch dort gut. </li><li>  Wo horizontale Skalierung wichtiger ist als vertikale Skalierung, funktioniert GCS auch dort einwandfrei. </li></ol><br><h3 id="kakie-eto-resheniya">  Was sind diese L√∂sungen? </h3><br><ol><li>  Alle Standardinfrastrukturdienste (Verzeichnisdienst, Mail, EDS, Dateiserver, kleine oder mittlere ERP- und BI-Systeme usw.).  Wir nennen dies "General Computing". </li><li>  Die Infrastruktur von Cloud-Anbietern, bei der es erforderlich ist, eine gro√üe Anzahl virtueller Maschinen f√ºr Clients schnell und standardisiert horizontal zu erweitern und einfach zu ‚Äûschneiden‚Äú. </li><li>  Infrastruktur f√ºr virtuelle Desktops (VDI), bei der viele Virtuala f√ºr kleine Benutzer gestartet werden und leise in einem einheitlichen Cluster "schweben". </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Filialnetzwerke, in denen Sie in jeder Filiale eine standardm√§√üige, fehlertolerante und gleichzeitig kosteng√ºnstige Infrastruktur mit 15 bis 20 virtuellen Maschinen ben√∂tigen. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jedes verteilte Computing (z. B. Big Data-Dienste). </font><font style="vertical-align: inherit;">Wo die Ladung nicht "tief" geht, sondern "breit".</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Testumgebungen, in denen zus√§tzliche kleine Verz√∂gerungen akzeptabel sind, aber Budgetbeschr√§nkungen bestehen, da es sich um Tests handelt. </font></font></li></ol><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Derzeit haben wir f√ºr diese Aufgaben AERODISK vAIR erstellt und konzentrieren uns darauf (bisher erfolgreich). </font><font style="vertical-align: inherit;">Vielleicht wird sich das bald √§ndern. </font><font style="vertical-align: inherit;">Die Welt steht nicht still.</font></font></p><br><h3 id="itak">  Also ... </h3><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Damit ist der erste Teil einer gro√üen Reihe von Artikeln abgeschlossen. Im n√§chsten Artikel werden wir √ºber die Architektur der L√∂sung und die verwendeten Komponenten sprechen. </font></font></p><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir freuen uns √ºber Fragen, Vorschl√§ge und konstruktive Streitigkeiten. </font></font></p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de469383/">https://habr.com/ru/post/de469383/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de469371/index.html">Beschreibung des Ansatzes zum Organisieren und Testen von Code mit Redux Thunk</a></li>
<li><a href="../de469373/index.html">Die Ergebnisse des Projekts zur Schaffung einer neuronalen Schnittstelle f√ºr vollst√§ndig gel√§hmte Patienten wurden in Frage gestellt</a></li>
<li><a href="../de469375/index.html">Warum stellen Mozilla, Coil und Creative Commons 100 Millionen US-Dollar f√ºr Open Source-Projekte zur Verf√ºgung?</a></li>
<li><a href="../de469379/index.html">Anwendung formaler Modellvalidierungsmethoden f√ºr die Benutzeroberfl√§che</a></li>
<li><a href="../de469381/index.html">Agones, erstellen Sie einen Mehrbenutzer-Spieleserver. Architektur und Installation</a></li>
<li><a href="../de469387/index.html">Die Geschichte eines "Entwicklers" oder wie ein Neuling eine Anwendung f√ºr iOS schreibt</a></li>
<li><a href="../de469389/index.html">Parametrisierung eines physikalischen Modells durch ein neuronales Netzwerk zur L√∂sung eines topologischen Optimierungsproblems</a></li>
<li><a href="../de469391/index.html">Audio-Schnittstellen: Ton als Informationsquelle auf der Stra√üe, im B√ºro und am Himmel</a></li>
<li><a href="../de469393/index.html">Flare-On 2019 Zuschreibung</a></li>
<li><a href="../de469395/index.html">Wo und wie werden Mehrspalten verwendet (CSS-Spalten)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>