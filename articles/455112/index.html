<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèø‚Äçüé® üëºüèº üî¨ Bitrix24: "No se considera que ha ca√≠do r√°pidamente" üôÇ üîµ üóº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hasta la fecha, el servicio Bitrix24 no tiene cientos de gigabits de tr√°fico, no hay una gran flota de servidores (aunque, por supuesto, hay muchos ex...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bitrix24: "No se considera que ha ca√≠do r√°pidamente"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/455112/">  Hasta la fecha, el servicio Bitrix24 no tiene cientos de gigabits de tr√°fico, no hay una gran flota de servidores (aunque, por supuesto, hay muchos existentes).  Pero para muchos clientes, es la herramienta principal para trabajar en la empresa, es una aplicaci√≥n cr√≠tica para el negocio real.  Por lo tanto, cayendo, bueno, de ninguna manera.  Pero, ¬øqu√© pasa si la ca√≠da sucedi√≥, pero el servicio "se rebel√≥" tan r√°pido que nadie not√≥ nada?  ¬øY c√≥mo logra implementar la conmutaci√≥n por error sin perder la calidad del trabajo y la cantidad de clientes?  Alexander Demidov, director de servicios en la nube de Bitrix24, cont√≥ a nuestro blog sobre c√≥mo ha evolucionado el sistema de respaldo durante los 7 a√±os de existencia del producto. <br><br><img src="https://habrastorage.org/webt/xp/iz/9_/xpiz9_ri1tdppyelfb8nu9tzeg8.jpeg"><br><a name="habracut"></a><br>  ‚ÄúEn forma de SaaS, lanzamos Bitrix24 hace 7 a√±os.  La principal dificultad, probablemente, fue la siguiente: antes de lanzarse en p√∫blico en forma de SaaS, este producto exist√≠a simplemente en el formato de una soluci√≥n en caja.  Los clientes nos lo compraron, lo colocaron en sus servidores, configuraron un portal corporativo, una soluci√≥n com√∫n para la comunicaci√≥n de los empleados, el almacenamiento de archivos, la gesti√≥n de tareas, CRM, eso es todo.  Y para 2012, decidimos que quer√≠amos lanzarlo como SaaS, administr√°ndolo nosotros mismos, brindando tolerancia a fallas y confiabilidad.  Adquirimos experiencia en el proceso, porque hasta entonces simplemente no lo ten√≠amos: solo √©ramos fabricantes de software, no proveedores de servicios. <br><br>  Al lanzar el servicio, entendimos que lo m√°s importante es garantizar la tolerancia a fallas, la confiabilidad y la disponibilidad constante del servicio, porque si tiene un sitio regular simple, una tienda, por ejemplo, y se cay√≥ de usted y yace una hora, solo usted mismo sufre, pierde pedidos , pierde clientes, pero para su propio cliente, para √©l esto no es muy cr√≠tico.  Estaba molesto, por supuesto, pero fue y compr√≥ en otro sitio.  Y si esta es una aplicaci√≥n con la que todo el trabajo dentro de la empresa, las comunicaciones y las soluciones est√°n vinculadas, lo m√°s importante es ganarse la confianza de los usuarios, es decir, no decepcionarlos y no caer.  Porque todo el trabajo puede levantarse si algo en el interior no funciona. <br><br><h4>  Bitrix.24 como SaaS </h4><br>  El primer prototipo que armamos un a√±o antes del lanzamiento p√∫blico, en 2011.  Reunidos en aproximadamente una semana, mirados, retorcidos, incluso estaba trabajando.  Es decir, fue posible ingresar al formulario, ingresar el nombre del portal all√≠, se estaba desplegando un nuevo portal, se estaba creando una base de usuarios.  Lo analizamos, evaluamos el producto en principio, lo apagamos y lo finalizamos un a√±o despu√©s.  Debido a que ten√≠amos una gran tarea: no quer√≠amos crear dos bases de c√≥digo diferentes, no quer√≠amos admitir un producto en caja separado, soluciones en la nube por separado; quer√≠amos hacer todo esto dentro del mismo c√≥digo. <br><br><img src="https://habrastorage.org/webt/lw/zu/fe/lwzufe1w3lchcx7iqtwwh04gqfu.jpeg"><br><br>  Una aplicaci√≥n web t√≠pica en ese momento es un servidor en el que se est√° ejecutando alg√∫n c√≥digo php, la base mysql, los archivos se est√°n descargando, los documentos, las im√°genes se colocan en la carga de pap√°, bueno, todo funciona.  Por desgracia, es imposible ejecutar un servicio web cr√≠ticamente sostenible en esto.  La cach√© distribuida no es compatible all√≠, la replicaci√≥n de la base de datos no es compatible. <br><br>  Formulamos los requisitos: esta capacidad para ubicarse en diferentes ubicaciones, para admitir la replicaci√≥n, idealmente para ubicarse en diferentes centros de datos distribuidos geogr√°ficamente.  Separe la l√≥gica del producto y, de hecho, el almacenamiento de datos.  Ser capaz de escalar din√°micamente de acuerdo con la carga, generalmente hacer la est√°tica.  A partir de estas consideraciones, de hecho, hab√≠a requisitos para el producto, que acabamos de desarrollar durante el a√±o.  Durante este tiempo, en una plataforma que result√≥ estar unificada, para soluciones en caja, para nuestro propio servicio, brindamos soporte para las cosas que necesit√°bamos.  Soporte para la replicaci√≥n de mysql a nivel del producto en s√≠: es decir, el desarrollador que escribe el c√≥digo no piensa en c√≥mo se distribuir√°n sus solicitudes, usa nuestra API y podemos distribuir correctamente las solicitudes de escritura y lectura entre maestros y esclavos. <br><br>  Hicimos soporte a nivel de producto para varias tiendas de objetos en la nube: almacenamiento de google, amazon s3, - plus, soporte para open stack swift.  Por lo tanto, fue conveniente tanto para nosotros como un servicio como para los desarrolladores que trabajan con una soluci√≥n en caja: si solo usan nuestra API para el trabajo, no piensan d√≥nde se guardar√° el archivo, ya sea localmente en el sistema de archivos o en el almacenamiento de archivos de objetos . <br><br>  Como resultado, decidimos inmediatamente que reservar√≠amos al nivel de un centro de datos completo.  En 2012, lanzamos completamente en Amazon AWS, porque ya ten√≠amos experiencia con esta plataforma: nuestro propio sitio web estaba alojado all√≠.  Nos atrajo el hecho de que en cada regi√≥n de Amazon hay varias zonas de acceso; de hecho, en su terminolog√≠a, varios centros de datos que son m√°s o menos independientes entre s√≠ y nos permiten reservar a nivel de un centro de datos completo: si falla repentinamente, las bases de datos master-master se replican, los servidores de aplicaciones web se reservan y la est√°tica se mueve al almacenamiento de objetos s3.  La carga est√° equilibrada, en ese momento el elba amaz√≥nica, pero un poco m√°s tarde llegamos a nuestros propios equilibradores, porque necesit√°bamos una l√≥gica m√°s compleja. <br><br><h4>  Lo que quer√≠an, lo consiguieron ... </h4><br>  Todas las cosas b√°sicas que quer√≠amos proporcionar, la tolerancia a fallas de los servidores, aplicaciones web, bases de datos, todo funcion√≥ bien.  El escenario m√°s simple: si algunas de las aplicaciones web fallan, entonces todo es simple: se desactivan de la balanza. <br><br><img src="https://habrastorage.org/webt/0y/vn/7h/0yvn7ho3syy9rpwkz8wo7govryu.jpeg"><br><br>  El equilibrador de la m√°quina (entonces era un elfo amaz√≥nico) que bloque√≥ la m√°quina como no saludable, apag√≥ la distribuci√≥n de carga en ellos.  El escalado autom√°tico de la Amazon√≠a funcion√≥: cuando la carga creci√≥, se agregaron autos nuevos al grupo de escalado autom√°tico, la carga se distribuy√≥ a los autos nuevos, todo estaba bien.  Con nuestros equilibradores, la l√≥gica es m√°s o menos la misma: si algo le sucede al servidor de aplicaciones, eliminamos las solicitudes, eliminamos estas m√°quinas, iniciamos nuevas y seguimos trabajando.  El esquema para todos estos a√±os ha cambiado un poco, pero contin√∫a funcionando: es simple, comprensible y no hay dificultades con esto. <br><br>  Trabajamos en todo el mundo, la carga m√°xima de clientes es completamente diferente y, en el buen sentido, deber√≠amos poder realizar ciertos trabajos de mantenimiento con cualquier componente de nuestro sistema en cualquier momento, de forma invisible para los clientes.  Por lo tanto, tenemos la oportunidad de cerrar la base de datos del trabajo, redistribuyendo la carga en el segundo centro de datos. <br><br>  ¬øC√≥mo funciona todo?  - Cambiamos el tr√°fico a un centro de datos en funcionamiento: si se trata de un accidente en un centro de datos, entonces completamente, si es nuestro trabajo planificado con cualquier base, entonces formamos parte del tr√°fico que sirve a estos clientes, cambiamos a un segundo centro de datos, se detiene replicaci√≥n  Si necesita m√°quinas nuevas para aplicaciones web, a medida que aumenta la carga en el segundo centro de datos, se inician autom√°ticamente.  Terminamos el trabajo, se restaura la replicaci√≥n y devolvemos toda la carga.  Si necesitamos reflejar alg√∫n trabajo en el segundo DC, por ejemplo, instalar actualizaciones del sistema o cambiar la configuraci√≥n en la segunda base de datos, entonces, en general, repetimos lo mismo, al rev√©s.  Y si esto es un accidente, entonces hacemos todo trivialmente: en el sistema de monitoreo usamos el mecanismo de manejo de eventos.  Si varias comprobaciones funcionan para nosotros y el estado se vuelve cr√≠tico, entonces se inicia este controlador, un controlador que puede ejecutar esta o aquella l√≥gica.  Para cada base de datos, hemos registrado qu√© servidor tiene conmutaci√≥n por error y d√≥nde debe cambiar el tr√°fico si no est√° disponible.  Nosotros, como se ha desarrollado hist√≥ricamente, utilizamos de una forma u otra nagios o cualquiera de sus tenedores.  En principio, existen mecanismos similares en casi cualquier sistema de monitoreo, todav√≠a no estamos usando algo m√°s complicado, pero quiz√°s alg√∫n d√≠a lo hagamos.  Ahora el monitoreo se activa por inaccesibilidad y tiene la capacidad de cambiar algo. <br><br><h4>  ¬øHemos reservado todo? </h4><br>  Tenemos muchos clientes de los EE. UU., Muchos clientes de Europa, muchos clientes que est√°n m√°s cerca del Este: Jap√≥n, Singapur, etc.  Por supuesto, una gran proporci√≥n de clientes en Rusia.  Es decir, el trabajo est√° lejos de estar en una regi√≥n.  Los usuarios desean una respuesta r√°pida, hay requisitos para observar varias leyes locales, y dentro de cada regi√≥n que reservamos para dos centros de datos, adem√°s hay algunos servicios adicionales que, nuevamente, son convenientes para colocar dentro de una regi√≥n, para los clientes que est√°n en esta regi√≥n de trabajo.  Los controladores REST, los servidores de autorizaci√≥n, son menos cr√≠ticos para el cliente en general, puede cambiar entre ellos con un peque√±o retraso aceptable, pero no desea inventar bicicletas, c√≥mo monitorearlas y qu√© hacer con ellas.  Por lo tanto, al m√°ximo intentamos utilizar las soluciones existentes y no desarrollar cierta competencia en productos adicionales.  Y en alg√∫n lugar, utilizamos trivialmente el cambio en el nivel dns, y determinamos la vivacidad del servicio con el mismo dns.  Amazon tiene un servicio de Route 53, pero no solo es dns en el que puede grabar todo, es mucho m√°s flexible y conveniente.  A trav√©s de √©l, puede crear servicios geo-distribuidos con geolocalizaciones, cuando lo usa para determinar de d√≥nde vino el cliente y darles ciertos registros; con √©l puede construir arquitecturas de conmutaci√≥n por error.  Las mismas comprobaciones de estado se configuran en la ruta 53, especifica puntos finales que se supervisan, establece m√©tricas y especifica qu√© protocolos determinan la vida √∫til del servicio: tcp, http, https;  establecer la frecuencia de las verificaciones que determinan si el servicio est√° en vivo o no.  Y en el dns mismo, prescribe qu√© ser√° primario, qu√© ser√° secundario, d√≥nde cambiar si se activa la verificaci√≥n de estado dentro de la ruta 53. Todo esto se puede hacer con otras herramientas, pero lo que es m√°s conveniente: lo configuramos una vez y luego no pensamos en c√≥mo hacemos cheques, c√≥mo cambiamos: todo funciona por s√≠ solo. <br><br>  <b>El primer "pero"</b> : ¬øc√≥mo y c√≥mo reservar la ruta 53?  ¬øOcurre si algo le sucede a √©l?  Afortunadamente, nunca hemos pisado este rastrillo, pero nuevamente, frente a m√≠, tendr√© una historia de por qu√© pensamos que todav√≠a tenemos que reservar.  Aqu√≠ ponemos la paja de antemano.  Varias veces al d√≠a hacemos una descarga completa de todas las zonas que tenemos en la ruta 53.  La API de Amazon les permite enviarlos de manera segura a JSON, y hemos creado varios servidores redundantes donde los convertimos, los cargamos en forma de configuraciones y, en t√©rminos generales, tenemos una configuraci√≥n de respaldo.  En cuyo caso podemos implementarlo r√°pidamente de forma manual, no perderemos los datos de configuraci√≥n de dns. <br><br>  <b>El segundo "pero"</b> : ¬øqu√© no est√° reservado en esta imagen?  El equilibrador mismo!  Hemos hecho que la distribuci√≥n de clientes por regi√≥n sea muy simple.  Tenemos dominios bitrix24.ru, bitrix24.com, .de, ahora hay 13 dominios diferentes que funcionan en zonas muy diferentes.  Hemos llegado a lo siguiente: cada regi√≥n tiene sus propios equilibradores.  Es m√°s conveniente distribuir por regi√≥n, dependiendo de d√≥nde est√© la carga m√°xima en la red.  Si se trata de una falla a nivel de cualquier equilibrador, simplemente se retira del servicio y se elimina de dns.  Si se produce alg√∫n tipo de problema con un grupo de equilibradores, entonces se reservan en otros sitios, y el cambio entre ellos se realiza utilizando la misma ruta53, porque debido a un ttl corto, el cambio se produce durante un m√°ximo de 2, 3, 5 minutos. <br><br>  <b>El tercer "pero"</b> : ¬øqu√© a√∫n no se ha reservado?  S3, a la derecha.  Nosotros, colocando los archivos que los usuarios almacenan en s3, cre√≠amos sinceramente que era una perforaci√≥n blindada y que no hab√≠a necesidad de reservar nada all√≠.  Pero la historia muestra lo que sucede de manera diferente.  En general, Amazon describe S3 como un servicio fundamental, porque Amazon mismo usa S3 para almacenar im√°genes de m√°quinas, configuraciones, im√°genes AMI, instant√°neas ... Y si s3 falla, como sucedi√≥ en estos 7 a√±os, cu√°nto bitrix24 hemos estado usando, es seguido por un ventilador saca un mont√≥n de todo: inaccesibilidad para iniciar m√°quinas virtuales, mal funcionamiento de la API, etc. <br><br>  Y S3 puede caer, sucedi√≥ una vez.  Por lo tanto, llegamos al siguiente esquema: hace unos a√±os no hab√≠a ning√∫n almacenamiento p√∫blico de objetos graves en Rusia, y est√°bamos considerando la opci√≥n de hacer algo por nuestra cuenta ... Afortunadamente, no comenzamos a hacer esto, porque investigar√≠amos ese examen que no hicimos. poseer, y probablemente lo habr√≠a hecho.  Ahora Mail.ru tiene almacenes compatibles con s3, Yandex lo tiene y varios proveedores a√∫n lo tienen.  Como resultado, llegamos a la conclusi√≥n de que queremos tener, en primer lugar, una copia de seguridad y, en segundo lugar, la capacidad de trabajar con copias locales.  Para una regi√≥n rusa en particular, utilizamos el servicio Mail.ru Hotbox, que es api compatible con s3.  No necesit√°bamos modificaciones serias en el c√≥digo dentro de la aplicaci√≥n, e hicimos el siguiente mecanismo: en s3 hay disparadores que funcionan en la creaci√≥n / eliminaci√≥n de objetos, Amazon tiene un servicio como Lambda: esta es la ejecuci√≥n de c√≥digo sin servidor, que se ejecutar√° solo cuando se activan ciertos desencadenantes. <br><br><img src="https://habrastorage.org/webt/yv/y5/cp/yvy5cp7xsn82ryeocsgos7ns7ss.jpeg"><br><br>  Lo hicimos de manera muy simple: si nuestro disparador se dispara, ejecutamos el c√≥digo que copiar√° el objeto al repositorio Mail.ru.  Para comenzar a trabajar completamente con copias locales de datos, tambi√©n necesitamos sincronizaci√≥n inversa, de modo que los clientes ubicados en el segmento ruso puedan trabajar con almacenamiento m√°s cercano a ellos.  El correo est√° a punto de completar los desencadenantes en su repositorio: ser√° posible realizar una sincronizaci√≥n inversa ya a nivel de infraestructura, pero por ahora lo estamos haciendo a nivel de nuestro propio c√≥digo.  Si vemos que el cliente ha colocado alg√∫n tipo de archivo, entonces a nuestro nivel de c√≥digo colocamos el evento en la cola, lo procesamos y hacemos la replicaci√≥n inversa.  ¬øPor qu√© es malo? Si tenemos alg√∫n tipo de trabajo con nuestros objetos fuera de nuestro producto, es decir, por alg√∫n medio externo, no lo tendremos en cuenta.  Por lo tanto, esperamos hasta el final cuando aparecen los desencadenantes en el nivel de almacenamiento, de modo que no importa desde d√≥nde ejecutamos el c√≥digo, el objeto que nos lleg√≥ se copia de la otra manera. <br><br>  A nivel de c√≥digo, para cada cliente, ambos repositorios est√°n registrados: uno se considera el principal, el otro es una copia de seguridad.  Si todo est√° bien, trabajamos con el almacenamiento que est√° m√°s cerca de nosotros: es decir, nuestros clientes que est√°n en Amazon, trabajan con S3, y aquellos que trabajan en Rusia, trabajan con Hotbox.  Si la casilla de verificaci√≥n funciona, la conmutaci√≥n por error deber√≠a conectarse con nosotros y cambiaremos los clientes a otro almacenamiento.  Podemos establecer este indicador de forma independiente por regi√≥n y podemos cambiarlos de un lado a otro.  En la pr√°ctica, a√∫n no lo hemos usado, pero hemos imaginado este mecanismo y creemos que alg√∫n d√≠a necesitaremos y usaremos este mismo interruptor.  Una vez que ya ha sucedido. <br><br><h4>  Ah, y tu Amazon escap√≥ ... </h4><br>  Este abril es el aniversario del inicio de las cerraduras de Telegram en Rusia.  El proveedor m√°s afectado que vino debajo de esto es Amazon.  Y, desafortunadamente, las compa√±√≠as rusas que trabajaron en todo el mundo sufrieron m√°s. <br><br>  Si la empresa es global y Rusia es un segmento muy peque√±o, 3-5%, bueno, de una forma u otra, puede donarlos. <br><br>  Si se trata de una empresa puramente rusa, estoy seguro de que necesita ubicarse localmente, bueno, es solo que los propios usuarios ser√°n convenientes, c√≥modos, habr√° menos riesgos. <br><br>  ¬øY si esta es una empresa que trabaja a nivel mundial, y tiene aproximadamente partes iguales de clientes de Rusia y de alg√∫n lugar del mundo?  La conectividad de los segmentos es importante, y de todos modos deben trabajar entre ellos. <br><br>  A finales de marzo de 2018, Roskomnadzor envi√≥ una carta a los operadores m√°s grandes indicando que planean bloquear varios millones de ip de Amazon para bloquear ... el mensajero Zello.  Gracias a estos mismos proveedores, filtraron con √©xito la carta a todos, y se entendi√≥ que la conectividad con Amazon podr√≠a desmoronarse.  Era viernes, nos encontramos con los colegas de los servidores.ru en p√°nico, con las palabras: "Amigos, necesitamos varios servidores que no estar√°n en Rusia, no en Amazon, sino, por ejemplo, en alg√∫n lugar de Amsterdam". para poder, al menos de alguna manera, poner nuestra propia VPN y proxy para algunos puntos finales en los que no podemos influir en absoluto, por ejemplo, los endponts del mismo s3, no podemos intentar aumentar un nuevo servicio y obtener otra ip, Todav√≠a necesitas llegar all√≠.  En unos pocos d√≠as, configuramos estos servidores, los elevamos y, en general, nos preparamos para el inicio de los bloqueos.  Es curioso que el ILV, mirando la exageraci√≥n y el p√°nico elevado, dijo: "No, no bloquearemos nada ahora".  (Pero esto es exactamente hasta el momento en que comenzaron a bloquear los telegramas). Habiendo configurado las opciones de derivaci√≥n y d√°ndonos cuenta de que no entraron en la cerradura, nosotros, sin embargo, no desmantelamos todo el asunto.  Entonces, por si acaso. <br><br><img src="https://habrastorage.org/webt/la/o1/c1/lao1c1iqnqljjdv76qb1s9hg-qo.jpeg"><br><br>  Y en 2019, todav√≠a vivimos en condiciones de cerraduras.  Mir√© anoche: alrededor de un mill√≥n de ip siguen bloqueados.  Es cierto, Amazon casi completamente desbloqueado, en el pico alcanz√≥ 20 millones de direcciones ... En general, la realidad es que la conectividad, buena conectividad, puede no serlo.  De repente  Puede que no sea por razones t√©cnicas: incendios, excavadoras, todo eso.  O, como hemos visto, no del todo t√©cnico.  Por lo tanto, alguien grande y grande, con su propio AS-kami, probablemente pueda dirigirlo de otras maneras: conexi√≥n directa y otras cosas ya est√°n en el nivel l2.  Pero en una versi√≥n simple, al igual que nosotros o incluso m√°s peque√±a, puede, por si acaso, tener redundancia a nivel de servidores criados en otros lugares, configurados por adelantado vpn, proxy, con la capacidad de cambiar r√°pidamente las configuraciones en aquellos segmentos que tiene conectividad cr√≠tica .  Esto nos fue √∫til m√°s de una vez, cuando se iniciaron los bloqueos de Amazon, dejamos pasar el tr√°fico S3 en el peor de los casos, pero gradualmente todo sali√≥ mal. <br><br><h4>  ¬øY c√≥mo reservar ... todo el proveedor? </h4><br>  Ahora no tenemos escenario en caso de falla de toda la Amazon√≠a.  Tenemos un escenario similar para Rusia.  Nosotros en Rusia fuimos alojados por un proveedor, de quien elegimos tener varios sitios.  Y hace un a√±o nos encontramos con un problema: a pesar de que estos son dos centros de datos, es posible que ya haya problemas a nivel de la configuraci√≥n de red del proveedor que afectar√°n a ambos centros de datos de todos modos.  Y podemos obtener inaccesibilidad en ambos sitios.  Por supuesto, eso es lo que pas√≥.  Eventualmente redefinimos la arquitectura interior.  No ha cambiado mucho, pero para Rusia ahora tenemos dos sitios, que no son un proveedor, sino dos diferentes.  Si uno de ellos falla, podemos cambiar a otro. <br><br>  Hipot√©ticamente, estamos considerando que Amazon reserve al nivel de otro proveedor;  tal vez Google, tal vez alguien m√°s ... Pero hasta ahora hemos observado en la pr√°ctica que si Amazon se bloquea al mismo nivel de zona de disponibilidad, los bloqueos a nivel de toda una regi√≥n son bastante raros.  Por lo tanto, te√≥ricamente tenemos la idea de que, tal vez, haremos una reserva "Amazon no es Amazon", pero en la pr√°ctica esto a√∫n no existe. <br><br><h4>  Algunas palabras sobre automatizaci√≥n </h4><br>  ¬øSiempre necesitas automatizaci√≥n?  Es apropiado recordar el efecto Dunning-Krueger.  En el eje x, nuestro conocimiento y experiencia, que estamos ganando, y en el eje y, confianza en nuestras acciones.  Al principio no sabemos nada y no estamos del todo seguros.  Entonces sabemos un poco y nos volvemos m√°s seguros: este es el llamado "pico de la estupidez", est√° bien ilustrado por la imagen "demencia y coraje".  Adem√°s, ya hemos aprendido un poco y estamos listos para la batalla.  Luego pisamos un rastrillo mega serio, caemos en un valle de desesperaci√≥n cuando parecemos saber algo, pero en realidad no sabemos mucho.  Luego, a medida que adquieres experiencia, nos volvemos m√°s seguros. <br><br><img src="https://habrastorage.org/webt/ha/-x/uy/ha-xuyf7ttz6wyojkkh5idgrli4.jpeg"><br><br>  Este gr√°fico describe muy bien nuestra l√≥gica acerca de varios cambios autom√°ticos a uno u otro accidente.   ‚Äî    ,     .   ,       , ,  .      -:    false positive,    - , , -,    . ,     - ‚Äî     .     ,       .       ,    .  Pero!     ,        ,  ,  , ,   ,     ‚Ä¶ <br><br><h4>  Conclusi√≥n </h4><br>  7      , ,  - , ‚Äî  -,  ,    ,   ,   ‚Äî   ‚Äî .    - ,     ,   ,   .      ‚Äî         ,    ,           ‚Äî    .     ,  -        ‚Äî    s3,     ,   .         ,      ,  - - .     .     ,      , ‚Äî  :  ,      ‚Äî            ? , -           ,      ,     -   ¬´,   ¬ª. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un compromiso razonable entre el perfeccionismo y las fuerzas reales, el tiempo, el dinero que puede gastar en el esquema que eventualmente tendr√°. </font></font><br><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este texto es una versi√≥n complementada y ampliada del informe de Alexander Demidov en la conferencia </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uptime d√≠a 4</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/455112/">https://habr.com/ru/post/455112/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455100/index.html">C√≥mo organizar el caf√© en la oficina.</a></li>
<li><a href="../455102/index.html">Conexi√≥n de interfaces seriales sobre IP</a></li>
<li><a href="../455104/index.html">BotAuth: inicio de sesi√≥n y registro con bots</a></li>
<li><a href="../455106/index.html">Desarrollo de software. Tendencias 2019</a></li>
<li><a href="../455110/index.html">¬øLa felicidad de los empleados depende de tareas interesantes? Juegos de Tell Badoo, SKB Kontur, Dodo Pizza, Staply y Alternativa</a></li>
<li><a href="../455114/index.html">Implementaci√≥n de tipo entero en CPython</a></li>
<li><a href="../455118/index.html">Hacia el futuro con la integraci√≥n del servicio Jenkins y Oracle APEX</a></li>
<li><a href="../455120/index.html">El wifi no es para todos. ¬øC√≥mo autorizar a los extranjeros en la red por ley?</a></li>
<li><a href="../455122/index.html">Alternativa de reflexi√≥n Java m√°s r√°pida</a></li>
<li><a href="../455126/index.html">Ropa inteligente del futuro: ¬øhay potencial?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>