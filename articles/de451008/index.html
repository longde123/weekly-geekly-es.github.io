<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçüéì üë®üèª‚Äçüöí ü§¶üèº End2End-Ansatz zum Verst√§ndnis der gesprochenen Sprache üë®‚Äçüî¨ ü•ò üëäüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es gibt verschiedene Ans√§tze zum Verst√§ndnis einer umgangssprachlichen Sprachmaschine: den klassischen Dreikomponentenansatz (der eine Spracherkennung...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>End2End-Ansatz zum Verst√§ndnis der gesprochenen Sprache</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ru_mts/blog/451008/">  <i>Es gibt verschiedene Ans√§tze zum Verst√§ndnis einer umgangssprachlichen Sprachmaschine: den klassischen Dreikomponentenansatz (der eine Spracherkennungskomponente, eine Komponente zum Verst√§ndnis nat√ºrlicher Sprache und eine Komponente enth√§lt, die f√ºr eine bestimmte Gesch√§ftslogik verantwortlich ist) und einen End2End-Ansatz, der vier Implementierungsmodelle umfasst: direkt, kollaborativ, mehrstufig und multitasking .</i>  <i>Lassen Sie uns alle Vor- und Nachteile dieser Ans√§tze betrachten, einschlie√ülich derer, die auf Googles Experimenten basieren, und im Detail analysieren, warum der End2End-Ansatz die Probleme des klassischen Ansatzes l√∂st.</i> <i><br></i> <br><img src="https://habrastorage.org/webt/4f/zx/o3/4fzxo3p37pnl9kprkwk1trmwxd4.png"><a name="habracut"></a><br><br>  Wir geben dem f√ºhrenden Entwickler des AI MTS-Zentrums Nikita Semenov das Wort. <br><br>  Hallo!  Als Vorwort m√∂chte ich die bekannten Wissenschaftler Jan Lekun, Joshua Benjio und Jeffrey Hinton zitieren - dies sind drei Pioniere der k√ºnstlichen Intelligenz, die k√ºrzlich eine der renommiertesten Auszeichnungen auf dem Gebiet der Informationstechnologie erhalten haben - den Turing Award.  In einer der Ausgaben des Nature-Magazins im Jahr 2015 ver√∂ffentlichten sie einen sehr interessanten Artikel ‚ÄûDeep Learning‚Äú, in dem es einen interessanten Satz gab: ‚ÄûDeep Learning versprach die F√§higkeit, mit Rohsignalen umzugehen, ohne dass handgefertigte Funktionen erforderlich sind.‚Äú  Es ist schwierig, es richtig zu √ºbersetzen, aber die Bedeutung ist ungef√§hr so: "Deep Learning war mit dem Versprechen verbunden, mit Rohsignalen umgehen zu k√∂nnen, ohne dass manuelle Zeichen erstellt werden m√ºssen."  Meiner Meinung nach ist dies f√ºr Entwickler der Hauptmotivator aller vorhandenen. <br><br><h4>  Klassischer Ansatz </h4><br>  Beginnen wir also mit dem klassischen Ansatz.  Wenn wir √ºber das Verstehen des Sprechens mit einer Maschine sprechen, meinen wir, dass wir eine bestimmte Person haben, die einige Dienste mit ihrer Stimme steuern m√∂chte oder das Bed√ºrfnis hat, dass ein System auf seine Sprachbefehle mit einer gewissen Logik reagiert. <br><br>  Wie wird dieses Problem gel√∂st?  In der klassischen Version wird ein System verwendet, das, wie oben erw√§hnt, aus drei gro√üen Komponenten besteht: einer Spracherkennungskomponente, einer Komponente zum Verstehen einer nat√ºrlichen Sprache und einer Komponente, die f√ºr eine bestimmte Gesch√§ftslogik verantwortlich ist.  Es ist klar, dass der Benutzer zun√§chst ein bestimmtes Tonsignal erzeugt, das auf die Spracherkennungskomponente f√§llt und vom Ton zum Text wechselt.  Dann f√§llt der Text in die Komponente des Verstehens der nat√ºrlichen Sprache, aus der eine bestimmte semantische Struktur herausgezogen wird, die f√ºr die f√ºr die Gesch√§ftslogik verantwortliche Komponente erforderlich ist. <br><br><img src="https://habrastorage.org/webt/sl/s9/a1/sls9a1uzwmia7h523tecvgssiec.png"><br><br>  Was ist eine semantische Struktur?  Dies ist eine Art Verallgemeinerung / Aggregation mehrerer Aufgaben zu einer - zum leichteren Verst√§ndnis.  Die Struktur umfasst drei wichtige Teile: die Klassifizierung der Dom√§ne (eine bestimmte Definition des Themas), die Klassifizierung der Absicht (Verst√§ndnis, was zu tun ist) und die Zuweisung benannter Entit√§ten zum Ausf√ºllen von Karten, die f√ºr bestimmte Gesch√§ftsaufgaben in der n√§chsten Phase erforderlich sind.  Um zu verstehen, was eine semantische Struktur ist, k√∂nnen Sie ein einfaches Beispiel betrachten, das Google am h√§ufigsten zitiert.  Wir haben eine einfache Anfrage: "Bitte spielen Sie ein Lied eines K√ºnstlers." <br><br><img src="https://habrastorage.org/webt/nf/an/6l/nfan6l3vzzjsl_r4iq9_3x491rk.png"><br><br>  Die Dom√§ne und der Gegenstand dieser Anfrage ist Musik;  Absicht - ein Lied spielen;  Attribute der Karte ‚ÄûEin Lied spielen‚Äú - welche Art von Lied, welche Art von K√ºnstler.  Eine solche Struktur ist das Ergebnis des Verst√§ndnisses einer nat√ºrlichen Sprache. <br><br>  Wenn wir √ºber die L√∂sung eines komplexen und mehrstufigen Problems des Verstehens der Umgangssprache sprechen, besteht es, wie gesagt, aus zwei Stufen: Die erste ist die Spracherkennung, die zweite ist das Verstehen der nat√ºrlichen Sprache.  Der klassische Ansatz beinhaltet eine vollst√§ndige Trennung dieser Stufen.  In einem ersten Schritt haben wir ein bestimmtes Modell, das am Eingang ein akustisches Signal empf√§ngt und am Ausgang mithilfe von Sprach- und Akustikmodellen und einem Lexikon die wahrscheinlichste verbale Hypothese aus diesem akustischen Signal ermittelt.  Dies ist eine v√∂llig probabilistische Geschichte - sie kann gem√§√ü der bekannten Bayes-Formel zerlegt werden und eine Formel erhalten, mit der Sie die Wahrscheinlichkeitsfunktion der Stichprobe schreiben und die Maximum-Likelihood-Methode verwenden k√∂nnen.  Wir haben eine bedingte Wahrscheinlichkeit des Signals X, vorausgesetzt, die Wortfolge W wird mit der Wahrscheinlichkeit dieser Wortfolge multipliziert. <br><br><img src="https://habrastorage.org/webt/u0/pz/y8/u0pzy8cvkx_texgnjfzun-keavu.png"><br><br>  Die erste Phase, die wir durchlaufen haben - wir haben eine verbale Hypothese aus dem Tonsignal erhalten.  Als n√§chstes kommt die zweite Komponente, die diese sehr verbale Hypothese aufgreift und versucht, die oben beschriebene semantische Struktur herauszuholen. <br><br>  Wir haben die Wahrscheinlichkeit der semantischen Struktur S, vorausgesetzt, die verbale Sequenz W befindet sich am Eingang. <br><br><img src="https://habrastorage.org/webt/yi/34/tx/yi34txzqvgevrvoauj4stho32im.png"><br><br>  Was ist das Schlechte an dem klassischen Ansatz, der aus diesen beiden Elementen / Schritten besteht, die getrennt unterrichtet werden (d. H. Wir trainieren zuerst das Modell des ersten Elements und dann das Modell des zweiten Elements)? <br><br><ul><li>  Die Komponente zum Verst√§ndnis der nat√ºrlichen Sprache arbeitet mit den von ASR generierten verbalen Hypothesen auf hoher Ebene.  Dies ist ein gro√ües Problem, da die erste Komponente (ASR selbst) mit Rohdaten auf niedriger Ebene arbeitet und eine verbale Hypothese auf hoher Ebene generiert. Die zweite Komponente verwendet die Hypothese als Eingabe - nicht die Rohdaten aus der Prim√§rquelle, sondern die Hypothese, die das erste Modell liefert - und erstellt ihre Hypothese √ºber die Hypothese der ersten Stufe.  Dies ist eine ziemlich problematische Geschichte, weil sie zu "bedingt" wird. </li><li>  Das n√§chste Problem: Wir k√∂nnen keinen Zusammenhang zwischen der Bedeutung von W√∂rtern herstellen, die zum Aufbau der sehr semantischen Struktur erforderlich sind, und dem, was die erste Komponente bevorzugt, indem wir eine eigene verbale Hypothese aufbauen.  Das hei√üt, wenn Sie umformulieren, erhalten wir, dass die Hypothese bereits erstellt wurde.  Es basiert, wie gesagt, auf drei Komponenten: dem akustischen Teil (der in die Eingabe kam und irgendwie modelliert ist), dem Sprachteil (modelliert alle Sprachgramme vollst√§ndig - die Wahrscheinlichkeit der Sprache) und dem Lexikon (Aussprache von W√∂rtern).  Dies sind drei gro√üe Teile, die kombiniert werden m√ºssen, und einige darin enthaltene Hypothesen.  Es gibt jedoch keine M√∂glichkeit, die Wahl derselben Hypothese zu beeinflussen, so dass diese Hypothese f√ºr die n√§chste Stufe wichtig ist (was im Prinzip der Punkt ist, an dem sie vollst√§ndig getrennt lernen und sich in keiner Weise gegenseitig beeinflussen). </li></ul><br><h4>  End2End-Ansatz </h4><br>  Wir haben verstanden, was der klassische Ansatz ist, welche Probleme er hat.  Versuchen wir, diese Probleme mit dem End2End-Ansatz zu l√∂sen. <br><br>  Mit End2End meinen wir ein Modell, das die verschiedenen Komponenten zu einer einzigen Komponente kombiniert.  Wir werden mit Modellen modellieren, die aus einer Encoder-Decoder-Architektur bestehen, die Module der Aufmerksamkeit (Aufmerksamkeit) enth√§lt.  Solche Architekturen werden h√§ufig bei Spracherkennungsproblemen und bei Aufgaben im Zusammenhang mit der Verarbeitung einer nat√ºrlichen Sprache, insbesondere der maschinellen √úbersetzung, verwendet. <br><br>  Es gibt vier M√∂glichkeiten f√ºr die Implementierung solcher Ans√§tze, die das vor uns liegende Problem des klassischen Ansatzes l√∂sen k√∂nnten: direkte, kollaborative, mehrstufige und multitasking-Modelle. <br><br><h4>  Direktes Modell </h4><br>  Das direkte Modell √ºbernimmt die eingegebenen Rohattribute auf niedriger Ebene, d.h.  Low-Level-Audiosignal, und am Ausgang erhalten wir sofort eine semantische Struktur.  Das hei√üt, wir erhalten ein Modul - die Eingabe des ersten Moduls aus dem klassischen Ansatz und die Ausgabe des zweiten Moduls aus demselben klassischen Ansatz.  Nur so eine "Black Box".  Von hier aus gibt es einige Vor- und Nachteile.  Das Modell lernt nicht, das Eingangssignal vollst√§ndig zu transkribieren - dies ist ein klares Plus, da wir kein gro√ües, gro√ües Markup sammeln m√ºssen, nicht viel Audiosignal sammeln m√ºssen und es dann den Accessoren zum Markup geben m√ºssen.  Wir brauchen nur dieses Audiosignal und die entsprechende semantische Struktur.  Und alle.  Dies reduziert den Arbeitsaufwand f√ºr das Markieren von Daten um ein Vielfaches.  Das wahrscheinlich gr√∂√üte Minus dieses Ansatzes ist, dass die Aufgabe f√ºr eine solche "Black Box" zu kompliziert ist, die versucht, zwei Probleme sofort und bedingt zu l√∂sen.  Zuerst versucht er in sich selbst, eine Art Transkription aufzubauen, und dann enth√ºllt er aus dieser Transkription die sehr semantische Struktur.  Dies wirft eine ziemlich schwierige Aufgabe auf - zu lernen, Teile der Transkription zu ignorieren.  Und es ist sehr schwierig.  Dieser Faktor ist ein ziemlich gro√ües und kolossales Minus dieses Ansatzes. <br><br>  Wenn wir √ºber Wahrscheinlichkeiten sprechen, l√∂st dieses Modell das Problem, die wahrscheinlichste semantische Struktur S aus dem akustischen Signal X mit den Modellparametern Œ∏ zu finden. <br><br><img src="https://habrastorage.org/webt/34/kl/m2/34klm26dj3vk5sd9kdcxckkenii.png"><br><br><h4>  Gemeinsames Modell </h4><br>  Was ist die Alternative?  Dies ist ein kollaboratives Modell.  Das hei√üt, einige Modelle sind einer geraden Linie sehr √§hnlich, aber mit einer Ausnahme: Die Ausgabe besteht f√ºr uns bereits aus verbalen Sequenzen und eine semantische Struktur wird einfach mit ihnen verkn√ºpft.  Das hei√üt, am Eingang haben wir ein Tonsignal und ein neuronales Netzwerkmodell, das am Ausgang bereits sowohl verbale Transkription als auch semantische Struktur liefert. <br><br><img src="https://habrastorage.org/webt/jz/kn/-f/jzkn-frploycnewpluip2kgoqb8.png"><br><br>  Von den Profis: Wir haben immer noch einen einfachen Encoder, einen einfachen Decoder.  Das Lernen wird erleichtert, da das Modell nicht versucht, zwei Probleme gleichzeitig zu l√∂sen, wie im Fall des direkten Modells.  Ein weiterer Vorteil besteht darin, dass diese Abh√§ngigkeit der semantischen Struktur von Low-Level-Sound-Attributen weiterhin besteht.  Denn wieder ein Encoder, ein Decoder.  Dementsprechend kann eines der Pluspunkte festgestellt werden, dass die Vorhersage dieser sehr semantischen Struktur und ihres Einflusses direkt von der Transkription selbst abh√§ngt - was uns im klassischen Ansatz nicht zusagte. <br><br>  Wieder m√ºssen wir die wahrscheinlichste Folge von W√∂rtern W und die entsprechenden semantischen Strukturen S aus dem akustischen Signal X mit den Parametern Œ∏ finden. <br><br><h4>  Multitasking-Modell </h4><br>  Der n√§chste Ansatz ist ein Multitasking-Modell.  Wieder der Encoder-Decoder-Ansatz, aber mit einer Ausnahme. <br><br><img src="https://habrastorage.org/webt/ym/kz/l3/ymkzl3t_nh892ohttlu84sjg98i.png"><br><br>  F√ºr jede Aufgabe, dh zum Erstellen einer verbalen Sequenz, zum Erstellen einer semantischen Struktur, haben wir einen eigenen Decoder, der eine gemeinsame versteckte Darstellung verwendet, die einen einzelnen Encoder generiert.  Ein sehr ber√ºhmter Trick beim maschinellen Lernen, der sehr oft in der Arbeit angewendet wird.  Das gleichzeitige L√∂sen von zwei verschiedenen Problemen hilft dabei, Abh√§ngigkeiten in den Quelldaten viel besser zu suchen.  Und als Folge davon - die beste Verallgemeinerungsf√§higkeit, da der optimale Parameter f√ºr mehrere Aufgaben gleichzeitig ausgew√§hlt wird.  Dieser Ansatz eignet sich am besten f√ºr Aufgaben mit weniger Daten.  Und Decoder verwenden einen verborgenen Vektorraum, in den ihr Encoder erstellt. <br><br><img src="https://habrastorage.org/webt/8l/-q/pj/8l-qpjo3dccdzmqh5a-fspmveiq.png"><br><br>  Es ist wichtig zu beachten, dass bereits wahrscheinlich eine Abh√§ngigkeit von den Parametern der Codierer- und Decodierermodelle besteht.  Und diese Parameter sind wichtig. <br><br><h4>  Mehrstufiges Modell </h4><br>  Wir wenden uns meiner Meinung nach dem interessantesten Ansatz zu: einem mehrstufigen Modell.  Wenn Sie genau hinschauen, k√∂nnen Sie sehen, dass dies mit einer Ausnahme tats√§chlich der gleiche klassische Zweikomponentenansatz ist. <br><br><img src="https://habrastorage.org/webt/fr/az/np/fraznpocvjmklcjjmau4i9v9ago.png"><br><br>  Hier ist es m√∂glich, eine Verbindung zwischen den Modulen herzustellen und sie zu einem Modul zu machen.  Daher wird die semantische Struktur als bedingt abh√§ngig von der Transkription angesehen.  Es gibt zwei M√∂glichkeiten, mit diesem Modell zu arbeiten.  Wir k√∂nnen diese beiden Mini-Bl√∂cke einzeln trainieren: den ersten und den zweiten Encoder-Decoder.  Oder kombinieren Sie sie und trainieren Sie beide Aufgaben gleichzeitig. <br><br>  Im ersten Fall h√§ngen die Parameter f√ºr die beiden Aufgaben nicht zusammen (wir k√∂nnen mit unterschiedlichen Daten trainieren).  Angenommen, wir haben einen gro√üen Klangk√∂rper und die entsprechenden verbalen Sequenzen und Transkriptionen.  Wir "fahren" sie, wir trainieren nur den ersten Teil.  Wir bekommen eine gute Transkriptionssimulation.  Dann nehmen wir den zweiten Teil, wir trainieren an einem anderen Fall.  Wir verbinden uns und erhalten eine L√∂sung, die in diesem Ansatz zu 100% mit dem klassischen Ansatz √ºbereinstimmt, da wir den ersten Teil und den zweiten Teil getrennt genommen und trainiert haben.  Und dann trainieren wir das verbundene Modell f√ºr den Fall, der bereits drei Daten enth√§lt: ein Audiosignal, die entsprechende Transkription und die entsprechende semantische Struktur.  Wenn wir ein solches Geb√§ude haben, k√∂nnen wir das Modell, das individuell an gro√üen Geb√§uden trainiert wurde, f√ºr unsere spezifische kleine Aufgabe trainieren und auf solch knifflige Weise den maximalen Genauigkeitsgewinn erzielen.  Dieser Ansatz erm√∂glicht es uns, die Bedeutung verschiedener Teile der Transkription und ihren Einfluss auf die Vorhersage der semantischen Struktur <i>zu ber√ºcksichtigen,</i> indem <i>wir die Fehler der</i> zweiten Stufe in der ersten <i>ber√ºcksichtigen</i> . <br><br>  Es ist wichtig anzumerken, dass die endg√ºltige Aufgabe dem klassischen Ansatz mit nur einem gro√üen Unterschied sehr √§hnlich ist: dem zweiten Term unserer Funktion, dem Logarithmus der Wahrscheinlichkeit der semantischen Struktur, vorausgesetzt, das akustische Eingangssignal X h√§ngt auch von den Parametern des <i>Modells der ersten Stufe ab</i> . <br><br><img src="https://habrastorage.org/webt/pa/24/xr/pa24xr-aaep-mqo7bzd3loksve4.png"><br><br>  Hierbei ist auch zu beachten, dass die zweite Komponente von den Parametern des ersten und zweiten Modells abh√§ngt. <br><br><h4>  Methodik zur Bewertung der Genauigkeit von Ans√§tzen </h4><br>  Nun lohnt es sich, die Methode zur Bewertung der Genauigkeit festzulegen.  Wie kann man diese Genauigkeit tats√§chlich messen, um Merkmale zu ber√ºcksichtigen, die im klassischen Ansatz nicht zu uns passen?  Es gibt klassische Bezeichnungen f√ºr diese separaten Aufgaben.  Um Spracherkennungskomponenten zu bewerten, k√∂nnen wir die klassische WER-Metrik verwenden.  Dies ist eine Wortfehlerrate.  Wir betrachten nach einer nicht sehr komplizierten Formel die Anzahl der Einf√ºgungen, Substitutionen, Permutationen des Wortes und dividieren sie durch die Anzahl aller W√∂rter.  Und wir erhalten ein bestimmtes gesch√§tztes Merkmal f√ºr die Qualit√§t unserer Anerkennung.  F√ºr eine semantische Struktur k√∂nnen wir komponentenweise einfach die F1-Punktzahl betrachten.  Dies ist auch eine klassische Metrik f√ºr das Klassifizierungsproblem.  Hier ist alles Plus oder Minus klar.  Es gibt F√ºlle, es gibt Genauigkeit.  Und das ist nur ein harmonisches Mittel zwischen ihnen. <br><br>  Es stellt sich jedoch die Frage, wie die Genauigkeit gemessen werden kann, wenn die Eingabetranskription und das Ausgabeargument nicht √ºbereinstimmen oder wenn die Ausgabe Audiodaten sind.  Google hat eine Metrik vorgeschlagen, die die Bedeutung der Vorhersage der ersten Komponente der Spracherkennung ber√ºcksichtigt, indem die Auswirkung dieser Erkennung auf die zweite Komponente selbst bewertet wird.  Sie nannten es Arg WER, das hei√üt, es wiegt WER √ºber den semantischen Strukturentit√§ten. <br><br>  Nehmen Sie die Anfrage entgegen: "Stellen Sie den Alarm f√ºr 5 Stunden ein."  Diese semantische Struktur enth√§lt ein Argument wie "f√ºnf Stunden", ein Argument vom Typ "Datum Uhrzeit".  Es ist wichtig zu verstehen, dass, wenn die Spracherkennungskomponente dieses Argument erzeugt, die Fehlermetrik dieses Arguments, dh WER, 0% betr√§gt.  Wenn dieser Wert nicht f√ºnf Stunden entspricht, hat die Metrik 100% WER.  Daher betrachten wir einfach den gewichteten Durchschnittswert f√ºr alle Argumente und erhalten im Allgemeinen eine bestimmte aggregierte Metrik, die die Bedeutung von Transkriptionsfehlern sch√§tzt, die die Spracherkennungskomponente erzeugen. <br><br>  Lassen Sie mich ein Beispiel f√ºr die Experimente von Google geben, die es in einer seiner Studien zu diesem Thema durchgef√ºhrt hat.  Sie verwendeten Daten aus f√ºnf Dom√§nen, f√ºnf Themen: Medien, Media_Control, Produktivit√§t, Freude, Keine - mit der entsprechenden Verteilung von Daten auf Trainingstestdatens√§tzen.  Es ist wichtig zu beachten, dass alle Modelle von Grund auf neu trainiert wurden.  Cross_entropy wurde verwendet, der Strahlensuchparameter war 8, der Optimierer, den sie verwendeten, nat√ºrlich Adam.  Betrachtet nat√ºrlich auf einer gro√üen Wolke ihrer TPU.  Was ist das Ergebnis?  Das sind interessante Zahlen: <br><br><img src="https://habrastorage.org/webt/cj/bb/of/cjbbofaddfuhufwqk3brhr2-l04.png"><br><br>  Zum Verst√§ndnis ist Baseline ein klassischer Ansatz, der, wie eingangs erw√§hnt, aus zwei Komponenten besteht.  Im Folgenden finden Sie Beispiele f√ºr direkte, verbundene, Multitask- und mehrstufige Modelle. <br><br>  Was kosten zwei mehrstufige Modelle?  Gerade an der Verbindungsstelle des ersten und zweiten Teils wurden verschiedene Schichten verwendet.  Im ersten Fall ist dies ArgMax, im zweiten Fall SampedSoftmax. <br><br>  Worauf sollte man achten?  Der klassische Ansatz verliert in allen drei Metriken, die eine Sch√§tzung der direkten Zusammenarbeit dieser beiden Komponenten darstellen.  Ja, wir sind nicht daran interessiert, wie gut die Transkription dort durchgef√ºhrt wird, wir sind nur daran interessiert, wie gut das Element, das die semantische Struktur vorhersagt, funktioniert.  Es wird anhand von drei Metriken ausgewertet: F1 - nach Thema, F1 - nach Absicht und ArgWer-Metrik, die von den Argumenten von Entit√§ten ber√ºcksichtigt wird.  F1 wird als gewichteter Durchschnitt zwischen Genauigkeit und Vollst√§ndigkeit angesehen.  Das hei√üt, der Standard ist 100. ArgWer ist im Gegenteil kein Erfolg, es ist ein Fehler, das hei√üt, hier ist der Standard 0. <br><br>  Es ist erw√§hnenswert, dass unsere gekoppelten und Multitasking-Modelle alle Klassifizierungsmodelle f√ºr Themen und Absichten vollst√§ndig √ºbertreffen.  Und das mehrstufige Modell weist einen sehr starken Anstieg des gesamten ArgWer auf.  Warum ist das wichtig?  Denn bei den Aufgaben, die mit dem Verstehen der Umgangssprache verbunden sind, ist die letzte Aktion wichtig, die in der f√ºr die Gesch√§ftslogik verantwortlichen Komponente ausgef√ºhrt wird.  Dies h√§ngt nicht direkt von den von ASR erstellten Transkriptionen ab, sondern von der Qualit√§t der ASR- und NLU-Komponenten, die zusammenarbeiten.  Daher ist eine Differenz von fast drei Punkten in der argWER-Metrik ein sehr cooler Indikator, der den Erfolg dieses Ansatzes anzeigt.  Es ist auch erw√§hnenswert, dass alle Ans√§tze durch Definition von Themen und Absichten vergleichbare Werte haben. <br><br>  Ich werde einige Beispiele f√ºr die Verwendung solcher Algorithmen zum Verst√§ndnis der Konversationssprache geben.  Wenn Google √ºber die Aufgaben des Verstehens von Konversationssprache spricht, stellt es in erster Linie die Mensch-Computer-Schnittstellen fest, dh alle Arten von virtuellen Assistenten wie Google Assistant, Apple Siri, Amazon Alexa usw.  Als zweites Beispiel ist ein Aufgabenpool wie Interactive Voice Response zu erw√§hnen.  Das hei√üt, dies ist eine bestimmte Geschichte, die sich mit der Automatisierung von Call Centern befasst. <br><br>  Daher untersuchten wir Ans√§tze mit der M√∂glichkeit der gemeinsamen Optimierung, um das Modell auf Fehler zu konzentrieren, die f√ºr SLUs wichtiger sind.  Diese Herangehensweise an die Aufgabe, die gesprochene Sprache zu verstehen, vereinfacht die Gesamtkomplexit√§t erheblich. <br><br>  Wir haben die M√∂glichkeit, eine logische Schlussfolgerung zu ziehen, dh eine Art Ergebnis zu erzielen, ohne dass zus√§tzliche Ressourcen wie das Lexikon, Sprachmodelle, Analysatoren usw. erforderlich sind (d. H. Dies sind alles Faktoren, die dem klassischen Ansatz inh√§rent sind).  Die Aufgabe wird ‚Äûdirekt‚Äú gel√∂st. <br><br>  In der Tat kann man dort nicht aufh√∂ren.  Und wenn wir jetzt die beiden Ans√§tze, die beiden Komponenten einer gemeinsamen Struktur, kombiniert haben, k√∂nnen wir mehr anstreben.  Kombinieren Sie sowohl die drei als auch die vier Komponenten - kombinieren Sie diese logische Kette einfach weiter und setzen Sie die Wichtigkeit von Fehlern angesichts der bereits vorhandenen Kritikalit√§t auf ein niedrigeres Niveau durch.  Dadurch k√∂nnen wir die Genauigkeit der Probleml√∂sung erh√∂hen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de451008/">https://habr.com/ru/post/de451008/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de450996/index.html">Spiel-Frameworks: JavaScript-Trends im Jahr 2019</a></li>
<li><a href="../de450998/index.html">Eine kurze Geschichte der 3D-Texturierung in Spielen</a></li>
<li><a href="../de451002/index.html">Benutzerdefinierte Computerbaugruppe, Teil 1</a></li>
<li><a href="../de451004/index.html">Technosph√§re. Vorlesung ‚ÄûIT-Projekt- und Produktmanagement‚Äú</a></li>
<li><a href="../de451006/index.html">Die Zusammenfassung der Ereignisse f√ºr HR-Experten im Bereich IT f√ºr Mai 2019</a></li>
<li><a href="../de451010/index.html">Oh √§tzend und nicht sehr</a></li>
<li><a href="../de451012/index.html">Zuf√§llige Permutationen und zuf√§llige Partitionen</a></li>
<li><a href="../de451014/index.html">Ansturm, Drang oder Durchbruch? Wir sagen die ganze Wahrheit √ºber den gr√∂√üten Hackathon des Landes</a></li>
<li><a href="../de451018/index.html">Geh dorthin - ich wei√ü nicht wo</a></li>
<li><a href="../de451020/index.html">Die Geschichte einer MySQL-Optimierung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>