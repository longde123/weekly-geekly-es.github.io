<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéÜ üî≤ üë©üèª‚Äçüé§ Ich habe in zwei Wochen mein eigenes Dipfake erstellt und $ 552 ‚ÜîÔ∏è üßùüèæ üßì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Durch das Erstellen dieses Videos habe ich viel gelernt 

 Die Dipfake- Technologie verwendet tiefe neuronale Netze , um eine Person in Video √ºberzeug...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ich habe in zwei Wochen mein eigenes Dipfake erstellt und $ 552</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482684/"><h3>  Durch das Erstellen dieses Videos habe ich viel gelernt </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/N6y0CA2Mcx8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <a href="https://ru.wikipedia.org/wiki/Deepfake" rel="nofollow">Die Dipfake-</a> Technologie verwendet <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25BB%25D1%2583%25D0%25B1%25D0%25BE%25D0%25BA%25D0%25BE%25D0%25B5_%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5" rel="nofollow">tiefe neuronale Netze</a> , um eine Person in Video √ºberzeugend durch eine andere zu ersetzen.  Diese Technologie kann offensichtlich b√∂swillig eingesetzt werden und wird immer h√§ufiger eingesetzt.  In Bezug auf die sozialen und politischen Folgen dieses Trends wurden bereits viele gute Artikel verfasst. <br><br>  Und das ist keiner von ihnen.  Stattdessen werde ich mir diese Technologie genauer ansehen: Wie funktioniert Diphey-Software?  Wie schwierig ist es, sie zu erstellen, und wie gut sind die Ergebnisse? <br><br>  Ich entschied, dass es am besten ist, diese Fragen zu beantworten, indem ich mein eigenes Dip-Video erstellte.  Die Redaktion gab mir ein paar Tage Zeit, um mit der Software zu spielen, und 1000 Dollar, um f√ºr Cloud Computing zu bezahlen.  Nach ein paar Wochen habe ich das Ergebnis im Video am Anfang des Artikels pr√§sentiert bekommen.  Ich begann mit einem Video, in dem Mark Zuckerberg mit dem Kongress sprach, und ersetzte sein Gesicht durch Lieutenant Commander Data (Brent Spiner) von Star Trek: The Next Generation.  Insgesamt wurden 552 US-Dollar ausgegeben. <br><a name="habracut"></a><br>  Das Video war nicht perfekt.  Alle Details des Gesichts von Data werden nicht √ºbertragen, und wenn Sie genau hinsehen, sind Artefakte an den R√§ndern sichtbar. <br><br>  Es ist dennoch bemerkenswert, dass ein Neuling wie ich ein √ºberzeugendes Video erstellen kann, und zwar so schnell und kosteng√ºnstig.  Es gibt allen Grund zu der Annahme, dass die Dipfeyk-Technologie in den kommenden Jahren nur noch besser, schneller und billiger wird. <br><br>  In diesem Artikel werde ich Sie auf meinem falschen Weg an der Hand f√ºhren.  Ich werde jeden Schritt erkl√§ren, den Sie ausf√ºhren m√ºssen, um ein Deepfake-Video zu erstellen.  Unterwegs werde ich erkl√§ren, wie diese Technologie funktioniert und welche Einschr√§nkungen sie hat. <br><br><h2>  Dipfeyks ben√∂tigen viel Rechenleistung und Daten </h2><br>  Wir nennen diese Videos Diphakes, weil sie mit Hilfe von tiefen neuronalen Netzen erstellt wurden.  In den letzten zehn Jahren haben Informatiker festgestellt, dass neuronale Netze durch das Hinzuf√ºgen zus√§tzlicher Schichten von Neuronen immer leistungsf√§higer werden.  Um das volle Potenzial der tiefen neuronalen Netze auszusch√∂pfen, sind jedoch viele Daten und eine enorme Rechenleistung erforderlich. <br><br>  Gleiches gilt f√ºr Dipfakes.  F√ºr dieses Projekt habe ich eine virtuelle Maschine mit vier leistungsstarken Grafikkarten gemietet.  Und trotz all dieser Pferde habe ich fast eine Woche gebraucht, um mein Modell zu trainieren. <br><br>  Ich brauchte auch einen Berg von Bildern von Mark Zuckerberg und Data.  Ich habe ein Video mit einer L√§nge von 38 Sekunden bekommen, aber zum Training brauchte ich viel l√§ngere Videos, sowohl Zuckerberg als auch Data. <br><br>  Dazu habe ich eine Reihe von Videos mit ihren Gesichtern heruntergeladen: 14 Clips mit Clips von Star Trek und neun Clips mit Mark Zuckerberg.  Unter den letzteren befanden sich offizielle Berichte, mehrere Interviews im Fernsehen und sogar ein Video, in dem Zuckerberg in seinem Garten ein Barbecue vorbereitete. <br><br>  Ich habe all diese Clips auf iMovie hochgeladen und Frames gel√∂scht, die nicht die Gesichter von Zuckerberg und Data enthielten.  Ich habe auch die l√§ngsten Passagen in St√ºcke geschnitten.  Ein Dipfake-Programm ben√∂tigt nicht nur eine gro√üe Anzahl von Bildern, sondern auch eine gro√üe Anzahl verschiedener Bilder.  Wir brauchten Gesichter aus verschiedenen Blickwinkeln, mit unterschiedlichen Gesichtsausdr√ºcken und unterschiedlicher Beleuchtung.  Ein einst√ºndiges Video, in dem Zuckerberg den Bericht liest, kann nicht wertvoller sein als ein f√ºnfmin√ºtiges Segment, da es aus dem gleichen Winkel, im gleichen Licht und mit dem gleichen Gesichtsausdruck aufgenommen wird.  Also habe ich ein paar Stunden Video auf 9 Minuten mit Data und bis zu 7 Minuten mit Zuckerberg beschnitten. <br><br><h2>  Faceswap: ein Softwarepaket zum Erstellen von Dipfakes </h2><br>  Dann ist es Zeit, die Software f√ºr dipheyka zu verwenden.  Zuerst habe ich das Programm DeepFaceLab ausprobiert und konnte ein ziemlich grobes Video erstellen.  Dann habe ich im SFWdeepfakes-Forum um Rat gefragt, und dann haben mich ein paar Leute bei Faceswap beraten.  Die Leute stellten fest, dass dieses Programm mehr Funktionen, eine bessere Dokumentation und eine bessere Online-Unterst√ºtzung bietet.  Ich beschloss, ihrem Rat zu folgen. <br><br>  Faceswap l√§uft unter Linux, Windows und Mac.  Das Paket enth√§lt Tools, mit denen Sie in allen Phasen der Erstellung eines falschen Videos arbeiten k√∂nnen, vom Importieren der Originalvideos bis zum Erstellen eines fertigen falschen Videos.  Die Software ist nicht intuitiv zu bedienen, es wird jedoch ein detailliertes Schulungsmaterial mitgeliefert, das alle Schritte des Prozesses abdeckt.  Das Material wurde von Matt Torah, dem Erfinder von Faceswap, geschrieben, der mir auch beim Chatten auf Discords Deepfake-Kanal sehr geholfen hat. <br><br>  Faceswap ben√∂tigt eine leistungsstarke Grafikkarte.  Ich wusste, dass mein MacBook Pro damit nicht umgehen konnte.  Ich bat die Techniker unserer Redaktion, mir eine virtuelle Maschine f√ºr Linux von einem f√ºhrenden Anbieter von Cloud-Diensten zu leihen.  Ich habe mit einer virtuellen Maschine mit einer Nvidia K80-GPU und 12 GB Videospeicher begonnen.  Einige Tage sp√§ter wechselte ich zu einem Modell mit zwei GPUs und dann zu 4 GPUs.  Sie hatte vier Nvidia T4 Tensor Core-GPUs mit jeweils 16 GB Arbeitsspeicher (und weitere 48 CPUs und 192 RAM, die gr√∂√ütenteils im Leerlauf waren). <br><br>  Nach zwei Wochen Arbeit erhielt ich eine Rechnung √ºber 522 US-Dollar.  Klar, ich habe ziemlich viel f√ºr die Bequemlichkeit ausgegeben, einen Computer zu mieten.  Die Torah sagte mir, dass die derzeit rentabelste Hardwareoption f√ºr eine F√§lschung eine Nvidia GTX 1070- oder 1080-Karte mit 8 GB Speicher ist.  Eine solche gebrauchte Karte ist mehrere hundert Dollar wert.  Eine 1080-Karte lehrt ein neuronales Netzwerk nicht so schnell wie vier meiner GPUs. Wenn Sie jedoch bereit sind, einige Wochen zu warten, erhalten Sie √§hnliche Ergebnisse. <br><br>  Der Workflow in Faceswap besteht aus drei grundlegenden Schritten: <br><br><ul><li>  Extraktion: Schneiden Sie das Video in Frames, suchen Sie nach Gesichtern in jedem Frame und zeigen Sie gut ausgerichtete und sorgf√§ltig zugeschnittene Bilder von jedem Gesicht an. </li><li>  Training: Verwenden Sie die erhaltenen Bilder zum Training des vorget√§uschten neuronalen Netzwerks.  Es nimmt ein Bild des Gesichts einer Person auf und erzeugt ein Bild des Gesichts einer anderen Person mit demselben Ausdruck, derselben Beleuchtung und derselben Position. </li><li>  Transformation: Wenden Sie das im vorherigen Schritt geschulte Modell auf ein bestimmtes Video an, um eine Tauchf√§lschung zu erzielen.  Nach dem Training des Modells kann es auf jedes Video angewendet werden, in dem sich Personen befinden, auf deren Gesicht es trainiert wurde. </li></ul><br>  F√ºr jeden der drei Schritte wird von der Person und der Maschine eine v√∂llig andere Zeit ben√∂tigt.  Die Bildwiederherstellungssoftware l√§uft einige Minuten, es kann jedoch Stunden dauern, bis eine Person die Ergebnisse √ºberpr√ºft hat.  Die Software merkt sich alle Gesichter in jedem Bild sowie eine ganze Reihe von Fehlalarmen.  Um gute Ergebnisse zu erzielen, muss eine Person alle Ergebnisse durchgehen und unn√∂tige Gesichter und alles, was die Software f√ºr eine Person ben√∂tigte, entfernen. <br><br>  Das Lernen ist einfach einzurichten und erfordert praktisch keine menschliche Beteiligung.  Es kann jedoch Tage oder sogar Wochen dauern, bis gute Ergebnisse erzielt werden.  Ich fing am 7. Dezember an, mein letztes Modell zu trainieren, und es funktionierte bis zum 13. Dezember.  Es ist m√∂glich, dass sich nach einer weiteren Arbeitswoche die Qualit√§t meines Dipfakes verbessert.  Au√üerdem habe ich mein Cloud-Monster mit vier fortschrittlichen Grafikkarten verwendet.  Wenn Sie an Ihrem Computer mit einer einzelnen GPU mit geringerer Leistung arbeiten, kann es viele Wochen dauern, bis ein gutes Modell trainiert ist. <br><br>  Der letzte Schritt, die Transformation, ist sowohl f√ºr eine Person als auch f√ºr einen Computer schnell.  Mit einem entsprechend geschulten Modell k√∂nnen Sie in weniger als einer Minute gef√§lschte Videos liefern. <br><br><h2>  Wie funktionieren Diphakes? </h2><br>  Bevor Sie den Faceswap-Lernprozess beschreiben, m√ºssen Sie erkl√§ren, wie die zugrunde liegende Technologie funktioniert. <br><br>  Das Herzst√ºck von Faceswap - und anderen f√ºhrenden Software-Paketen zur Erstellung von Diphakes - ist der Auto-Encoder.  Dies ist ein neuronales Netzwerk, das darauf trainiert ist, ein Eingabebild zu empfangen und ein identisches Bild zu erzeugen.  Diese Fertigkeit an sich mag nicht so n√ºtzlich sein, aber wie wir sp√§ter sehen werden, ist sie ein Schl√ºsselbaustein bei der Erstellung eines Dipfakes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/63f/e44/76b/63fe4476bd395c863cd0c9b71bf4cfbf.jpg"><br><br>  Der Auto-Encoder ist nach dem Prinzip zweier durch ein schmales Ende verbundener Trichter aufgebaut.  Auf einer Seite des Netzwerks befindet sich ein Encoder, der ein Bild empf√§ngt und auf eine kleine Anzahl von Variablen komprimiert.  In dem Modell, das ich in Faceswap verwendet habe, sind dies 1024 32-Bit-Gleitkommazahlen.  Auf der anderen Seite des neuronalen Netzwerks befindet sich ein Decoder.  Er nimmt diese kompakte Darstellung, die als "latenter Raum" bekannt ist, und versucht sie zu erweitern, nachdem er das urspr√ºngliche Bild erhalten hat. <br><br>  Durch die k√ºnstliche Begrenzung der vom Codierer zum Decodierer √ºbertragenen Datenmenge entwickeln diese beiden Netzwerke eine kompakte Darstellung des menschlichen Gesichts.  Ein Encoder ist so etwas wie ein verlustbehafteter Komprimierungsalgorithmus, der versucht, so viele Informationen √ºber ein Gesicht wie m√∂glich zu speichern und gleichzeitig die Speicherkapazit√§t zu begrenzen.  Der latente Raum muss irgendwie wichtige Details extrahieren, zum Beispiel, in welche Richtung das Subjekt schaut, seine Augen offen oder geschlossen sind, er l√§chelt oder die Stirn runzelt. <br><br>  Es ist wichtig, dass der Auto-Encoder nur die Merkmale des Gesichts speichert, die sich mit der Zeit √§ndern.  Er muss keine unver√§nderten Dinge wie Augenfarbe oder Nasenform speichern.  Wenn er auf jedem Foto von Zuckerberg blaue Augen hat, lernt sein Netzwerkdecoder, sein Gesicht automatisch mit blauen Augen zu zeichnen.  Es ist nicht erforderlich, Informationen in einen engen latenten Raum zu stopfen, der sich beim √úbergang von einem Bild zum anderen nicht √§ndert.  Wie wir sp√§ter sehen werden, ist die Tatsache, dass Auto-Encoder unterschiedliche Einstellungen zu konstanten und sich √§ndernden Gesichtsmerkmalen haben, √§u√üerst wichtig f√ºr ihre F√§higkeit, Diphfakes auszugeben. <br><br>  Jeder Algorithmus zum Trainieren eines neuronalen Netzwerks ben√∂tigt eine M√∂glichkeit, die Qualit√§t des Netzwerks zu bewerten, damit es verbessert werden kann.  In vielen F√§llen erfolgt dies durch Training mit dem Lehrer, wenn die Person die richtige Antwort f√ºr jedes Element aus dem Trainingsdatensatz bereitstellt.  Auto-Encoder arbeiten anders.  Da sie lediglich versuchen, ihre eigenen Eingabedaten zu reproduzieren, kann die Trainingssoftware ihre Arbeitsqualit√§t automatisch beurteilen.  Im Fachjargon des maschinellen Lernens wird dies als Lernen ohne Lehrer bezeichnet. <br><br>  Wie jedes neuronale Netzwerk werden Autoencoder in Faceswap mit Backpropagation trainiert.  Der Trainingsalgorithmus speist ein bestimmtes Bild in das neuronale Netzwerk ein und √ºberpr√ºft, welche Pixel in der Ausgabe nicht mit der Eingabe √ºbereinstimmen.  Dann berechnet er, welches der Neuronen der letzten Schicht den gr√∂√üten Beitrag zu den Fehlern geleistet hat, und korrigiert die Parameter jedes Neurons geringf√ºgig, um bessere Ergebnisse zu erzielen. <br><br>  Dann breiten sich diese Fehler zur√ºck zur vorherigen Schicht aus, wo die Parameter jedes Neurons erneut korrigiert werden.  Fehler breiten sich auf diese Weise weiter aus, bis alle Parameter des neuronalen Netzwerks - sowohl der Codierer als auch der Decodierer - korrigiert sind. <br><br>  Dann liefert der Trainingsalgorithmus ein weiteres Bild des Netzwerks, und der gesamte Vorgang wird erneut wiederholt.  Hunderttausende solcher Wiederholungen sind m√∂glicherweise erforderlich, um einen automatischen Codierer zu erstellen, der seine eigene Eingabe gut reproduziert. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/335/661/bb1/335661bb13300b80164987a1aa1dd420.jpg"><br><br>  Die Dipfake-Software trainiert gleichzeitig zwei Auto-Encoder, einen f√ºr das urspr√ºngliche und einen f√ºr das neue Gesicht.  W√§hrend des Trainingsprozesses erh√§lt jeder Auto-Encoder Bilder von nur einer Person, und er wird darauf trainiert, Bilder zu erzeugen, die dem Original sehr √§hnlich sind. <br><br>  Es gibt jedoch einen Haken: Beide Netzwerke verwenden denselben Encoder.  Decoder - Neuronen auf der rechten Seite des Netzwerks - bleiben getrennt, und jeder von ihnen ist darauf trainiert, ein anderes Gesicht zu geben.  Die Neuronen auf der linken Seite des Netzwerks haben jedoch gemeinsame Parameter, die sich jedes Mal √§ndern, wenn einer der Autocodierer trainiert wird.  Wenn das Zuckerberg-Netzwerk auf dem Zuckerberg-Gesicht trainiert wird, √§ndert dies die H√§lfte des Netzwerks, das zum Encoder und zum Netzwerk f√ºr Daten geh√∂rt.  Jedes Mal, wenn das Netzwerk von Data auf das Gesicht von Data trainiert wird, √ºbernimmt der Zuckerberg-Encoder diese √Ñnderungen. <br><br>  Infolgedessen verf√ºgen zwei Auto-Encoder √ºber einen gemeinsamen Encoder, der entweder das Gesicht von Zuckerberg oder das von Data ‚Äûlesen‚Äú kann.  Der Encoder dient dazu, die gleiche Darstellung von Dingen wie den Winkel des Kopfes oder die Position der Augenbrauen zu verwenden, unabh√§ngig davon, ob er am Eingang ein Foto von Zuckerberg oder ein Foto von Data erhalten hat.  Und dies bedeutet wiederum, dass Sie Ihr Gesicht, wenn Sie es mit dem Encoder zusammengedr√ºckt haben, mit jedem Decoder auspacken k√∂nnen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ce8/0b6/670/ce80b6670b49be4b905f9ce520810931.jpg"><br><br>  Nachdem Sie also ein paar Auto-Encoder auf diese Weise trainiert haben, bleibt ein einfacher Schritt zum Erstellen eines Dip-Fake: Sie tauschen Decoder aus.  Sie codieren ein Zuckerberg-Foto, verwenden jedoch im Decodierungsschritt den Datendecoder.  Das Ergebnis ist ein rekonstruiertes Foto von Data - jedoch mit derselben Kopfposition und demselben Gesichtsausdruck wie das Originalfoto von Zuckerberg. <br><br>  Denken Sie daran, ich erw√§hnte, dass der latente Raum die unterschiedlichen Gesichtsz√ºge einer Person erfasst - Ausdruck, Blickrichtung, Position der Augenbrauen - und solche konstanten Dinge wie die Farbe der Augen oder die Form des Mundes, die dem Decoder gegeben sind.  Dies bedeutet, dass Sie, wenn Sie das Zuckerberg-Bild codieren und anschlie√üend mit dem Datendecoder decodieren, ein Gesicht mit permanenten Datenmerkmalen erhalten, z. B. einer Gesichtsform, jedoch mit dem Ausdruck und der Ausrichtung des urspr√ºnglichen Zuckerberg-Gesichts. <br><br>  Wenn Sie diese Technik auf aufeinanderfolgende Einzelbilder eines Videos mit Zuckerberg anwenden, erhalten Sie ein neues Video, in dem Datas Gesicht dieselben Bewegungen ausf√ºhrt - l√§chelt, blinkt, dreht den Kopf -, die Zuckerberg im Originalvideo ausgef√ºhrt hat. <br><br>  Diese Situation ist symmetrisch.  Wenn Sie ein neuronales Netzwerk trainieren, um ein Foto von Zuckerberg zu erhalten und ein Foto von Data zu erstellen, trainieren Sie es gleichzeitig, um ein Foto von Data zu erhalten und ein Foto von Zuckerberg zu erstellen.  Das Tool zum Konvertieren von Videos aus Faceswap - der letzte Schritt bei der Erstellung eines Dipfake - enth√§lt ein n√ºtzliches Kontrollk√§stchen zum Austauschen von Modellen, mit dem der Benutzer Decoder austauschen kann.  Infolgedessen ersetzt das Programm nicht das Gesicht von Data, sondern das Gesicht von Zuckerberg, und erzeugt so sehr lustige Videos wie diese: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Fi7sj7SU1Vg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Trainingsdaten </h2><br>  In der Praxis ist es nicht einfach, gute Ergebnisse beim Erstellen eines Dipfakes zu erzielen. <br><br>  Wie gesagt, ich habe sieben Minuten Video f√ºr Data und neun Minuten f√ºr Zuckerberg bekommen.  Dann habe ich das Faceswap-Tool zum Extrahieren von Bildern verwendet, um das Video auszuschneiden und zugeschnittene Bilder der Gesichter beider M√§nner zu erhalten.  Das Video enth√§lt ungef√§hr 30 Bilder pro Sekunde, aber ich habe nur alle sechs extrahiert - diese Vorgehensweise wird in der Faceswap-Dokumentation empfohlen.  Dies liegt daran, dass eine Vielzahl von Bildern mehr bedeutet als nur ihre Anzahl, und das Speichern jedes Einzelbilds w√ºrde zu einer gro√üen Anzahl sehr √§hnlicher Bilder f√ºhren. <br><br>  Das Faceswap-Extraktionswerkzeug erzeugte eine ganze Reihe von Fehlalarmen.  Er fand auch echte Gesichter im Hintergrund einiger Aufnahmen.  Einige Stunden lang l√∂schte ich manuell alle extrahierten Fotos, die keinem meiner beiden Versuchstiere geh√∂rten.  Als Ergebnis erhielt ich 2598 Bilder von Datas Gesicht und 2224 Bilder von Zuckerbergs Gesicht. <br><br>  Und in diesem Moment war es endlich an der Zeit, zu einem echten Modelltraining √ºberzugehen.  Jetzt wird Faceswap mit 10 verschiedenen Dipfake-Algorithmen ausgeliefert, die unterschiedliche Bildgr√∂√üen unterst√ºtzen und unterschiedliche Rechenleistungen erfordern.  Zu den unpr√§tenti√∂sesten geh√∂rt ein ‚Äûleichtes‚Äú Modell, das mit Gesichtsbildern mit einer Gr√∂√üe von nicht mehr als 64 Pixeln arbeitet.  Es kann auf einem Computer mit nicht mehr als 2 GB Videospeicher ausgef√ºhrt werden.  Andere Modelle arbeiten mit Bildern mit einer Gr√∂√üe von 128, 256 oder sogar 512 Pixeln. Sie ben√∂tigen jedoch viel mehr Videospeicher sowie mehr Einarbeitungszeit. <br><br>  Ich habe angefangen, das DFL-SAE-Modell zu trainieren, das von den Algorithmen von DeepFaceLab abgeleitet wurde.  In der Faceswap-Dokumentation wurde jedoch gewarnt, dass dieses Modell unter einem ‚ÄûIdentit√§tsleck‚Äú leidet, bei dem einige Merkmale eines Gesichts in ein anderes eindringen k√∂nnen.  Ich hatte den Eindruck, dass ich so etwas in ein paar ersten Testvideos gesehen habe. Einen Tag sp√§ter wechselte ich zum Villain-Modell, das mit 128-Pixel-Bildern arbeitet.  Das Faceswap-Handbuch beschreibt es als sehr anspruchsvoll f√ºr VRAM und als "eine gute Wahl f√ºr diejenigen, die ein Modell mit h√∂herer Aufl√∂sung erhalten m√∂chten, ohne irgendwelche Parameter anzupassen". <br><br>  Dann habe ich gewartet.  Und er wartete.  Der Lernprozess war noch nicht zu Ende, als meine Deadline am Freitag kam - und das nach sechs Tagen Training.  Zu dieser Zeit hat mein Modell ein ziemlich gutes Dipfake produziert.  Die Geschwindigkeit des Fortschritts verlangsamte sich, aber es ist m√∂glich, dass ich ein besseres Ergebnis erzielt h√§tte, wenn ich noch eine Woche Computerzeit gehabt h√§tte. <br><br>  Faceswap eignet sich gut f√ºr lange Computerarbeiten.  Wenn Sie das Trainingsteam √ºber die grafische Oberfl√§che starten, aktualisiert die Programmoberfl√§che regelm√§√üig den Vorschaubildschirm, auf dem Sie Beispiele daf√ºr sehen k√∂nnen, wie die Software Portr√§ts von Data und Zuckerberg erstellt.  Wenn Sie das Training lieber von der Kommandozeile aus durchf√ºhren m√∂chten, ist dies ebenfalls m√∂glich.  Die Faceswap-Oberfl√§che verf√ºgt √ºber eine n√ºtzliche Schaltfl√§che zum Generieren, die den genauen Befehl angibt, den Sie zum Trainieren des Modells mit den aktuellen Einstellungen in der Oberfl√§che ausf√ºhren m√ºssen. <br><br><h2>  Wie gut war das Dipfake? </h2><br>  W√§hrend des Lernprozesses zeigt Faceswap st√§ndig eine numerische Sch√§tzung des ‚ÄûVerlusts‚Äú f√ºr jeden der beiden Auto-Encoder an.  Diese Sch√§tzungen zeigen, wie gut der Auto-Encoder von Zuckerberg die Fotos von Zuckerberg wiedergeben kann - und wie gut der Auto-Encoder von Data die Fotos von Data wiedergeben kann.  Und diese Zahlen gingen immer weiter zur√ºck, als ich am Freitag aufh√∂rte zu lernen, obwohl sich die Geschwindigkeit der Fortschritte erheblich verlangsamte. <br><br>  In der Tat ist es nat√ºrlich wichtig f√ºr uns, wie gut der Decoder von Data Zuckerbergs Gesicht in das von Data verwandeln kann.  Wir wissen nicht, wie das "Endergebnis" aussehen soll, daher ist es unm√∂glich, die Qualit√§t der Arbeit in genauen Zahlen zu messen.  Das Beste, was wir tun k√∂nnen, ist, das Video zu √ºberpr√ºfen und zu entscheiden, ob es realistisch aussieht. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/zZEi1lHTdpY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das obige Video zeigt die Qualit√§t des Dipfakes in den vier Phasen des Lernprozesses. Die Videos vom 10. und 12. Dezember zeigen das teilweise trainierte Villain-Modell. Das Video oben links vom 6. Dezember ist ein fr√ºher Test mit einem anderen Modell. Unten rechts ist das Endergebnis. W√§hrend des Trainings wurden die Details seines Gesichts klarer und glaubw√ºrdiger. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Am 9. Dezember ver√∂ffentlichte ich nach drei Trainingstagen ein vorl√§ufiges Video im internen Kanal der Redaktion in Slak. Das Video √§hnelte dem in der oberen linken Ecke. Unser Designguru Aurich Lawson reagierte sarkastisch auf ihn. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄûIm Allgemeinen sieht es schlecht aus‚Äú, schrieb er und f√ºgte hinzu, dass es ‚Äûnicht √ºberzeugend aussieht. Ich warte auf eines dieser Videos, die nicht falsch aussehen. "</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich denke, dass es in seiner Kritik einen rationalen Kern gibt. Ich war √ºberrascht, wie schnell Faceswap Bilder von Gesichtern erstellen konnte, die eher Brent Spiner als Zuckerberg √§hnelten. Wenn Sie jedoch genau hinsehen, werden Sie die charakteristischen Anzeichen von digitalem Betrug erkennen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In einigen Bildern sieht die Grenze zwischen Datas falschem Gesicht und Zuckerbergs Kopf nicht ganz richtig aus. Manchmal sp√§ht Zuckerbergs Augenbraue unter Datas Gesicht hervor. An anderen Stellen sind die R√§nder des falschen Gesichts mit einigen Pixeln auf Zuckerbergs Ohren bedeckt. Es kann m√∂glich sein, diese Probleme mit der Komposition bei der manuellen Nachbearbeitung durch eine Person zu beheben. Jemand muss jedoch das Video Bild f√ºr Bild scrollen und die Maske f√ºr jedes Bild anpassen.</font></font><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/d8XknbnMs4c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein grundlegenderes Problem ist jedoch, dass die Diphfake-Algorithmen noch nicht in der Lage sind, die kleinsten Details menschlicher Gesichter gut genug zu reproduzieren. Dies ist ziemlich offensichtlich, wenn Sie sich die Start- und Endvideos parallel ansehen. Faceswap zeigte √ºberraschend gut die Gesamtstruktur des Gesichts von Data. Aber auch nach einer Woche Training sieht das Gesicht verschwommen aus und es sind nicht gen√ºgend wichtige Details enthalten. Zum Beispiel kann Software f√ºr Dipheyka das Zeichnen menschlicher Z√§hne kaum bew√§ltigen. Manchmal werden die Z√§hne deutlich sichtbar, und im n√§chsten Bild verschwinden sie und hinterlassen Schw√§rze.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Einer der Hauptgr√ºnde daf√ºr ist, dass die Faceswap-Aufgabe mit h√∂heren Aufl√∂sungen exponentiell komplizierter wird. Auto-Encoder leisten gute Arbeit mit 64x64 Pixel-Bildern. Die feineren Details von 128x128 Pixel-Bildern zu reproduzieren - ganz zu schweigen von Bildern mit 256 Pixeln oder mehr - ist jedoch schon viel schwieriger. Vielleicht ist dies einer der Gr√ºnde, warum die beeindruckendsten Diphfes einen ziemlich weiten Blickwinkel haben, ohne Nahaufnahmen von Gesichtern. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie sollten dies jedoch nicht als grundlegende Einschr√§nkung der Diphake-Technologie betrachten. In den kommenden Jahren k√∂nnen Forscher m√∂glicherweise Technologien entwickeln, mit denen diese Einschr√§nkungen √ºberwunden werden k√∂nnen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Oft wird die Basis von Software f√ºr ein Dipheyka f√§lschlicherweise als generativ-adversariales Netzwerk (GSS) oder solche neuronalen Netzwerke beschrieben, die es der Software erm√∂glichen, "darzustellen"</font></font><a href="https://thispersondoesnotexist.com/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nicht existierende Personen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , Gegenst√§nde oder Landschaften. </font><font style="vertical-align: inherit;">In der Tat arbeiten Dipfeyki mit Autoencodern. </font><font style="vertical-align: inherit;">Die j√ºngsten Fortschritte in der GSS-Technologie lassen jedoch vermuten, dass bei den Tauchf√§lschungen noch Verbesserungspotenzial besteht. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GSS, das erstmals im Jahr 2014 erschien, konnte nur grobe Bilder mit niedriger Aufl√∂sung produzieren. </font><font style="vertical-align: inherit;">Vor kurzem haben Forscher </font></font><a href="https://arxiv.org/abs/1710.10196" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">herausgefunden</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , wie ein GSS erstellt werden kann, das fotorealistische Bilder mit einer Gr√∂√üe von bis zu 1024 Pixel erzeugt. </font><font style="vertical-align: inherit;">Die spezifischen Techniken, die in diesen wissenschaftlichen Arbeiten verwendet werden, sind m√∂glicherweise nicht f√ºr die Erstellung eines Diphakes anwendbar, aber es ist leicht vorstellbar, wie jemand eine √§hnliche Technologie f√ºr Autocodierer entwickeln wird - oder vielleicht eine v√∂llig neue neuronale Netzwerkarchitektur, die Gesichter ersetzen soll.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Dipfake-Perspektive </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Anstieg der Popularit√§t von Dipfakes ist offensichtlich alarmierend. Bis vor kurzem konnten Leute ein Video mit einer Person zum Nennwert leicht machen. Das Aufkommen der Dipheyka-Software und anderer digitaler Tools hat uns jetzt skeptisch gegen√ºber Videos gemacht. Wenn wir ein Video sehen, in dem eine Person etwas Skandal√∂ses behauptet oder sich auszieht, sollten wir die M√∂glichkeit in Betracht ziehen, dass jemand dieses Video gef√§lscht hat, um diese Person zu diskreditieren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mein Experiment betont jedoch die Grenzen der Dipfake-Technologie - zumindest in ihrer gegenw√§rtigen Form. Um ein vollst√§ndig √ºberzeugendes virtuelles Gesicht zu erstellen, sind umfangreiche Kenntnisse und Anstrengungen erforderlich. Es gelang mir nicht und ich bin mir nicht sicher, ob jemand in der Lage war, ein gef√§lschtes Video zu produzieren, das wirklich nicht von dem echten zu unterscheiden ist.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dar√ºber hinaus befassen sich Tools wie Faceswap heute nur noch mit Gesichtsver√§nderungen. Sie ver√§ndern nicht Stirn, Haare, Arme und Beine. Und selbst wenn das Gesicht perfekt ist, ist es m√∂glich, das gef√§lschte Video anhand von Elementen zu bestimmen, die nicht richtig aussehen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diese Einschr√§nkungen der Dipfake-Technologie k√∂nnen jedoch verschwinden. In einigen Jahren kann die Software lernen, Videos zu produzieren, die nicht von den realen zu unterscheiden sind. Was dann?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Fall ist es hilfreich, sich daran zu erinnern, dass andere Medientypen lange Zeit leicht zu f√§lschen waren. </font><font style="vertical-align: inherit;">Die einfache Aufgabe w√§re, einen Screenshot einer E-Mail zu erstellen, in der jemand etwas schreibt, das er nicht geschrieben hat. </font><font style="vertical-align: inherit;">Und dies f√ºhrte weder zu einem Anstieg der Anzahl kaputter Steinbr√ºche aufgrund betr√ºgerischer E-Mails noch zu einer Diskreditierung von Screenshots von Briefen als Beweismittel f√ºr √∂ffentliche Diskussionen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Leute wissen jedoch, dass E-Mails gef√§lscht werden k√∂nnen, und suchen in solchen F√§llen nach einer zus√§tzlichen Best√§tigung. </font><font style="vertical-align: inherit;">Welche Ereigniskette hat die Aufmerksamkeit der √ñffentlichkeit auf die Briefe gelenkt? </font><font style="vertical-align: inherit;">Haben andere Personen Kopien dieser E-Mail erhalten, als sie geschrieben werden sollte? </font><font style="vertical-align: inherit;">Hat der mutma√üliche Verfasser des Schreibens seine Urheberschaft anerkannt oder behauptet er F√§lschungen? </font><font style="vertical-align: inherit;">Antworten auf solche Fragen helfen den Menschen zu entscheiden, wie ernst sie einen ver√∂ffentlichten Brief nehmen k√∂nnen.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sie k√∂nnen einmal get√§uscht werden </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So ist es auch mit Videos. </font><font style="vertical-align: inherit;">Vielleicht wird es eine kurze Zeitspanne geben, in der Betr√ºger die Karriere einer Person zerst√∂ren k√∂nnen, indem sie ein Video ver√∂ffentlichen, in dem sie etwas Unversch√§mtes sagt oder tut. </font><font style="vertical-align: inherit;">Aber bald wird die Gesellschaft lernen, Videos mit Skepsis zu behandeln, es sei denn, der Videoclip enth√§lt irgendwelche dokumentarischen Beweise, Zeugen oder andere unterst√ºtzende Faktoren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich denke, dass dies auch bei den unversch√§mtesten Missbr√§uchen der Diphey-Technologie funktioniert: Einf√ºgen des Gesichts einer Person in ein pornografisches Video. </font><font style="vertical-align: inherit;">Dies ist offensichtlich respektlos und inakzeptabel. </font><font style="vertical-align: inherit;">Aber die Leute bef√ºrchten, dass solche Videos ihren Ruf und ihre Karriere zerst√∂ren k√∂nnen. </font><font style="vertical-align: inherit;">Ich denke das ist nicht so.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tats√§chlich finden Sie im Internet vollst√§ndige Bilder ber√ºhmter Pers√∂nlichkeiten (haupts√§chlich Frauen), deren K√∂pfe mithilfe von Photoshop an den K√∂rpern von Pornostars befestigt sind. Das Leiden der Frauen ist verst√§ndlich. Die √ñffentlichkeit kommt jedoch nicht automatisch zu dem Schluss, dass diese Frauen nackt posierten - wir kennen die Existenz von Photoshop und die M√∂glichkeit, gef√§lschte Fotos zu erstellen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gleiches gilt f√ºr Deep Pornography. Nat√ºrlich ist es nicht gut, mit Ihrer Teilnahme gef√§lschte Pornos zu machen. Aber die Ver√∂ffentlichung eines gef√§lschten Videos mit einer Person hat keine so verheerende Wirkung wie ein echtes Sex-Video. Wenn keine Beweise f√ºr die Echtheit des Videos vorliegen, wird die √ñffentlichkeit zu dem Schluss kommen, dass es sich um eine F√§lschung handelt.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Matt Torah, der Autor von Faceswap, sagt mir, dass diese √úberlegung eine der Komponenten f√ºr seine Motivation war, das Paket zu erstellen. Er glaubt, dass zwangsl√§ufig Software f√ºr den Wandel der Menschen entwickelt wird. Er hofft, dass er durch die Entwicklung eines benutzerfreundlichen Tools f√ºr den Austausch von Open Source-Anwendern dazu beitr√§gt, die Geheimhaltung mit dieser Technologie zu beseitigen und die √ñffentlichkeit √ºber ihre F√§higkeiten und Einschr√§nkungen zu informieren. Dies wird uns wiederum dabei helfen, schnell zu einem Punkt zu gelangen, an dem die √ñffentlichkeit Videos gegen√ºber skeptisch ist, die sich als F√§lschung herausstellen k√∂nnten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Langfristig besteht die Gefahr, dass das Pendel der √ñffentlichkeitsarbeit zu stark in die andere Richtung schwingt und die M√∂glichkeit, dass es zu falschen Ergebnissen kommt, den Glauben an die Beweiskraft von Videos zunichte macht. </font><font style="vertical-align: inherit;">Einige Politiker haben es sich bereits zur Gewohnheit gemacht, Medienkritik als "falsche Nachricht" abzulehnen. </font><font style="vertical-align: inherit;">Diese Taktik wird mit zunehmendem Bewusstsein der Gesellschaft f√ºr die Technologie der Tauchf√§lschungen effektiver.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de482684/">https://habr.com/ru/post/de482684/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de482674/index.html">‚ÄûUnmittelbar nach den Ferien‚Äú: Seminare, Meisterkurse und Technologiewettbewerbe an der ITMO University</a></li>
<li><a href="../de482676/index.html">Effektive Sortierung von Daten vom Typ Struct</a></li>
<li><a href="../de482678/index.html">Wie ich mich f√ºr eine SMS-Suche f√ºr iOS entschieden habe und was dabei herauskam</a></li>
<li><a href="../de482680/index.html">Crypt, XOR, Hacken von unverschl√ºsseltem ZIP und PRSP. Probleml√∂sung mit r0ot-mi Crypto. Teil 2</a></li>
<li><a href="../de482682/index.html">Qt Tool Automation</a></li>
<li><a href="../de482686/index.html">Wissenschaftler automatisieren Tierverhaltensforschung, um die Gehirnfunktion zu entschl√ºsseln</a></li>
<li><a href="../de482690/index.html">Erstellen Sie Ihren Publisher in Combine</a></li>
<li><a href="../de482692/index.html">Intel NUC Home f√ºr Arbeit und Virtualisierung</a></li>
<li><a href="../de482694/index.html">Gibt es ein Leben nach Windows oder wo kann man 2020 einen Windows-Systemadministrator / -ingenieur entwickeln?</a></li>
<li><a href="../de482696/index.html">Vollst√§ndige Festplattenverschl√ºsselung von Windows Linux-Systemen. Verschl√ºsselter Multiboot</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>