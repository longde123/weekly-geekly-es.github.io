<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¤™ğŸ½ ğŸ›¤ï¸ â™Ÿï¸ Masalah Bandit Multi-Senjata - Bandingkan Strategi Epsilon-Greedy dan Thompson Sampling ğŸ¤²ğŸ¼ ğŸ‘ ğŸ‘¨â€ğŸ­</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! Saya mempersembahkan bagi Anda halaman depan Solving bandit multiarmed: Sebuah perbandingan artikel sampel epsilon-serakah dan Thompson . ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Masalah Bandit Multi-Senjata - Bandingkan Strategi Epsilon-Greedy dan Thompson Sampling</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425619/">  <i>Halo, Habr!</i>  <i>Saya mempersembahkan bagi Anda halaman depan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Solving bandit multiarmed: Sebuah perbandingan</a> artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sampel epsilon-serakah dan Thompson</a> .</i> <br><br><h1>  Masalah bandit multi-bersenjata </h1><br><p>  Masalah bandit multi-bersenjata adalah salah satu tugas paling dasar dalam ilmu solusi.  Yaitu, ini adalah masalah alokasi sumber daya yang optimal dalam kondisi ketidakpastian.  Nama "multi-bersenjata bandit" sendiri berasal dari mesin slot lama yang dikendalikan oleh pegangan.  Senapan serbu ini dijuluki "bandit," karena setelah berbicara dengan mereka, orang biasanya merasa dirampok.  Sekarang bayangkan ada beberapa mesin seperti itu dan peluang untuk menang melawan mobil yang berbeda berbeda.  Karena kami mulai bermain dengan mesin ini, kami ingin menentukan peluang mana yang lebih tinggi dan menggunakan mesin ini lebih sering daripada yang lain. </p><br><p>  Masalahnya adalah ini: bagaimana kita paling efisien memahami mesin mana yang paling cocok, dan pada saat yang sama mencoba banyak fitur secara real time?  Ini bukan semacam masalah teoretis, ini adalah masalah yang dihadapi bisnis setiap saat.  Misalnya, perusahaan memiliki beberapa opsi untuk pesan yang perlu diperlihatkan kepada pengguna (misalnya, pesan menyertakan iklan, situs, gambar) sehingga pesan yang dipilih memaksimalkan tugas bisnis tertentu (konversi, klik, dll.) </p><br><a name="habracut"></a><p>  Cara khas untuk mengatasi masalah ini adalah dengan menjalankan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tes A / B</a> berkali-kali.  Yaitu, selama beberapa minggu untuk menunjukkan masing-masing opsi secara merata, dan kemudian, berdasarkan uji statistik, tentukan pilihan mana yang lebih baik.  Metode ini cocok ketika ada beberapa opsi, katakanlah, 2 atau 4. Tetapi ketika ada banyak opsi, pendekatan ini menjadi tidak efektif - baik dalam waktu yang hilang maupun laba yang hilang. </p><br><p>  Dari mana waktu yang hilang berasal harus mudah dimengerti.  Lebih banyak opsi - lebih banyak tes A / B dibutuhkan - lebih banyak waktu diperlukan untuk membuat keputusan.  Terjadinya laba yang hilang tidak begitu jelas.  Kehilangan peluang (opportunity cost) - biaya yang terkait dengan fakta bahwa alih-alih satu tindakan kami melakukan yang lain, yaitu, secara sederhana, inilah yang kami hilangkan dengan berinvestasi di A, bukannya B. Berinvestasi dalam B adalah laba yang hilang dari investasi di A. Sama dengan opsi memeriksa.  Tes A / B tidak boleh diganggu sampai selesai.  Ini berarti bahwa eksperimen tidak tahu opsi mana yang lebih baik sampai pengujian selesai.  Namun, masih diyakini bahwa satu opsi akan lebih baik daripada yang lain.  Ini berarti bahwa dengan memperpanjang tes A / B, kami tidak menunjukkan opsi terbaik untuk jumlah pengunjung yang cukup besar (meskipun kami tidak tahu opsi mana yang bukan yang terbaik), sehingga kehilangan keuntungan kami.  Ini adalah manfaat yang hilang dari pengujian A / B.  Jika hanya ada satu tes A / B, maka mungkin untung yang hilang sama sekali tidak bagus.  Sejumlah besar tes A / B berarti bahwa untuk waktu yang lama kami harus menunjukkan kepada pelanggan banyak pilihan bukan yang terbaik.  Akan lebih baik jika Anda dapat dengan cepat membuang opsi yang buruk secara real time, dan hanya kemudian, ketika ada beberapa pilihan yang tersisa, gunakan tes A / B untuk mereka. </p><br><p>  Pengambil sampel atau agen adalah cara untuk menguji dan mengoptimalkan distribusi opsi dengan cepat.  Pada artikel ini, saya akan memperkenalkan Anda pada <i>pengambilan sampel Thompson</i> dan propertinya.  Saya juga akan membandingkan pengambilan sampel Thompson dengan algoritma epsilon-serakah, pilihan populer lainnya untuk masalah bandit multi-bersenjata.  Semuanya akan diimplementasikan dalam Python dari awal - semua kode dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . </p><br><h2>  Kamus Singkat Konsep </h2><br><p></p><ul><li>  Agent, sampler, bandit ( <i>agent, sampler, bandit</i> ) - sebuah algoritma yang membuat keputusan tentang opsi mana yang akan ditampilkan. </li><li>  Varian - varian berbeda dari pesan yang dilihat pengunjung. </li><li>  Tindakan - tindakan yang dipilih algoritma (opsi untuk ditampilkan). </li><li>  Gunakan ( <i>exploit</i> ) - buat pilihan untuk memaksimalkan total hadiah berdasarkan data yang tersedia. </li><li>  Jelajahi, <i>jelajahi</i> - buat pilihan untuk lebih memahami pengembalian untuk setiap opsi. </li><li>  Penghargaan, poin ( <i>skor, hadiah</i> ) - tugas bisnis, misalnya konversi atau clickability.  Untuk kesederhanaan, kami percaya bahwa itu didistribusikan secara biner dan sama dengan 1 atau 0 - diklik atau tidak. </li><li>  Lingkungan - konteks di mana agen beroperasi - opsi dan "pengembalian" tersembunyi untuk pengguna. </li><li>  Payback, probabilitas keberhasilan ( <i>payout rate</i> ) - variabel tersembunyi sama dengan probabilitas mendapatkan skor = 1, untuk setiap opsi berbeda.  Tetapi pengguna tidak melihatnya. </li><li>  Coba ( <i>uji coba</i> ) - pengguna mengunjungi halaman. </li><li>  Penyesalan adalah perbedaan antara apa yang akan menjadi hasil terbaik dari semua opsi yang tersedia dan apa hasil dari opsi yang tersedia dalam upaya saat ini.  Semakin menyesali tindakan yang sudah dilakukan, semakin baik. </li><li>  Pesan ( <i>pesan</i> ) - spanduk, opsi halaman, dan banyak lagi, versi berbeda yang ingin kami coba. </li><li>  Sampling - pembuatan sampel dari distribusi yang diberikan. </li></ul><br><h2>  Jelajahi dan eksploitasi </h2><br><p>  Agen adalah algoritma yang mencari pendekatan untuk pengambilan keputusan real-time untuk mencapai keseimbangan antara menjelajahi ruang opsi dan menggunakan opsi terbaik.  Keseimbangan ini sangat penting.  Ruang opsi harus diselidiki untuk memiliki gagasan tentang opsi mana yang terbaik.  Jika kami pertama kali menemukan opsi yang paling optimal ini, dan kemudian menggunakannya sepanjang waktu, kami akan memaksimalkan total hadiah yang tersedia bagi kami dari lingkungan.  Di sisi lain, kami juga ingin menjelajahi opsi lain yang mungkin - bagaimana jika mereka akan berubah menjadi lebih baik di masa depan, tetapi kami belum tahu?  Dengan kata lain, kami ingin memastikan kemungkinan kerugian, mencoba bereksperimen sedikit dengan opsi suboptimal untuk mengklarifikasi sendiri pengembalian mereka.  Jika pengembalian mereka sebenarnya lebih tinggi, mereka dapat ditampilkan lebih sering.  Kelebihan lainnya dari meneliti opsi adalah bahwa kita dapat lebih memahami tidak hanya pengembalian rata-rata, tetapi juga bagaimana kira-kira pembayaran dikembalikan, yaitu kita dapat memperkirakan ketidakpastian dengan lebih baik. <br>  Masalah utama, oleh karena itu, adalah untuk memecahkan - apa jalan keluar terbaik dari dilema antara eksplorasi dan eksploitasi (tradeoff eksplorasi-eksploitasi). </p><br><h2>  Algoritma Epsilon-serakah </h2><br><p>  Jalan keluar khas dari dilema ini adalah algoritma epsilon-serakah.  "Serakah" berarti apa yang Anda pikirkan.  Setelah beberapa periode awal, ketika kita secara tidak sengaja melakukan upaya - katakan, 1000 kali, algoritme dengan penuh semangat memilih opsi terbaik k dalam setiap persen upaya.  Misalnya, jika <i>e</i> = 0,05, algoritma 95% dari waktu memilih opsi terbaik, dan sisanya 5% dari waktu itu memilih upaya acak.  Sebenarnya, ini adalah algoritma yang agak efektif, namun, mungkin tidak cukup untuk menjelajahi ruang pilihan, dan oleh karena itu, tidak akan cukup baik untuk mengevaluasi opsi mana yang terbaik, untuk terjebak pada opsi yang kurang optimal.  Mari kita tunjukkan dalam kode bagaimana algoritma ini bekerja. </p><br><p>  Tapi pertama-tama, beberapa ketergantungan.  Kita harus mendefinisikan lingkungan.  Ini adalah konteks di mana algoritma akan berjalan.  Dalam hal ini, konteksnya sangat sederhana.  Dia memanggil agen sehingga agen memutuskan tindakan mana yang harus dipilih, kemudian konteks meluncurkan tindakan ini dan mengembalikan poin yang diterima untuk kembali ke agen (yang entah bagaimana memperbarui keadaannya). </p><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Environment</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, variants, payouts, n_trials, variance=False)</span></span></span><span class="hljs-function">:</span></span> self.variants = variants <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> variance: self.payouts = np.clip(payouts + np.random.normal(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.04</span></span>, size=len(variants)), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">.2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: self.payouts = payouts <span class="hljs-comment"><span class="hljs-comment">#self.payouts[5] = self.payouts[5] if i &lt; n_trials/2 else 0.1 self.n_trials = n_trials self.total_reward = 0 self.n_k = len(variants) self.shape = (self.n_k, n_trials) def run(self, agent): """Run the simulation with the agent. agent must be a class with choose_k and update methods.""" for i in range(self.n_trials): # agent makes a choice x_chosen = agent.choose_k() # Environment returns reward reward = np.random.binomial(1, p=self.payouts[x_chosen]) # agent learns of reward agent.reward = reward # agent updates parameters based on the data agent.update() self.total_reward += reward agent.collect_data() return self.total_reward</span></span></code> </pre> <br>  Poin didistribusikan secara biner dengan probabilitas p tergantung pada jumlah tindakan (sama seperti mereka dapat didistribusikan terus menerus, esensi tidak akan berubah).  Saya juga akan mendefinisikan kelas BaseSampler - diperlukan hanya untuk menyimpan log dan berbagai atribut. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">BaseSampler</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_samples=None, n_learning=None, e=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.05</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.env = env self.shape = (env.n_k, n_samples) self.variants = env.variants self.n_trials = env.n_trials self.payouts = env.payouts self.ad_i = np.zeros(env.n_trials) self.r_i = np.zeros(env.n_trials) self.thetas = np.zeros(self.n_trials) self.regret_i = np.zeros(env.n_trials) self.thetaregret = np.zeros(self.n_trials) self.a = np.ones(env.n_k) self.b = np.ones(env.n_k) self.theta = np.zeros(env.n_k) self.data = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.total_reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.k = <span class="hljs-number"><span class="hljs-number">0</span></span> self.i = <span class="hljs-number"><span class="hljs-number">0</span></span> self.n_samples = n_samples self.n_learning = n_learning self.e = e self.ep = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, size=env.n_trials) self.exploit = (<span class="hljs-number"><span class="hljs-number">1</span></span> - e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">collect_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.data = pd.DataFrame(dict(ad=self.ad_i, reward=self.r_i, regret=self.regret_i))</code> </pre> <br>  Di bawah ini kami menetapkan 10 opsi dan pembayaran untuk masing-masing.  Opsi terbaik adalah opsi 9 dengan pengembalian 0,11%. <br><br><pre> <code class="python hljs">variants = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>] payouts = [<span class="hljs-number"><span class="hljs-number">0.023</span></span>, <span class="hljs-number"><span class="hljs-number">0.03</span></span>, <span class="hljs-number"><span class="hljs-number">0.029</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.06</span></span>, <span class="hljs-number"><span class="hljs-number">0.0234</span></span>, <span class="hljs-number"><span class="hljs-number">0.035</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.11</span></span>]</code> </pre> <br>  Untuk membangun sesuatu, kami juga mendefinisikan kelas RandomSampler.  Kelas ini diperlukan sebagai model dasar.  Dia hanya secara acak memilih opsi pada setiap upaya dan tidak memperbarui parameternya. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RandomSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.k = np.random.choice(self.variants) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.k <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># nothing to update #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.thetaregret) - self.theta[self.k] #self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.theta) - self.theta[self.k] self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><p>  Model-model lain memiliki struktur sebagai berikut.  Semua memiliki select_k dan memperbarui metode.  select_k mengimplementasikan metode dimana agen memilih opsi.  perbarui memperbarui parameter agen - metode ini mencirikan bagaimana kemampuan agen untuk memilih perubahan opsi (dengan RandomSampler kemampuan ini tidak berubah dengan cara apa pun).  Kami menjalankan agen di lingkungan menggunakan pola berikut. </p><br><pre> <code class="python hljs">en0 = Environment(machines, payouts, n_trials=<span class="hljs-number"><span class="hljs-number">10000</span></span>) rs = RandomSampler(env=en0) en0.run(agent=rs)</code> </pre> <br><p>  Inti dari algoritma epsilon-serakah adalah sebagai berikut. <br><br></p><ol><li>  Pilih secara acak k untuk n percobaan. </li><li>  Pada setiap percobaan, untuk setiap opsi, evaluasi keuntungan. </li><li>  Setelah semua n upaya: </li><li>  Dengan probabilitas 1 - <i>e</i> pilih k dengan gain tertinggi; </li><li>  Dengan probabilitas <i>e</i> pilih K secara acak. </li></ol><br>  Kode Epsilon-serakah: <br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">eGreedy</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_learning, e)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env, n_learning, e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># e% of the time take a random draw from machines # random k for n learning trials, then the machine with highest theta self.k = np.random.choice(self.variants) if self.i &lt; self.n_learning else np.argmax(self.theta) # with 1 - e probability take a random sample (explore) otherwise exploit self.k = np.random.choice(self.variants) if self.ep[self.i] &gt; self.exploit else self.k return self.k # every 100 trials update the successes # update the count of successes for the chosen machine def update(self): # update the probability of payout for each machine self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b #self.total_reward += self.reward #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] #self.thetaregret[self.i] = self.thetaregret[self.i] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><br><p>  Di bawah ini pada grafik, Anda dapat melihat hasil dari sampel acak murni, yaitu, dengan kata lain, tidak ada model di sini.  Grafik menunjukkan pilihan algoritma yang dibuat pada setiap upaya, jika ada 10 ribu upaya.  Algoritma hanya mencoba, tetapi tidak belajar.  Secara total, ia mencetak 418 poin. <br> <a href=""><img src="https://habrastorage.org/webt/sn/ql/2r/snql2roqbdiruuskxsathithz8i.jpeg"></a> </p><br><p>  Mari kita lihat bagaimana algoritma epsilon-serakah berperilaku di lingkungan yang sama.  Jalankan algoritme untuk 10 ribu upaya dengan <i>e</i> = 0,1 dan n_learning = 500 (agen cukup mencoba 500 upaya pertama, kemudian mencoba dengan probabilitas <i>e</i> = 0,1).  Mari kita evaluasi algoritme berdasarkan jumlah total poin yang dihasilkannya dalam lingkungan. </p><br><pre> <code class="python hljs">en1 = Environment(machines, payouts, n_trials) eg = eGreedy(env=en1, n_learning=<span class="hljs-number"><span class="hljs-number">500</span></span>, e=<span class="hljs-number"><span class="hljs-number">0.1</span></span>) en1.run(agent=eg)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/-f/zf/14/-fzf14djbqtdl5-0vyrapuhcp5c.jpeg"></a> <br><p>  Algoritma Epsilon-serakah mencetak 788 poin, hampir 2 kali lebih baik daripada algoritma acak - super!  Grafik kedua menjelaskan algoritma ini dengan cukup baik.  Kita melihat bahwa untuk 500 langkah pertama aksi didistribusikan secara merata dan K dipilih secara acak.  Namun, kemudian mulai sangat mengeksploitasi opsi 5 - ini adalah opsi yang cukup kuat, tetapi bukan yang terbaik.  Kami juga melihat bahwa agen masih secara acak memilih 10% dari waktu. </p><br><p>  Ini cukup keren - kami hanya menulis beberapa baris kode, dan sekarang kami sudah memiliki algoritma yang cukup kuat yang dapat menjelajahi ruang pilihan dan membuat keputusan yang mendekati optimal.  Di sisi lain, algoritma tidak menemukan opsi terbaik.  Ya, kami dapat meningkatkan jumlah langkah untuk belajar, tetapi dengan cara ini kami akan menghabiskan lebih banyak waktu untuk pencarian acak, semakin memperburuk hasil akhir.  Juga, keacakan dijahit ke dalam proses ini secara default - algoritma terbaik mungkin tidak ditemukan. </p><br><p>  Nanti saya akan menjalankan masing-masing algoritma berkali-kali sehingga kita dapat membandingkannya relatif satu sama lain.  Tetapi untuk sekarang, mari kita lihat pengambilan sampel Thompson dan mengujinya di lingkungan yang sama. </p><br><h2>  Sampling Thompson </h2><br><p>  Sampling Thompson pada dasarnya berbeda dari algoritma epsilon-serakah oleh tiga poin utama: <br><br></p><ol><li>  Itu tidak serakah. </li><li>  Itu membuat upaya dengan cara yang lebih canggih. </li><li>  Itu adalah Bayesian. </li></ol><br>  Poin utamanya adalah paragraf 3, paragraf 1 dan 2 ikuti darinya. <br><p>  Inti dari algoritma ini adalah: <br><br></p><ol><li>  Tetapkan distribusi beta awal antara 0 dan 1 untuk pengembalian setiap opsi. </li><li>  Cicipi pilihan dari distribusi ini, pilih parameter Theta maksimum. </li><li>  Pilih opsi k yang dikaitkan dengan theta terbesar. </li><li>  Lihat berapa banyak poin yang telah dicetak, perbarui parameter distribusi. </li></ol><br>  Baca lebih lanjut tentang distribusi beta di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br>  Dan tentang penggunaannya dalam Python - di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><p>  Kode Algoritma: <br><br></p><pre> <code class="python hljs"> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ThompsonSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># sample from posterior (this is the thompson sampling approach) # this leads to more exploration because machines with &gt; uncertainty can then be selected as the machine self.theta = np.random.beta(self.a, self.b) # select machine with highest posterior p of payout self.k = self.variants[np.argmax(self.theta)] #self.k = np.argmax(self.a/(self.a + self.b)) return self.k def update(self): #update dist (a, b) = (a, b) + (r, 1 - r) self.a[self.k] += self.reward self.b[self.k] += 1 - self.reward # ie only increment b when it's a swing and a miss. 1 - 0 = 1, 1 - 1 = 0 #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br>  Notasi formal dari algoritma terlihat seperti ini. <br> <a href=""><img src="https://habrastorage.org/webt/5f/n6/xe/5fn6xew2i7v1jjh_10h9jqkjdzu.png"></a> <br><p>  Mari kita programkan algoritma ini.  Seperti agen lainnya, ThompsonSampler mewarisi dari BaseSampler dan menentukan select_k dan memperbarui metode sendiri.  Sekarang luncurkan agen baru kami. </p><br><pre> <code class="python hljs"> en2 = Environment(machines, payouts, n_trials) tsa = ThompsonSampler(env=en2) en2.run(agent=tsa)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/ml/kj/1p/mlkj1pvs8xnpxtkqmehmm_3xhgo.jpeg"></a> <br><p>  Seperti yang Anda lihat, dia mencetak lebih dari algoritma epsilon-serakah.  Hebat!  Mari kita lihat grafik pemilihan usaha.  Dua hal menarik terlihat di sana.  Pertama, agen dengan benar menemukan opsi terbaik (opsi 9) dan menggunakannya secara maksimal.  Kedua, agen menggunakan opsi lain, tetapi dengan cara yang lebih sulit - setelah sekitar 1000 upaya, agen, selain opsi utama, terutama menggunakan opsi yang paling kuat di antara yang lain.  Dengan kata lain, dia tidak memilih secara acak, tetapi lebih kompeten. </p><br><p>  Mengapa ini bekerja?  Sederhana - ketidakpastian dalam distribusi posterior dari manfaat yang diharapkan untuk setiap opsi berarti bahwa setiap opsi dipilih dengan probabilitas yang kira-kira sebanding dengan bentuknya, ditentukan oleh parameter alfa dan beta.  Dengan kata lain, pada setiap percobaan, pengambilan sampel Thompson memicu pilihan sesuai dengan probabilitas posterior bahwa itu memiliki manfaat maksimal.  Secara kasar, setelah dari informasi distribusi tentang ketidakpastian, agen memutuskan kapan untuk memeriksa lingkungan dan kapan menggunakan informasi tersebut.  Sebagai contoh, opsi lemah dengan ketidakpastian posterior tinggi mungkin membayar paling untuk upaya ini.  Tetapi untuk sebagian besar upaya, semakin kuat distribusi posteriornya, semakin besar rata-rata dan semakin sedikit standar deviasinya, dan karenanya, semakin besar peluang untuk memilihnya. </p><br><p>  Properti luar biasa lainnya dari algoritma Thompson: karena ini adalah Bayesian, kita dapat memperkirakan ketidakpastian dalam estimasi pengembalian untuk setiap opsi menggunakan parameternya.  Grafik di bawah ini menunjukkan distribusi posterior pada 6 titik berbeda dan dalam 20.000 upaya.  Anda melihat bagaimana distribusi secara bertahap mulai menyatu dengan opsi dengan pengembalian terbaik. </p><br> <a href=""><img src="https://habrastorage.org/webt/bb/ka/fb/bbkafb4nv1pajwkygxy2brtmowy.jpeg"></a> <br><p>  Sekarang bandingkan ketiga agen dalam 100 simulasi.  1 simulasi adalah peluncuran agen pada 10.000 upaya. </p><br> <a href=""><img src="https://habrastorage.org/webt/j6/v1/sm/j6v1smcrwkwyhlo27ffhly13pwk.jpeg"></a> <br><p>  Seperti yang dapat Anda lihat dari grafik, baik strategi epsilon-serakah dan pengambilan sampel Thompson jauh lebih baik daripada pengambilan sampel acak.  Anda mungkin terkejut bahwa strategi epsilon-serakah dan pengambilan sampel Thompson sebenarnya sebanding dalam hal kinerja mereka.  Strategi Epsilon-serakah bisa sangat efektif, tetapi lebih berisiko, karena bisa terjebak pada opsi suboptimal - ini dapat dilihat pada kegagalan dalam grafik.  Tapi pengambilan sampel Thompson tidak bisa, karena itu membuat pilihan dalam ruang pilihan dengan cara yang lebih kompleks. </p><br><h2>  Menyesal </h2><br><p>  Cara lain untuk mengevaluasi seberapa baik algoritma bekerja adalah untuk mengevaluasi penyesalan.  Secara kasar, semakin kecil, dalam kaitannya dengan tindakan yang telah diambil, semakin baik.  Di bawah ini adalah grafik dari total penyesalan dan penyesalan atas kesalahan.  Sekali lagi - semakin sedikit penyesalan, semakin baik. </p><br> <a href=""><img src="https://habrastorage.org/webt/8p/kd/o3/8pkdo3bilrde28bwsimdnbesqwg.jpeg"></a> <br><p>  Pada grafik atas, kita melihat penyesalan total, dan pada penyesalan yang lebih rendah upaya.  Seperti dapat dilihat dari grafik, sampling Thompson konvergen ke penyesalan minimal jauh lebih cepat daripada strategi epsilon-serakah.  Dan itu konvergen ke level yang lebih rendah.  Dengan pengambilan sampel Thompson, agen tersebut menyesal lebih sedikit karena ia dapat lebih baik mendeteksi opsi terbaik dan mencoba opsi yang paling menjanjikan dengan lebih baik - sehingga pengambilan sampel Thompson sangat cocok untuk kasus penggunaan yang lebih maju, seperti model statistik atau jaringan saraf untuk memilih k. </p><br><h2>  Kesimpulan </h2><br><p>  Ini adalah posting teknis yang cukup panjang.  Sebagai rangkuman, kita dapat menggunakan metode pengambilan sampel yang cukup canggih jika kita memiliki banyak opsi yang ingin kita uji secara real time.  Salah satu fitur yang sangat baik dari pengambilan sampel Thompson adalah menyeimbangkan penggunaan dan eksplorasi dengan cara yang agak rumit.  Artinya, kita bisa membiarkannya mengoptimalkan distribusi opsi solusi secara real time.  Ini adalah algoritma yang keren, dan harus lebih berguna untuk bisnis daripada tes A / B. </p><br><p>  <b>Penting!</b>  <b>Sampling Thompson tidak berarti Anda tidak perlu melakukan tes A / B.</b>  <b>Biasanya, mereka pertama kali menemukan opsi terbaik dengan bantuannya, dan kemudian melakukan tes A / B pada mereka.</b> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id425619/">https://habr.com/ru/post/id425619/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id425605/index.html">Penerimaan pembayaran dari kartu tanpa jur. wajah di Yandex.Money</a></li>
<li><a href="../id425607/index.html">Identifikasi penipuan menggunakan set data Enron. Bagian 2, menemukan model terbaik</a></li>
<li><a href="../id425609/index.html">Game Theory: Pengambilan Keputusan dengan Contoh di Kotlin</a></li>
<li><a href="../id425611/index.html">Arsitektur frontend tingkat atas. Kuliah Yandex</a></li>
<li><a href="../id425613/index.html">Bagaimana saya menggabungkan data plugin Tempo untuk Jira Server dan Jira Cloud dan memigrasikannya kembali ke Jira Cloud</a></li>
<li><a href="../id425621/index.html">Perusahaan yang memanfaatkan karbon dioksida atmosfer meluncurkan produksi metana</a></li>
<li><a href="../id425623/index.html">Tur foto Coworking â€œKunciâ€</a></li>
<li><a href="../id425625/index.html">Menghabiskan, atau Mengapa pelokalan menerjemahkan game dengan buruk</a></li>
<li><a href="../id425627/index.html">IaaS untuk mengembangkan layanan: siapa dan mengapa beralih ke infrastruktur virtual</a></li>
<li><a href="../id425629/index.html">Bagaimana kami membuat permainan papan dengan remote control</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>