<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äçü§ù‚Äçüë©üèæ ü§Ø üßëüèø‚Äçü§ù‚Äçüßëüèø KVM (sous) VDI avec des machines virtuelles uniques utilisant bash ‚öôÔ∏è üêÜ üï¥üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√Ä qui s'adresse cet article? 
 Cet article peut int√©resser les administrateurs syst√®me qui ont √©t√© confront√©s √† la t√¢che de cr√©er un service de t√¢ches...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>KVM (sous) VDI avec des machines virtuelles uniques utilisant bash</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462203/"><h4>  √Ä qui s'adresse cet article? </h4><br>  Cet article peut int√©resser les administrateurs syst√®me qui ont √©t√© confront√©s √† la t√¢che de cr√©er un service de t√¢ches ¬´ponctuel¬ª. <br><br><h4>  Prologue </h4><br>  Le d√©partement de support informatique d'une jeune entreprise en d√©veloppement dynamique avec un petit r√©seau r√©gional a √©t√© charg√© d'organiser des ¬´stations libre-service¬ª √† l'usage de leurs clients externes.  Les donn√©es de la station √©taient cens√©es √™tre utilis√©es pour l'enregistrement sur les portails externes de l'entreprise, le t√©l√©chargement de donn√©es √† partir d'appareils externes et la collaboration avec les portails gouvernementaux. <br><br>  Un aspect important a √©t√© le fait que la plupart des logiciels sont ¬´affin√©s¬ª pour MS Windows (par exemple, ¬´D√©claration¬ª), et malgr√© l'√©volution vers des formats ouverts, MS Office reste la norme dominante dans l'√©change de documents √©lectroniques.  Ainsi, nous ne pouvions pas refuser MS Windows lors de la r√©solution de ce probl√®me. <br><a name="habracut"></a><br>  Le principal probl√®me √©tait la possibilit√© d'accumuler diverses donn√©es des sessions des utilisateurs, ce qui pouvait entra√Æner leur fuite √† des tiers.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Cette situation a d√©j√† laiss√© tomber le MFC</a> .  Mais contrairement au MFC quasi judiciaire (institution autonome de l'√âtat), les organisations non √©tatiques seront beaucoup plus punies pour de telles lacunes.  Le prochain probl√®me critique √©tait la n√©cessit√© de travailler avec des supports de stockage externes, sur lesquels, par tous les moyens, il y aura un tas de logiciels malveillants.  La probabilit√© d'entr√©e de logiciels malveillants √† partir d'Internet a √©t√© jug√©e moins probable en raison de la restriction de l'acc√®s √† Internet via une liste blanche d'adresses. Les employ√©s d'autres d√©partements se sont associ√©s pour d√©finir les exigences, formuler leurs exigences et leurs souhaits, les exigences finales √©taient les suivantes: <br><br>  <b>Exigences SI</b> <br><br><ul><li>  Apr√®s utilisation, toutes les donn√©es utilisateur (y compris les fichiers temporaires et les cl√©s de registre) doivent √™tre supprim√©es. </li><li>  Tous les processus lanc√©s par l'utilisateur doivent √™tre termin√©s √† la fin du travail. </li><li>  Acc√®s √† Internet via une liste blanche d'adresses. </li><li>  Restrictions sur la possibilit√© d'ex√©cuter du code tiers. </li><li>  Si la session est inactive pendant plus de 5 minutes, la session doit se terminer automatiquement, la station doit effectuer un nettoyage. </li></ul><br>  <b>Besoins du client</b> <br><br><ul><li>  Le nombre de postes clients par succursale ne d√©passe pas 4. </li><li>  Le temps d'attente minimum pour la pr√©paration du syst√®me, √† partir du moment o√π je me suis ¬´assis sur une chaise¬ª jusqu'au d√©but du travail avec le logiciel client. </li><li>  La possibilit√© de connecter des p√©riph√©riques (scanners, lecteurs flash) directement depuis le site d'installation de la ¬´station libre-service¬ª. </li><li>  Souhaits du client </li><li>  D√©monstration de mat√©riel publicitaire (photos) lors de la fermeture du complexe. </li></ul><cut></cut><br><h4>  Farine de cr√©ativit√© </h4><br>  Ayant suffisamment jou√© avec Windows Live, nous sommes arriv√©s √† la conclusion unanime que la solution r√©sultante ne satisfait pas au moins 3 points critiques.  Ils sont soit charg√©s pendant longtemps, soit pas tout √† fait vivants, ou leur personnalisation a √©t√© associ√©e √† une douleur sauvage.  Peut-√™tre que nous avons mal recherch√©, et vous pouvez conseiller un ensemble d'outils, je vous en serai reconnaissant. <br><br>  De plus, nous avons commenc√© √† nous tourner vers le VDI, mais pour cette t√¢che, la plupart des solutions sont trop co√ªteuses ou n√©cessitent une attention particuli√®re.  Et je voulais un outil simple avec un minimum de magie, dont la plupart des probl√®mes pourraient √™tre r√©solus en red√©marrant / red√©marrant simplement le service.  Heureusement, nous avions du mat√©riel serveur, bas de gamme dans les succursales, du service d√©class√©, que nous pouvions utiliser pour la base technologique. <br><br>  Quel est le r√©sultat?  Mais je ne serai pas en mesure de vous dire ce qui s'est pass√© √† la fin, parce que le NDA, mais en cours de recherche, nous avons d√©velopp√© un sch√©ma int√©ressant qui s'est bien montr√© dans les tests de laboratoire, bien qu'il ne soit pas entr√© en s√©rie. <br><br>  Quelques avertissements: l'auteur ne pr√©tend pas que la solution propos√©e r√©sout compl√®tement toutes les t√¢ches et le fait volontairement et avec la chanson.  L'auteur souscrit √† l'avance √† l'affirmation selon laquelle Sein Englishe sprache est zehr schlecht.  Comme la solution ne se d√©veloppe plus, vous ne pouvez pas compter sur une correction de bogue ou un changement de fonctionnalit√©, tout est entre vos mains.  L'auteur suppose que vous √™tes au moins un peu familier avec KVM et que vous avez lu un article de synth√®se sur le protocole Spice, et que vous avez un peu travaill√© avec Centos ou une autre distribution GNU Linux. <br><br>  Dans cet article, je voudrais analyser l'√©pine dorsale de la solution r√©sultante, √† savoir l'interaction du client et du serveur et l'essence des processus sur le cycle de vie des machines virtuelles dans le cadre de la solution en question.  Si l'article sera int√©ressant pour le public, je d√©crirai les d√©tails de l'impl√©mentation d'images en direct pour cr√©er des clients l√©gers bas√©s sur Fedora et raconterai les d√©tails de l'optimisation des machines virtuelles et des serveurs KVM pour optimiser les performances et la s√©curit√©. <br><br>  Si vous prenez du papier de couleur, <br>  Peintures, pinceaux et colle, <br>  Et un peu plus de dext√©rit√© ... <br>  Vous pouvez faire cent roubles! <br><br><h4>  Sch√©ma et description du banc d'essai </h4><br><img src="https://habrastorage.org/webt/pu/tu/rk/puturkaiwqcpbp4wld7ezdk_lcw.png"><br><br>  Tous les √©quipements sont situ√©s √† l'int√©rieur du r√©seau de succursales, seul le canal Internet s'√©teint.  Historiquement, il y a d√©j√† eu un serveur proxy, ce n'est rien d'extraordinaire.  Mais c'est sur celui-ci, entre autres, que le trafic des machines virtuelles sera filtr√© (abr√©viation VM plus loin dans le texte).  Rien n'emp√™che de placer ce service sur le serveur KVM, la seule chose √† surveiller est de savoir comment sa charge sur le sous-syst√®me de disque change. <br><br>  Station client - en fait, ¬´stations libre-service¬ª, ¬´front-end¬ª de notre service.  Sont des nettops de Lenovo IdeaCentre.  √Ä quoi sert cette unit√©?  Oui, presque tout le monde, particuli√®rement satisfait du grand nombre de connecteurs USB et de lecteurs de cartes sur le panneau avant.  Dans notre sch√©ma, une carte SD avec protection mat√©rielle en √©criture est ins√©r√©e dans le lecteur de carte, sur laquelle est enregistr√©e l'image en direct modifi√©e de Fedora 28. Bien s√ªr, un moniteur, un clavier et une souris sont connect√©s au nettop. <br><br>  Switch - un commutateur mat√©riel banal du deuxi√®me niveau, il est dans la salle des serveurs et clignote avec des lumi√®res.  Il n'est connect√© √† aucun r√©seau √† l'exception du r√©seau des ¬´stations libre-service¬ª. <br><br>  KVM_Server est le c≈ìur du circuit; dans les tests de banc du Core 2 Quad Q9650 avec 8 Go de RAM, il a tir√© en toute confiance 3 machines virtuelles Windows 10 sur lui-m√™me.  Sous-syst√®me de disque - adaptec 3405 2 disques Raid 1 + SSD.  Lors des essais sur le terrain du Xeon 1220, le SSD LSI 9260 +, plus s√©rieux, a facilement tir√© 5-6 VM.  Nous obtiendrions le serveur du service retir√©, il n'y aurait pas beaucoup de co√ªts en capital.  Le syst√®me de virtualisation KVM avec pool de machines virtuelles pool_Vm est d√©ploy√© sur ce ou ces serveurs. <br><br>  VM est une machine virtuelle, le backend de notre service.  C'est le travail de l'utilisateur. <br><br>  Enp5s0 est une interface r√©seau qui regarde vers le r√©seau des "stations libre-service", dhcpd, ntpd, httpd en direct, et xinetd √©coute le port "signal". <br><br>  Lo0 est la pseudo-interface de bouclage.  Standard. <br><br>  Spice_console - Une chose tr√®s int√©ressante, le fait est que, contrairement au RDP classique, lorsque vous tournez le bundle de protocole KVM + Spice, une entit√© suppl√©mentaire appara√Æt - le port de console de machine virtuelle.  En fait, en se connectant √† ce port TCP, on obtient la console Vm, sans avoir besoin de se connecter √† Vm via son interface r√©seau.  Toute interaction avec Vm pour la transmission du signal, le serveur prend le relais.  La fonction analogique la plus proche est IPKVM.  C'est-√†-dire  Une image d'un moniteur VM est transf√©r√©e sur ce port, des donn√©es sur le mouvement de la souris lui sont transmises et (surtout) l'interaction via le protocole Spice vous permet de rediriger de mani√®re transparente des p√©riph√©riques USB vers une machine virtuelle, comme si ce p√©riph√©rique √©tait connect√© √† Vm lui-m√™me.  Test√© pour les lecteurs flash, scanners, webcams. <br><br>  Les cartes Vnet0, virbr0 et r√©seau virtuel Vm forment un r√©seau de machines virtuelles. <br><br><h4>  Comment √ßa marche </h4><br>  Depuis la station client <br><br>  La station client est charg√©e en mode graphique √† partir d'une image en direct modifi√©e de Fedora 28, re√ßoit l'adresse IP par DHCP √† partir de l'espace d'adressage r√©seau 169.254.24.0/24.  Pendant le processus de d√©marrage, des r√®gles de pare-feu sont cr√©√©es qui permettent des connexions aux ports de serveur ¬´signal¬ª et ¬´pimenter¬ª.  Une fois le t√©l√©chargement termin√©, la station attend l'autorisation de l'utilisateur client.  Apr√®s l'autorisation de l'utilisateur, le gestionnaire de bureau ¬´openbox¬ª est lanc√© et le script de d√©marrage automatique d√©marre automatiquement au nom de l'utilisateur autoris√©.  Entre autres choses, le script d'ex√©cution automatique ex√©cute le script remote.sh. <br><br><div class="spoiler">  <b class="spoiler_title">$ HOME / .config / openbox / scripts / remote.sh</b> <div class="spoiler_text"><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh server_ip=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "server_ip" \ |/usr/bin/cut -d "=" -f2) vdi_signal_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_signal_port" \ |/usr/bin/cut -d "=" -f2) vdi_spice_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_spice_port" \ |/usr/bin/cut -d "=" -f2) animation_folder=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "animation_folder" \ |/usr/bin/cut -d "=" -f2) process=/usr/bin/remote-viewer while true do if [ -z `/usr/bin/pidof feh` ] then /usr/bin/echo $animation_folder /usr/bin/feh -N -x -D1 $animation_folder &amp; else /usr/bin/echo fi /usr/bin/nc -i 1 $server_ip $vdi_signal_port |while read line do if /usr/bin/echo "$line" |/usr/bin/grep "RULE ADDED, CONNECT NOW!" then /usr/bin/killall feh pid_process=$($process "spice://$server_ip:$vdi_spice_port" \ "--spice-disable-audio" "--spice-disable-effects=animation" \ "--spice-preferred-compression=auto-glz" "-k" \ "--kiosk-quit=on-disconnect" | /bin/echo $!) /usr/bin/wait $pid_process /usr/bin/killall -u $USER exit else /usr/bin/echo $line &gt;&gt; /var/log/remote.log fi done done</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/client.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">server_ip=169.254.24.1 vdi_signal_port=5905 vdi_spice_port=5906 animation_folder=/usr/share/backgrounds/animation background_folder=/usr/share/backgrounds2/fedora-workstation</code> </pre><br></div></div><br>  Description des variables du fichier client.conf <br>  server_ip - adresse KVM_Server <br>  vdi_signal_port - port KVM_Server sur lequel xinetd "se trouve" <br>  vdi_spice_port - port r√©seau KVM_Server, √† partir duquel la demande de connexion sera redirig√©e du client de visualisation √† distance vers le port spice du Vm s√©lectionn√© (d√©tails ci-dessous) <br>  animation_folder - dossier dans lequel les images sont prises pour une animation de d√©monstration de conneries <br>  background_folder - le dossier d'o√π les images sont prises pour les pr√©sentations en attente.  Plus d'informations sur l'animation dans la prochaine partie de l'article. <br><br>  Le script remote.sh prend les param√®tres du fichier de configuration /etc/client.conf et utilise nc pour se connecter au port "vdi_signal_port" du serveur KVM et re√ßoit un flux de donn√©es du serveur, parmi lequel il attend la cha√Æne "RULE ADDED, CONNECT NOW".  Lorsque la ligne souhait√©e est re√ßue, le processus de visionneuse √† distance d√©marre en mode kiosque, √©tablissant une connexion au port du serveur ¬´vdi_spice_port¬ª.  L'ex√©cution du script est suspendue jusqu'√† la fin de l'ex√©cution de la visionneuse √† distance. <br><br>  La visionneuse √† distance se connectant au port "vdi_spice_port", en raison d'une redirection c√¥t√© serveur, arrive au port "spice_console" de l'interface lo0 ie  √† la console de la machine virtuelle et le travail de l'utilisateur s'effectue directement.  En attendant la connexion, l'utilisateur voit une animation de conneries, sous la forme d'un diaporama de fichiers jpeg, le chemin vers le r√©pertoire avec des images est d√©termin√© par la valeur de la variable animation_folder du fichier de configuration. <br><br>  Si la connexion au port ¬´spice_console¬ª de la machine virtuelle est perdue, ce qui signale l'arr√™t / red√©marrage de la machine virtuelle (c'est-√†-dire la fin r√©elle de la session utilisateur), tous les processus en cours d'ex√©cution pour le compte de l'utilisateur autoris√© sont interrompus, ce qui conduit au red√©marrage de lightdm et revient √† l'√©cran d'autorisation . <br><br><h4>  Du c√¥t√© du serveur KVM </h4><br>  Sur le port ¬´signal¬ª de la carte r√©seau, enp5s0 attend la connexion xinetd.  Apr√®s la connexion au port ¬´signal¬ª, xinetd ex√©cute le script vm_manager.sh sans lui passer de param√®tres d'entr√©e et redirige le r√©sultat du script vers la session nc de la station client. <br><br><div class="spoiler">  <b class="spoiler_title">/etc/xinetd.d/test-server</b> <div class="spoiler_text"><pre> <code class="bash hljs">service vdi_signal { port = 5905 socket_type = stream protocol = tcp <span class="hljs-built_in"><span class="hljs-built_in">wait</span></span> = no user = root server = /home/admin/scripts_vdi_new/vm_manager.sh }</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_manager.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" export "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL export SRV_START_PORT_POOL=$SRV_START_PORT_POOL SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR date=$(/usr/bin/date) #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# /usr/bin/echo "# $date START EXECUTE VM_MANAGER.SH #" make_connect_to_vm() { #&lt;READING CLEAR.LIST AND CHECK PORT FOR NETWORK STATE&gt;# /usr/bin/echo "READING CLEAN.LIST AND CHECK PORT STATE" #&lt;CHECK FOR NO ONE PORT IN CLEAR.LIST&gt;# if [ -z `/usr/bin/cat $SRV_TMP_DIR/clear.list` ] then /usr/bin/echo "NO AVALIBLE PORTS IN CLEAN.LIST FOUND" /usr/bin/echo "Will try to make housekeeper, and create new vm" make_housekeeper else #&lt;MINIMUN ONE PORT IN CLEAR.LIST FOUND&gt;# /usr/bin/cat $SRV_TMP_DIR/clear.list |while read line do clear_vm_port=$(($line)) /bin/echo "FOUND PORT $clear_vm_port IN CLEAN.LIST. TRY NETSTAT" \ "CHECK FOR PORT=$clear_vm_port" #&lt;NETSTAT LISTEN CHECK FOR PORT FROM CLEAN.LIST&gt;# if /usr/bin/netstat -lnt |/usr/bin/grep ":$clear_vm_port" &gt; /dev/null then /bin/echo "$clear_vm_port IS LISTEN" #&lt;PORT IS LISTEN. CHECK FOR IS CONNECTED NOW&gt;# if /usr/bin/netstat -nt |/usr/bin/grep ":$clear_vm_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then #&lt;PORT LISTEN AND ALREADY CONNECTED! MOVE PORT FROM CLEAR.LIST # TO WASTE.LIST&gt;# /bin/echo "$clear_vm_port IS ALREADY CONNECTED, MOVE PORT TO WASTE.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list else #&lt;PORT LISTEN AND NO ONE CONNECT NOW. MOVE PORT FROM CLEAR.LIST TO # CONN_WAIT.LIST AND CREATE IPTABLES RULES&gt;## /usr/bin/echo "OK, $clear_vm_port IS NOT ALREADY CONNECTED" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/conn_wait.list $SRV_SCRIPTS_DIR/vm_connect.sh $clear_vm_port #&lt;TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW WM&gt;# /bin/echo "TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW VM" make_housekeeper /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH#" exit fi else #&lt;PORT IS NOT A LISTEN. MOVE PORT FROM CLEAR.LIST TO WASTE.LIST&gt;# /bin/echo " "$clear_vm_port" is NOT LISTEN. REMOVE PORT FROM CLEAR.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list make_housekeeper fi done fi } make_housekeeper() { /usr/bin/echo "=Execute housekeeper=" /usr/bin/cat $SRV_TMP_DIR/waste.list |while read line do /usr/bin/echo "$line" if /usr/bin/netstat -lnt |/usr/bin/grep ":$line" &gt; /dev/null then /bin/echo "port_alive, vm is running" if /usr/bin/netstat -nt |/usr/bin/grep ":$line" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "port_in_use can't delete vm!!!" else /bin/echo "port_not in use. Deleting vm" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh $line fi else /usr/bin/echo "posible vm is already off. Deleting vm" /usr/bin/echo "MOVE VM IN OFF STATE $line FROM WASTE.LIST TO" \ "RECYCLE.LIST AND DELETE VM" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh "$line" fi done create_clear_vm } create_clear_vm() { /usr/bin/echo "=Create new VM=" while [ $SRV_POOL_SIZE -gt 0 ] do new_vm_port=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) /usr/bin/echo "new_vm_port=$new_vm_port" if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/clear.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in clear.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/waste.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in waste.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/recycle.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN RECYCLE LIST" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/conn_wait.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN CONN_WAIT LIST" else /usr/bin/echo "PORT IN NOT DEFINED IN NO ONE LIST WILL CREATE" \ "VM ON PORT $new_vm_port" /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_create.sh $new_vm_port fi fi fi fi SRV_POOL_SIZE=$(($SRV_POOL_SIZE-1)) done /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH #" } make_connect_to_vm |/usr/bin/tee -a /var/log/vm_manager.log</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/vm_manager.conf</b> <div class="spoiler_text">  srv_scripts_dir = / home / admin / scripts_vdi_new <br>  srv_pool_size = 4 <br>  srv_start_port_pool = 5920 <br>  srv_tmp_dir = / tmp / vm_state <br>  base_host = win10_2 <br>  input_iface = enp5s0 <br>  vdi_spice_port = 5906 <br>  count_conn_tryes = 10 <br></div></div><br><br>  Description des variables du fichier de configuration vm_manager.conf <br>  srv_scripts_dir - dossier d'emplacement de script vm_manager.sh, vm_connect.sh, vm_delete.sh, vm_create.sh, vm_clear.sh <br>  srv_pool_size - Taille du pool Vm <br>  srv_start_port_pool - le port initial, apr√®s quoi les ports spice des consoles de machines virtuelles commenceront <br>  srv_tmp_dir - dossier pour les fichiers temporaires <br>  base_host - base Vm (image dor√©e) √† partir de laquelle les clones Vm seront transform√©s en pool <br>  input_iface - l'interface r√©seau du serveur, en regardant vers les postes clients <br>  vdi_spice_port - le port r√©seau du serveur √† partir duquel la demande de connexion sera redirig√©e du client visualiseur distant vers le port spice du Vm s√©lectionn√© <br>  count_conn_tryes - un temporisateur d'attente, apr√®s quoi il est consid√©r√© qu'une connexion √† Vm ne s'est pas produite (pour plus de d√©tails, voir vm_connect.sh) <br><br>  Le script vm_manager.sh lit le fichier de configuration √† partir du fichier vm_manager.conf, √©value l'√©tat des machines virtuelles dans le pool en fonction de plusieurs param√®tres, √† savoir: combien de VM sont d√©ploy√©es, s'il existe des VM propres et libres.  Pour ce faire, il lit le fichier clear.list qui contient le nombre de ports ¬´spice_console¬ª des machines virtuelles ¬´nouvellement cr√©√©es¬ª (voir cycle de cr√©ation de VM ci-dessous) et recherche une connexion √©tablie avec eux.  Si un port avec une connexion r√©seau √©tablie est d√©tect√© (ce qui ne devrait absolument pas l'√™tre), un avertissement s'affiche et le port est transf√©r√© vers waste.list Lorsque le premier port est trouv√© dans le fichier clear.list avec lequel il n'y a actuellement aucune connexion, vm_manager.sh appelle le script vm_connect.sh et passe lui comme param√®tre le num√©ro de ce port. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_connect.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh date=$(/usr/bin/date) /usr/bin/echo "#" "$date" "START EXECUTE VM_CONNECT.SH#" #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# free_port="$1" input_iface=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "input_iface" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "input_iface=$input_iface" vdi_spice_port=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "vdi_spice_port" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "vdi_spice_port=$vdi_spice_port" count_conn_tryes=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "count_conn_tryes" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "count_conn_tryes=$count_conn_tryes" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# #&lt;CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# /usr/bin/echo "create rule for port" $free_port /usr/sbin/iptables -I INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -I OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/sbin/iptables -t nat -I PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/bin/echo "RULE ADDED, CONNECT NOW!" #&lt;/CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# #&lt;WAIT CONNECT ESTABLISHED AND ACTIVATE CONNECT TIMER&gt;# while [ $count_conn_tryes -gt 0 ] do if /usr/bin/netstat -nt |/usr/bin/grep ":$free_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "$free_port NOW in use!!!" /usr/bin/sleep 1s /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/waste.list return else /usr/bin/echo "$free_port NOT IN USE" /usr/bin/echo "RULE ADDED, CONNECT NOW!" /usr/bin/sleep 1s fi count_conn_tryes=$((count_conn_tryes-1)) done #&lt;/WAIT CONNECT ESTABLISED AND ACTIVATE CONNECT TIMER&gt;# #&lt;IF COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT \ # VM TO CLEAR.LIST&gt;# /usr/bin/echo "REVERT IPTABLES RULE AND REVERT VM TO CLEAN \ LIST $free_port" /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport $free_port \ -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/clear.list #&lt;/COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT VM \ #TO CLEAR.LIST&gt;# /usr/bin/echo "#" "$date" "END EXECUTE VM_CONNECT.SH#" # Attention! Must Be! sysctl net.ipv4.conf.all.route_localnet=1</span></span></code> </pre><br></div></div><br>  Le script vm_connect.sh introduit des r√®gles de pare-feu qui cr√©ent une redirection "vdi_spice_port" du port de serveur de l'interface enp5s0 vers le "port de console spice" de la machine virtuelle situ√©e sur l'interface de serveur lo0, transmise comme param√®tre de d√©marrage.  Le port est transf√©r√© vers conn_wait.list, la machine virtuelle est consid√©r√©e comme √©tant en attente de connexion.  La ligne RULE ADDED, CONNECT NOW est envoy√©e √† la session Client Station sur le port ¬´signal¬ª du serveur, ce qui est attendu par le script remote.sh qui s'ex√©cute dessus.  Un cycle d'attente de connexion commence par le nombre de tentatives d√©termin√© par la valeur de la variable "count_conn_tryes" du fichier de configuration.  Chaque seconde dans la session nc, la cha√Æne "RULE ADDED, CONNECT NOW" sera donn√©e et la connexion √©tablie au port "spice_console" sera v√©rifi√©e. <br><br>  Si la connexion a √©chou√© pendant le nombre de tentatives d√©fini, le port spice_console est retransf√©r√© vers clear.list L'ex√©cution de vm_connect.sh est termin√©e, l'ex√©cution de vm_manager.sh reprend, ce qui d√©marre le cycle de nettoyage. <br><br>  Si la station cliente se connecte au port spice_console sur l'interface lo0, les r√®gles de pare-feu qui cr√©ent une redirection entre le port du serveur spice et le port spice_console sont supprim√©es et la connexion est maintenue par un m√©canisme permettant de d√©terminer l'√©tat du pare-feu.  En cas de connexion d√©connect√©e, la reconnexion au port spice_console √©chouera.  Le port spice_console est transf√©r√© vers waste.list, la machine virtuelle est consid√©r√©e comme sale et elle ne peut pas retourner au pool de machines virtuelles propres sans passer par le nettoyage.  L'ex√©cution de vm_connect.sh est termin√©e, l'ex√©cution de vm_manager.sh reprend, ce qui d√©marre le cycle de nettoyage. <br><br>  Le cycle de nettoyage commence par regarder le fichier waste.list, dans lequel les num√©ros spice_console des ports de machine virtuelle vers lesquels la connexion est √©tablie sont transf√©r√©s.  La pr√©sence d'une connexion active est d√©termin√©e sur chaque port spice_console de la liste.  S'il n'y a pas de connexion, il est consid√©r√© que la machine virtuelle n'est plus utilis√©e et le port est transf√©r√© vers recycle.list et le processus de suppression de la machine virtuelle (voir ci-dessous) √† laquelle ce port appartenait est d√©marr√©.  Si une connexion r√©seau active est d√©tect√©e sur le port, il est suppos√© que la machine virtuelle est utilis√©e, aucune action n'est entreprise pour elle.  Si le port n'est pas exploit√©, il est suppos√© que la machine virtuelle est d√©sactiv√©e et n'est plus n√©cessaire.  Le port est transf√©r√© vers recycle.list et le processus de suppression de la machine virtuelle d√©marre.  Pour ce faire, le script vm_delete.sh est appel√©, auquel le num√©ro "spice_console" est transf√©r√© au port VM en tant que param√®tre, qui doit √™tre supprim√©. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_delete.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh #&lt;Set local VARIABLES&gt;# port_to_delete="$1" date=$(/usr/bin/date) #&lt;/Set local VARIABLES&gt;# /usr/bin/echo "# $date START EXECUTE VM_DELETE.SH#" /usr/bin/echo "TRY DELETE VM ON PORT: $vm_port" #&lt;VM NAME SETUP&gt;# vm_name_part1=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep 'base_host' \ |/usr/bin/cut -d'=' -f2) vm_name=$(/usr/bin/echo "$vm_name_part1""-""$port_to_delete") #&lt;/VM NAME SETUP&gt;# #&lt;SHUTDOWN AND DELETE VM&gt;# /usr/bin/virsh destroy $vm_name /usr/bin/virsh undefine $vm_name /usr/bin/rm -f /var/lib/libvirt/images_write/$vm_name.qcow2 /usr/bin/sed -i "/$port_to_delete/d" $SRV_TMP_DIR/recycle.list #&lt;/SHUTDOWN AND DELETE VM&gt;# /usr/bin/echo "VM ON PORT $vm_port HAS BEEN DELETE AND REMOVE" \ "FROM RECYCLE.LIST. EXIT FROM VM_DELETE.SH" /usr/bin/echo "# $date STOP EXECUTE VM_DELETE.SH#" exit</span></span></code> </pre><br></div></div><br>  La suppression d'une machine virtuelle est une op√©ration assez banale, le script vm_delete.sh d√©termine le nom de la machine virtuelle qui poss√®de le port transmis comme param√®tre de d√©marrage.  La machine virtuelle est oblig√©e de s'arr√™ter, la machine virtuelle est supprim√©e de l'hyperviseur, le disque dur virtuel de cette machine virtuelle est supprim√©.  Le port spice_console est supprim√© de recycle.list.  L'ex√©cution de vm_delete.sh se termine, l'ex√©cution de vm_manager.sh reprend <br><br>  Le script vm_manager.sh, √† la fin des op√©rations de nettoyage des machines virtuelles inutiles de la liste waste.list, d√©marre le cycle de cr√©ation de machines virtuelles dans le pool. <br><br>  Le processus commence par la d√©termination des ports spice_console disponibles pour l'h√©bergement.  Pour ce faire, en fonction du param√®tre du fichier de configuration "srv_start_port_pool" qui d√©finit le port de d√©part pour le pool "spice_console" des machines virtuelles et du param√®tre "srv_pool_size", qui d√©termine la limite du nombre de machines virtuelles, toutes les variantes de port possibles sont tri√©es s√©quentiellement.  Pour chaque port sp√©cifique, il est recherch√© dans clear.list, waste.list, conn_wait.list, recycle.list.  Si un port est trouv√© dans l'un de ces fichiers, le port est consid√©r√© comme occup√© et est ignor√©.  Si le port n'est pas trouv√© dans les fichiers sp√©cifi√©s, il est entr√© dans le fichier recycle.list et le processus de cr√©ation d'une nouvelle machine virtuelle commence.  Pour ce faire, le script vm_create.sh est appel√© auquel le num√©ro spice_console du port pour lequel vous souhaitez cr√©er une VM est pass√© en param√®tre. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_create.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh /usr/bin/echo "#" "$date" "START RUNNING VM_CREATE.SH#" new_vm_port=$1 date=$(/usr/bin/date) a=0 /usr/bin/echo SRV_TMP_DIR=$SRV_TMP_DIR #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# base_host=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "base_host" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "base_host=$base_host" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# hdd_image_locate() { /bin/echo "Run STEP 1 - hdd_image_locate" hdd_base_image=$(/usr/bin/virsh dumpxml $base_host \ |/usr/bin/grep "source file" |/usr/bin/grep "qcow2" |/usr/bin/head -n 1 \ |/usr/bin/cut -d "'" -f2) if [ -z "$hdd_base_image" ] then /bin/echo "base hdd image not found!" else /usr/bin/echo "hdd_base_image found is a $hdd_base_image. Run next step 2" #&lt; CHECK FOR SNAPSHOT ON BASE HDD &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" | /usr/bin/grep -c "Snapshot"` ] then /usr/bin/echo "base image haven't snapshot, run NEXT STEP 3" else /usr/bin/echo "base hdd image have a snapshot, can't use this image" exit fi #&lt;/ CHECK FOR SNAPSHOT ON BASE HDD &gt;# #&lt; CHECK FOR HDD IMAGE IS LINK CLONE &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" |/usr/bin/grep -c "backing file" then /usr/bin/echo "base image is not a linked clone, NEXT STEP 4" /usr/bin/echo "Base image check complete!" else /usr/bin/echo "base hdd image is a linked clone, can't use this image" exit fi fi #&lt;/ CHECK FOR HDD IMAGE IS LINK CLONE &gt;# cloning } cloning() { # &lt;Step_1 turn the base VM off &gt;# /usr/bin/virsh shutdown $base_host &gt; /dev/null 2&gt;&amp;1 # &lt;/Step_1 turn the base VM off &gt;# #&lt;Create_vm_config&gt;# /usr/bin/echo "Free port for Spice VM is $new_vm_port" #&lt;Setup_name_for_new_VM&gt;# new_vm_name=$(/bin/echo $base_host"-"$new_vm_port) #&lt;/Setup_name_for_new_VM&gt;# #&lt;Make_base_config_as_clone_base_VM&gt;# /usr/bin/virsh dumpxml $base_host &gt; $SRV_TMP_DIR/$new_vm_name.xml #&lt;Make_base_config_as_clone_base_VM&gt;# ##&lt;Setup_New_VM_Name_in_config&gt;## /usr/bin/sed -i "s%&lt;name&gt;$base_host&lt;/name&gt;%&lt;name&gt;$new_vm_name&lt;/name&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Setup_New_VM_Name_in_config&gt;# #&lt;UUID Changing&gt;# old_uuid=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "&lt;uuid&gt;") /usr/bin/echo old UUID $old_uuid new_uuid_part1=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 1,2) new_uuid_part2=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 4,5) new_uuid=$(/bin/echo $new_uuid_part1"-"$new_vm_port"-"$new_uuid_part2) /usr/bin/echo $new_uuid /usr/bin/sed -i "s%$old_uuid%$new_uuid%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/UUID Changing&gt;# #&lt;Spice port replace&gt;# old_spice_port=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml \ |/usr/bin/grep "graphics type='spice' port=") /bin/echo old spice port $old_spice_port new_spice_port=$(/usr/bin/echo "&lt;graphics type='spice' port='$new_vm_port' autoport='no' listen='127.0.0.1'&gt;") /bin/echo $new_spice_port /usr/bin/sed -i "s%$old_spice_port%$new_spice_port%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Spice port replace&gt;# #&lt;MAC_ADDR_GENERATE&gt;# mac_new=$(/usr/bin/hexdump -n6 -e '/1 ":%02X"' /dev/random|/usr/bin/sed s/^://g) /usr/bin/echo New Mac is $mac_new #&lt;/MAC_ADDR_GENERATE&gt;# #&lt;GET OLD MAC AND REPLACE&gt;# mac_old=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "mac address=") /usr/bin/echo old mac is $mac_old /usr/bin/sed -i "s%$mac_old%$mac_new%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;GET OLD MAC AND REPLACE&gt;# #&lt;new_disk_create&gt;# /usr/bin/qemu-img create -f qcow2 -b $hdd_base_image /var/lib/libvirt/images_write/$new_vm_name.qcow2 #&lt;/new_disk_create&gt;# #&lt;attach_new_disk_in_confiig&gt;# /usr/bin/echo hdd base image is $hdd_base_image /usr/bin/sed -i "s%&lt;source file='$hdd_base_image'/&gt;%&lt;source file='/var/lib/libvirt/images_write/$new_vm_name.qcow2'/&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/attach_new_disk_in_confiig&gt;# starting_vm #&lt;/Create_vm config&gt;# } starting_vm() { /usr/bin/virsh define $SRV_TMP_DIR/$new_vm_name.xml /usr/bin/virsh start $new_vm_name while [ $a -ne 1 ] do if /usr/bin/virsh list --all |/usr/bin/grep "$new_vm_name" |/usr/bin/grep "running" &gt; /dev/null 2&gt;&amp;1 then a=1 /usr/bin/sed -i "/$new_vm_port/d" $SRV_TMP_DIR/recycle.list /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/clear.list /usr/bin/echo "#" "$date" "VM $new_vm_name IS STARTED #" else /usr/bin/echo "#VM $new_vm_name is not ready#" a=0 /usr/bin/sleep 2s fi done /usr/bin/echo "#$date EXIT FROM VM_CREATE.SH#" exit } hdd_image_locate</span></span></code> </pre><br></div></div><br>  Le processus de cr√©ation d'une nouvelle machine virtuelle <br><br>  Le script vm_create.sh lit dans le fichier de configuration la valeur de la variable "base_host" qui d√©termine l'exemple de machine virtuelle sur la base de laquelle le clone sera effectu√©.  D√©charge la configuration xml de la machine virtuelle de la base de donn√©es de l'hyperviseur, effectue une s√©rie de v√©rifications qcow de l'image disque de la machine virtuelle et, une fois termin√©e, cr√©e le fichier de configuration xml pour la nouvelle machine virtuelle et l'image disque "clone li√©" de la nouvelle machine virtuelle.  Apr√®s cela, la configuration xml de la nouvelle machine virtuelle est charg√©e dans la base de donn√©es de l'hyperviseur et la machine virtuelle d√©marre.  Le port spice_console est transf√©r√© de recycle.list vers clear.list.  L'ex√©cution de vm_create.sh se termine et l'ex√©cution de vm_manager.sh se termine. <br>  La prochaine fois que vous vous connectez, cela recommence depuis le d√©but. <br><br>  Pour les cas d'urgence, le kit comprend un script vm_clear.sh qui traverse de force toutes les machines virtuelles du pool et les supprime en mettant √† z√©ro les valeurs des listes.  L'appeler √† l'√©tape de chargement vous permet de d√©marrer (sous) VDI √† partir de z√©ro. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_clear.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #set VARIABLES# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL #Set VARIABLES# /usr/bin/echo "= Cleanup ALL VM=" /usr/bin/mkdir $SRV_TMP_DIR /usr/sbin/service iptables restart /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/clear.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/waste.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/recycle.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/conn_wait.list port_to_delete=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) while [ "$port_to_delete" -gt "$SRV_START_PORT_POOL" ] do $SRV_SCRIPTS_DIR/vm_delete.sh $port_to_delete port_to_delete=$(($port_to_delete-1)) done /usr/bin/echo "= EXIT FROM VM_CLEAR.SH="</span></span></code> </pre><br></div></div><br>  Sur ce, je voudrais terminer la premi√®re partie de mon histoire.  Les √©l√©ments ci-dessus devraient suffire aux administrateurs syst√®me pour essayer underVDI en entreprise.  Si la communaut√© trouve ce sujet int√©ressant, dans la seconde partie je parlerai de la modification de livecd Fedora et de sa transformation en kiosque. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr462203/">https://habr.com/ru/post/fr462203/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr462181/index.html">R√®gles pour une communication efficace dans les chats de groupe</a></li>
<li><a href="../fr462185/index.html">La r√©volution est finie. Existe-t-il une alternative √† une batterie lithium-ion?</a></li>
<li><a href="../fr462189/index.html">Gravure de donn√©es avec travajs</a></li>
<li><a href="../fr462191/index.html">Mus√©e DataArt: visite du nord de l'Italie</a></li>
<li><a href="../fr462197/index.html">Conseils pour lib√©rer votre esprit et augmenter votre cr√©ativit√©</a></li>
<li><a href="../fr462205/index.html">Gagner PHDays 9 The Standoff: la chronique de l'√©quipe True0xA3</a></li>
<li><a href="../fr462209/index.html">Solutions de vid√©oconf√©rence Polycom. Des souvenirs 6 ans plus tard ... √âtape 2. Partie 1. RMX1500</a></li>
<li><a href="../fr462213/index.html">Apprendre et travailler: l'exp√©rience des √©tudiants de premier cycle √† la Facult√© des technologies de l'information et de la programmation</a></li>
<li><a href="../fr462221/index.html">Quelle d√©ception je suis sur Google Play</a></li>
<li><a href="../fr462227/index.html">Moscou, 9 ao√ªt - Backend Stories 4.0</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>