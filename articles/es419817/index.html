<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏂🏻 🧔🏻 👩🏼‍🚀 Lanzamiento del clúster RabbitMQ en Kubernetes 🥗 ◼️ 🤾🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En el caso de la organización de microservicios de la aplicación, el trabajo sustancial se basa en los mecanismos de la conexión de integración de mic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Lanzamiento del clúster RabbitMQ en Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/419817/">  En el caso de la organización de microservicios de la aplicación, el trabajo sustancial se basa en los mecanismos de la conexión de integración de microservicios.  Además, esta integración debe ser tolerante a fallas, con un alto grado de disponibilidad. <br><br>  En nuestras soluciones, utilizamos la integración con Kafka, gRPC y RabbitMQ. <br><br>  En este artículo, compartiremos nuestra experiencia de agrupar RabbitMQ, cuyos nodos están alojados en Kubernetes. <br><br><img src="https://habrastorage.org/webt/dx/ll/-h/dxll-hzomoco0zcfp7esju8pena.jpeg" alt="imagen"><br><br>  Antes de RabbitMQ versión 3.7, agruparlo en K8S no era una tarea muy trivial, con muchos hacks y soluciones no muy hermosas.  En la versión 3.6, se usó un plugin de autocluster de la comunidad RabbitMQ.  Y en 3.7 apareció Kubernetes Peer Discovery Backend.  Está integrado por el complemento en la entrega básica de RabbitMQ y no requiere montaje e instalación por separado. <br><br>  Describiremos la configuración final como un todo, mientras comentamos lo que está sucediendo. <br><a name="habracut"></a><br><h2>  En teoría </h2><br>  El complemento tiene un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio en el github</a> , en el que hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un ejemplo de uso básico</a> . <br>  Este ejemplo no está destinado a Producción, lo cual está claramente indicado en su descripción, y además, algunos de los ajustes en él están configurados en contra de la lógica de uso en el producto.  Además, en el ejemplo, la persistencia del almacenamiento no se menciona en absoluto, por lo que en cualquier situación de emergencia nuestro clúster se convertirá en un zilch. <br><br><h2>  En la practica </h2><br>  Ahora le diremos a qué se enfrentó y cómo instalar y configurar RabbitMQ. <br><br>  Describamos las configuraciones de todas las partes de RabbitMQ como un servicio en K8s.  Aclararemos de inmediato que instalamos RabbitMQ en K8s como StatefulSet.  En cada nodo del clúster K8s, una instancia de RabbitMQ siempre funcionará (un nodo en la configuración clásica del clúster).  También instalaremos el panel de control RabbitMQ en K8 y daremos acceso a este panel fuera del clúster. <br><br><h3>  Derechos y roles: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_rbac.yaml</b> <div class="spoiler_text"><pre><code class="plaintext hljs">--- apiVersion: v1 kind: ServiceAccount metadata: name: rabbitmq --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader rules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader subjects: - kind: ServiceAccount name: rabbitmq roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: endpoint-reader</code> </pre> </div></div><br>  Los derechos de acceso para RabbitMQ se toman completamente del ejemplo, no se requieren cambios en ellos.  Creamos una cuenta de servicio para nuestro clúster y le damos permisos de lectura a los puntos finales K8. <br><br><h3>  Almacenamiento persistente: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pv.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolume apiVersion: v1 metadata: name: rabbitmq-data-sigma labels: type: local annotations: volume.alpha.kubernetes.io/storage-class: rabbitmq-data-sigma spec: storageClassName: rabbitmq-data-sigma capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle hostPath: path: "/opt/rabbitmq-data-sigma"</code> </pre> </div></div><br>  Aquí tomamos el caso más simple como el almacenamiento persistente: hostPath (una carpeta regular en cada nodo K8s), pero puede usar cualquiera de los muchos tipos de volúmenes persistentes admitidos por K8s. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pvc.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolumeClaim apiVersion: v1 metadata: name: rabbitmq-data spec: storageClassName: rabbitmq-data-sigma accessModes: - ReadWriteMany resources: requests: storage: 10Gi</code> </pre> </div></div><br>  Crear reclamo de volumen en el volumen creado en el paso anterior.  Este reclamo se utilizará en StatefulSet como un almacén de datos persistente. <br><br><h3>  Servicios: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq-internal labels: app: rabbitmq spec: clusterIP: None ports: - name: http protocol: TCP port: 15672 - name: amqp protocol: TCP port: 5672 selector: app: rabbitmq</code> </pre> </div></div><br>  Creamos un servicio interno sin cabeza a través del cual funcionará el complemento Peer Discovery. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service_ext.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq labels: app: rabbitmq type: LoadBalancer spec: type: NodePort ports: - name: http protocol: TCP port: 15672 targetPort: 15672 nodePort: 31673 - name: amqp protocol: TCP port: 5672 targetPort: 5672 nodePort: 30673 selector: app: rabbitmq</code> </pre> </div></div><br>  Para que las aplicaciones en K8 funcionen con nuestro clúster, creamos un servicio equilibrador. <br><br>  Dado que necesitamos acceso al clúster RabbitMQ fuera de K8, pasamos por NodePort.  RabbitMQ estará disponible al acceder a cualquier nodo del clúster K8s en los puertos 31673 y 30673. En el trabajo real, no hay una gran necesidad de esto.  Cuestión de usabilidad del panel de administración de RabbitMQ. <br><br>  Al crear un servicio con el tipo NodePort en K8, también se crea implícitamente un servicio con el tipo ClusterIP para servirlo.  Por lo tanto, las aplicaciones en K8 que necesitan trabajar con nuestro RabbitMQ podrán acceder al clúster en <i>amqp: // rabbitmq: 5672</i> <br><br><h3>  Configuración: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_configmap.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-config data: enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s]. rabbitmq.conf: | cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443 ### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal queue_master_locator=min-masters cluster_formation.randomized_startup_delay_range.min = 0 cluster_formation.randomized_startup_delay_range.max = 2 cluster_formation.k8s.service_name = rabbitmq-internal cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> </div></div><br>  Creamos archivos de configuración RabbitMQ.  La magia principal. <br><br><pre> <code class="plaintext hljs">enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s].</code> </pre><br>  Agregue los complementos necesarios a los permitidos para la descarga.  Ahora podemos usar el Peer Discovery automático en el K8S. <br><br><pre> <code class="plaintext hljs">cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s</code> </pre><br>  Exponemos el complemento necesario como back-end para el descubrimiento de pares. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443</code> </pre><br>  Especifique la dirección y el puerto a través del cual puede comunicarse con kubernetes apiserver.  Aquí puede especificar la dirección IP directamente, pero será más hermoso hacerlo. <br><br>  En el espacio de nombres predeterminado, generalmente se crea un servicio con el nombre kubernetes que conduce a k8-apiserver.  En diferentes opciones de instalación de K8S, el espacio de nombres, el nombre del servicio y el puerto pueden ser diferentes.  Si algo en una instalación particular es diferente, debe solucionarlo en consecuencia. <br><br>  Por ejemplo, nos enfrentamos al hecho de que en algunos grupos el servicio está en el puerto 443 y en algunos en 6443. Será posible comprender que algo está mal en los registros de inicio de RabbitMQ, el tiempo de conexión a la dirección especificada aquí está claramente resaltado allí. <br><br><pre> <code class="plaintext hljs">### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname</code> </pre><br>  De manera predeterminada, el ejemplo especifica el tipo de dirección del nodo del clúster RabbitMQ por dirección IP.  Pero cuando reinicia el pod, obtiene una nueva IP cada vez.  Sorpresa!  El grupo está muriendo en agonía. <br><br>  Cambie el direccionamiento a nombre de host.  StatefulSet nos garantiza la invariabilidad del nombre de host dentro del ciclo de vida de todo el StatefulSet, lo que nos conviene por completo. <br><br><pre> <code class="plaintext hljs">cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true</code> </pre><br>  Como cuando perdemos uno de los nodos, suponemos que se recuperará tarde o temprano, deshabilitamos la eliminación automática por un grupo de nodos inaccesibles.  En este caso, tan pronto como el nodo regrese en línea, ingresará al clúster sin perder su estado anterior. <br><br><pre> <code class="plaintext hljs">cluster_partition_handling = autoheal</code> </pre> <br>  Este parámetro determina las acciones del clúster en caso de pérdida de quórum.  Aquí solo necesita leer la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentación sobre este tema</a> y comprender por sí mismo lo que está más cerca de un caso de uso específico. <br><br><pre> <code class="plaintext hljs">queue_master_locator=min-masters</code> </pre> <br>  Determine la selección del asistente para nuevas colas.  Con esta configuración, el asistente seleccionará el nodo con el menor número de colas, por lo que las colas se distribuirán de manera uniforme entre los nodos del clúster. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.service_name = rabbitmq-internal</code> </pre> <br>  Nombramos el servicio K8s sin cabeza (creado por nosotros anteriormente) a través del cual los nodos RabbitMQ se comunicarán entre sí. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> <br>  Una cosa importante para direccionar en un clúster es el nombre de host.  El FQDN del hogar K8s se forma como un nombre corto (rabbitmq-0, rabbitmq-1) + sufijo (parte del dominio).  Aquí indicamos este sufijo.  En K8S, se ve así <b>. &lt;Nombre del servicio&gt;. &lt;Nombre del espacio de nombres&gt; .svc.cluster.local</b> <br><br>  kube-dns resuelve los nombres de la forma rabbitmq-0.rabbitmq-internal.our-namespace.svc.cluster.local en la dirección IP de un pod específico sin ninguna configuración adicional, lo que hace posible toda la magia de la agrupación por nombre de host. <br><br><h3>  Configuración StatefulSet RabbitMQ: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_statefulset.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: rabbitmq spec: serviceName: rabbitmq-internal replicas: 3 template: metadata: labels: app: rabbitmq annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } } spec: serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 containers: - name: rabbitmq-k8s image: rabbitmq:3.7 volumeMounts: - name: config-volume mountPath: /etc/rabbitmq - name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia ports: - name: http protocol: TCP containerPort: 15672 - name: amqp protocol: TCP containerPort: 5672 livenessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 10 readinessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 10 imagePullPolicy: Always env: - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local" - name: K8S_SERVICE_NAME value: "rabbitmq-internal" - name: RABBITMQ_ERLANG_COOKIE value: "mycookie" volumes: - name: config-volume configMap: name: rabbitmq-config items: - key: rabbitmq.conf path: rabbitmq.conf - key: enabled_plugins path: enabled_plugins - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> </div></div><br>  En realidad, StatefulSet sí mismo.  Tomamos nota de puntos interesantes. <br><br><pre> <code class="plaintext hljs">serviceName: rabbitmq-internal</code> </pre> <br>  Escribimos el nombre del servicio sin cabeza a través del cual los pods se comunican en StatefulSet. <br><br><pre> <code class="plaintext hljs">replicas: 3</code> </pre> <br>  Establezca el número de réplicas en el clúster.  En nuestro país, es igual al número de nodos de trabajo K8. <br><br><pre> <code class="plaintext hljs">annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } }</code> </pre> <br>  Cuando uno de los nodos K8 cae, el conjunto con estado busca preservar el número de instancias en el conjunto, por lo tanto, crea varios hogares en el mismo nodo K8.  Este comportamiento es completamente indeseable y, en principio, no tiene sentido.  Por lo tanto, prescribimos una regla antiafinidad para los conjuntos de hogares de setful.  Hacemos que la regla sea difícil (obligatorio) para que kube-Scheduler no pueda romperla cuando se planifican pods. <br><br>  La esencia es simple: está prohibido que el planificador coloque (dentro del espacio de nombres) más de un pod con la <i>aplicación: etiqueta rabbitmq</i> en cada nodo.  Distinguimos los <i>nodos</i> por el valor de la etiqueta <i>kubernetes.io/hostname</i> .  Ahora, si por alguna razón la cantidad de nodos K8S en funcionamiento es menor que la cantidad requerida de réplicas en StatefulSet, no se crearán nuevas réplicas hasta que aparezca un nodo libre nuevamente. <br><br><pre> <code class="plaintext hljs">serviceAccountName: rabbitmq</code> </pre> <br>  Registramos ServiceAccount, bajo el cual funcionan nuestros pods. <br><br><pre> <code class="plaintext hljs">image: rabbitmq:3.7</code> </pre> <br>  La imagen de RabbitMQ es completamente estándar y se toma del docker hub; no requiere ninguna reconstrucción ni revisión de archivos. <br><br><pre> <code class="plaintext hljs">- name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia</code> </pre><br>  Los datos persistentes de RabbitMQ se almacenan en / var / lib / rabbitmq / mnesia.  Aquí montamos nuestro Reclamo de volumen persistente en esta carpeta para que al reiniciar los hogares / nodos o incluso todo el StatefulSet, los datos (tanto el servicio, incluido el clúster ensamblado y los datos del usuario) estén sanos y salvos.  Hay algunos ejemplos en los que toda la carpeta / var / lib / rabbitmq / se hace persistente.  Llegamos a la conclusión de que esta no es la mejor idea, ya que al mismo tiempo toda la información establecida por las configuraciones de Rabbit comienza a ser recordada.  Es decir, para cambiar algo en el archivo de configuración, debe limpiar el almacenamiento persistente, lo cual es muy inconveniente en la operación. <br><br><pre> <code class="plaintext hljs"> - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local"</code> </pre><br>  Con este conjunto de variables de entorno, primero le decimos a RabbitMQ que use el nombre FQDN como identificador para los miembros del clúster y, en segundo lugar, establecemos el formato de este nombre.  El formato se describió anteriormente al analizar la configuración. <br><br><pre> <code class="plaintext hljs">- name: K8S_SERVICE_NAME value: "rabbitmq-internal"</code> </pre> <br>  El nombre del servicio sin cabeza para la comunicación entre los miembros del clúster. <br><br><pre> <code class="plaintext hljs">- name: RABBITMQ_ERLANG_COOKIE value: "mycookie"</code> </pre> <br>  El contenido de la cookie Erlang debe ser el mismo en todos los nodos del clúster, debe registrar su propio valor.  Un nodo con una cookie diferente no puede ingresar al clúster. <br><br><pre> <code class="plaintext hljs">volumes: - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> <br>  Defina el volumen asignado a partir del Reclamo de volumen persistente creado anteriormente. <br><br>  Aquí es donde hemos terminado con la configuración en los K8.  El resultado es un clúster RabbitMQ, que distribuye uniformemente las colas entre los nodos y es resistente a problemas en el entorno de tiempo de ejecución. <br><br><img src="https://habrastorage.org/webt/_j/ky/mw/_jkymwmxe7syyjfa7h2idbcxosc.png" alt="imagen"><br><br>  Si uno de los nodos del clúster no está disponible, las colas contenidas en él dejarán de ser accesibles, todo lo demás seguirá funcionando.  Tan pronto como el nodo regrese a la operación, volverá al clúster, y las colas para las cuales era un Maestro volverán a estar operativas, preservando todos los datos contenidos en ellas (si el almacenamiento persistente no se ha roto, por supuesto).  Todos estos procesos son completamente automáticos y no requieren intervención. <br><br><h2>  Bono: personalizar HA </h2><br>  Uno de los proyectos fue un matiz.  Los requisitos sonaron como un reflejo completo de todos los datos contenidos en el clúster.  Esto es necesario para que en una situación en la que al menos un nodo del clúster esté operativo, todo continúe funcionando desde el punto de vista de la aplicación.  Este momento no tiene nada que ver con los K8, lo describimos simplemente como un mini tutorial. <br><br>  Para habilitar la HA completa, debe crear una Política en el panel de RabbitMQ en la pestaña <i>Admin -&gt; Políticas</i> .  El nombre es arbitrario, el Patrón está vacío (todas las colas), en las Definiciones agregue dos parámetros: <i>ha-mode: all</i> , <i>ha-sync-mode: automatic</i> . <br><br><img src="https://habrastorage.org/webt/jz/tn/vu/jztnvu5zygtv56hurbyss1w9ljm.png" alt="imagen"><br><br><img src="https://habrastorage.org/webt/_6/in/om/_6inoma38lvluhpaet1g66uus_u.png" alt="imagen"><br><br>  Después de eso, todas las colas creadas en el clúster estarán en modo de alta disponibilidad: si el nodo maestro no está disponible, uno de los esclavos será seleccionado automáticamente por el nuevo asistente.  Y los datos que entran en la cola se reflejarán en todos los nodos del clúster.  Que, de hecho, se requería para recibir. <br><br><img src="https://habrastorage.org/webt/0v/m3/je/0vm3jem0bi4fqckj8ucmiy5zcxe.png" alt="imagen"><br><br>  Lea más sobre HA en RabbitMQ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aquí</a> <br><br><h2>  Literatura útil: </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Repository RabbitMQ Peer Discovery Kubernetes Plugin</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ejemplo de configuración para la implementación de RabbitMQ en K8S</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Descripción de los principios de formación de clúster, mecanismo de descubrimiento de pares y complementos para ello</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Discusión épica sobre la configuración de descubrimiento basada en el nombre de host</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RabbitMQ Clustering Guide</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Descripción de problemas y soluciones de agrupamiento de cerebro dividido</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Colas de alta disponibilidad en RabbitMQ</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Configurar políticas</a> </li></ul><br>  Buena suerte </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es419817/">https://habr.com/ru/post/es419817/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es419805/index.html">The Super Tiny Compiler - ahora en ruso</a></li>
<li><a href="../es419807/index.html">Glaucoma: cómo no quedarse ciego: hablemos sobre el tratamiento ...</a></li>
<li><a href="../es419811/index.html">La evolución de las pantallas flexibles.</a></li>
<li><a href="../es419813/index.html">Seminarios web de Skillbox: selección del viernes</a></li>
<li><a href="../es419815/index.html">Secretos de tolerancia a fallas de nuestra oficina principal</a></li>
<li><a href="../es419819/index.html">Biomarcadores del envejecimiento. Fragilidad de panel. Parte 2</a></li>
<li><a href="../es419823/index.html">Dúo inusual: frases de contraseña e imágenes mnemotécnicas</a></li>
<li><a href="../es419825/index.html">Probar el rendimiento de varios tipos de unidades en un entorno virtual</a></li>
<li><a href="../es419829/index.html">El cifrado de clave predeterminado de OpenSSH es peor que ninguno</a></li>
<li><a href="../es419831/index.html">Cómo funciona JS: elementos personalizados</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>