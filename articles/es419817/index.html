<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèÇüèª üßîüèª üë©üèº‚ÄçüöÄ Lanzamiento del cl√∫ster RabbitMQ en Kubernetes ü•ó ‚óºÔ∏è ü§æüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En el caso de la organizaci√≥n de microservicios de la aplicaci√≥n, el trabajo sustancial se basa en los mecanismos de la conexi√≥n de integraci√≥n de mic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Lanzamiento del cl√∫ster RabbitMQ en Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/419817/">  En el caso de la organizaci√≥n de microservicios de la aplicaci√≥n, el trabajo sustancial se basa en los mecanismos de la conexi√≥n de integraci√≥n de microservicios.  Adem√°s, esta integraci√≥n debe ser tolerante a fallas, con un alto grado de disponibilidad. <br><br>  En nuestras soluciones, utilizamos la integraci√≥n con Kafka, gRPC y RabbitMQ. <br><br>  En este art√≠culo, compartiremos nuestra experiencia de agrupar RabbitMQ, cuyos nodos est√°n alojados en Kubernetes. <br><br><img src="https://habrastorage.org/webt/dx/ll/-h/dxll-hzomoco0zcfp7esju8pena.jpeg" alt="imagen"><br><br>  Antes de RabbitMQ versi√≥n 3.7, agruparlo en K8S no era una tarea muy trivial, con muchos hacks y soluciones no muy hermosas.  En la versi√≥n 3.6, se us√≥ un plugin de autocluster de la comunidad RabbitMQ.  Y en 3.7 apareci√≥ Kubernetes Peer Discovery Backend.  Est√° integrado por el complemento en la entrega b√°sica de RabbitMQ y no requiere montaje e instalaci√≥n por separado. <br><br>  Describiremos la configuraci√≥n final como un todo, mientras comentamos lo que est√° sucediendo. <br><a name="habracut"></a><br><h2>  En teor√≠a </h2><br>  El complemento tiene un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio en el github</a> , en el que hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un ejemplo de uso b√°sico</a> . <br>  Este ejemplo no est√° destinado a Producci√≥n, lo cual est√° claramente indicado en su descripci√≥n, y adem√°s, algunos de los ajustes en √©l est√°n configurados en contra de la l√≥gica de uso en el producto.  Adem√°s, en el ejemplo, la persistencia del almacenamiento no se menciona en absoluto, por lo que en cualquier situaci√≥n de emergencia nuestro cl√∫ster se convertir√° en un zilch. <br><br><h2>  En la practica </h2><br>  Ahora le diremos a qu√© se enfrent√≥ y c√≥mo instalar y configurar RabbitMQ. <br><br>  Describamos las configuraciones de todas las partes de RabbitMQ como un servicio en K8s.  Aclararemos de inmediato que instalamos RabbitMQ en K8s como StatefulSet.  En cada nodo del cl√∫ster K8s, una instancia de RabbitMQ siempre funcionar√° (un nodo en la configuraci√≥n cl√°sica del cl√∫ster).  Tambi√©n instalaremos el panel de control RabbitMQ en K8 y daremos acceso a este panel fuera del cl√∫ster. <br><br><h3>  Derechos y roles: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_rbac.yaml</b> <div class="spoiler_text"><pre><code class="plaintext hljs">--- apiVersion: v1 kind: ServiceAccount metadata: name: rabbitmq --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader rules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader subjects: - kind: ServiceAccount name: rabbitmq roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: endpoint-reader</code> </pre> </div></div><br>  Los derechos de acceso para RabbitMQ se toman completamente del ejemplo, no se requieren cambios en ellos.  Creamos una cuenta de servicio para nuestro cl√∫ster y le damos permisos de lectura a los puntos finales K8. <br><br><h3>  Almacenamiento persistente: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pv.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolume apiVersion: v1 metadata: name: rabbitmq-data-sigma labels: type: local annotations: volume.alpha.kubernetes.io/storage-class: rabbitmq-data-sigma spec: storageClassName: rabbitmq-data-sigma capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle hostPath: path: "/opt/rabbitmq-data-sigma"</code> </pre> </div></div><br>  Aqu√≠ tomamos el caso m√°s simple como el almacenamiento persistente: hostPath (una carpeta regular en cada nodo K8s), pero puede usar cualquiera de los muchos tipos de vol√∫menes persistentes admitidos por K8s. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pvc.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolumeClaim apiVersion: v1 metadata: name: rabbitmq-data spec: storageClassName: rabbitmq-data-sigma accessModes: - ReadWriteMany resources: requests: storage: 10Gi</code> </pre> </div></div><br>  Crear reclamo de volumen en el volumen creado en el paso anterior.  Este reclamo se utilizar√° en StatefulSet como un almac√©n de datos persistente. <br><br><h3>  Servicios: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq-internal labels: app: rabbitmq spec: clusterIP: None ports: - name: http protocol: TCP port: 15672 - name: amqp protocol: TCP port: 5672 selector: app: rabbitmq</code> </pre> </div></div><br>  Creamos un servicio interno sin cabeza a trav√©s del cual funcionar√° el complemento Peer Discovery. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service_ext.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq labels: app: rabbitmq type: LoadBalancer spec: type: NodePort ports: - name: http protocol: TCP port: 15672 targetPort: 15672 nodePort: 31673 - name: amqp protocol: TCP port: 5672 targetPort: 5672 nodePort: 30673 selector: app: rabbitmq</code> </pre> </div></div><br>  Para que las aplicaciones en K8 funcionen con nuestro cl√∫ster, creamos un servicio equilibrador. <br><br>  Dado que necesitamos acceso al cl√∫ster RabbitMQ fuera de K8, pasamos por NodePort.  RabbitMQ estar√° disponible al acceder a cualquier nodo del cl√∫ster K8s en los puertos 31673 y 30673. En el trabajo real, no hay una gran necesidad de esto.  Cuesti√≥n de usabilidad del panel de administraci√≥n de RabbitMQ. <br><br>  Al crear un servicio con el tipo NodePort en K8, tambi√©n se crea impl√≠citamente un servicio con el tipo ClusterIP para servirlo.  Por lo tanto, las aplicaciones en K8 que necesitan trabajar con nuestro RabbitMQ podr√°n acceder al cl√∫ster en <i>amqp: // rabbitmq: 5672</i> <br><br><h3>  Configuraci√≥n: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_configmap.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-config data: enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s]. rabbitmq.conf: | cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443 ### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal queue_master_locator=min-masters cluster_formation.randomized_startup_delay_range.min = 0 cluster_formation.randomized_startup_delay_range.max = 2 cluster_formation.k8s.service_name = rabbitmq-internal cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> </div></div><br>  Creamos archivos de configuraci√≥n RabbitMQ.  La magia principal. <br><br><pre> <code class="plaintext hljs">enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s].</code> </pre><br>  Agregue los complementos necesarios a los permitidos para la descarga.  Ahora podemos usar el Peer Discovery autom√°tico en el K8S. <br><br><pre> <code class="plaintext hljs">cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s</code> </pre><br>  Exponemos el complemento necesario como back-end para el descubrimiento de pares. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443</code> </pre><br>  Especifique la direcci√≥n y el puerto a trav√©s del cual puede comunicarse con kubernetes apiserver.  Aqu√≠ puede especificar la direcci√≥n IP directamente, pero ser√° m√°s hermoso hacerlo. <br><br>  En el espacio de nombres predeterminado, generalmente se crea un servicio con el nombre kubernetes que conduce a k8-apiserver.  En diferentes opciones de instalaci√≥n de K8S, el espacio de nombres, el nombre del servicio y el puerto pueden ser diferentes.  Si algo en una instalaci√≥n particular es diferente, debe solucionarlo en consecuencia. <br><br>  Por ejemplo, nos enfrentamos al hecho de que en algunos grupos el servicio est√° en el puerto 443 y en algunos en 6443. Ser√° posible comprender que algo est√° mal en los registros de inicio de RabbitMQ, el tiempo de conexi√≥n a la direcci√≥n especificada aqu√≠ est√° claramente resaltado all√≠. <br><br><pre> <code class="plaintext hljs">### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname</code> </pre><br>  De manera predeterminada, el ejemplo especifica el tipo de direcci√≥n del nodo del cl√∫ster RabbitMQ por direcci√≥n IP.  Pero cuando reinicia el pod, obtiene una nueva IP cada vez.  Sorpresa!  El grupo est√° muriendo en agon√≠a. <br><br>  Cambie el direccionamiento a nombre de host.  StatefulSet nos garantiza la invariabilidad del nombre de host dentro del ciclo de vida de todo el StatefulSet, lo que nos conviene por completo. <br><br><pre> <code class="plaintext hljs">cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true</code> </pre><br>  Como cuando perdemos uno de los nodos, suponemos que se recuperar√° tarde o temprano, deshabilitamos la eliminaci√≥n autom√°tica por un grupo de nodos inaccesibles.  En este caso, tan pronto como el nodo regrese en l√≠nea, ingresar√° al cl√∫ster sin perder su estado anterior. <br><br><pre> <code class="plaintext hljs">cluster_partition_handling = autoheal</code> </pre> <br>  Este par√°metro determina las acciones del cl√∫ster en caso de p√©rdida de qu√≥rum.  Aqu√≠ solo necesita leer la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n sobre este tema</a> y comprender por s√≠ mismo lo que est√° m√°s cerca de un caso de uso espec√≠fico. <br><br><pre> <code class="plaintext hljs">queue_master_locator=min-masters</code> </pre> <br>  Determine la selecci√≥n del asistente para nuevas colas.  Con esta configuraci√≥n, el asistente seleccionar√° el nodo con el menor n√∫mero de colas, por lo que las colas se distribuir√°n de manera uniforme entre los nodos del cl√∫ster. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.service_name = rabbitmq-internal</code> </pre> <br>  Nombramos el servicio K8s sin cabeza (creado por nosotros anteriormente) a trav√©s del cual los nodos RabbitMQ se comunicar√°n entre s√≠. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> <br>  Una cosa importante para direccionar en un cl√∫ster es el nombre de host.  El FQDN del hogar K8s se forma como un nombre corto (rabbitmq-0, rabbitmq-1) + sufijo (parte del dominio).  Aqu√≠ indicamos este sufijo.  En K8S, se ve as√≠ <b>. &lt;Nombre del servicio&gt;. &lt;Nombre del espacio de nombres&gt; .svc.cluster.local</b> <br><br>  kube-dns resuelve los nombres de la forma rabbitmq-0.rabbitmq-internal.our-namespace.svc.cluster.local en la direcci√≥n IP de un pod espec√≠fico sin ninguna configuraci√≥n adicional, lo que hace posible toda la magia de la agrupaci√≥n por nombre de host. <br><br><h3>  Configuraci√≥n StatefulSet RabbitMQ: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_statefulset.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: rabbitmq spec: serviceName: rabbitmq-internal replicas: 3 template: metadata: labels: app: rabbitmq annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } } spec: serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 containers: - name: rabbitmq-k8s image: rabbitmq:3.7 volumeMounts: - name: config-volume mountPath: /etc/rabbitmq - name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia ports: - name: http protocol: TCP containerPort: 15672 - name: amqp protocol: TCP containerPort: 5672 livenessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 10 readinessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 10 imagePullPolicy: Always env: - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local" - name: K8S_SERVICE_NAME value: "rabbitmq-internal" - name: RABBITMQ_ERLANG_COOKIE value: "mycookie" volumes: - name: config-volume configMap: name: rabbitmq-config items: - key: rabbitmq.conf path: rabbitmq.conf - key: enabled_plugins path: enabled_plugins - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> </div></div><br>  En realidad, StatefulSet s√≠ mismo.  Tomamos nota de puntos interesantes. <br><br><pre> <code class="plaintext hljs">serviceName: rabbitmq-internal</code> </pre> <br>  Escribimos el nombre del servicio sin cabeza a trav√©s del cual los pods se comunican en StatefulSet. <br><br><pre> <code class="plaintext hljs">replicas: 3</code> </pre> <br>  Establezca el n√∫mero de r√©plicas en el cl√∫ster.  En nuestro pa√≠s, es igual al n√∫mero de nodos de trabajo K8. <br><br><pre> <code class="plaintext hljs">annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } }</code> </pre> <br>  Cuando uno de los nodos K8 cae, el conjunto con estado busca preservar el n√∫mero de instancias en el conjunto, por lo tanto, crea varios hogares en el mismo nodo K8.  Este comportamiento es completamente indeseable y, en principio, no tiene sentido.  Por lo tanto, prescribimos una regla antiafinidad para los conjuntos de hogares de setful.  Hacemos que la regla sea dif√≠cil (obligatorio) para que kube-Scheduler no pueda romperla cuando se planifican pods. <br><br>  La esencia es simple: est√° prohibido que el planificador coloque (dentro del espacio de nombres) m√°s de un pod con la <i>aplicaci√≥n: etiqueta rabbitmq</i> en cada nodo.  Distinguimos los <i>nodos</i> por el valor de la etiqueta <i>kubernetes.io/hostname</i> .  Ahora, si por alguna raz√≥n la cantidad de nodos K8S en funcionamiento es menor que la cantidad requerida de r√©plicas en StatefulSet, no se crear√°n nuevas r√©plicas hasta que aparezca un nodo libre nuevamente. <br><br><pre> <code class="plaintext hljs">serviceAccountName: rabbitmq</code> </pre> <br>  Registramos ServiceAccount, bajo el cual funcionan nuestros pods. <br><br><pre> <code class="plaintext hljs">image: rabbitmq:3.7</code> </pre> <br>  La imagen de RabbitMQ es completamente est√°ndar y se toma del docker hub; no requiere ninguna reconstrucci√≥n ni revisi√≥n de archivos. <br><br><pre> <code class="plaintext hljs">- name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia</code> </pre><br>  Los datos persistentes de RabbitMQ se almacenan en / var / lib / rabbitmq / mnesia.  Aqu√≠ montamos nuestro Reclamo de volumen persistente en esta carpeta para que al reiniciar los hogares / nodos o incluso todo el StatefulSet, los datos (tanto el servicio, incluido el cl√∫ster ensamblado y los datos del usuario) est√©n sanos y salvos.  Hay algunos ejemplos en los que toda la carpeta / var / lib / rabbitmq / se hace persistente.  Llegamos a la conclusi√≥n de que esta no es la mejor idea, ya que al mismo tiempo toda la informaci√≥n establecida por las configuraciones de Rabbit comienza a ser recordada.  Es decir, para cambiar algo en el archivo de configuraci√≥n, debe limpiar el almacenamiento persistente, lo cual es muy inconveniente en la operaci√≥n. <br><br><pre> <code class="plaintext hljs"> - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local"</code> </pre><br>  Con este conjunto de variables de entorno, primero le decimos a RabbitMQ que use el nombre FQDN como identificador para los miembros del cl√∫ster y, en segundo lugar, establecemos el formato de este nombre.  El formato se describi√≥ anteriormente al analizar la configuraci√≥n. <br><br><pre> <code class="plaintext hljs">- name: K8S_SERVICE_NAME value: "rabbitmq-internal"</code> </pre> <br>  El nombre del servicio sin cabeza para la comunicaci√≥n entre los miembros del cl√∫ster. <br><br><pre> <code class="plaintext hljs">- name: RABBITMQ_ERLANG_COOKIE value: "mycookie"</code> </pre> <br>  El contenido de la cookie Erlang debe ser el mismo en todos los nodos del cl√∫ster, debe registrar su propio valor.  Un nodo con una cookie diferente no puede ingresar al cl√∫ster. <br><br><pre> <code class="plaintext hljs">volumes: - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> <br>  Defina el volumen asignado a partir del Reclamo de volumen persistente creado anteriormente. <br><br>  Aqu√≠ es donde hemos terminado con la configuraci√≥n en los K8.  El resultado es un cl√∫ster RabbitMQ, que distribuye uniformemente las colas entre los nodos y es resistente a problemas en el entorno de tiempo de ejecuci√≥n. <br><br><img src="https://habrastorage.org/webt/_j/ky/mw/_jkymwmxe7syyjfa7h2idbcxosc.png" alt="imagen"><br><br>  Si uno de los nodos del cl√∫ster no est√° disponible, las colas contenidas en √©l dejar√°n de ser accesibles, todo lo dem√°s seguir√° funcionando.  Tan pronto como el nodo regrese a la operaci√≥n, volver√° al cl√∫ster, y las colas para las cuales era un Maestro volver√°n a estar operativas, preservando todos los datos contenidos en ellas (si el almacenamiento persistente no se ha roto, por supuesto).  Todos estos procesos son completamente autom√°ticos y no requieren intervenci√≥n. <br><br><h2>  Bono: personalizar HA </h2><br>  Uno de los proyectos fue un matiz.  Los requisitos sonaron como un reflejo completo de todos los datos contenidos en el cl√∫ster.  Esto es necesario para que en una situaci√≥n en la que al menos un nodo del cl√∫ster est√© operativo, todo contin√∫e funcionando desde el punto de vista de la aplicaci√≥n.  Este momento no tiene nada que ver con los K8, lo describimos simplemente como un mini tutorial. <br><br>  Para habilitar la HA completa, debe crear una Pol√≠tica en el panel de RabbitMQ en la pesta√±a <i>Admin -&gt; Pol√≠ticas</i> .  El nombre es arbitrario, el Patr√≥n est√° vac√≠o (todas las colas), en las Definiciones agregue dos par√°metros: <i>ha-mode: all</i> , <i>ha-sync-mode: automatic</i> . <br><br><img src="https://habrastorage.org/webt/jz/tn/vu/jztnvu5zygtv56hurbyss1w9ljm.png" alt="imagen"><br><br><img src="https://habrastorage.org/webt/_6/in/om/_6inoma38lvluhpaet1g66uus_u.png" alt="imagen"><br><br>  Despu√©s de eso, todas las colas creadas en el cl√∫ster estar√°n en modo de alta disponibilidad: si el nodo maestro no est√° disponible, uno de los esclavos ser√° seleccionado autom√°ticamente por el nuevo asistente.  Y los datos que entran en la cola se reflejar√°n en todos los nodos del cl√∫ster.  Que, de hecho, se requer√≠a para recibir. <br><br><img src="https://habrastorage.org/webt/0v/m3/je/0vm3jem0bi4fqckj8ucmiy5zcxe.png" alt="imagen"><br><br>  Lea m√°s sobre HA en RabbitMQ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> <br><br><h2>  Literatura √∫til: </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Repository RabbitMQ Peer Discovery Kubernetes Plugin</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ejemplo de configuraci√≥n para la implementaci√≥n de RabbitMQ en K8S</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Descripci√≥n de los principios de formaci√≥n de cl√∫ster, mecanismo de descubrimiento de pares y complementos para ello</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Discusi√≥n √©pica sobre la configuraci√≥n de descubrimiento basada en el nombre de host</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RabbitMQ Clustering Guide</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Descripci√≥n de problemas y soluciones de agrupamiento de cerebro dividido</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Colas de alta disponibilidad en RabbitMQ</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Configurar pol√≠ticas</a> </li></ul><br>  Buena suerte </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es419817/">https://habr.com/ru/post/es419817/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es419805/index.html">The Super Tiny Compiler - ahora en ruso</a></li>
<li><a href="../es419807/index.html">Glaucoma: c√≥mo no quedarse ciego: hablemos sobre el tratamiento ...</a></li>
<li><a href="../es419811/index.html">La evoluci√≥n de las pantallas flexibles.</a></li>
<li><a href="../es419813/index.html">Seminarios web de Skillbox: selecci√≥n del viernes</a></li>
<li><a href="../es419815/index.html">Secretos de tolerancia a fallas de nuestra oficina principal</a></li>
<li><a href="../es419819/index.html">Biomarcadores del envejecimiento. Fragilidad de panel. Parte 2</a></li>
<li><a href="../es419823/index.html">D√∫o inusual: frases de contrase√±a e im√°genes mnemot√©cnicas</a></li>
<li><a href="../es419825/index.html">Probar el rendimiento de varios tipos de unidades en un entorno virtual</a></li>
<li><a href="../es419829/index.html">El cifrado de clave predeterminado de OpenSSH es peor que ninguno</a></li>
<li><a href="../es419831/index.html">C√≥mo funciona JS: elementos personalizados</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>