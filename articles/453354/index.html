<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏽‍🚒 ⚛️ 🔑 Clasificación de la cobertura del suelo utilizando eo-learn. Parte 3 🔭 👨🏿 👨🏿‍💼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cuando necesita mejores resultados que satisfactorios 


 Parte 1 
 Parte 2 





 La transición de la zona de invierno a verano se compone de imágene...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Clasificación de la cobertura del suelo utilizando eo-learn. Parte 3</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/453354/"><p>  Cuando necesita mejores resultados que satisfactorios </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2</a> </p><br><p><img src="https://habrastorage.org/webt/c0/ls/b2/c0lsb2it_c9qwggm74kdk3uglw4.png"></p><br><p> <em>La transición de la zona de invierno a verano se compone de imágenes Sentinel-2.</em>  <em>Puede notar algunas diferencias en los tipos de cobertura en la nieve, que se describió en un artículo anterior.</em> </p><a name="habracut"></a><br><h2 id="predislovie">  Prólogo </h2><br><p> Las últimas dos semanas han sido muy difíciles.  Publicamos la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primera</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">segunda</a> parte de nuestros artículos sobre la clasificación de la cobertura en todo el país utilizando el marco <code>eo-learn</code> .  <code>eo-learn</code> es una biblioteca de código abierto para crear una capa entre la recepción y el procesamiento de imágenes satelitales y el aprendizaje automático.  En artículos anteriores en los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ejemplos,</a> indicamos solo un pequeño subconjunto de los datos y mostramos los resultados solo en un pequeño porcentaje del área de interés completa (AOI - área de interés).  Sé que esto parece al menos no muy impresionante, y quizás muy grosero de nuestra parte.  Todo este tiempo has sido atormentado por preguntas sobre cómo puedes usar este conocimiento y transferirlo al <em>siguiente</em> nivel. </p><br><p>  ¡No se preocupe, para eso es el tercer artículo de esta serie!  Toma una taza de café y toma asiento ... </p><br><h2 id="all-our-data-are-belong-to-you">  ¡Todos nuestros datos te pertenecen! </h2><br><p>  Ya estas sentado  Tal vez deje el café sobre la mesa por otro segundo, porque ahora escuchará las mejores noticias de hoy ... <br>  En Sinergise decidimos publicar el conjunto de datos completo para Eslovenia para 2017.  Gratis  ¡Puede acceder libremente a 200 GB de datos en forma de ~ 300 fragmentos de EOPatch, cada uno aproximadamente en el tamaño de 1000x1000, en una resolución de 10 m!  Puede leer más sobre el formato EOPatch en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">última publicación</a> sobre <code>eo-learn</code> , pero de hecho es un contenedor de datos <em>geo-temporales</em> EO (Observación de la Tierra) y no EO: por ejemplo, imágenes de satélite, máscaras, mapas, etc. </p><br><p><img src="https://habrastorage.org/webt/dc/nt/gy/dcntgywsu4la7pdpwegv5m6eskc.png"><br>  <em>Estructura EOPatch</em> ) </p><br><p>  No pirateamos cuando descargamos estos datos.  ¡Cada EOPatch contiene imágenes de Sentinel-2 L1C, su máscara <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">s2cloudless</a> correspondiente y el mapa oficial de cobertura del suelo en formato ráster! </p><br><p>  Los datos se almacenan en AWS S3 en: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">http://eo-learn.sentinel-hub.com/</a> </p><br><p>  Deserializar un objeto EOPatch es bastante simple: </p><br><pre> <code class="python hljs">EOPatch.load(<span class="hljs-string"><span class="hljs-string">'path_to_eopatches/eopatch-0x6/'</span></span>)</code> </pre> <br><p>  Como resultado, obtienes un objeto de la siguiente estructura: </p><br><pre> <code class="python hljs">EOPatch( data: { BANDS: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>), dtype=float32) } mask: { CLM: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_DATA: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_VALID: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=bool) } mask_timeless: { LULC: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) VALID_COUNT: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=int64) } meta_info: { maxcc: <span class="hljs-number"><span class="hljs-number">0.8</span></span> service_type: <span class="hljs-string"><span class="hljs-string">'wcs'</span></span> size_x: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> size_y: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> time_difference: datetime.timedelta(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">86399</span></span>) time_interval: (datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>), datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">31</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)) } bbox: BBox(((<span class="hljs-number"><span class="hljs-number">370230.5261411405</span></span>, <span class="hljs-number"><span class="hljs-number">5085303.344972428</span></span>), (<span class="hljs-number"><span class="hljs-number">380225.31836121203</span></span>, <span class="hljs-number"><span class="hljs-number">5095400.767924464</span></span>)), crs=EPSG:<span class="hljs-number"><span class="hljs-number">32633</span></span>) timestamp: [datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>), ..., datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>)], length=<span class="hljs-number"><span class="hljs-number">80</span></span> )</code> </pre> <br><p>  El acceso a los diversos atributos de EOPatch es el siguiente: </p><br><pre> <code class="python hljs">eopatch.timestamp eopatch.mask[<span class="hljs-string"><span class="hljs-string">'LULC'</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'CLM'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'BANDS'</span></span>][<span class="hljs-number"><span class="hljs-number">5</span></span>][..., [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]]</code> </pre> <br><h3 id="eoexecute-order-66">  EO Ejecutar orden 66 </h3><br><p>  Genial, los datos se están cargando.  Mientras esperamos la finalización de este proceso, echemos un vistazo a las capacidades de una clase que aún no se ha discutido en estos artículos: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><code>EOExecutor</code></a> .  Este módulo se dedica a la ejecución y monitoreo de la tubería y permite el uso de subprocesos múltiples sin esfuerzos innecesarios.  No más búsquedas en Stack Overflow sobre cómo paralelizar la tubería correctamente o cómo hacer que la barra de progreso funcione en este modo: ¡ya hemos hecho todo por usted! </p><br><p>  Además, maneja los errores que ocurren y puede generar un breve resumen del proceso de ejecución.  Este último es el momento más importante para estar seguro de la repetibilidad de sus resultados en el futuro, para que el usuario no tenga que pasar un tiempo de trabajo precioso buscando los parámetros que utilizó el jueves pasado a las 9 a.m. después de una noche de juerga (no mezcle alcohol y programación) vale la pena!).  ¡Esta clase también puede generar un buen gráfico de dependencia para la tubería, que puede mostrarle a su jefe! </p><br><p><img src="https://habrastorage.org/webt/_o/x7/0q/_ox70q41_uiebqp7opyqbeu0nx0.png"><br>  <em>Gráfico de dependencia de canalización generado por <code>eo-learn</code></em> </p><br><h3 id="eksperimenty-s-mashinnym-obucheniem">  Experimentos de aprendizaje automático </h3><br><p>  Como se prometió, este artículo está destinado principalmente a estudiar diferentes modelos con <code>eo-learn</code> utilizando los datos que proporcionamos.  A continuación, hemos preparado dos experimentos en los que estudiamos el efecto de las nubes y diferentes algoritmos de remuestreo durante la interpolación temporal sobre el resultado final.  Después de todo esto, comenzaremos a trabajar con redes de convolución (CNN) y compararemos los resultados de dos enfoques: el análisis píxel por píxel del árbol de decisión y el aprendizaje profundo utilizando redes neuronales convolucionales. </p><br><p>  Lamentablemente, no se puede dar una respuesta inequívoca sobre las decisiones que se deben tomar durante los experimentos.  Puede estudiar el área temática más profundamente y hacer suposiciones para decidir si el juego vale la pena, pero en última instancia, el trabajo se reducirá a prueba y error. </p><br><h3 id="igraem-s-oblakami">  Jugar con las nubes </h3><br><p>  Las nubes son un gran dolor en el mundo de EO, especialmente cuando se trata de algoritmos de aprendizaje automático, donde desea determinarlos y eliminarlos del conjunto de datos para la interpolación basada en valores perdidos.  Pero, ¿qué tan grande es el beneficio de este procedimiento?  ¿Vale la pena?  Rußwurm y Körner, en su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">artículo Clasificación de la cobertura de la tierra</a> multitemporal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">con codificadores secuenciales recurrentes,</a> incluso demostraron que para el aprendizaje profundo, el proceso de filtrado de nubes probablemente no tenga ninguna importancia, ya que el clasificador mismo puede detectar nubes e ignorarlas. </p><br><p><img src="https://habrastorage.org/webt/gz/c8/zs/gzc8zsp0nrdjtgbewqqysxulaiu.png"><br>  Activación de la capa de entrada (arriba) y la capa de modulación (abajo) en la secuencia de imágenes de un fragmento específico para una red neuronal.  Puede notar que este fragmento de red aprendió a crear máscaras de nube y filtrar los resultados obtenidos.  (Página 9 en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://www.researchgate.net/publication/322975904_Multi-Temporal_Land_Cover_Classification_with_Sequential_Recurrent_Encoders</a> ) </p><br><p>  Recordamos brevemente la estructura del paso de filtrado de datos (para más detalles, consulte [artículo anterior] ()).  Después de tomar instantáneas de Sentinel-2, comenzamos a filtrar instantáneas en la nube.  Todas las imágenes en las que el número de píxeles no nublados no supere el 80% están sujetas a selección (los valores de umbral pueden diferir para diferentes áreas de interés).  Después de eso, para obtener valores de píxeles en días arbitrarios, se usan máscaras de nubes para no tener en cuenta dichos datos. </p><br><p>  En total, cuatro comportamientos son posibles: </p><br><ol><li>  <strong>con</strong> filtro de imagen, máscaras de nubes <strong>dadas</strong> </li><li>  <strong>sin</strong> filtro de instantáneas, <strong>con</strong> máscaras de nube <strong>dadas</strong> </li><li>  <strong>con</strong> filtro de imagen, excluyendo máscaras de nubes </li><li>  <strong>sin</strong> filtro de imagen, <strong>sin incluir</strong> máscaras de nube </li></ol><br><p><img src="https://habrastorage.org/webt/rd/3i/ne/rd3ineypd8f0akhs41yve8mtgso.png"><br>  <em>Visualización visual de la pila de imágenes del satélite Sentinel-2.</em>  <em>Los píxeles transparentes a la izquierda significan píxeles faltantes debido a la capa de nubes.</em>  <em>La pila central muestra los valores de píxeles después de filtrar imágenes e interpolarlas con una máscara de nubes (Caso 4), y la pila de la derecha muestra el resultado de la interpolación en el caso sin filtrar imágenes y sin máscaras de nubes (1).</em>  <em>(Nota: aparentemente, el artículo contiene un error tipográfico, y significaba lo contrario: el caso 1 en el centro y el 4 a la derecha).</em> </p><br><p>  En el último artículo, ya realizamos una variación del caso 1 y mostramos los resultados, por lo que los utilizaremos para comparar.  Preparar otros transportadores y entrenar el modelo parece una tarea simple: solo necesita asegurarse de que estamos comparando los valores correctos.  Para hacer esto, solo tome el mismo conjunto de píxeles para entrenar y validar el modelo. </p><br><p>  Los resultados se muestran en la tabla a continuación.  ¡Puede ver que, en general, la influencia de las nubes en el resultado del modelo es bastante baja!  Esto puede deberse al hecho de que la tarjeta de referencia es de muy buena calidad y el modelo puede ignorar la mayoría de las imágenes.  En cualquier caso, este comportamiento no puede garantizarse para ningún AOI, ¡así que tómate tu tiempo para eliminar este paso de tus modelos! </p><br><div class="scrollable-table"><table><thead><tr><th>  Modelo </th><th>  Precisión [%] </th><th>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">F_1</a> [%] </th></tr></thead><tbody><tr><td>  Sin filtros, sin máscara. </td><td>  92,8 </td><td>  92,6 </td></tr><tr><td>  Sin filtros, con máscara. </td><td>  94,2 </td><td>  93,9 </td></tr><tr><td>  Con filtro, sin mascarilla </td><td>  94,0 </td><td>  93,8 </td></tr><tr><td>  Con filtro, con mascarilla </td><td>  94,4 </td><td>  94,1 </td></tr></tbody></table></div><br><h3 id="vliyanie-raznyh-podhodov-k-resemplingu">  El impacto de diferentes enfoques de remuestreo </h3><br><p>  La elección de las opciones de remuestreo temporal no es obvia.  Por un lado, necesitamos una matriz detallada de imágenes que muestren bien los detalles de las imágenes de origen; queremos incluir el número de imágenes más cercano posible a los datos de origen.  Por otro lado, estamos limitados por los recursos informáticos.  La reducción del paso de remuestreo duplica el número de fotogramas después de la interpolación y, por lo tanto, aumenta el número de atributos que se utilizan en el entrenamiento.  ¿Vale esa mejora el costo de los recursos?  Esto es lo que tenemos que descubrir. </p><br><p>  Para este experimento, utilizaremos la variación 1 del paso anterior.  Después de la interpolación, volvemos a muestrear con las siguientes variaciones: </p><br><ol><li>  Muestreo uniforme con un intervalo de 16 días. </li><li>  Muestreo uniforme con un intervalo de 8 días. </li><li>  La elección de las "mejores" fechas, el número coincide con el caso 2. </li></ol><br><p>  La muestra en el caso 3 se basa en el mayor número de fechas comunes para todos los EOPatch en el AOI seleccionado <br><img src="https://habrastorage.org/webt/xg/qa/9w/xgqa9w17-oe4dbtxca22yejwhzo.png"><br>  <em>El gráfico muestra el número de fragmentos de EOPatch que contienen datos para cada día de 2017 (azul).</em>  <em>Las líneas rojas muestran las fechas óptimas para el remuestreo, que se basan en las fechas de las imágenes de Sentinel-2 para el AOI 2017 dado.</em> </p><br><p>  Mirando la tabla a continuación, puede ver que los resultados no son muy impresionantes, como en la experiencia pasada.  Para los casos 2 y 3, la cantidad de tiempo invertido se duplica, pero la diferencia con el enfoque inicial es inferior al 1%.  Dichas mejoras son demasiado discretas para un uso práctico, por lo que podemos considerar el intervalo de 16 días adecuado para la tarea. </p><br><div class="scrollable-table"><table><thead><tr><th>  Modelo </th><th>  Precisión [%] </th><th>  F_1 [%] </th></tr></thead><tbody><tr><td>  Uniformemente cada 16 días </td><td>  94,4 </td><td>  94,1 </td></tr><tr><td>  Uniformemente cada 8 días </td><td>  94,5 </td><td>  94,3 </td></tr><tr><td>  Elegir las mejores fechas </td><td>  94,6 </td><td>  94,4 </td></tr></tbody></table></div><br><p>  <em>Resultados de precisión general y F1 ponderada para diferentes tuberías con un cambio en el enfoque de remuestreo.</em> </p><br><h2 id="glubokoe-obuchenie-ispolzuem-svyortochnuyu-neyronnuyu-set-cnn">  Aprendizaje profundo: uso de la red neuronal convolucional (CNN) </h2><br><p>  El aprendizaje profundo se ha convertido en el enfoque estándar para muchas tareas, como la visión por computadora, el procesamiento de palabras en lenguaje natural y el procesamiento de señales.  Esto se debe a su capacidad para extraer patrones de entradas multidimensionales complejas.  Los enfoques clásicos de aprendizaje automático (como los árboles de decisión) se han utilizado en muchas tareas de geodatos temporales.  Las redes convolucionales, por otro lado, se utilizaron para analizar la correlación espacial entre imágenes adyacentes.  Básicamente, su uso se limitó a trabajar con imágenes individuales. </p><br><p>  Queríamos estudiar la arquitectura de los modelos de aprendizaje profundo e intentar elegir uno que sea capaz de analizar los aspectos espaciales y temporales de los datos satelitales al mismo tiempo. </p><br><p>  Para hacer esto, utilizamos Netvork temporal totalmente convolucional, TFCN, o más bien, la extensión temporal a U-Net, implementada en TensorFlow.  Más específicamente, la arquitectura utiliza correlaciones espacio-temporales para mejorar el resultado.  Una ventaja adicional es que la estructura de red le permite representar mejor las relaciones espaciales a diferentes escalas gracias al proceso de codificación / decodificación en U-net.  Como en los modelos clásicos, en la salida obtenemos una matriz bidimensional de etiquetas, que compararemos con la verdad. </p><br><p><img src="https://habrastorage.org/webt/p0/jl/mg/p0jlmgxi9euwvodwonx4zrmezsw.png"></p><br><p>  Utilizamos el modelo entrenado para predecir las marcas en el conjunto de prueba, y los valores obtenidos se verificaron con la verdad.  En general, la precisión fue del 84,4% y F1 fue del 85,4%. </p><br><p><img src="https://habrastorage.org/webt/ol/z2/zj/olz2zjp3waghaak9hnirzcwa258.png"></p><br><p>  <em>Comparación de diferentes predicciones para nuestra tarea.</em>  <em>Imagen visual (arriba a la izquierda), mapa de referencia real (arriba a la derecha), predicción del modelo LightGBM (abajo a la izquierda) y predicción de U-net (abajo a la derecha)</em> </p><br><p>  Estos resultados muestran solo el trabajo inicial en este prototipo, que no está altamente optimizado para la tarea actual.  A pesar de esto, los resultados concuerdan con algunas estadísticas obtenidas en la región.  Para liberar el potencial de una red neuronal, es necesario optimizar la arquitectura (conjunto de atributos, profundidad de red, número de convoluciones), así como establecer hiperparámetros (velocidad de aprendizaje, número de eras, ponderación de clase).  Esperamos profundizar aún más en este tema (ja, ja) aún más, y planeamos distribuir nuestro código cuando esté en una forma aceptable. </p><br><h3 id="drugie-eksperimenty">  Otros experimentos </h3><br><p>  Puede encontrar <em>muchas</em> formas de mejorar sus resultados actuales, pero no podemos resolverlas ni probarlas todas.  ¡Es en ese momento que apareces en la escena!  ¡Muestre lo que puede hacer con este conjunto de datos y ayúdenos a mejorar los resultados! </p><br><p>  Por ejemplo, en el futuro cercano, uno de nuestros colegas participará en la clasificación de la cobertura basada en la pila temporal de imágenes <em>individuales</em> utilizando redes de convolución.  La idea es que algunas superficies, por ejemplo, las artificiales, se pueden distinguir sin características temporales, bastante espaciales.  ¡Estaremos encantados de escribir un artículo separado cuando este trabajo conduzca a resultados! </p><br><h3 id="ot-perevodchika">  Del traductor </h3><br><p>  Desafortunadamente, la siguiente parte de esta serie de artículos no salió, lo que significa que los autores no mostraron ejemplos de código fuente con la construcción de U-Net.  Como alternativa, puedo ofrecer las siguientes fuentes: </p><br><ol><li>  <em>U-Net: redes convolucionales para la segmentación de imágenes biomédicas - Olaf Ronneberger, Philipp Fischer, Thomas Brox</em> es uno de los artículos básicos sobre arquitectura U-Net que no involucra datos temporales. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html</a> : la página de documentación de eo-learn, donde (posiblemente) se encuentra una versión más reciente de tuberías de 1.2 partes. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/divamgupta/image-segmentation-keras</a> - Un repositorio con varias redes implementadas usando keras.  Tengo algunas preguntas sobre las implementaciones (son ligeramente diferentes de las descritas en los artículos originales), pero en general, las soluciones se adaptan fácilmente para fines personales y funcionan bastante bien. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/453354/">https://habr.com/ru/post/453354/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../453342/index.html">Mitos sobre empleados remotos que nos destruimos</a></li>
<li><a href="../453346/index.html">Tecnologías de almacenamiento y protección de datos: el tercer día en VMware EMPOWER 2019</a></li>
<li><a href="../453348/index.html">¿Qué hay dentro de asyncio?</a></li>
<li><a href="../453350/index.html">Transmisión abierta de la sala principal de RIT ++ 2019</a></li>
<li><a href="../453352/index.html">Cómo los drones entregan medicamentos vitales en Ghana</a></li>
<li><a href="../453356/index.html">Tendencias actuales y recomendaciones sobre aglomeración de grandes instituciones financieras</a></li>
<li><a href="../453360/index.html">Ciudad sin atascos</a></li>
<li><a href="../453362/index.html">HabraConf # 1: hacia atrás para backend</a></li>
<li><a href="../453364/index.html">Una historia de lanzamiento que afectó todo</a></li>
<li><a href="../453366/index.html">Cómo usar comas en inglés: 15 reglas y ejemplos de error</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>