<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñïüèª ‚òùüèº ‚öîÔ∏è An√°lisis de rendimiento de VM en VMware vSphere. Parte 2: memoria üë©üèæ‚Äç‚úàÔ∏è üï≥Ô∏è ü§Ω</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parte 1. Acerca de la CPU 
 Parte 3. Acerca del almacenamiento 

 En este art√≠culo, hablaremos sobre los contadores de rendimiento de RAM en vSphere. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>An√°lisis de rendimiento de VM en VMware vSphere. Parte 2: memoria</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dataline/blog/455820/"><img src="https://habrastorage.org/webt/el/am/7y/elam7yyhc6vrowmmrt5ofxgj-r0.png"><br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1. Acerca de la CPU</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 3. Acerca del almacenamiento</a> <br><br>  En este art√≠culo, hablaremos sobre los contadores de rendimiento de RAM en vSphere. <br>  Parece que la memoria es cada vez m√°s inequ√≠voca que con el procesador: si hay problemas de rendimiento en la VM, es dif√≠cil no notarlos.  Pero si aparecen, tratar con ellos es mucho m√°s dif√≠cil.  Pero lo primero es lo primero. <a name="habracut"></a><br><br><h3>  Poco de teor√≠a </h3><br>  La RAM de las m√°quinas virtuales se toma de la memoria del servidor en el que se ejecutan las m√°quinas virtuales.  Esto es bastante obvio :).  Si la RAM del servidor no es suficiente para todos, ESXi comienza a aplicar t√©cnicas de recuperaci√≥n de memoria.  De lo contrario, los sistemas operativos VM se bloquear√≠an con errores de acceso a RAM. <br><br>  Qu√© t√©cnicas usar ESXi decide seg√∫n la carga de RAM: <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Estado de la memoria</b> <br></td><td>  <b>La frontera</b> <br></td><td>  <b>Acciones</b> <br></td></tr><tr><td>  Alta <br></td><td> 400% de min Gratis <br></td><td>  Despu√©s de alcanzar el l√≠mite superior, las p√°ginas grandes de memoria se dividen en peque√±as (TPS funciona en modo est√°ndar). <br></td></tr><tr><td>  Claro <br></td><td>  100% de min Gratis <br></td><td>  Las p√°ginas grandes de memoria se dividen en peque√±as, TPS funciona a la fuerza. <br></td></tr><tr><td>  Suave <br></td><td>  64% de minFree <br></td><td>  TPS + globo <br></td></tr><tr><td>  Duro <br></td><td>  32% de min Gratis <br></td><td>  TPS + Comprimir + Intercambiar <br></td></tr><tr><td>  Bajo <br></td><td>  16% de min Gratis <br></td><td>  Comprimir + Intercambiar + Bloquear <br></td></tr></tbody></table></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Fuente</a> <br><br>  minFree es la RAM necesaria para que funcione el hipervisor. <br><br>  Antes de ESXi 4.1 inclusive, minFree se reparaba de manera predeterminada: 6% de la RAM del servidor (el porcentaje se pod√≠a cambiar a trav√©s de la opci√≥n Mem.MinFreePct en ESXi).  En versiones posteriores, debido al aumento en los vol√∫menes de memoria en los servidores minFree, comenz√≥ a calcularse en funci√≥n del tama√±o de la memoria del host y no como un valor de porcentaje fijo. <br><br>  El valor minFree (predeterminado) se calcula de la siguiente manera: <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Porcentaje de memoria reservada para minFree</b> <br></td><td>  <b>Rango de memoria</b> <br></td></tr><tr><td>  6% <br></td><td>  0-4 GB <br></td></tr><tr><td>  4% <br></td><td>  4-12 GB <br></td></tr><tr><td>  2% <br></td><td>  12-28 GB <br></td></tr><tr><td>  1% <br></td><td>  Memoria restante <br></td></tr></tbody></table></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Fuente</a> <br><br>  Por ejemplo, para un servidor con 128 GB de RAM, el valor MinFree ser√≠a: <br>  MinFree = 245.76 + 327.68 + 327.68 + 1024 = 1925.12 MB = 1.88 GB <br>  El valor real puede diferir en un par de cientos de MB, depende del servidor y la RAM. <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Porcentaje de memoria reservada para minFree</b> <br></td><td>  <b>Rango de memoria</b> <br></td><td>  <b>Valor por 128 GB</b> <br></td></tr><tr><td>  6% <br></td><td>  0-4 GB <br></td><td>  245,76 MB <br></td></tr><tr><td>  4% <br></td><td>  4-12 GB <br></td><td>  327,68 MB <br></td></tr><tr><td>  2% <br></td><td>  12-28 GB <br></td><td>  327,68 MB <br></td></tr><tr><td>  1% <br></td><td>  Memoria restante (100 GB) <br></td><td>  1024 MB <br></td></tr></tbody></table></div><br><br>  T√≠picamente, para rodales productivos, solo Alto puede considerarse normal.  Para bancos de pruebas y desarrollo, pueden ser aceptables condiciones claras / suaves.  Si queda menos del 64% de MinFree de RAM en el host, entonces las m√°quinas virtuales que se ejecutan en √©l definitivamente experimentar√°n problemas de rendimiento. <br><br>  En cada estado, se aplican ciertas t√©cnicas de recuperaci√≥n de memoria comenzando con TPS, que pr√°cticamente no afecta el rendimiento de la VM, y termina con Swapping.  Te contar√© m√°s sobre ellos. <br><br>  <b>Uso compartido de p√°gina transparente (TPS).</b>  TPS es, en t√©rminos generales, deduplicaci√≥n de las p√°ginas de RAM de m√°quinas virtuales en un servidor. <br><br>  ESXi busca p√°ginas id√©nticas de RAM de m√°quina virtual, contando y comparando la suma de hash de p√°ginas, y elimina p√°ginas duplicadas, reemplaz√°ndolas con enlaces a la misma p√°gina en la memoria f√≠sica del servidor.  Como resultado, se reduce el consumo de memoria f√≠sica y se puede volver a suscribir la memoria con poca o ninguna p√©rdida de rendimiento. <br><br><img src="https://habrastorage.org/webt/ul/fz/1i/ulfz1i0bomyhsarceziylov-o6i.jpeg"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Fuente</a> <br><br>  Este mecanismo funciona solo para p√°ginas de 4 kB (p√°ginas peque√±as).  P√°ginas de 2 MB de tama√±o (p√°ginas grandes) que el hipervisor ni siquiera intenta deduplicar: la posibilidad de encontrar p√°ginas id√©nticas de este tama√±o no es grande. <br><br>  Por defecto, ESXi asigna memoria a p√°ginas grandes.  La divisi√≥n de p√°ginas grandes en peque√±as comienza cuando se alcanza el umbral del estado Alto y se fuerza cuando se alcanza el estado Borrar (consulte la tabla de estados del hipervisor). <br><br>  Si desea que TPS comience a funcionar sin esperar a que se llene la RAM del host, en Advanced Options ESXi debe establecer el valor <i>"Mem.AllocGuestLargePage"</i> en 0 (el valor predeterminado es 1).  Luego, se deshabilitar√° la asignaci√≥n de p√°ginas grandes de memoria para m√°quinas virtuales. <br><br>  Desde diciembre de 2014, en todas las versiones de ESXi, el TPS entre m√°quinas virtuales se ha deshabilitado de forma predeterminada, ya que se ha encontrado una vulnerabilidad que te√≥ricamente permite acceder a la RAM de otra m√°quina virtual desde una m√°quina virtual.  Detalles aqu√≠  Informaci√≥n sobre la implementaci√≥n pr√°ctica de la explotaci√≥n de la vulnerabilidad TPS que no he conocido. <br><br>  La pol√≠tica de TPS se controla a trav√©s de la opci√≥n avanzada <i>"Mem.ShareForceSalting"</i> en ESXi: <br>  0: TPS entre m√°quinas virtuales.  TPS funciona para p√°ginas de diferentes m√°quinas virtuales; <br>  1 - TPS para m√°quinas virtuales con el mismo valor "sched.mem.pshare.salt" en VMX; <br>  2 (predeterminado): Intra-VM TPS.  TPS funciona para p√°ginas dentro de una VM. <br><br>  Definitivamente tiene sentido apagar p√°ginas grandes y habilitar Inter-VM TPS en bancos de prueba.  Tambi√©n se puede usar para stands con una gran cantidad de m√°quinas virtuales del mismo tipo.  Por ejemplo, en stands con VDI, el ahorro de memoria f√≠sica puede alcanzar decenas de por ciento. <br><br>  <b>Globos de memoria.</b>  El globo ya no es una t√©cnica tan inofensiva y transparente para el sistema operativo VM como TPS.  Pero con el uso adecuado con el globo puede vivir e incluso trabajar. <br><br>  Junto con Vmware Tools, se instala un controlador especial en la VM, llamado Balloon Driver (tambi√©n conocido como vmmemctl).  Cuando el hipervisor comienza a quedarse sin memoria f√≠sica y entra en el estado suave, ESXi le pide a la m√°quina virtual que devuelva la RAM no utilizada a trav√©s de este controlador de globo.  El controlador, a su vez, funciona a nivel del sistema operativo y solicita memoria libre de √©l.  El hipervisor ve qu√© p√°ginas de memoria f√≠sica ha tomado Balloon Driver, toma la memoria de la m√°quina virtual y la devuelve al host.  No hay problemas con el funcionamiento del sistema operativo, ya que a nivel del sistema operativo la memoria est√° ocupada por Balloon Driver.  De forma predeterminada, Balloon Driver puede ocupar hasta el 65% de la memoria de VM. <br><br>  Si las herramientas de VMware no est√°n instaladas en la m√°quina virtual o la funci√≥n de globo est√° desactivada (no lo recomiendo, pero hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">KB</a> :), el hipervisor cambia inmediatamente a m√©todos m√°s estrictos para eliminar la memoria.  Conclusi√≥n: aseg√∫rese de que las herramientas de VMware en la VM lo sean. <br><br><img src="https://habrastorage.org/webt/ey/pm/s1/eypms1ugdkmhr0r4odhga1locao.png"><br>  <i>La operaci√≥n de Balloon Driver se puede verificar desde el sistema operativo a trav√©s de VMware Tools</i> . <br><br>  <b>Compresi√≥n de la memoria</b>  Esta t√©cnica se usa cuando ESXi llega a Hard.  Como su nombre indica, ESXi est√° tratando de comprimir 4 KB de p√°ginas RAM a 2 KB y, por lo tanto, liberar algo de espacio en la memoria f√≠sica del servidor.  Esta t√©cnica aumenta significativamente el tiempo de acceso a los contenidos de las p√°ginas de la memoria RAM de la m√°quina virtual, ya que la p√°gina debe limpiarse primero.  A veces, no todas las p√°ginas se pueden comprimir y el proceso en s√≠ lleva algo de tiempo.  Por lo tanto, esta t√©cnica no es muy efectiva en la pr√°ctica. <br><br>  <b>Intercambio de memoria.</b>  Despu√©s de una breve fase, Memory Compression ESXi casi inevitablemente (si las m√°quinas virtuales no fueron a otros hosts o se cerraron) va a Swapping.  Y si queda muy poca memoria (estado bajo), el hipervisor tambi√©n deja de asignar p√°ginas de memoria de VM, lo que puede causar problemas en las m√°quinas virtuales invitadas. <br><br>  As√≠ es como funciona el intercambio.  Cuando enciende la m√°quina virtual, se crea un archivo con la extensi√≥n .vswp.  En tama√±o, es igual a la RAM no reservada de la VM: esta es la diferencia entre la memoria configurada y reservada.  Cuando se trabaja con Swapping, ESXi descarga las p√°ginas de memoria de la m√°quina virtual en este archivo y comienza a trabajar con √©l en lugar de la memoria f√≠sica del servidor.  Por supuesto, tal memoria "RAM" es varios √≥rdenes de magnitud m√°s lenta que la memoria real, incluso si .vswp est√° en almacenamiento r√°pido. <br><br>  A diferencia del globo, cuando las p√°ginas no utilizadas se seleccionan de una VM, las p√°ginas que el sistema operativo o las aplicaciones usan activamente dentro de la VM pueden ir al disco durante el intercambio.  Como resultado, el rendimiento de la VM disminuye hasta que se bloquea.  VM funciona formalmente y al menos se puede desactivar correctamente desde el sistema operativo.  Si vas a ser paciente;) <br><br>  Si las m√°quinas virtuales se han cambiado, esta es una situaci√≥n anormal que es mejor evitar si es posible. <br><br><h3>  Contadores b√°sicos de rendimiento de memoria de m√°quina virtual </h3><br>  Entonces llegamos a lo principal.  Para monitorear el estado de la memoria en la VM, est√°n disponibles los siguientes contadores: <br><br>  <b>Activo</b> : muestra la cantidad de RAM (Kbytes) a la que la VM obtuvo acceso en el per√≠odo de medici√≥n anterior. <br><br>  <b>El uso</b> es el mismo que Activo, pero como un porcentaje de la memoria de VM configurada.  Se calcula usando la siguiente f√≥rmula: activo √∑ tama√±o de memoria configurado de m√°quina virtual. <br>  High Usage y Active, respectivamente, no siempre son indicativos de problemas de rendimiento de VM.  Si una VM usa agresivamente memoria (al menos tiene acceso a ella), esto no significa que no hay suficiente memoria.  M√°s bien, esta es una ocasi√≥n para ver lo que est√° sucediendo en el sistema operativo. <br>  Hay una alarma est√°ndar sobre el uso de memoria para m√°quinas virtuales: <br><br><img src="https://habrastorage.org/webt/8u/ca/n8/8ucan84mevajwvlnr-4ov9boyro.png"><br><br>  <b>Compartido</b> : la cantidad de RAM en una VM deduplicada usando TPS (dentro de una VM o entre VM). <br><br>  <b>Concedido</b> : la cantidad de memoria f√≠sica del host (Kbytes) que se le dio a la VM.  Incluye Compartido. <br><br>  <b>Consumido</b> (concedido - compartido): la cantidad de memoria f√≠sica (Kbytes) que la VM consume del host.  No incluye Compartido. <br><br>  Si parte de la memoria de la VM no se asigna desde la memoria f√≠sica del host, sino desde el archivo de intercambio o la memoria se tom√≥ de la VM a trav√©s del controlador de globo, esta cantidad no se tiene en cuenta en Granted and Consumed. <br>  Los valores altos de Concedido y Consumido son perfectamente normales.  El sistema operativo elimina gradualmente la memoria del hipervisor y no devuelve.  Con el tiempo, con una VM que trabaja activamente, los valores de estos contadores se acercan a la cantidad de memoria configurada y permanecen all√≠. <br><br>  <b>Cero</b> : la cantidad de RAM en la VM (Kbytes), que contiene ceros.  Dicha memoria se considera un hipervisor gratuito y se puede dar a otras m√°quinas virtuales.  Despu√©s de que el SO hu√©sped lo recibi√≥, escribi√≥ algo en la memoria nula, va a Consumed y no regresa. <br><br>  <b>Gastos generales reservados</b> : la cantidad de RAM en la VM (Kbytes) reservada por el hipervisor para que la VM funcione.  Esta es una cantidad peque√±a, pero debe estar disponible en el host, de lo contrario, la VM no se iniciar√°. <br><br>  <b>Globo</b> : la cantidad de RAM (KB) incautada de la VM utilizando el controlador de globo. <br><br>  <b>Comprimido</b> : la cantidad de RAM (KB) que se pudo comprimir. <br><br>  <b>Intercambiado</b> : la cantidad de RAM (Kbytes), que por falta de memoria f√≠sica en el servidor se movi√≥ al disco. <br>  Los contadores de globo y otras t√©cnicas de recuperaci√≥n de memoria son cero. <br><br>  As√≠ es como se ve el gr√°fico con los contadores de memoria de una VM que funciona normalmente con 150 GB de RAM. <br><br><img src="https://habrastorage.org/webt/0l/pp/w3/0lppw3nz9iqzcnuergtxiseb67s.png"><br><br>  En el gr√°fico a continuaci√≥n, la VM tiene problemas obvios.  El gr√°fico muestra que para esta VM se utilizaron todas las t√©cnicas descritas para trabajar con RAM.  El globo para esta VM es mucho m√°s grande que Consumido.  De hecho, la VM est√° m√°s probablemente muerta que viva. <br><br><img src="https://habrastorage.org/webt/f4/ic/xk/f4icxkpxpykxua_gp-sirlzuu_u.png"><br><br><h3>  ESXTOP </h3><br>  Al igual que con la CPU, si desea evaluar r√°pidamente la situaci√≥n en el host, as√≠ como su din√°mica con un intervalo de hasta 2 segundos, vale la pena usar ESXTOP. <br><br>  La pantalla ESXTOP Memory se llama con la tecla "m" y tiene este aspecto (campos B, D, H, J, K, L, O seleccionados): <br><br><img src="https://habrastorage.org/webt/rm/wj/4k/rmwj4krvvizdtcizkrid0zjuml8.png"><br><br>  Los siguientes par√°metros nos ser√°n interesantes: <br><br>  Promedio de exceso de memoria: el valor promedio de una suscripci√≥n excesiva de memoria en un host durante 1, 5 y 15 minutos.  Si est√° por encima de cero, esta es una ocasi√≥n para ver qu√© sucede, pero no siempre es un indicador de la presencia de problemas. <br><br>  En las l√≠neas <b>PMEM / MB</b> y <b>VMKMEM / MB</b> : informaci√≥n sobre la memoria f√≠sica del servidor y la memoria disponible para VMkernel.  Desde lo interesante aqu√≠ puede ver el valor minfree (en MB), el estado del host desde la memoria (en nuestro caso, alto). <br><br>  En la l√≠nea <b>NUMA / MB,</b> puede ver la distribuci√≥n de RAM por NUMA-nodos (sockets).  En este ejemplo, la distribuci√≥n es desigual, lo que en principio no es muy bueno. <br><br>  El siguiente es un resumen de las estad√≠sticas del servidor para las t√©cnicas de recuperaci√≥n de memoria: <br><br>  <b>PSHARE / MB</b> es estad√≠sticas TPS; <br><br>  <b>SWAP / MB</b> : estad√≠sticas sobre el uso de Swap; <br><br>  <b>ZIP / MB</b> : estad√≠sticas de compresi√≥n de p√°ginas de memoria; <br><br>  <b>MEMCTL / MB</b> : estad√≠sticas de uso del controlador de globo. <br><br>  Para m√°quinas virtuales individuales, podemos estar interesados ‚Äã‚Äãen la siguiente informaci√≥n.  Escond√≠ los nombres de las m√°quinas virtuales para no avergonzar a la audiencia :).  Si la m√©trica ESXTOP es la misma que el contador en vSphere, cito el contador correspondiente. <br><br>  <b>MEMSZ</b> es la cantidad de memoria configurada en la VM (MB). <br>  MEMSZ = GRANT + MCTLSZ + SWCUR + sin tocar. <br><br>  <b>CONCESI√ìN</b> - Concedida en MB. <br><br>  <b>TCHD</b> : activo en MB. <br><br>  <b>MCTL?</b>  - est√° instalado en VM Balloon Driver. <br><br>  <b>MCTLSZ</b> - Globo en MB. <br><br>  <b>MCTLGT</b> es la cantidad de RAM (MB) que ESXi desea eliminar de la VM a trav√©s del controlador de globo (Destino de Memctl). <br><br>  <b>MCTLMAX</b> : la cantidad m√°xima de RAM (MB) que ESXi puede eliminar de la VM a trav√©s del controlador de globo. <br><br>  <b>SWCUR</b> : la cantidad actual de RAM (MB) dada a la VM desde el archivo de intercambio. <br><br>  <b>SWGT</b> : la cantidad de RAM (MB) que ESXi quiere dar a las m√°quinas virtuales desde un archivo de intercambio (Swap Target). <br><br>  Tambi√©n a trav√©s de ESXTOP puede ver informaci√≥n m√°s detallada sobre la topolog√≠a de VM NUMA.  Para hacer esto, seleccione los campos D, G: <br><br><img src="https://habrastorage.org/webt/ff/7y/zd/ff7yzdsjedyntnpj4duwv0c731m.png"><br><br>  <b>NHN</b> : nodos NUMA en los que se encuentra la VM.  Aqu√≠ puede ver inmediatamente una amplia vm que no cabe en un nodo NUMA. <br><br>  <b>NRMEM</b> : cu√°ntos megabytes de memoria ocupa la VM del nodo NUMA remoto. <br><br>  <b>NLMEM</b> : cu√°ntos megabytes de memoria ocupa la m√°quina virtual del nodo NUMA local. <br><br>  <b>N% L</b> : porcentaje de memoria de VM en el nodo NUMA local (si es inferior al 80%, pueden producirse problemas de rendimiento). <br><br><h3>  Memoria en el hipervisor. </h3><br>  Si los contadores de CPU en el hipervisor generalmente no son de especial inter√©s, entonces la situaci√≥n es lo contrario con la memoria.  Un uso elevado de memoria en la VM no siempre indica un problema de rendimiento, pero un uso elevado de memoria en el hipervisor solo inicia al t√©cnico de administraci√≥n de memoria y causa problemas con el rendimiento de la VM.  Las alarmas de uso de la memoria del host deben monitorearse y las m√°quinas virtuales no deben ingresar a Swap. <br><br><img src="https://habrastorage.org/webt/h2/x_/59/h2x_59kddpe84yzudcq03fq1rmc.png"><br><br><img src="https://habrastorage.org/webt/oc/w7/c9/ocw7c9vmbrqogjhmtpbotng4-6y.png"><br><br><h3>  Unswap </h3><br>  Si la VM entr√≥ en Swap, su rendimiento se reduce considerablemente.  Los rastros de globo y compresi√≥n desaparecen r√°pidamente despu√©s de la aparici√≥n de RAM libre en el host, pero la m√°quina virtual no tiene prisa por volver de Swap a la RAM del servidor. <br>  Antes de ESXi 6.0, la √∫nica forma confiable y r√°pida de sacar las m√°quinas virtuales de Swap era reiniciando (m√°s precisamente, apagando / encendiendo el contenedor).  Comenzando con ESXi 6.0, apareci√≥ una forma no tan oficial pero confiable y funcional de sacar las m√°quinas virtuales de Swap.  En una de las conferencias, logr√© hablar con uno de los ingenieros de VMware responsables del Programador de CPU.  Confirm√≥ que el m√©todo funciona bastante y es seguro.  En nuestra experiencia, tampoco se notaron problemas con √©l. <br><br>  Duncan Epping <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">describi√≥</a> los comandos reales para generar m√°quinas virtuales desde Swap.  No repetir√© la descripci√≥n detallada, solo dar√© un ejemplo de su uso.  Como se puede ver en la captura de pantalla, alg√∫n tiempo despu√©s de la ejecuci√≥n de los comandos de intercambio especificados en la VM desaparece. <br><br><img src="https://habrastorage.org/webt/e5/lm/7e/e5lm7e0e6i_yxrrlm7dfwixptv0.png"><br><br><h3>  Consejos para administrar RAM en ESXi </h3><br>  En conclusi√≥n, le dar√© algunos consejos para ayudarlo a evitar problemas con el rendimiento de la VM debido a la RAM: <br><br><ul><li>  Evite la suscripci√≥n excesiva en RAM en cl√∫steres productivos.  Siempre es aconsejable tener ~ 20-30% de memoria libre en el cl√∫ster, de modo que DRS (y el administrador) tengan espacio para maniobrar, y las m√°quinas virtuales no vayan a Swap durante la migraci√≥n.  Adem√°s, no olvide el margen de tolerancia a fallas.  Es desagradable cuando, cuando falla un servidor y la VM se reinicia usando HA, algunas de las m√°quinas tambi√©n van a Swap. </li><li>  En infraestructuras altamente consolidadas, intente NO crear m√°quinas virtuales con m√°s de la mitad de la memoria del host.  Esto, nuevamente, ayudar√° a DRS a distribuir m√°quinas virtuales entre los servidores del cl√∫ster sin ning√∫n problema.  Esta regla, por supuesto, no es universal :). </li><li>  Tenga cuidado con la alarma de uso de memoria del host. </li><li>  No olvide poner VMware Tools en la VM y no desactive el globo. </li><li>  Considere habilitar TPS entre m√°quinas virtuales y deshabilitar p√°ginas grandes en VDI y entornos de prueba. </li><li>  Si la VM est√° experimentando problemas de rendimiento, verifique si est√° usando memoria de un nodo NUMA remoto. </li><li>  ¬°Obtenga m√°quinas virtuales de Swap lo m√°s r√°pido posible!  Entre otras cosas, si la VM est√° en Swap, por razones obvias, el sistema de almacenamiento sufre. </li></ul><br>  Eso es todo por RAM.  A continuaci√≥n hay art√≠culos relacionados para aquellos que desean profundizar en los detalles.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El pr√≥ximo art√≠culo</a> estar√° dedicado a la historia. <br><br><div class="spoiler">  <b class="spoiler_title">Enlaces utiles</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">http://www.yellow-bricks.com/2015/03/02/what-happens-at-which-vsphere-memory-state/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">http://www.yellow-bricks.com/2013/06/14/how-does-mem-minfreepct-work-with-vsphere-5-0-and-up/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://www.vladan.fr/vmware-transparent-page-sharing-tps-explained/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">http://www.yellow-bricks.com/2016/06/02/memory-pages-swapped-can-unswap/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://kb.vmware.com/s/article/1002586</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://www.vladan.fr/what-is-vmware-memory-ballooning/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://kb.vmware.com/s/article/2080735</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://kb.vmware.com/s/article/2017642</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://labs.vmware.com/vmtj/vmware-esx-memory-resource-management-swap</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://blogs.vmware.com/vsphere/2013/10/understanding-vsphere-active-memory.html</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://www.vmware.com/support/developer/converter-sdk/conv51_apireference/memory_counters.html</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://docs.vmware.com/en/VMware-vSphere/6.5/vsphere-esxi-vcenter-server-65-monitoring-performance-guide.pdf</a> <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/455820/">https://habr.com/ru/post/455820/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455802/index.html">C√≥mo armar los Juegos Ol√≠mpicos a trav√©s de boletines electr√≥nicos. Estuche Black Star</a></li>
<li><a href="../455806/index.html">Nacimiento y muerte de un √°lbum: entendemos c√≥mo los formatos de m√∫sica han cambiado en los √∫ltimos 100 a√±os.</a></li>
<li><a href="../455808/index.html">Obtenga extractos del registro en el sitio web de FTS usando python</a></li>
<li><a href="../455812/index.html">Creaci√≥n de una arquitectura de microservicios en Golang y gRPC, parte 2 (acoplador)</a></li>
<li><a href="../455816/index.html">C√≥mo crear una acci√≥n genial para el Asistente de Google. Lifehacks de Just AI</a></li>
<li><a href="../455826/index.html">Riego autom√°tico controlado a distancia</a></li>
<li><a href="../455828/index.html">Los cient√≠ficos han descubierto nuevas formas ex√≥ticas de sincronizaci√≥n</a></li>
<li><a href="../455832/index.html">Historia de una sola investigaci√≥n SQL</a></li>
<li><a href="../455834/index.html">Benchmarks para servidores Linux: 5 herramientas abiertas</a></li>
<li><a href="../455840/index.html">C√≥mo trabajar con m√∫ltiples consultas. Composici√≥n, Reductor, FP</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>