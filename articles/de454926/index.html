<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚åöÔ∏è üêä üë®‚Äçüç≥ Alles, was Sie √ºber word2vec wussten, ist nicht wahr üç± üòª üë®üèæ‚Äçüöí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die klassische Erkl√§rung von word2vec als Skip-Gramm-Architektur mit negativem Beispiel im urspr√ºnglichen wissenschaftlichen Artikel und in unz√§hligen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Alles, was Sie √ºber word2vec wussten, ist nicht wahr</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454926/">  Die klassische Erkl√§rung von word2vec als Skip-Gramm-Architektur mit negativem Beispiel im urspr√ºnglichen wissenschaftlichen Artikel und in unz√§hligen Blog-Posts sieht folgenderma√üen aus: <br><br><pre><code class="plaintext hljs">while(1) { 1. vf = vector of focus word 2. vc = vector of focus word 3. train such that (vc . vf = 1) 4. for(0 &lt;= i &lt;= negative samples): vneg = vector of word *not* in context train such that (vf . vneg = 0) }</code> </pre> <br>  In der Tat, wenn Sie [word2vec skipgram] googeln, was wir sehen: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wikipedia-Seite, die den Algorithmus auf hohem Niveau beschreibt</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow-Seite mit der gleichen Erkl√§rung</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auf dem Weg zum Data Science-Blog mit einer Beschreibung desselben Algorithmus</a> , und die Liste geht weiter. </li></ul><br>  <b>Aber all diese Implementierungen sind falsch</b> . <br><a name="habracut"></a><br>  Die urspr√ºngliche Implementierung von word2vec in C funktioniert anders und unterscheidet sich <i>grundlegend</i> davon.  Diejenigen, die Systeme mit Worteinbettungen von word2vec professionell implementieren, f√ºhren eine der folgenden Aktionen aus: <br><br><ol><li>  Rufen Sie direkt die urspr√ºngliche Implementierung von C auf. <br></li><li>  Verwenden Sie die <code>gensim</code> Implementierung, die von der Quelle C <code>gensim</code> , <code>gensim</code> die Variablennamen √ºbereinstimmen. </li></ol><br>  In der Tat ist <code>gensim</code> die <i>einzige echte C-Implementierung, die mir bekannt ist</i> . <br><br><h3>  C Implementierung </h3><br>  Die C-Implementierung unterst√ºtzt tats√§chlich <i>zwei Vektoren f√ºr jedes Wort</i> .  Ein Vektor f√ºr das Wort steht im Fokus und der zweite f√ºr das Wort im Kontext.  (Kommt Ihnen das bekannt vor? Richtig, GloVe-Entwickler haben eine Idee von word2vec ausgeliehen, ohne diese Tatsache zu erw√§hnen!) <br><br>  Die Implementierung in C-Code ist au√üerordentlich kompetent: <br><br><ul><li>  Das <code>syn0</code> Array enth√§lt die Vektoreinbettung des Wortes, wenn es als fokussiertes Wort <code>syn0</code> .  Hier ist eine <b>zuf√§llige Initialisierung</b> . <br><br><pre> <code class="cpp hljs">https:<span class="hljs-comment"><span class="hljs-comment">//github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L369 for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) { next_random = next_random * (unsigned long long)25214903917 + 11; syn0[a * layer1_size + b] = (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size; }</span></span></code> </pre> </li><li>  Ein anderes <code>syn1neg</code> Array enth√§lt den Vektor des Wortes, wenn es als Kontextwort erscheint.  Hier ist die <b>Initialisierung Null</b> . <br></li><li>  W√§hrend des Trainings (Skip-Gramm, negative Stichprobe, obwohl andere F√§lle ungef√§hr gleich sind) w√§hlen wir zuerst das Fokuswort aus.  Es wird w√§hrend des gesamten Trainings an positiven und negativen Beispielen beibehalten.  Die Gradienten des Fokusvektors werden im Puffer akkumuliert und nach dem Training an positiven und negativen Beispielen auf das Fokuswort angewendet. <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (negative &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (d = <span class="hljs-number"><span class="hljs-number">0</span></span>; d &lt; negative + <span class="hljs-number"><span class="hljs-number">1</span></span>; d++) { <span class="hljs-comment"><span class="hljs-comment">// if we are performing negative sampling, in the 1st iteration, // pick a word from the context and set the dot product target to 1 if (d == 0) { target = word; label = 1; } else { // for all other iterations, pick a word randomly and set the dot //product target to 0 next_random = next_random * (unsigned long long)25214903917 + 11; target = table[(next_random &gt;&gt; 16) % table_size]; if (target == 0) target = next_random % (vocab_size - 1) + 1; if (target == word) continue; label = 0; } l2 = target * layer1_size; f = 0; // find dot product of original vector with negative sample vector // store in f for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2]; // set g = sigmoid(f) (roughly, the actual formula is slightly more complex) if (f &gt; MAX_EXP) g = (label - 1) * alpha; else if (f &lt; -MAX_EXP) g = (label - 0) * alpha; else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha; // 1. update the vector syn1neg, // 2. DO NOT UPDATE syn0 // 3. STORE THE syn0 gradient in a temporary buffer neu1e for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2]; for (c = 0; c &lt; layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1]; } // Finally, after all samples, update syn1 from neu1e https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L541 // Learn weights input -&gt; hidden for (c = 0; c &lt; layer1_size; c++) syn0[c + l1] += neu1e[c];</span></span></code> </pre> </li></ul><br><h3>  Warum zuf√§llige und Null-Initialisierung? </h3><br>  Noch einmal, da dies in den Originalartikeln <i>und irgendwo im Internet</i> √ºberhaupt nicht erkl√§rt wird, kann ich nur spekulieren. <br><br>  Die Hypothese ist, dass Sie, wenn negative Stichproben aus dem gesamten Text stammen und nicht nach H√§ufigkeit gewichtet werden, ein <i>beliebiges Wort</i> ausw√§hlen <i>k√∂nnen</i> , und meistens ein Wort, dessen <i>Vektor √ºberhaupt nicht trainiert wird</i> .  Wenn dieser Vektor eine Bedeutung hat, verschiebt er das wirklich wichtige Wort zuf√§llig in den Fokus. <br><br>  Unter dem Strich werden alle negativen Beispiele auf Null gesetzt, sodass <i>nur Vektoren, die mehr oder weniger h√§ufig auftreten</i> , die Darstellung eines anderen Vektors beeinflussen. <br><br>  Das ist eigentlich ziemlich knifflig und ich hatte nie dar√ºber nachgedacht, wie wichtig Initialisierungsstrategien sind. <br><br><h3>  Warum schreibe ich das? </h3><br>  Ich habe zwei Monate meines Lebens damit verbracht, word2vec wie in der wissenschaftlichen Originalver√∂ffentlichung und in unz√§hligen Artikeln im Internet beschrieben zu reproduzieren, bin aber gescheitert.  Ich konnte nicht die gleichen Ergebnisse wie word2vec erzielen, obwohl ich mein Bestes versucht habe. <br><br>  Ich konnte mir nicht vorstellen, dass die Autoren der Ver√∂ffentlichung buchst√§blich einen Algorithmus erfunden haben, der nicht funktioniert, w√§hrend die Implementierung etwas v√∂llig anderes macht. <br><br>  Am Ende entschied ich mich, die Quelle zu studieren.  Drei Tage lang war ich zuversichtlich, den Code falsch verstanden zu haben, da buchst√§blich alle im Internet √ºber eine andere Implementierung sprachen. <br><br>  Ich habe keine Ahnung, warum die Originalver√∂ffentlichung und die Artikel im Internet nichts √ºber den <i>tats√§chlichen</i> Mechanismus von word2vec aussagen. Deshalb habe ich beschlossen, diese Informationen selbst zu ver√∂ffentlichen. <br><br>  Dies erkl√§rt auch die radikale Entscheidung von GloVe, separate Vektoren f√ºr den negativen Kontext festzulegen - sie haben einfach das getan, was word2vec tut, aber den Leuten davon erz√§hlt :). <br><br>  Ist das ein wissenschaftlicher Trick?  Ich wei√ü nicht, eine schwierige Frage.  Aber um ehrlich zu sein, bin ich unglaublich w√ºtend.  Wahrscheinlich werde ich die Erkl√§rung von Algorithmen beim maschinellen Lernen nie wieder ernst nehmen k√∂nnen: Beim n√§chsten Mal werde ich mir <i>sofort</i> die Quellen ansehen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de454926/">https://habr.com/ru/post/de454926/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de454916/index.html">Neuronale Netzwerkarchitektur zur Implementierung des RL-Algorithmus mit der F√§higkeit, gleichzeitig laufende Aktionen festzulegen</a></li>
<li><a href="../de454918/index.html">So kombinieren Sie den R√ºcken zweier Einzelh√§ndler in SAP in 12 Stunden</a></li>
<li><a href="../de454920/index.html">Front-End-Leistung: Analysieren wichtiger Metriken</a></li>
<li><a href="../de454922/index.html">Geschichten √ºber ausl√§ndische Kunden und ihre Arbeitsmerkmale in Russland nach dem PD-Gesetz</a></li>
<li><a href="../de454924/index.html">Authentifizierungseinstellungen in Veeam Backup f√ºr Microsoft Office 365 v3</a></li>
<li><a href="../de454928/index.html">M√∂glichkeit, den Windows-Sperrbildschirm in RDP-Sitzungen zu umgehen</a></li>
<li><a href="../de454932/index.html">Investitionen und Software: 5 Handelsterminals f√ºr den Handel an der B√∂rse</a></li>
<li><a href="../de454936/index.html">Vivaldi: Das Blockieren von Anzeigen sollte die Wahl des Nutzers sein</a></li>
<li><a href="../de454938/index.html">Entwicklung eines eigenen Kerns zur Einbettung in ein auf FPGA basierendes Prozessorsystem</a></li>
<li><a href="../de454940/index.html">Reisekrankenversicherung: detaillierte Anweisungen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>