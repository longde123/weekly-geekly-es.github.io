<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ûñ üë©‚Äçüë¶ üíÖüèæ Manchmal ist mehr weniger. Wenn eine Abnahme der Last zu einer Zunahme der Verz√∂gerung f√ºhrt üí≥ üë®üèΩ‚Äçüåæ üëàüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wie in den meisten Beitr√§gen gab es ein Problem mit einem verteilten Dienst. Nennen wir diesen Dienst Alvin. Diesmal habe ich das Problem selbst nicht...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Manchmal ist mehr weniger. Wenn eine Abnahme der Last zu einer Zunahme der Verz√∂gerung f√ºhrt</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/451904/"> Wie in den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">meisten Beitr√§gen</a> gab es ein Problem mit einem verteilten Dienst. Nennen wir diesen Dienst Alvin.  Diesmal habe ich das Problem selbst nicht gefunden, informierten mich die Jungs vom Client-Teil. <br><br>  Einmal bin ich aus einem ver√§rgerten Brief aufgewacht, wegen der gro√üen Verz√∂gerungen von Alvin, den wir in naher Zukunft starten wollten.  Insbesondere stie√ü der Kunde auf eine Verz√∂gerung von 99. Perzentilen um 50 ms, die weit √ºber unserem Verz√∂gerungsbudget lag.  Dies war √ºberraschend, da ich den Service gr√ºndlich getestet habe, insbesondere auf Verz√∂gerungen, da dies h√§ufig beanstandet wird. <br><br>  Bevor ich Alvin zum Testen gab, f√ºhrte ich viele Experimente mit 40.000 Anfragen pro Sekunde (QPS) durch, die alle eine Verz√∂gerung von weniger als 10 ms zeigten.  Ich war bereit zu erkl√§ren, dass ich mit ihren Ergebnissen nicht einverstanden bin.  Aber als ich den Brief noch einmal betrachtete, machte ich auf etwas Neues aufmerksam: Ich habe die genannten Bedingungen definitiv nicht getestet, ihr QPS war viel niedriger als meiner.  Ich habe auf 40k QPS getestet, und sie nur auf 1k.  Ich habe ein weiteres Experiment durchgef√ºhrt, diesmal mit niedrigerem QPS, nur um ihnen zu gefallen. <br><a name="habracut"></a><br>  Da ich dar√ºber in meinem Blog schreibe, haben Sie wahrscheinlich schon verstanden: Ihre Zahlen haben sich als richtig herausgestellt.  Ich habe meinen virtuellen Client immer wieder getestet, alle mit dem gleichen Ergebnis: Eine geringe Anzahl von Anforderungen erh√∂ht nicht nur die Verz√∂gerung, sondern auch die Anzahl von Anforderungen mit einer Verz√∂gerung von mehr als 10 ms.  Mit anderen Worten, wenn bei 40k QPS etwa 50 Anforderungen pro Sekunde 50 ms √ºberstiegen, dann gab es bei 1k QPS pro Sekunde 100 Anforderungen √ºber 50 ms.  Paradox! <br><br><img src="https://habrastorage.org/webt/xd/86/x-/xd86x-er30ek6tg4hkv4qdevvgq.png"><br><br><h1>  Grenzen Sie Ihre Suche ein </h1><br>  Angesichts des Problems der Verz√∂gerung in einem verteilten System mit vielen Komponenten m√ºssen Sie zun√§chst eine kurze Liste der Verd√§chtigen erstellen.  Wir gehen etwas tiefer in Alvins Architektur ein: <br><br><img src="https://habrastorage.org/webt/lh/is/s5/lhiss5mqv9dq2h9moyhlss_chlk.png"><br><br>  Ein guter Ausgangspunkt ist eine Liste abgeschlossener E / A-√úberg√§nge (Netzwerkanrufe / Festplattensuchen usw.).  Versuchen wir herauszufinden, wo die Verz√∂gerung liegt.  Neben der offensichtlichen E / A mit dem Kunden unternimmt Alvin einen weiteren Schritt: Er greift auf das Data Warehouse zu.  Dieser Speicher funktioniert jedoch mit Alvin im selben Cluster, sodass weniger Verz√∂gerungen auftreten sollten als mit dem Client.  Also die Liste der Verd√§chtigen: <br><br><ol><li>  Netzwerkanruf vom Client an Alvin. <br></li><li>  Netzwerkanruf von Alvin an das Data Warehouse. <br></li><li>  Suche auf Datentr√§ger im Data Warehouse. <br></li><li>  Netzwerkanruf vom Data Warehouse an Alvin. <br></li><li>  Netzwerkanruf von Alvin an den Client. </li></ol><br>  Versuchen wir, einige Punkte zu streichen. <br><br><h3>  Data Warehouse </h3><br>  Als erstes habe ich Alvin in einen Ping-Ping-Server konvertiert, der keine Anforderungen verarbeitet.  Nach Erhalt der Anfrage wird eine leere Antwort zur√ºckgegeben.  Wenn die Verz√∂gerung abnimmt, ist ein Fehler bei der Implementierung von Alvin oder des Data Warehouse nichts Unbekanntes.  Im ersten Experiment erhalten wir die folgende Grafik: <br><br><img src="https://habrastorage.org/webt/i4/zs/9s/i4zs9saymr5ra4tmry3diqbgdyy.png"><br><br>  Wie Sie sehen, gibt es bei Verwendung des Ping-Ping-Servers keine Verbesserungen.  Dies bedeutet, dass das Data Warehouse die Verz√∂gerung nicht erh√∂ht und die Liste der Verd√§chtigen halbiert wird: <br><br><ol><li>  Netzwerkanruf vom Client an Alvin. <br></li><li>  Netzwerkanruf von Alvin an den Client. </li></ol><br>  Wow!  Die Liste wird immer kleiner.  Ich dachte, ich h√§tte fast den Grund herausgefunden. <br><br><h3>  gRPC </h3><br>  Jetzt ist es an der Zeit, Ihnen einen neuen Spieler vorzustellen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gRPC</a> .  Dies ist eine Open Source-Bibliothek von Google f√ºr die In-Process- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RPC-</a> Kommunikation.  Obwohl <code>gRPC</code> gut optimiert und weit verbreitet ist, habe ich es zum ersten Mal auf einem System dieser Gr√∂√üenordnung verwendet und erwartet, dass meine Implementierung - gelinde gesagt - nicht optimal ist. <br><br>  Das Vorhandensein von <code>gRPC</code> im Stapel warf eine neue Frage auf: Vielleicht ist dies meine Implementierung oder verursacht <code>gRPC</code> selbst ein Verz√∂gerungsproblem?  Zur Liste des neuen Verd√§chtigen hinzuf√ºgen: <br><br><ol><li>  Der Client ruft die <code>gRPC</code> Bibliothek auf <br></li><li>  Die <code>gRPC</code> Bibliothek auf dem Client ruft die <code>gRPC</code> Bibliothek auf dem Server im Netzwerk auf <br></li><li>  <code>gRPC</code> Bibliothek greift auf Alvin zu (keine Operation bei Ping-Pong-Server) </li></ol><br>  Damit Sie verstehen, wie der Code aussieht, unterscheidet sich meine Client / Alvin-Implementierung nicht wesentlich von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">asynchronen</a> Client-Server- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beispielen</a> . <br><br><blockquote>  <i>Hinweis: Die obige Liste ist etwas vereinfacht, da Sie mit <code>gRPC</code> Ihr eigenes (Vorlagen-?) Stream-Modell verwenden k√∂nnen, in dem der <code>gRPC</code> Ausf√ºhrungsstapel und die Benutzerimplementierung miteinander verflochten sind.</i>  <i>Der Einfachheit halber werden wir uns an dieses Modell halten.</i> </blockquote><br><h3>  Durch die Profilerstellung wird alles behoben </h3><br>  Als ich die Data Warehouses durchgestrichen hatte, dachte ich, ich w√§re fast fertig: ‚ÄûJetzt einfach!  Wir werden das Profil anwenden und herausfinden, wo die Verz√∂gerung auftritt. "  Ich bin ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gro√üer Fan von genauem Profiling,</a> weil die CPUs sehr schnell sind und meistens keinen Engpass darstellen.  Die meisten Verz√∂gerungen treten auf, wenn der Prozessor die Verarbeitung stoppen muss, um etwas anderes zu tun.  Genau daf√ºr wurde eine genaue Profilerstellung der CPU vorgenommen: Sie zeichnet alle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kontextwechsel</a> genau auf und macht deutlich, wo Verz√∂gerungen auftreten. <br><br>  Ich habe vier Profile erstellt: unter hohem QPS (niedrige Latenz) und mit einem Ping-Pong-Server mit niedrigem QPS (hohe Latenz), sowohl auf der Clientseite als auch auf der Serverseite.  Und f√ºr alle F√§lle habe ich auch ein Beispielprozessorprofil genommen.  Beim Vergleichen von Profilen suche ich normalerweise nach einem abnormalen Aufrufstapel.  Auf der schlechten Seite mit einer hohen Verz√∂gerung gibt es beispielsweise viel mehr Kontextwechsel (10 oder mehr Mal).  In meinem Fall stimmte die Anzahl der Kontextwechsel jedoch fast √ºberein.  Zu meinem Entsetzen gab es dort nichts Bedeutendes. <br><br><h1>  Zus√§tzliches Debugging </h1><br>  Ich war verzweifelt.  Ich wusste nicht, welche anderen Werkzeuge verwendet werden k√∂nnten, und mein n√§chster Plan bestand im Wesentlichen darin, Experimente mit verschiedenen Variationen zu wiederholen und das Problem nicht eindeutig zu diagnostizieren. <br><br><h3>  Was w√§re wenn </h3><br>  Ich war von Anfang an besorgt √ºber die spezifische Verz√∂gerungszeit von 50 ms.  Dies ist eine sehr gro√üe Zeit.  Ich beschloss, die Teile aus dem Code herauszuschneiden, bis ich genau herausfinden konnte, welcher Teil diesen Fehler verursachte.  Dann folgte ein Experiment, das funktionierte. <br><br>  Wie immer scheint im Hinterkopf alles offensichtlich zu sein.  Ich habe den Client mit Alvin auf denselben Computer gestellt - und die Anfrage an <code>localhost</code> gesendet.  Und die Zunahme der Verz√∂gerung ist verschwunden! <br><br><img src="https://habrastorage.org/webt/kl/bk/fv/klbkfv7ajppr9uqp_tywvxwgjre.png"><br><br>  Mit dem Netzwerk stimmte etwas nicht. <br><br><h3>  Erlernen der F√§higkeiten eines Netzwerktechnikers </h3><br>  Ich muss zugeben: Mein Wissen √ºber Netzwerktechnologien ist schrecklich, insbesondere angesichts der Tatsache, dass ich t√§glich mit ihnen arbeite.  Aber das Netzwerk war der Hauptverd√§chtige, und ich musste lernen, wie man es debuggt. <br><br>  Gl√ºcklicherweise liebt das Internet diejenigen, die lernen wollen.  Die Kombination von Ping und Tracert schien ein guter Anfang zu sein, um Netzwerktransportprobleme zu beheben. <br><br>  Zuerst habe ich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PsPing</a> auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Alvins</a> TCP-Port ausgef√ºhrt.  Ich habe die Standardoptionen verwendet - nichts Besonderes.  Von den mehr als tausend Pings √ºberschritt keiner 10 ms, mit Ausnahme des ersten zum Aufw√§rmen.  Dies widerspricht dem beobachteten Anstieg der Verz√∂gerung von 50 ms im 99. Perzentil: Dort sollten wir f√ºr jeweils 100 Anfragen etwa eine Anfrage mit einer Verz√∂gerung von 50 ms sehen. <br><br>  Dann habe ich es mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tracert</a> versucht: Vielleicht liegt das Problem an einem der Knoten entlang der Route zwischen Alvin und dem Client.  Aber der Tracer kam mit leeren H√§nden zur√ºck. <br><br>  Der Grund f√ºr die Verz√∂gerung war also nicht mein Code, nicht die Implementierung von gRPC und nicht das Netzwerk.  Ich habe bereits angefangen, mir Sorgen zu machen, dass ich das nie verstehen werde. <br><br><h3>  Auf welchem ‚Äã‚ÄãBetriebssystem sind wir jetzt? </h3><br>  <code>gRPC</code> unter Linux weit verbreitet, f√ºr Windows jedoch exotisch.  Ich entschied mich f√ºr ein Experiment, das funktionierte: Ich erstellte eine virtuelle Linux-Maschine, kompilierte Alvin f√ºr Linux und stellte sie bereit. <br><br><img src="https://habrastorage.org/webt/z1/t8/tk/z1t8tkyhrobvurzcdqlmpdtetzc.png"><br><br>  Und hier ist, was passiert ist: Der Ping-Pong-Linux-Server hatte keine Verz√∂gerungen wie ein √§hnlicher Windows-Knoten, obwohl sich die Datenquelle nicht unterschied.  Es stellt sich heraus, dass das Problem in der Implementierung von gRPC f√ºr Windows liegt. <br><br><h3>  Nagle-Algorithmus </h3><br>  Die ganze Zeit dachte ich, ich h√§tte die <code>gRPC</code> Flagge verpasst.  Jetzt wurde mir klar, dass das Windows-Flag in <code>gRPC</code> fehlt.  Ich fand die interne RPC-Bibliothek, in der ich sicher war, dass sie f√ºr alle installierten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Winsock-</a> Flags gut funktioniert.  Dann f√ºgte er all diese Flags zu gRPC hinzu und stellte Alvin f√ºr Windows auf dem festen Ping-Pong-Server f√ºr Windows bereit! <br><br><img src="https://habrastorage.org/webt/7o/it/sf/7oitsfp2rzxotix0cf1xhktbrri.png"><br><br>  <i>Fast</i> fertig: Ich fing an, die hinzugef√ºgten Flags einzeln zu l√∂schen, bis die Regression zur√ºckkehrte, damit ich die Ursache ermitteln konnte.  Es war der ber√ºchtigte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TCP_NODELAY</a> , ein Schalter des Nagle-Algorithmus. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der Neigl-Algorithmus</a> versucht, die Anzahl der √ºber das Netzwerk gesendeten Pakete zu reduzieren, indem die √úbertragung von Nachrichten verz√∂gert wird, bis die Paketgr√∂√üe eine bestimmte Anzahl von Bytes √ºberschreitet.  Obwohl dies f√ºr den durchschnittlichen Benutzer angenehm sein kann, ist es f√ºr Echtzeitserver destruktiv, da das Betriebssystem einige Nachrichten verz√∂gert und Verz√∂gerungen bei niedrigem QPS verursacht.  <code>gRPC</code> hatte dieses Flag in der Linux-Implementierung f√ºr TCP-Sockets gesetzt, jedoch nicht f√ºr Windows.  Ich habe es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">repariert</a> . <br><br><h1>  Fazit </h1><br>  Eine gro√üe Verz√∂gerung bei niedrigem QPS wurde durch die Betriebssystemoptimierung verursacht.  R√ºckblickend hat die Profilerstellung keine Verz√∂gerung festgestellt, da sie im Kernelmodus und nicht im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Benutzermodus ausgef√ºhrt wurde</a> .  Ich wei√ü nicht, ob es m√∂glich ist, den Nagle-Algorithmus durch ETW-Captures zu beobachten, aber das w√§re interessant. <br><br>  Das localhost-Experiment hat wahrscheinlich den tats√§chlichen Netzwerkcode nicht ber√ºhrt, und der Neigl-Algorithmus wurde nicht gestartet, sodass die Verz√∂gerungsprobleme verschwanden, als der Client Alvin √ºber localhost kontaktierte. <br><br>  Wenn Sie das n√§chste Mal eine Erh√∂hung der Latenz feststellen und gleichzeitig die Anzahl der Anforderungen pro Sekunde verringern, sollte der Neigl-Algorithmus auf Ihrer Liste der Verd√§chtigen stehen! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de451904/">https://habr.com/ru/post/de451904/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de451894/index.html">Ein kurzer und √ºbersichtlicher √úberblick √ºber die Compiler-Architektur</a></li>
<li><a href="../de451896/index.html">Eine "unzerbrechliche" eyeDisk ist durch einen Iris-Scan gesch√ºtzt, √ºbertr√§gt jedoch ein Passwort im Klartext</a></li>
<li><a href="../de451898/index.html">Innovation auf Russisch</a></li>
<li><a href="../de451900/index.html">Erster Beitrag zur Browser-API von Facebook</a></li>
<li><a href="../de451902/index.html">Microsoft Azure Developer Camp Russland</a></li>
<li><a href="../de451906/index.html">Exchange-Sicherheitsanf√§lligkeit: Ermitteln der Erh√∂hung der Berechtigung f√ºr einen Dom√§nenadministrator</a></li>
<li><a href="../de451908/index.html">Die Geschichte der Computer: eine Nacht im Yandex Museum</a></li>
<li><a href="../de451912/index.html">Das tiefe neuronale Netzwerk von MuseNet schreibt Musik</a></li>
<li><a href="../de451916/index.html">Asynchrones PHP und die Geschichte eines Fahrrads</a></li>
<li><a href="../de451918/index.html">Auf die Frage von TI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>