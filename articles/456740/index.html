<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëåüèª ‚úçüèº üõ†Ô∏è Inmersi√≥n en redes neuronales convolucionales. Parte 5/1 - 9 üë®üèª‚Äçüè´ ‚öôÔ∏è üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El curso completo en ruso se puede encontrar en este enlace . 
 El curso de ingl√©s original est√° disponible en este enlace . 



 Nuevas conferencias ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Inmersi√≥n en redes neuronales convolucionales. Parte 5/1 - 9</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456740/"><p>  El curso completo en ruso se puede encontrar en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . <br>  El curso de ingl√©s original est√° disponible en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . </p><br><p><img src="https://habrastorage.org/webt/1m/hz/qn/1mhzqnwa288uxjmptqhexriiahc.png"><br>  <i>Nuevas conferencias est√°n programadas cada 2-3 d√≠as.</i> </p><a name="habracut"></a><br><h1>  Contenido </h1><br><ol><li>  Entrevista con Sebastian Trun </li><li>  Introduccion </li><li>  Conjunto de datos de perros y gatos </li><li>  Im√°genes de varios tama√±os. </li><li>  Im√°genes a color.  Parte 1 </li><li>  Im√°genes a color.  Parte 2 </li><li>  Operaci√≥n de convoluci√≥n en im√°genes en color </li><li>  La operaci√≥n de submuestreo por el valor m√°ximo en im√°genes en color </li><li>  CoLab: gatos y perros </li><li>  Softmax y sigmoide </li><li>  Cheque </li><li>  Extensi√≥n de imagen </li><li>  Excepci√≥n </li><li>  CoLab: perros y gatos.  Repetici√≥n </li><li>  Otras t√©cnicas para prevenir el reentrenamiento </li><li>  Ejercicios: clasificaci√≥n de im√°genes en color. </li><li>  Soluci√≥n: clasificaci√≥n de im√°genes en color </li><li>  Resumen </li></ol><br><h1>  Entrevista con Sebastian Trun </h1><br><p>  - Entonces, hoy estamos aqu√≠ nuevamente, junto con Sebastian y hablaremos sobre el reciclaje.  Este tema es muy interesante para nosotros, especialmente en las partes pr√°cticas del curso actual sobre c√≥mo trabajar con TensorFlow. <br>  - Sebastian, ¬øalguna vez has encontrado un sobreajuste?  Si dices que no te has encontrado, ¬°definitivamente dir√© que no puedo creerte! <br>  - Entonces, la raz√≥n para la reentrenamiento es la denominada <strong>compensaci√≥n de desviaci√≥n de sesgo</strong> (un compromiso entre los valores del par√°metro de sesgo y su propagaci√≥n).  Una red neuronal en la que una peque√±a cantidad de pesas no puede aprender una cantidad suficiente de ejemplos, una situaci√≥n similar en el aprendizaje autom√°tico se llama distorsi√≥n. <br>  - si. <br>  - Una red neuronal con tantos par√°metros puede elegir arbitrariamente una soluci√≥n que no le guste, solo por la gran cantidad de estos par√°metros.  El resultado de elegir una soluci√≥n de red neuronal depende de la variabilidad de los datos de origen.  Por lo tanto, se puede formular una regla simple: cuantos m√°s par√°metros haya en la red con respecto al tama√±o (cantidad) de datos, es m√°s probable que obtenga una soluci√≥n aleatoria en lugar de la correcta.  Por ejemplo, te preguntas: "¬øQui√©n es el hombre en esta habitaci√≥n y qui√©n es la mujer?"  Una red neuronal compleja puede decirle que, por ejemplo, todos aquellos cuyos nombres comienzan con T son hombres y nunca se vuelven a entrenar.  Hay dos soluciones  El primero de ellos utiliza un conjunto de datos de reserva (una peque√±a cantidad del conjunto de entrenamiento para validar la precisi√≥n del modelo).  Puede tomar los datos, dividirlos en dos partes: 90% para entrenamiento y 10% para probar y realizar la llamada validaci√≥n cruzada, donde verifica la precisi√≥n del modelo en los datos que la red neuronal no vio, tan pronto como comienza el valor de error crecer despu√©s de cierto ciclo de entrenamiento: es hora de dejar de aprender.  La segunda soluci√≥n es introducir restricciones en la red neuronal.  Por ejemplo, para limitar los valores de los par√°metros de desplazamientos y pesos, acerc√°ndolos cada vez m√°s a cero.  Cuanto m√°s limitados sean los pesos, menos entrenado ser√° el modelo. <br>  - Entiendo correctamente que podemos tener conjuntos de datos tanto para capacitaci√≥n como para pruebas y validaci√≥n, ¬øverdad? <br>  Eso es correcto.  Si tiene un conjunto de datos para la validaci√≥n, debe tener un conjunto de datos que nunca haya tocado o mostrado a su red neuronal.  Si le mostr√≥ al modelo un determinado conjunto de datos muchas veces, entonces, por supuesto, comenzar√° el proceso de reciclaje, lo cual es muy malo para nosotros. <br>  - ¬øQuiz√°s recuerde los casos m√°s interesantes cuando su modelo fue reentrenado? <br>  - Ah, s√≠ ... hubo un incidente en mi juventud cuando estaba desarrollando una red neuronal para jugar al ajedrez.  Fue en 1993. Lo interesante fue que a partir de los datos de ajedrez en los que se entren√≥ la red neuronal, la red r√°pidamente determin√≥ que si un experto mueve a la reina al centro del tablero de ajedrez, entonces hay un 60% de posibilidades de ganar.  Lo que ella comenz√≥ a hacer fue abrir el "pasaje" con un pe√≥n y mover a la reina al centro del tablero de ajedrez.  Fue una decisi√≥n tan est√∫pida para cualquier jugador de ajedrez, lo que claramente atestigu√≥ el reciclaje del modelo. <br>  - Genial!  Por lo tanto, hemos discutido varias t√©cnicas sobre c√≥mo mejorar nuestros modelos.  ¬øCu√°l crees que es el lado m√°s subestimado del aprendizaje profundo? <br>  - El 90% de su trabajo se subestima, porque el 90% de su trabajo consistir√° en la limpieza de datos. <br>  - ¬°Aqu√≠ estoy completamente de acuerdo contigo! <br>  - Como muestra la pr√°ctica, cualquier conjunto de datos contiene alg√∫n tipo de basura.  Es muy dif√≠cil llevar los datos al tipo correcto, para que sean consistentes, es un proceso que lleva mucho tiempo. <br>  - S√≠, incluso si trabaja con conjuntos de datos como im√°genes o videos, donde, al parecer, toda la informaci√≥n ya est√° all√≠, en el interior, todav√≠a es necesario procesar previamente las im√°genes. <br>  - ¬°Las √∫nicas personas para quienes los datos son ideales son los profesores, porque tienen la oportunidad de pretender en una presentaci√≥n en PowerPoint que todo es como debe ser y todo es perfecto!  En realidad, el 90% de su tiempo estar√° ocupado por la limpieza de datos. <br>  - Genial  Entonces, descubramos m√°s sobre el reciclaje y las t√©cnicas que nos permitir√°n mejorar nuestros modelos de aprendizaje profundo. </p><br><h1>  Introduccion </h1><br><p>  - hola!  Y de nuevo, ¬°bienvenido al curso! <br>  - En la √∫ltima lecci√≥n, desarrollamos una peque√±a red neuronal convolucional para clasificar im√°genes de elementos de ropa en tonos de gris del conjunto de datos FASHION MNIST.  Hemos visto en la pr√°ctica que nuestra peque√±a red neuronal puede clasificar las im√°genes entrantes con una precisi√≥n bastante alta.  Sin embargo, en el mundo real tenemos que trabajar con im√°genes de alta resoluci√≥n y varios tama√±os.  Una de las grandes ventajas del SNA es el hecho de que pueden funcionar igual de bien con im√°genes en color.  Por lo tanto, comenzaremos nuestra lecci√≥n actual explorando c√≥mo funciona el SNA con im√°genes en color. <br>  - M√°s tarde, en la misma frecuencia, construir√° una red neuronal convolucional que puede clasificar im√°genes de gatos y perros.  En el camino hacia la implementaci√≥n de una red neuronal convolucional capaz de clasificar im√°genes de gatos y perros, tambi√©n aprenderemos c√≥mo usar diversas t√©cnicas para resolver uno de los problemas m√°s comunes con las redes neuronales: el reentrenamiento.  Y al final de esta lecci√≥n, en la parte pr√°ctica, desarrollar√° su propia red neuronal convolucional para clasificar im√°genes en color.  ¬°Empecemos! </p><br><h1>  Conjunto de datos de gatos y perros </h1><br><p>  Hasta ese momento, solo trabaj√°bamos con im√°genes en escala de grises y tama√±os 28x28 del conjunto de datos FASHION MNIST. </p><br><p><img src="https://habrastorage.org/webt/e4/1o/cb/e41ocb39ngplbr8osyfccvji0mm.png"></p><br><p>  En aplicaciones reales, nos vemos obligados a encontrar im√°genes de varios tama√±os, por ejemplo, las que se muestran a continuaci√≥n: </p><br><p><img src="https://habrastorage.org/webt/xs/oc/u2/xsocu2t1m1ywlkqk5_qdfslglow.png"></p><br><p>  Como mencionamos al comienzo de esta lecci√≥n, en esta lecci√≥n desarrollaremos una red neuronal convolucional que puede clasificar im√°genes en color de perros y gatos. </p><br><p>  Para implementar nuestros planes, utilizaremos im√°genes de gatos y perros del conjunto de datos de Microsoft Asirra.  Cada imagen en este conjunto de datos se etiqueta como 1 o 0 si hay un perro o un gato en la imagen, respectivamente. </p><br><p><img src="https://habrastorage.org/webt/pn/4e/uf/pn4euf-gaaxlv8_2eakpvntjtkw.png"></p><br><p>  A pesar de que el conjunto de datos de Microsoft Asirra contiene m√°s de 3 millones de im√°genes etiquetadas de gatos y perros, solo 25,000 est√°n disponibles p√∫blicamente.  La capacitaci√≥n de nuestra red neuronal convolucional en estas 25,000 im√°genes llevar√° mucho tiempo.  Es por eso que utilizaremos una peque√±a cantidad de im√°genes para entrenar nuestra red neuronal convolucional de los 25,000 disponibles. </p><br><p>  Nuestro subconjunto de im√°genes de entrenamiento consta de 2,000 pcs y 1,000 pcs de im√°genes para la validaci√≥n del modelo.  En el conjunto de datos de entrenamiento, 1,000 im√°genes contienen gatos y las otras 1,000 im√°genes contienen perros.  Hablaremos sobre el conjunto de datos para la validaci√≥n un poco m√°s adelante en esta parte de la lecci√≥n. </p><br><p><img src="https://habrastorage.org/webt/oa/wh/dg/oawhdglv3_dtccwky_e1pjwygog.png"></p><br><p>  Al trabajar con este conjunto de datos, encontraremos dos dificultades principales: trabajar con im√°genes de diferentes tama√±os y trabajar con im√°genes en color. </p><br><p>  Comencemos a explorar c√≥mo trabajar con im√°genes de varios tama√±os. </p><br><h1>  Im√°genes de varios tama√±os. </h1><br><p>  Nuestra primera prueba ser√° resolver el problema del procesamiento de im√°genes de varios tama√±os.  Esto se debe a que una red neuronal en la entrada necesita datos de tama√±o fijo. </p><br><p> Como ejemplo, puede recordar nuestras partes anteriores utilizando el par√°metro <code>input_shape</code> al crear una capa <code>Flatten</code> : </p><br><p><img src="https://habrastorage.org/webt/v5/tt/hk/v5tthkilik-9reer8owxjpv-x3m.png"></p><br><p>  Antes de transmitir la imagen de un elemento de ropa a una red neuronal, la convertimos en una matriz 1D de tama√±o fijo: 28x28 = 784 elementos (p√≠xeles).  Como las im√°genes del conjunto de datos Fashion MNIST ten√≠an el mismo tama√±o, la matriz unidimensional resultante ten√≠a el mismo tama√±o y constaba de 784 elementos. </p><br><p>  Sin embargo, al trabajar con im√°genes de varios tama√±os (alto y ancho) y transformarlas en matrices unidimensionales, obtenemos matrices de diferentes tama√±os. </p><br><p>  Dado que las redes neuronales en la entrada requieren datos del mismo tama√±o, no es suficiente simplemente salirse con la conversi√≥n a una matriz unidimensional de valores de p√≠xeles. </p><br><p>  Para resolver los problemas de clasificaci√≥n de im√°genes, siempre recurrimos a una de las opciones para unificar los datos de entrada, reduciendo el tama√±o de las im√°genes a valores comunes (cambio de tama√±o). </p><br><p><img src="https://habrastorage.org/webt/dt/rt/7f/dtrt7frns4fdinexvf0e3bow5gi.png"></p><br><p>  En este tutorial, recurriremos a cambiar el tama√±o de todas las im√°genes a tama√±os de 150 p√≠xeles de alto y 150 p√≠xeles de ancho.  Al convertir im√°genes a un tama√±o √∫nico, garantizamos que la imagen del tama√±o correcto llegar√° a la entrada de la red neuronal y, cuando se transfiere a una capa <code>flatten</code> , obtenemos una matriz unidimensional del mismo tama√±o. </p><br><pre> <code class="python hljs">tf.keras.layers.Flatten(input_shape(<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre> <br><p>  Como resultado, obtuvimos una matriz unidimensional que consta de 150x150 = 22,500 valores (p√≠xeles). </p><br><p>  El siguiente problema que enfrentaremos ser√° el problema del color: las im√°genes en color.  Hablaremos de ellos en la siguiente parte. </p><br><h1>  Im√°genes a color.  Parte 1 </h1><br><p>  Para comprender y entender c√≥mo funcionan las redes neuronales convolucionales con im√°genes en color, debemos profundizar en c√≥mo funciona exactamente el SNA en general.  Actualicemos lo que ya sabemos. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/3b5/c0c/687/3b5c0c68738975962dc882b9ab1522b2.png" alt="imagen"></p><br><p>  Un ejemplo anterior es una imagen en escala de grises y c√≥mo la computadora la interpreta como una matriz bidimensional de valores de p√≠xeles. </p><br><p>  Un ejemplo a continuaci√≥n es una imagen, esta vez un <strong>color,</strong> y c√≥mo la computadora lo interpreta como una matriz tridimensional de valores de p√≠xeles. </p><br><p><img src="https://habrastorage.org/webt/n-/2u/ic/n-2uicbpmraz7i2sggtwqckexbc.png"></p><br><p>  La altura y el ancho de la matriz 3D estar√°n determinados por la altura y el ancho de la imagen, y la profundidad (profundidad) determina el n√∫mero de canales de color de la imagen. </p><br><p>  La mayor√≠a de las im√°genes en color se pueden representar mediante tres canales de color: rojo (rojo), verde (verde) y azul (azul). </p><br><p><img src="https://habrastorage.org/webt/kn/14/zm/kn14zmbazrhrnbhtweoil9m-ipk.png"></p><br><p>  Las im√°genes que consisten en canales rojo, verde y azul se denominan im√°genes RGB.  La combinaci√≥n de estos tres canales da como resultado una imagen en color.  En cada una de las im√°genes RGB, cada canal est√° representado por una matriz de p√≠xeles bidimensional separada. </p><br><p><img src="https://habrastorage.org/webt/56/h5/g6/56h5g6loe_bu4_oiuu0-vy_unoc.png"></p><br><p>  Dado que el n√∫mero de canales que tenemos es tres, como resultado tendremos tres matrices bidimensionales.  Por lo tanto, una imagen en color que consta de 3 canales de color tendr√° la siguiente representaci√≥n: </p><br><p><img src="https://habrastorage.org/webt/nn/2r/q6/nn2rq6itb9gz5suamhcl5kvjwtu.png"></p><br><h1>  Im√°genes a color.  Parte 2 </h1><br><p>  Entonces, dado que nuestra imagen ahora constar√° de 3 colores, lo que significa que ser√° una matriz tridimensional de valores de p√≠xeles, entonces nuestro c√≥digo deber√° cambiarse en consecuencia. </p><br><p>  Si observa el c√≥digo que usamos en nuestra √∫ltima lecci√≥n cuando resolvimos el problema de clasificar elementos de ropa en im√°genes, podemos ver que indicamos la dimensi√≥n de los datos de entrada: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Los dos primeros par√°metros de la tupla <code>(28,28,1)</code> son los valores de la altura y el ancho de la imagen.  Las im√°genes en el conjunto de datos Fashion MNIST ten√≠an un tama√±o de 28x28 p√≠xeles.  El √∫ltimo par√°metro en la tupla <code>(28,28,1)</code> denota el n√∫mero de canales de color.  En el conjunto de datos Fashion MNIST, las im√°genes solo estaban en tonos de gris - 1 canal de color. </p><br><p>  Ahora que la tarea se ha vuelto un poco m√°s complicada, y nuestras im√°genes de gatos y perros se han hecho de diferentes tama√±os (pero convertidas en una sola - 150x150 p√≠xeles) y contienen 3 canales de color, entonces la tupla de valores tambi√©n deber√≠a ser diferente: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)))</code> </pre> <br><p>  En la siguiente parte, veremos c√≥mo se calcula la convoluci√≥n en presencia de tres canales de color en la imagen. </p><br><h1>  Operaci√≥n de convoluci√≥n en im√°genes en color </h1><br><p>  En lecciones pasadas, aprendimos c√≥mo realizar una operaci√≥n de convoluci√≥n en im√°genes en escala de grises.  Pero, ¬øc√≥mo realizar una operaci√≥n de convoluci√≥n en im√°genes en color?  Comencemos repitiendo c√≥mo se realiza la operaci√≥n de convoluci√≥n en im√°genes en escala de grises. </p><br><p>  Todo comienza con un filtro (n√∫cleo) de cierto tama√±o. </p><br><p><img src="https://habrastorage.org/webt/hv/jo/1h/hvjo1h8-b8xxaxnd5yzoj_6cr78.png"></p><br><p>  El filtro se encuentra en un p√≠xel de imagen espec√≠fico para convertir, luego cada valor de filtro se multiplica por el valor de p√≠xel correspondiente en la imagen y se suman todos estos valores.  El valor final del p√≠xel se establece en la nueva imagen en el lugar donde se encontraba el p√≠xel original convertido.  La operaci√≥n se repite para cada p√≠xel de la imagen original. </p><br><p>  Tambi√©n vale la pena recordar que durante la operaci√≥n de convoluci√≥n, para no perder informaci√≥n en los bordes de la imagen, podemos aplicar la alineaci√≥n y rellenar los bordes de la imagen con ceros: </p><br><p><img src="https://habrastorage.org/webt/fc/wa/1s/fcwa1sx3ekaufxf0zzstuzzk7bo.png"></p><br><p>  Ahora veamos c√≥mo podemos realizar la operaci√≥n de convoluci√≥n en im√°genes en color. </p><br><p>  Al igual que cuando se convierte una imagen en tonos de gris, comenzamos eligiendo el tama√±o del filtro (n√∫cleo) de cierto tama√±o. </p><br><p><img src="https://habrastorage.org/webt/ho/s2/8d/hos28dmxwzluclof1hpjtwa8eq0.png"></p><br><p>  La √∫nica diferencia ahora ser√° que ahora el filtro en s√≠ ser√° tridimensional, y el valor del par√°metro de profundidad ser√° igual al valor del n√∫mero de canales de color en la imagen - 3 (en nuestro caso, RGB).  Para cada "capa" del canal de color, tambi√©n aplicaremos la operaci√≥n de convoluci√≥n con un filtro del tama√±o seleccionado.  Veamos c√≥mo ser√° un ejemplo. </p><br><p><img src="https://habrastorage.org/webt/77/yz/ms/77yzmskwfveucvryfrqnfqk9ti0.png"></p><br><p>  Imagine que tenemos una imagen RGB y queremos aplicar la operaci√≥n de convoluci√≥n con el siguiente filtro 3D.  Vale la pena prestar atenci√≥n al hecho de que nuestro filtro consta de 3 filtros bidimensionales.  Para simplificar, imaginemos que nuestra imagen RGB tiene un tama√±o de 5x5 p√≠xeles. </p><br><p><img src="https://habrastorage.org/webt/eg/46/54/eg4654ymcuktxfo-sybo7c2jw9q.png"></p><br><p>  Recuerde tambi√©n que cada canal de color es una matriz bidimensional de valores de color de p√≠xeles. </p><br><p><img src="https://habrastorage.org/webt/th/sk/9y/thsk9yyqd91koooaqcftklhyq-i.png"></p><br><p>  Al igual que con la operaci√≥n de convoluci√≥n sobre im√°genes en tonos de gris, as√≠ como con im√°genes en color, realizaremos la alineaci√≥n y complementaremos la imagen en los bordes con ceros para evitar la p√©rdida de informaci√≥n en los bordes. </p><br><p><img src="https://habrastorage.org/webt/us/vl/h7/usvlh7czpbmfn2iwbibynpr_aem.png"></p><br><p>  ¬°Ahora estamos listos para la operaci√≥n de convoluci√≥n! </p><br><p>  El mecanismo de convoluci√≥n para im√°genes en color ser√° similar al proceso que realizamos con im√°genes en escala de grises.  La √∫nica diferencia entre las operaciones realizadas en im√°genes en escala de grises y en color es que la operaci√≥n de convoluci√≥n ahora debe realizarse 3 veces para cada canal de color. </p><br><p><img src="https://habrastorage.org/webt/-8/mq/x5/-8mqx5ehxcqfg_4jttweanhhrhc.png"></p><br><p>  Luego, despu√©s de haber realizado la operaci√≥n de convoluci√≥n en cada canal de color, sume los tres valores obtenidos y agregue 1 a ellos (el valor est√°ndar utilizado al realizar operaciones de este tipo).  El nuevo valor resultante se fija en la misma posici√≥n en la nueva imagen, en qu√© posici√≥n estaba el p√≠xel convertido actual. </p><br><p>  Realizamos una operaci√≥n de conversi√≥n similar (una operaci√≥n de convoluci√≥n) para cada p√≠xel en nuestra imagen original y para cada canal de color. </p><br><p>  En este ejemplo particular, la imagen resultante tiene el mismo tama√±o en altura y ancho que nuestra imagen RGB original. </p><br><p>  Como puede ver, la aplicaci√≥n de la operaci√≥n de convoluci√≥n con un solo filtro 3D da como resultado un √∫nico valor de salida. </p><br><p><img src="https://habrastorage.org/webt/y_/kq/c7/y_kqc7d4p07hq7v-qkfxwakwuwg.png"></p><br><p>  Sin embargo, cuando se trabaja con redes neuronales convolucionales, es una pr√°ctica com√∫n usar m√°s de un filtro 3D.  Si utilizamos m√°s de un filtro 3D, el resultado ser√° varios valores de salida: cada valor es el resultado de un filtro. </p><br><p><img src="https://habrastorage.org/webt/36/ci/01/36ci013hfcfdcdthe23etkcg-_g.png"></p><br><p>  En nuestro ejemplo anterior, dado que usamos 3 filtros, la representaci√≥n 3D resultante tendr√° una profundidad de 3: cada capa corresponder√° al valor de salida de la conversi√≥n de un filtro sobre la imagen con todos sus canales de color. </p><br><p>  Si, por ejemplo, en lugar de 3 filtros, decidimos usar 16, entonces la representaci√≥n 3D de salida contendr√≠a 16 capas de profundidad. </p><br><p>  En el c√≥digo, podemos controlar la cantidad de filtros creados al pasar el valor apropiado para el par√°metro de <code>filters</code> : </p><br><pre> <code class="python hljs">tf.keras.layers.Conv2D(filters, kernel_size, ...)</code> </pre> <br><p>  Tambi√©n podemos especificar el tama√±o del filtro a trav√©s del par√°metro <code>kernel_size</code> .  Por ejemplo, para crear 3 filtros de tama√±o 3x3, como fue el caso en nuestro ejemplo anterior, podemos escribir el c√≥digo de la siguiente manera: </p><br><pre> <code class="python hljs">tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">3</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), ...)</code> </pre> <br><p>  Recuerde que durante el entrenamiento de la red neuronal convolucional, los valores en los filtros 3D se actualizar√°n para minimizar el valor de la funci√≥n de p√©rdida. </p><br><p>  Ahora que sabemos c√≥mo realizar la operaci√≥n de convoluci√≥n en im√°genes en color, es hora de descubrir c√≥mo aplicar la operaci√≥n de submuestreo al resultado m√°ximo por el valor m√°ximo (la misma agrupaci√≥n m√°xima). </p><br><h1>  La operaci√≥n de submuestreo por el valor m√°ximo en im√°genes en color </h1><br><p>  Aprendamos ahora c√≥mo realizar la operaci√≥n de submuestreo con el valor m√°ximo en im√°genes en color.  De hecho, la operaci√≥n de submuestreo por el valor m√°ximo funciona de la misma manera que funciona con im√°genes en tonos de gris con una ligera diferencia: la operaci√≥n de submuestreo ahora debe aplicarse a cada representaci√≥n de salida que recibimos como resultado de la aplicaci√≥n de filtros.  Veamos un ejemplo. </p><br><p>  Para simplificar, imaginemos que nuestra vista de salida se ve as√≠: </p><br><p><img src="https://habrastorage.org/webt/b0/-k/u1/b0-ku1roxi7hyplaadecnribfvw.png"></p><br><p>  Como antes, utilizaremos un kernel 2x2 y el paso 2 para realizar la operaci√≥n de submuestreo en el valor m√°ximo.  La operaci√≥n de submuestreo por el valor m√°ximo comienza con la "instalaci√≥n" de un n√∫cleo de 2x2 en la esquina superior izquierda de cada representaci√≥n de salida (la representaci√≥n que se obtuvo despu√©s de aplicar la operaci√≥n de convoluci√≥n). </p><br><p><img src="https://habrastorage.org/webt/sc/hv/68/schv68ab1pzdg-lcazzelhmmvr8.png"></p><br><p>  Ahora podemos comenzar la operaci√≥n de submuestreo en el valor m√°ximo.  Por ejemplo, en nuestra primera representaci√≥n de salida, los siguientes valores cayeron en el n√∫cleo 2x2: 1, 9, 5, 4. Dado que el valor m√°ximo en este n√∫cleo es 9, es el que se env√≠a a la nueva representaci√≥n de salida.  Se repite una operaci√≥n similar para cada representaci√≥n de entrada. </p><br><p>  Como resultado, deber√≠amos obtener el siguiente resultado: </p><br><p><img src="https://habrastorage.org/webt/9v/um/_0/9vum_0x2p4a78inha_xv3rnce8y.png"></p><br><p>  Despu√©s de realizar la operaci√≥n de submuestreo por el valor m√°ximo, el resultado es 3 matrices bidimensionales, cada una de las cuales es 2 veces m√°s peque√±a que la representaci√≥n de entrada original. </p><br><p>  Por lo tanto, en este caso particular, al realizar la operaci√≥n de submuestreo por el valor m√°ximo sobre la representaci√≥n de entrada tridimensional, obtenemos una representaci√≥n de salida tridimensional de la misma profundidad, pero con los valores de altura y anchura la mitad de los valores iniciales. </p><br><p>  Entonces, esta es toda la teor√≠a que necesitaremos para seguir trabajando.  ¬°Ahora veamos c√≥mo funciona esto en c√≥digo! </p><br><h1>  CoLab: gatos y perros </h1><br><p>  Original CoLab en ingl√©s est√° disponible en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . <br>  CoLab en ruso est√° disponible en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . </p><br><p>  En este tutorial, discutiremos c√≥mo clasificar las im√°genes de gatos y perros.  Desarrollaremos un clasificador de im√°genes utilizando el modelo <code>tf.keras.Sequential</code> y usaremos <code>tf.keras.Sequential</code> para cargar los datos. </p><br><h3 id="idei-kotorye-budut-zatronuty-v-etoy-chasti">  Ideas para ser cubiertas en esta parte: </h3><br><p>  Obtendremos experiencia pr√°ctica en el desarrollo de un clasificador y desarrollaremos una comprensi√≥n intuitiva de los siguientes conceptos: </p><br><ol><li>  Construir un modelo de flujo de datos ( <em>tuber√≠as de entrada de datos</em> ) usando <code>tf.keras.preprocessing.image.ImageDataGenerator</code> (¬øC√≥mo trabajar eficientemente con datos en el disco interactuando con el modelo?) </li><li>  Reciclaje: ¬øqu√© es y c√≥mo determinarlo? </li></ol><br><p>  <strong>Antes de comenzar ...</strong> </p><br><p>  Antes de comenzar el c√≥digo en el editor, le recomendamos que restablezca todas las configuraciones en <strong>Runtime -&gt; Restablecer todo</strong> en el men√∫ superior.  Tal acci√≥n ayudar√° a evitar problemas con la falta de memoria, si trabaj√≥ en paralelo o est√° trabajando con varios editores. </p><br><h1 id="importirovanie-paketov">  Importar paquetes </h1><br><p>  Comencemos importando los paquetes que necesita: </p><br><ul><li>  <code>os</code> - leer archivos y estructuras de directorios; </li><li>  <code>numpy</code> : para algunas operaciones matriciales fuera de TensorFlow; </li><li>  <code>matplotlib.pyplot</code> : trazar y mostrar im√°genes de un conjunto de datos de prueba y validaci√≥n. </li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np</code> </pre> <br><p>  Importar <code>TensorFlow</code> : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><h1 id="zagruzka-dannyh">  Carga de datos </h1><br><p>  Comenzamos el desarrollo de nuestro clasificador cargando un conjunto de datos.  El conjunto de datos que utilizamos es una versi√≥n filtrada del conjunto de datos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Dogs vs Cats</a> del servicio Kaggle (al final, Microsoft Research proporciona este conjunto de datos). </p><br><p>  En el pasado, CoLab y yo usamos un conjunto de datos del propio m√≥dulo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TensorFlow Dataset</a> , que es extremadamente conveniente para el trabajo y las pruebas.  En este CoLab, sin embargo, utilizaremos la clase <code>tf.keras.preprocessing.image.ImageDataGenerator</code> para leer datos del disco.  Por lo tanto, primero debemos descargar el conjunto de datos Dog VS Cats y descomprimirlo. </p><br><pre> <code class="python hljs">_URL = <span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'</span></span> zip_dir = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'cats_and_dogs_filterted.zip'</span></span>, origin=_URL, extract=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><p>  El conjunto de datos que descargamos tiene la siguiente estructura: </p><br><pre> <code class="plaintext hljs">cats_and_dogs_filtered |__ train |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...] |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...] |__ validation |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...] |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]</code> </pre> <br><p>  Para obtener una lista completa de directorios, puede usar el siguiente comando: </p><br><pre> <code class="python hljs">zip_dir_base = os.path.dirname(zip_dir) !find $zip_dir_base -type d -<span class="hljs-keyword"><span class="hljs-keyword">print</span></span></code> </pre> <br><p>  Como resultado, obtenemos algo similar: </p><br><pre> <code class="plaintext hljs">/root/.keras/datasets /root/.keras/datasets/cats_and_dogs_filtered /root/.keras/datasets/cats_and_dogs_filtered/train /root/.keras/datasets/cats_and_dogs_filtered/train/dogs /root/.keras/datasets/cats_and_dogs_filtered/train/cats /root/.keras/datasets/cats_and_dogs_filtered/validation /root/.keras/datasets/cats_and_dogs_filtered/validation/dogs /root/.keras/datasets/cats_and_dogs_filtered/validation/cats</code> </pre> <br><p>  Ahora asigne las rutas correctas a los directorios con los conjuntos de datos para capacitaci√≥n y validaci√≥n de las variables: </p><br><pre> <code class="python hljs">base_dir = os.path.join(os.path.dirname(zip_dir), <span class="hljs-string"><span class="hljs-string">'cats_and_dogs_filtered'</span></span>) train_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'train'</span></span>) validation_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'validation'</span></span>) train_cats_dir = os.path.join(train_dir, <span class="hljs-string"><span class="hljs-string">'cats'</span></span>) train_dogs_dir = os.path.join(train_dir, <span class="hljs-string"><span class="hljs-string">'dogs'</span></span>) validation_cats_dir = os.path.join(validation_dir, <span class="hljs-string"><span class="hljs-string">'cats'</span></span>) validation_dogs_dir = os.path.join(validation_dir, <span class="hljs-string"><span class="hljs-string">'dogs'</span></span>)</code> </pre> <br><h4 id="razbiraemsya-s-dannymi-i-ih-strukturoy">  Comprender los datos y su estructura. </h4><br><p>  Veamos cu√°ntas im√°genes de gatos y perros tenemos en los conjuntos de datos de prueba y validaci√≥n (directorios). </p><br><pre> <code class="python hljs">num_cats_tr = len(os.listdir(train_cats_dir)) num_dogs_tr = len(os.listdir(train_dogs_dir)) num_cats_val = len(os.listdir(validation_cats_dir)) num_dogs_val = len(os.listdir(validation_dogs_dir)) total_train = num_cats_tr + num_dogs_tr total_val = num_cats_val + num_dogs_val</code> </pre> <br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_cats_tr) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_dogs_tr) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_cats_val) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_dogs_val) print(<span class="hljs-string"><span class="hljs-string">'--'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'     : '</span></span>, total_train) print(<span class="hljs-string"><span class="hljs-string">'     : '</span></span>, total_val)</code> </pre> <br><p>  La salida del √∫ltimo bloque ser√° la siguiente: </p><br><pre> <code class="plaintext hljs">    : 1000     : 1000     : 500     : 500 --      : 2000      : 1000</code> </pre> <br><h1 id="ustanovka-parametrov-modeli">  Establecer par√°metros del modelo </h1><br><p>  Para mayor comodidad, colocaremos la instalaci√≥n de las variables que necesitamos para un mayor procesamiento de datos y capacitaci√≥n de modelos en un anuncio separado: </p><br><pre> <code class="python hljs">BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">100</span></span> <span class="hljs-comment"><span class="hljs-comment">#          IMG_SHAPE = 150 #  150x150      </span></span></code> </pre> <br><h1 id="podgotovka-dannyh">  Preparaci√≥n de datos </h1><br><p>  Antes de que las im√°genes se puedan usar como entrada para nuestra red, deben convertirse a tensores con valores de coma flotante.  Lista de pasos a seguir para hacer esto: </p><br><ol><li>  Leer im√°genes del disco </li><li>  Decodifique el contenido de la imagen y convi√©rtalo al formato deseado teniendo en cuenta el perfil RGB </li><li>  Convertir a tensores con valores de coma flotante </li><li>  Para normalizar los valores del tensor del intervalo de 0 a 255 al intervalo de 0 a 1, ya que las redes neuronales funcionan mejor con valores de entrada peque√±os. </li></ol><br><p>  Afortunadamente, todas estas operaciones se pueden realizar utilizando la clase <code>tf.keras.preprocessing.image.ImageDataGenerator</code> . </p><br><p>  Podemos hacer todo esto usando varias l√≠neas de c√≥digo: </p><br><pre> <code class="python hljs">train_image_generator = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>) validation_image_generator = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>)</code> </pre> <br><p>  Despu√©s de haber definido los generadores para un conjunto de datos de prueba y validaci√≥n, el m√©todo <strong>flow_from_directory</strong> cargar√° im√°genes del disco, normalizar√° los datos y redimensionar√° las im√°genes con solo una l√≠nea de c√≥digo: </p><br><pre> <code class="python hljs">train_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE, directory=train_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, target_size=(IMG_SHAPE,IMG_SHAPE), class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>)</code> </pre> <br><p>  Conclusi√≥n </p><br><pre> <code class="plaintext hljs">Found 2000 images belonging to 2 classes.</code> </pre> <br><p>  Generador de datos de validaci√≥n: </p><br><pre> <code class="python hljs">val_data_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE, directory=validation_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, target_size=(IMG_SHAPE,IMG_SHAPE), class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>)</code> </pre> <br><p>  Conclusi√≥n </p><br><pre> <code class="plaintext hljs">Found 1000 images belonging to 2 classes.</code> </pre> <br><h4 id="vizualiziruem-izobrazheniya-iz-trenirovochnogo-nabora">  Visualice las im√°genes del conjunto de entrenamiento. </h4><br><p>  Podemos visualizar im√°genes de un conjunto de datos de entrenamiento usando <code>matplotlib</code> : </p><br><pre> <code class="python hljs">sample_training_images, _ = next(train_data_gen)</code> </pre> <br><p>  La <code>next</code> funci√≥n devuelve un bloque de im√°genes del conjunto de datos.  Un bloque es una tupla de <em>(muchas im√°genes, muchas etiquetas)</em> .  En este momento, dejaremos caer las etiquetas, ya que no las necesitamos, estamos interesados ‚Äã‚Äãen las im√°genes mismas. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#        15 def plotImages(images_arr): fig, axes = plt.subplots(1, 5, figsize=(20, 20)) axes = axes.flatten() for img, ax in zip(images_arr, axes): ax.imshow(img) plt.tight_layout() plt.show()</span></span></code> </pre> <br><pre> <code class="python hljs">plotImages(sample_training_images[:<span class="hljs-number"><span class="hljs-number">5</span></span>]) <span class="hljs-comment"><span class="hljs-comment">#   0-4</span></span></code> </pre> <br><p>  Ejemplo de salida (2 im√°genes en lugar de las 5): </p><br><p><img src="https://habrastorage.org/webt/ag/sd/hr/agsdhrbpj3jqtgd-xwbnzoh-7go.png"></p><br><h1 id="sozdanie-modeli">  Creaci√≥n de modelos </h1><br><h3 id="opisyvaem-model">  Describimos el modelo </h3><br><p>  El modelo consta de 4 bloques de convoluci√≥n, despu√©s de cada uno de los cuales hay un bloque con una capa de submuestra.  A continuaci√≥n, tenemos una capa totalmente conectada con 512 neuronas y una <code>relu</code> activaci√≥n <code>relu</code> .  El modelo dar√° una distribuci√≥n de probabilidad para dos clases, perros y gatos, utilizando <code>softmax</code> . </p><br><pre> <code class="python hljs">model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(IMG_SHAPE, IMG_SHAPE, <span class="hljs-number"><span class="hljs-number">3</span></span>)), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Flatten(), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ])</code> </pre> <br><h4 id="kompilirovanie-modeli">  Compilaci√≥n de modelos </h4><br><p>  Como antes, usaremos el optimizador <code>adam</code> .  Usamos <code>sparse_categorical_crossentropy</code> como una funci√≥n de p√©rdida.  Tambi√©n queremos monitorear la precisi√≥n del modelo en cada iteraci√≥n de entrenamiento, por lo que pasamos el valor de <code>accuracy</code> al par√°metro de <code>metrics</code> : </p><br><pre> <code class="python hljs">model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><h4 id="predstavlenie-modeli">  Vista modelo </h4><br><p>  Echemos un vistazo a la estructura de nuestro modelo por niveles utilizando el m√©todo de <strong>resumen</strong> : </p><br><pre> <code class="python hljs">model.summary()</code> </pre> <br><p>  Conclusi√≥n </p><br><pre> <code class="plaintext hljs">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 148, 148, 32) 896 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 74, 74, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 72, 72, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 34, 34, 128) 73856 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 15, 15, 128) 147584 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 6272) 0 _________________________________________________________________ dense (Dense) (None, 512) 3211776 _________________________________________________________________ dense_1 (Dense) (None, 2) 1026 ================================================================= Total params: 3,453,634 Trainable params: 3,453,634 Non-trainable params: 0</code> </pre> <br><h4 id="trenirovka-modeli">   </h4><br><p>    ! </p><br><p>         ( <code>ImageDataGenerator</code> )    <code>fit_generator</code>     <code>fit</code> : </p><br><pre> <code class="python hljs">EPOCHS = <span class="hljs-number"><span class="hljs-number">100</span></span> history = model.fit_generator( train_data_gen, steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))), epochs=EPOCHS, validation_data=val_data_gen, validation_steps=int(np.ceil(total_val / float(BATCH_SIZE))) )</code> </pre> <br><h4 id="vizualizaciya-rezultatov-trenirovki">    </h4><br><p>       : </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">'./foo.png'</span></span>) plt.show()</code> </pre> <br><p>  Conclusi√≥n </p><br><p><img src="https://habrastorage.org/webt/z5/wn/we/z5wnwe2v8nmxrkwlrpgwgdr38qg.png"></p><br><p>     ,                   70%      (    ). </p><br><p>     .          ,             . </p><br><p> <em> ‚Ä¶   .</em> </p><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ... y un llamado a la acci√≥n est√°ndar: reg√≠strese, ponga un plus y comparta :) </font></font></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Telegrama</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VKontakte</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/456740/">https://habr.com/ru/post/456740/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456724/index.html">5 maneras extremadamente simples de acelerar significativamente su aplicaci√≥n VueJS</a></li>
<li><a href="../456730/index.html">Libro "{No sabes JS} Tipos y construcciones gramaticales"</a></li>
<li><a href="../456732/index.html">Ser un mentor</a></li>
<li><a href="../456736/index.html">Recetas PostgreSQL: cURL: obtener, publicar y ... correo electr√≥nico</a></li>
<li><a href="../456738/index.html">Redes neuronales y aprendizaje profundo, cap√≠tulo 1: uso de redes neuronales para reconocer n√∫meros escritos a mano</a></li>
<li><a href="../456744/index.html">10 problemas que resolv√≠ con recordatorios en mi tel√©fono inteligente</a></li>
<li><a href="../456746/index.html">Big data: gran responsabilidad, gran estr√©s y mucho dinero</a></li>
<li><a href="../456748/index.html">Impresora t√©rmica 2003 de un mercado de pulgas: ¬øqu√© puede hacer en 2019?</a></li>
<li><a href="../456754/index.html">GitOps: comparaci√≥n de los m√©todos Pull y Push</a></li>
<li><a href="../456756/index.html">¬øPor qu√© CockroachDB cambia la licencia de c√≥digo abierto?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>