<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë≤üèø ‚úãüèΩ üßóüèº Welche Video-Codecs (nicht) verwenden Browser f√ºr Videoanrufe? üîì üêç üà∑Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eine typische Anfrage des technischen Supports von Voximplant: ‚ÄûWarum sieht ein Videoanruf zwischen zwei Chrome besser aus als ein Videoanruf zwischen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Welche Video-Codecs (nicht) verwenden Browser f√ºr Videoanrufe?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/419949/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/bp/z1/1m/bpz11m_orhftn04lqz-4xbh6pzm.png"></div><br>  Eine typische Anfrage des technischen Supports von Voximplant: ‚ÄûWarum sieht ein Videoanruf zwischen zwei Chrome besser aus als ein Videoanruf zwischen MS Edge und einer nativen iOS-Anwendung?‚Äú  Kollegen reagieren normalerweise neutral - "weil die Codecs."  Aber wir IT-Leute sind neugierig.  Auch wenn ich kein neues Skype-for-Web entwickle, bereichert das Lesen von ‚ÄûWas kann ein Browser?‚Äú Und wie sie ein Video in mehrere Streams unterschiedlicher Qualit√§t aufteilen, das Bild der Welt und gibt ein neues Thema zur Diskussion im Raucherraum.  Erfolgreich aufgetaucht Artikel aus weithin bekannten in engen Kreisen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dr. Alex</a> (mit der besten Erkl√§rung des Begriffs "Media Engine" aus allem, was ich gesehen habe), ein wenig unserer Erfahrung, ein paar Abende im "Dial" - und eine f√ºr Habr angepasste √úbersetzung wartet unter dem Schnitt! <br><a name="habracut"></a><br><h2>  Codecs und Kanalbreite </h2><br>  Wenn es um Video-Codecs geht, wird meistens das Gleichgewicht zwischen Qualit√§t und Breite des verwendeten Kanals er√∂rtert.  Und sie ignorieren gerne Probleme mit der Prozessorlast und wie man Videos technisch √ºbertr√§gt.  Ziemlich vern√ºnftig, wenn wir √ºber die Codierung eines bereits aufgenommenen Videos sprechen. <br><br>  Wenn Sie ein fertiges Video haben, gibt es keinen gro√üen Unterschied, es wird ein paar Minuten, ein paar Stunden oder sogar ein paar Tage komprimieren.  Alle Prozessor- und Speicherkosten sind gerechtfertigt, da dies eine einmalige Investition ist und Sie das Video dann an Millionen von Benutzern verteilen k√∂nnen.  Die besten Video-Codecs komprimieren Videos in mehreren Durchg√§ngen: <br><br><ol><li>  Pass Nr. 1: Das Video ist in Teile mit gemeinsamen Merkmalen unterteilt: Die Aktion findet auf demselben Hintergrund, in derselben schnellen oder langsamen Szene und dergleichen statt. </li><li>  Pass Nr. 2: Sammeln Sie Statistiken zur Codierung und Informationen dar√ºber, wie sich Frames im Laufe der Zeit √§ndern (um solche Informationen zu erhalten, ben√∂tigen Sie mehrere Frames). </li><li>  Pass Nr. 3: Jeder Teil wird mit seinen eigenen Codec-Einstellungen und unter Verwendung der im zweiten Schritt erhaltenen Informationen codiert. </li></ol><br>  Streaming ist eine ganz andere Sache.  Niemand wird bis zum Ende eines Podcasts, Streams oder einer Show warten, bevor er mit der Codierung des Videos beginnt.  Sofort verschl√ºsseln und senden.  Lebe davon und lenke, dass die minimale Verz√∂gerung am wichtigsten wird. <br><br>  Bei Verwendung von physischen Medien, DVDs oder Blu-ray-Discs ist die Videogr√∂√üe festgelegt und der Codec steht vor der Aufgabe, die maximale Qualit√§t f√ºr eine bestimmte Gr√∂√üe sicherzustellen.  Wenn das Video √ºber das Netzwerk verteilt wird, besteht die Aufgabe des Codecs darin, solche Dateien vorzubereiten, um die maximale Qualit√§t bei fester Kanalbreite oder die minimale Kanalbreite bei fester Qualit√§t zu erzielen, wenn Sie den Preis senken m√ºssen.  In diesem Fall k√∂nnen Netzwerkverz√∂gerungen ignoriert und auf der Clientseite f√ºr so viele Sekunden Video wie n√∂tig gepuffert werden.  F√ºr das Streaming m√ºssen jedoch weder Gr√∂√üe noch Qualit√§t festgelegt werden. Der Codec hat eine andere Aufgabe: Verz√∂gerungen um jeden Preis zu reduzieren. <br><br>  Schlie√ülich haben die Codec-Hersteller lange Zeit nur einen Anwendungsfall im Auge behalten: Auf dem Computer des Benutzers wird nur ein einziges Video abgespielt.  Was dar√ºber hinaus fast immer durch die Kr√§fte eines Videochips dekodiert werden kann.  Dann gab es mobile Plattformen.  Und dann WebRTC, um die minimale Latenz sicherzustellen, f√ºr die die Entwickler wirklich Server der Selective Forwarding Unit verwenden wollten. <br><br>  Die Verwendung von Codecs f√ºr Videoanrufe unterscheidet sich so stark von der herk√∂mmlichen Verwendung beim Abspielen von Videos, dass ein Vergleich von Codecs ‚Äûfrontal‚Äú sinnlos wird.  Beim Vergleich von VP8 und H.264 zu Beginn von WebRTC ging es in einer der hei√üesten Diskussionen um Codec-Einstellungen: Sie sollten mit unzuverl√§ssigen Netzwerken ‚Äûrealistisch‚Äú oder f√ºr maximale Videoqualit√§t ‚Äûideal‚Äú sein.  Die K√§mpfer f√ºr einen ‚Äûsauberen Codec-Vergleich‚Äú argumentierten ernsthaft, dass Codecs verglichen werden sollten, ohne Paketverlust, Jitter und andere Netzwerkprobleme zu ber√ºcksichtigen. <br><br><h2>  Was ist jetzt mit Codecs? </h2><br><ul><li>  H.264 und VP8 sind hinsichtlich des Verh√§ltnisses von Videoqualit√§t und verwendeter Kanalbreite ungef√§hr gleich; </li><li>  H.265 und VP9 entsprechen ebenfalls in etwa einander und zeigen im Durchschnitt 30% bessere Ergebnisse im Vergleich zu Codecs der vorherigen Generation, da die Prozessorlast um 20% gestiegen ist. </li><li>  Der neue AV1-Codec, eine explosive Mischung aus VP10, Daala und Thor, ist besser als die Codecs der vorherigen Generation und besser als ihre Vorg√§nger. </li></ul><br>  Und jetzt eine √úberraschung: Diese Unterschiede sind allen egal, wenn es um Videoanrufe und Videokonferenzen geht.  Das Wichtigste ist, wie der Codec in einem Team mit dem Rest der Infrastruktur spielt.  Die Entwickler befassen sich mit dem so genannten neuen Begriff <b>Media Engine</b> : Wie erfasst ein Browser oder eine mobile Anwendung Videos, codiert / decodiert sie, zerlegt sie in RTP-Pakete und geht mit Netzwerkproblemen um (erinnern Sie sich an das Video aus unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Erfahrung</a> ? Also wurden Medien dort verglichen Motor - Anmerkung des √úbersetzers).  Wenn der Encoder nicht mit einer starken Verringerung der Kanalbreite arbeiten oder 20 Bilder pro Sekunde stabil halten kann, wenn der Decoder nicht mit dem Verlust eines Netzwerkpakets arbeiten kann, welchen Unterschied macht es dann, wie gut der Codec das Video komprimiert?  Es ist leicht zu verstehen, warum Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stanfords Forschung</a> zur besten Interaktion zwischen Codec und Netzwerk unterst√ºtzt.  Dies ist die Zukunft der Videokommunikation. <br><br><h2>  Codecs und Media Engine: Alles ist kompliziert </h2><br>  Videoanrufe und Videokonferenzen haben <b>fast</b> die gleichen Aufgaben wie herk√∂mmliche Medien.  Nur die Priorit√§ten sind <b>v√∂llig</b> unterschiedlich: <br><br><ol><li>  Es dauert 30 Bilder pro Sekunde (Codec-Geschwindigkeit). </li><li>  Ben√∂tigen Sie 30 Bilder pro Sekunde mit Interaktivit√§t (minimale Verz√∂gerung). </li></ol><br>  Wir haben auch das Internet zwischen den Teilnehmern, dessen Qualit√§t wir nur erraten k√∂nnen.  Normalerweise ist es schlimmer.  Deshalb: <br><br><ol><li>  Sie m√ºssen kleine √Ñnderungen in der Breite des Kanals erfahren, wenn ein anderer Besucher zum Coworking kommt. </li><li>  Sie m√ºssen zumindest irgendwie starke √Ñnderungen in der Kanalbreite feststellen, wenn dieser Besucher mit dem Herunterladen von Torrents beginnt. </li><li>  Sie m√ºssen sich um Jitter sorgen (zuf√§llige Verz√∂gerungen zwischen empfangenen Paketen, aufgrund derer sie nicht nur verz√∂gern k√∂nnen, sondern in der falschen Reihenfolge ankommen, in der sie gesendet wurden). </li><li>  Sorgen Sie sich um Paketverlust. </li></ol><br><h2>  3.1.  Hauptaufgaben der Media Engine </h2><br>  Was bedeutet "30 Bilder pro Sekunde ben√∂tigen"?  Dies bedeutet, dass die Media Engine 33 Millisekunden Zeit hat, um Videos von der Kamera aufzunehmen, Ton vom Mikrofon zu h√∂ren, ihn mit einem Codec zu komprimieren, ihn in RTP-Pakete aufzuteilen, die √ºbertragenen Daten zu sch√ºtzen (SRTP = RTP + AES) und √ºber das Netzwerk (UDP oder TCP) zu senden in den allermeisten F√§llen UDP).  All dies ist auf der sendenden Seite.  Und auf der Empfangsseite - in umgekehrter Reihenfolge wiederholen.  Da das Codieren normalerweise schwieriger ist als das Decodieren, f√§llt es der sendenden Seite schwerer. <br><br>  Auf der technischen Seite ist das Ziel ‚Äû30 Bilder pro Sekunde‚Äú mit Verz√∂gerungen erreichbar.  Und je gr√∂√üer die Verz√∂gerung ist, desto einfacher ist es, das Ziel zu erreichen: Wenn Sie auf der Sendeseite nicht mehrere Frames gleichzeitig, sondern mehrere gleichzeitig codieren, k√∂nnen Sie die Kanalbreite erheblich einsparen (Codecs komprimieren mehrere Frames besser, indem sie nicht nur die √Ñnderungen zwischen allen analysieren zwischen aktuell und vorher).  Gleichzeitig nimmt die Verz√∂gerung zwischen dem Empfangen des Videostreams von der Kamera und dem Senden √ºber das Netzwerk proportional zur Anzahl der gepufferten Frames zu, und die Komprimierung wird aufgrund zus√§tzlicher Berechnungen langsamer.  Viele Sites verwenden diesen Trick und geben die Antwortzeit zwischen dem Senden und Empfangen von Netzwerkpaketen zwischen Teilnehmern an Videoanrufen an.  Die Verz√∂gerung beim Codieren und Decodieren, sie schweigen. <br><br>  Damit Videoanrufe wie pers√∂nliche Kommunikation aussehen, lehnen die Ersteller von Kommunikationsdiensten alle Einstellungen und Codec-Profile ab, die zu Verz√∂gerungen f√ºhren k√∂nnen.  Es stellt sich heraus, dass sich moderne Codecs zu einer Einzelbildkomprimierung verschlechtern.  Eine solche Situation erregte zun√§chst Ablehnung und Kritik bei Codec-Entwicklern.  Aber die Zeiten haben sich ge√§ndert, und jetzt haben moderne Codecs zus√§tzlich zu den traditionellen Voreinstellungen "minimale Gr√∂√üe" und "maximale Qualit√§t" eine Reihe von "Echtzeit" -Einstellungen hinzugef√ºgt.  Gleichzeitig ist ‚ÄûBildschirmfreigabe‚Äú auch f√ºr Videoanrufe vorgesehen (es hat seine eigenen Besonderheiten - gro√üe Aufl√∂sung, sich leicht √§nderndes Bild, verlustfreie Komprimierung, sonst schwebt der Text). <br><br><h2>  3.2.  Media Engine und √∂ffentliche Netzwerke </h2><br><blockquote>  Kleine √Ñnderungen in der Kanalbreite </blockquote><br>  Bisher konnten Codecs die Bitrate nicht √§ndern: Zu Beginn der Komprimierung nahmen sie die Zielbitrate als Einstellung und gaben dann eine feste Anzahl von Megabyte Video pro Minute aus.  In jenen alten Tagen waren Videoanrufe und Videokonferenzen die Menge lokaler Netzwerke und redundanter Bandbreite.  Bei Problemen wurde der Name des Administrators genannt, der die Reservierung der Kanalbreite auf tsiska korrigierte. <br><br>  Die erste evolution√§re √Ñnderung war die Technologie der "adaptiven Bitrate".  Der Codec verf√ºgt √ºber viele Einstellungen, die sich auf die Bitrate auswirken: Videoaufl√∂sung, leichte Verringerung der fps von 30 auf 25 Bilder pro Sekunde, Quantisierung des Videosignals.  Das letzte auf dieser Liste ist die ‚ÄûVergr√∂berung‚Äú des √úbergangs zwischen Farben, deren geringf√ºgige √Ñnderungen f√ºr das menschliche Auge kaum wahrnehmbar sind.  Meistens war die pr√§zise Quantisierung die Hauptabstimmung f√ºr die adaptive Bitrate.  Und die Media Engine teilte dem Codec die Kanalbreite mit. <br><br><blockquote>  Gro√üe √Ñnderungen in der Kanalbreite </blockquote><br>  Der adaptive Bitratenmechanismus hilft der Media Engine, die Video√ºbertragung mit geringf√ºgigen √Ñnderungen der Kanalbreite fortzusetzen.  Wenn Ihr Kollege jedoch mit dem Herunterladen von Torrents begonnen hat und der verf√ºgbare Kanal zwei- oder dreimal eingetaucht ist, hilft die adaptive Bitrate nicht weiter.  Das Verringern der Aufl√∂sung und der Bildrate hilft.  Letzteres ist vorzuziehen, da unsere Augen weniger empfindlich auf die Anzahl der Bilder pro Sekunde als auf die Aufl√∂sung des Videos reagieren.  Normalerweise √ºberspringt der Codec ein oder zwei Frames und reduziert die Framerate von 30 auf 15 oder sogar auf 10. <br><br>  Wichtiges Detail: Die Media Engine √ºberspringt Frames auf der sendenden Seite.  Wenn wir eine Videokonferenz f√ºr mehrere Teilnehmer oder eine Sendung haben und der Absender kein Netzwerkproblem hat, verschlechtert ein ‚Äûschwaches Glied‚Äú die Videoqualit√§t f√ºr alle Teilnehmer.  In dieser Situation hilft eine Reihe von Simulcasts (die Sendeseite gibt mehrere Videostreams unterschiedlicher Qualit√§t gleichzeitig ab) und SFU (Selective Forwarding Unit, der Server gibt jedem Teilnehmer eine Videokonferenz oder sendet einen Stream der gew√ºnschten Qualit√§t).  Einige Codecs k√∂nnen mehrere Simulcast-Streams erstellen, SVCs, die sich gegenseitig erg√§nzen: Clients mit dem schw√§chsten Kanal erhalten einen Stream von minimaler Qualit√§t, Clients mit einem besseren Kanal erhalten denselben Stream plus das erste ‚ÄûUpgrade‚Äú, Clients mit einem noch besseren Kanal bereits zwei Streams von "Upgrade" und so weiter.  Diese Methode erm√∂glicht es Ihnen, nicht dieselben Daten in mehrere Streams zu √ºbertragen, und spart etwa 20% des Datenverkehrs im Vergleich zur Codierung mehrerer vollwertiger Videostreams.  Es vereinfacht auch den Betrieb des Servers - es besteht keine Notwendigkeit, den Datenfluss zu wechseln, es reicht aus, keine Pakete mit einem ‚ÄûUpgrade‚Äú an Clients zu √ºbertragen.  Trotzdem kann jeder Codec f√ºr Simulcasts verwendet werden. Er ist eine Funktion der Media Engine und der Organisation von RTP-Paketen, kein Codec. <br><br><blockquote>  Jitter und Paketverlust </blockquote><br>  Verluste sind am schwierigsten zu bew√§ltigen.  Jitter ist etwas einfacher - erstellen Sie einfach einen Puffer auf der Empfangsseite, in dem Sie versp√§tete und verwirrte Pakete sammeln k√∂nnen.  Kein zu gro√üer Puffer, sonst k√∂nnen Sie die Echtzeit unterbrechen und ein pufferndes YouTube-Video werden. <br><br>  Paketverlust wird normalerweise mit Re-Forwarding (RTX) bek√§mpft.  Wenn der Absender eine gute Kommunikation mit der SFU hat, kann der Server ein verlorenes Paket anfordern, es abrufen und trotzdem 33 Millisekunden einhalten.  Wenn die Netzwerkverbindung unzuverl√§ssig ist (mehr als 0,01% Paketverlust), ben√∂tigen wir komplexe Algorithmen f√ºr die Verlustarbeit, wie z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FEC</a> . <br><br>  Die bisher beste L√∂sung ist die Verwendung von SVC-Codecs.  In diesem Fall werden zum Empfangen von mindestens einigen Videos nur "Referenz" -Pakete mit einem Stream von minimaler Qualit√§t ben√∂tigt. Diese Pakete sind weniger. Daher ist es einfacher, sie erneut zu senden. Dies reicht aus, um selbst bei einem sehr schlechten Netzwerk "zu √ºberleben" (mehr als 1% Paketverlust).  Wenn Sie mit Simulcast + SFU mit Kanalsenkungen umgehen k√∂nnen, l√∂st Simulcast mithilfe des SVC-Codecs + SFU sowohl die Kanalbreite als auch Probleme mit verlorenen Paketen. <br><br><h2>  Welche Browser unterst√ºtzen jetzt </h2><br>  Firefox und Safari verwenden die Media Engine von Google und aktualisieren libwebrtc von Zeit zu Zeit.  Sie tun dies viel seltener als Chrome, dessen neue Version alle 6 Wochen ver√∂ffentlicht wird.  Von Zeit zu Zeit bleiben sie weit zur√ºck, synchronisieren sich dann aber wieder.  Mit Ausnahme der Unterst√ºtzung des VP8-Codecs in Safari.  Fragen Sie nicht einmal. <br><br>  Vor kat eine Tabelle mit einem vollst√§ndigen Vergleich, wer was unterst√ºtzt, aber im Allgemeinen ist alles ganz einfach.  Edge wird normalerweise von allen ignoriert.  Sie haben die Wahl zwischen der Unterst√ºtzung der mobilen Version von Safari und einer guten Videoqualit√§t.  iOS Safari unterst√ºtzt nur H.264-Videocodec, w√§hrend libwebrtc nur Simulcasting mit VP8-Codecs (verschiedene Streams mit unterschiedlichen Bildraten) und VP9 (SVC-Unterst√ºtzung) zul√§sst.  Sie k√∂nnen libwebrtc jedoch unter iOS lesen und verwenden, indem Sie eine native Anwendung erstellen.  Mit Simulcast ist dann alles in Ordnung und die Benutzer erhalten die h√∂chstm√∂gliche Videoqualit√§t mit einer instabilen Internetverbindung.  Einige Beispiele: <br><br><ul><li>  <b>Highfive</b> - eine Desktop-Anwendung auf Electron (Chromium) mit H.264-Simulcast (libwebrtc) und Soundcodecs von Dolby; </li><li>  <b>Attlasian</b> - Eine interessante L√∂sung des Clients f√ºr React Native und libwebrtc f√ºr Simulcast; </li><li>  <b>Symphony</b> - Electron f√ºr den Desktop, React Native f√ºr mobile Ger√§te und Simulcast werden hier und da unterst√ºtzt + zus√§tzliche Sicherheitsfunktionen, die mit den W√ºnschen der Banken kompatibel sind; </li><li>  <b>Tokbox</b> - VP8 mit Simulcast im mobilen SDK, verwenden Sie die gepatchte Version von libvpx in libwebrtc. </li></ul><br><h2>  Die Zukunft </h2><br>  Es ist bereits klar, dass es in Safari kein VP8 und VP9 geben wird (im Gegensatz zu Edge, das VP8 unterst√ºtzt). <br><br>  Obwohl Apple die Aufnahme von H.265 in WebRTC unterst√ºtzte, deuten die j√ºngsten Nachrichten und eine Reihe indirekter Anzeichen darauf hin, dass AV1 das ‚Äûn√§chste gro√üe Ding‚Äú ist.  Im Gegensatz zum Rest des Artikels ist dies meine pers√∂nliche Meinung.  Die Spezifikation f√ºr die Daten√ºbertragung AV1 ist bereits fertig, am Codec wird jedoch noch gearbeitet.  Jetzt zeigt die Referenzimplementierung des Encoders traurige 0,3 Bilder pro Sekunde.  Dies ist kein Problem beim Abspielen vorkomprimierter Inhalte, gilt jedoch noch nicht f√ºr die Echtzeitkommunikation.  In der Zwischenzeit k√∂nnen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">versuchen</a> , AV1-Videos in Firefox abzuspielen, obwohl dies nicht mit RTC zusammenh√§ngt.  Implementierung durch das Bitmovin-Team, das MPEG-DASH entwickelt und 30 Millionen Investitionen in die Schaffung der Infrastruktur der n√§chsten Generation f√ºr die Arbeit mit Videos erhalten hat. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de419949/">https://habr.com/ru/post/de419949/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de419939/index.html">Wie ich die Navigation in React Native nicht so schrecklich gemacht habe</a></li>
<li><a href="../de419941/index.html">Audiomania Office Fototour: Teil Eins</a></li>
<li><a href="../de419943/index.html">Was wir im Juli gelesen haben: Wie man Zeit zum Lesen findet, f√ºnf B√ºcher f√ºr Teamleiter und einige neue Artikel</a></li>
<li><a href="../de419945/index.html">So bereiten Sie sich auf ein Interview bei Google vor und geben es nicht weiter. Zweimal</a></li>
<li><a href="../de419947/index.html">Stellen Sie mit Raspbian Stretch Lite eine Verbindung zu PiZeroW her, ohne zus√§tzliche Adapter und einen Monitor</a></li>
<li><a href="../de419951/index.html">Erfahrung mit WebRTC. Yandex Vortrag</a></li>
<li><a href="../de419953/index.html">Ich schreibe ein Buch √ºber das erste ‚Äûunser‚Äú Startup, das die Welt erobert hat: Hilfe</a></li>
<li><a href="../de419955/index.html">Funktionen des FIFO-UART-Puffers in ESP32</a></li>
<li><a href="../de419961/index.html">Die Zusammenfassung interessanter Materialien f√ºr den mobilen Entwickler # 265 (6. August - 12. August)</a></li>
<li><a href="../de419963/index.html">Wir stellen eine "intelligente" Steuerung f√ºr die Klimaanlage des ESP8266 her</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>