<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåπ üòø üê∂ Monte Carlo Blackjack Strategieoptimierung üòê üë¥üèº ü§ôüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die √úbersetzung des Artikels wurde speziell f√ºr Studierende des Kurses Maschinelles Lernen erstellt . 



 Verst√§rktes Training eroberte die Welt der ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Monte Carlo Blackjack Strategieoptimierung</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477042/">  <i>Die √úbersetzung des Artikels wurde speziell f√ºr Studierende des Kurses <a href="https://otus.pw/Zkti/">Maschinelles Lernen erstellt</a> .</i> <br><hr><br><img src="https://habrastorage.org/webt/dj/mh/7h/djmh7hubq1mecnsb-gilezo8qyi.png"><br><br>  Verst√§rktes Training eroberte die Welt der k√ºnstlichen Intelligenz.  Ausgehend von AlphaGo und <a href="https://www.space.com/alphastar-artificial-intelligence-starcraft-2-grandmaster.html">AlphaStar</a> werden immer mehr Aktivit√§ten, die zuvor vom Menschen dominiert wurden, jetzt von KI-Agenten auf der Grundlage von Verst√§rkungstraining erobert.  Kurz gesagt, diese Erfolge h√§ngen von der Optimierung der Aktionen des Agenten in einer bestimmten Umgebung ab, um eine maximale Belohnung zu erzielen.  In den letzten Artikeln von <a href="https://medium.com/gradientcrescent">GradientCrescent haben</a> wir uns mit verschiedenen grundlegenden Aspekten des verst√§rkten Lernens befasst, von den Grundlagen der <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-the-k-bandit-problem-illustrat-940eea430296">Banditensysteme</a> und <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">politischen</a> Ans√§tzen zur Optimierung des <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">belohnungsbasierten</a> Verhaltens in <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82">Markov-Umgebungen</a> .  All diese Ans√§tze erforderten eine vollst√§ndige Kenntnis unserer Umwelt.  <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310">Die dynamische Programmierung</a> setzt beispielsweise eine vollst√§ndige Wahrscheinlichkeitsverteilung aller m√∂glichen Zustands√ºberg√§nge voraus.  In der Realit√§t stellen wir jedoch fest, dass die meisten Systeme nicht vollst√§ndig interpretiert werden k√∂nnen und dass Wahrscheinlichkeitsverteilungen aufgrund von Komplexit√§t, inh√§renter Unsicherheit oder Einschr√§nkungen der Rechenkapazit√§ten nicht explizit erhalten werden k√∂nnen.  Betrachten Sie als Analogie die Aufgabe des Meteorologen - die Anzahl der Faktoren, die bei der Wettervorhersage eine Rolle spielen, kann so gro√ü sein, dass es unm√∂glich ist, die Wahrscheinlichkeit genau zu berechnen. <a name="habracut"></a><br><br>  In solchen F√§llen sind Lehrmethoden wie Monte Carlo die L√∂sung.  Der Begriff Monte Carlo wird allgemein verwendet, um einen beliebigen Ansatz zur Sch√§tzung von Zufallsstichproben zu beschreiben.  Mit anderen Worten, wir sagen kein Wissen √ºber unsere Umwelt voraus, sondern lernen aus Erfahrungen, indem wir beispielhafte Folgen von Zust√§nden, Handlungen und Belohnungen durchlaufen, die durch die Interaktion mit der Umwelt entstehen.  Diese Methoden arbeiten, indem sie die vom Modell w√§hrend des normalen Betriebs zur√ºckgegebenen Belohnungen direkt beobachten, um den Durchschnittswert seiner Bedingungen zu beurteilen.  Interessanterweise k√∂nnen wir auch ohne Kenntnis der Dynamik der Umgebung (die als Wahrscheinlichkeitsverteilung von Zustands√ºberg√§ngen betrachtet werden sollte) immer noch ein optimales Verhalten erzielen, um die Belohnungen zu maximieren. <br><br>  Betrachten Sie als Beispiel das Ergebnis eines W√ºrfels mit 12 W√ºrfeln.  Wenn wir diese W√ºrfe als einen einzelnen Zustand betrachten, k√∂nnen wir diese Ergebnisse mitteln, um n√§her an das tats√§chlich vorhergesagte Ergebnis heranzukommen.  Je gr√∂√üer die Stichprobe, desto genauer n√§hern wir uns dem tats√§chlich erwarteten Ergebnis. <br><br><img src="https://habrastorage.org/webt/xr/85/d6/xr85d6ugn6dbszaqh5b-nbq-usq.png"><br>  <i>Die durchschnittlich erwartete Menge von 12 W√ºrfeln f√ºr 60 Sch√ºsse betr√§gt 41,57</i> <br><br>  Diese Art der stichprobenbasierten Bewertung mag dem Leser vertraut erscheinen, da eine solche Stichprobe auch f√ºr k-Banditensysteme durchgef√ºhrt wird.  Anstatt verschiedene Banditen zu vergleichen, werden Monte-Carlo-Methoden verwendet, um verschiedene Richtlinien in Markov-Umgebungen zu vergleichen und den Wert des Staates zu bestimmen, w√§hrend eine bestimmte Richtlinie befolgt wird, bis die Arbeit abgeschlossen ist. <br><br><h3>  Monte-Carlo-Sch√§tzung des Zustandswertes </h3><br>  Monte-Carlo-Methoden sind eine M√∂glichkeit, im Kontext des verst√§rkenden Lernens die Signifikanz des Zustands eines Modells durch Mittelung der Stichprobenergebnisse zu bewerten.  Aufgrund der Notwendigkeit eines Endzustands sind Monte-Carlo-Methoden von Natur aus auf episodische Umgebungen anwendbar.  Aufgrund dieser Einschr√§nkung werden Monte-Carlo-Methoden normalerweise als "autonom" betrachtet, bei denen alle Aktualisierungen nach Erreichen des Terminalzustands durchgef√ºhrt werden.  Es kann eine einfache Analogie zum Herausfinden eines Weges aus einem Labyrinth gegeben werden - ein autonomer Ansatz w√ºrde den Agenten dazu zwingen, das Ende zu erreichen, bevor er die gesammelten Zwischenerfahrungen nutzt, um die Zeit zu verk√ºrzen, die er ben√∂tigt, um durch das Labyrinth zu gehen.  Andererseits wird der Agent bei der Online-Ann√§herung sein Verhalten bereits w√§hrend des Durchgangs des Labyrinths st√§ndig √§ndern. Vielleicht bemerkt er, dass die gr√ºnen Korridore zu Sackgassen f√ºhren und entscheidet, sie beispielsweise zu meiden.  In einem der folgenden Artikel werden wir Online-Ans√§tze diskutieren. <br><br>  Die Monte-Carlo-Methode kann wie folgt formuliert werden: <br><br><img src="https://habrastorage.org/webt/u4/kd/o_/u4kdo_uc3dj64rihmdjhnyj1r7y.png"><br><br>  Um die Funktionsweise der Monte-Carlo-Methode besser zu verstehen, sehen Sie sich das folgende Zustands√ºbergangsdiagramm an.  Die Belohnung f√ºr jeden Zustands√ºbergang wird in schwarz angezeigt, ein Abzinsungsfaktor von 0,5 wird darauf angewendet.  Lassen Sie uns den tats√§chlichen Wert des Zustands beiseite legen und uns auf die Berechnung der Ergebnisse eines Wurfs konzentrieren. <br><br><img src="https://habrastorage.org/webt/oc/he/n9/ochen91kvgbix5pgltzijpbozgg.png"><br>  <i>Zustands√ºbergangsdiagramm.</i>  <i>Die Statusnummer wird rot angezeigt, das Ergebnis ist schwarz.</i> <br>  Da der Endzustand ein Ergebnis gleich 0 zur√ºckgibt, berechnen wir das Ergebnis jedes Zustands, beginnend mit dem Endzustand (G5).  Bitte beachten Sie, dass wir den Abzinsungsfaktor auf 0,5 eingestellt haben, was zu einer Gewichtung gegen√ºber den sp√§teren Staaten f√ºhrt. <br><br><img src="https://habrastorage.org/webt/uw/y6/qq/uwy6qq_amgfxpsyj2_3gpij498o.png"><br><br>  Oder allgemeiner: <br><br><img src="https://habrastorage.org/webt/k7/i4/ua/k7i4ua-g8vaph8w62-rfpc3r8sk.png"><br><br>  Um zu vermeiden, dass alle Ergebnisse in der Liste gespeichert werden, k√∂nnen Sie den Statuswert in der Monte-Carlo-Methode schrittweise aktualisieren, indem Sie eine Gleichung verwenden, die einige √Ñhnlichkeiten mit der herk√∂mmlichen Gradientenabnahme aufweist: <br><br><img src="https://habrastorage.org/webt/lv/tm/nk/lvtmnki_ff4csblmytygnnxjn6u.png"><br>  <i>Inkrementelle Aktualisierung nach Monte Carlo.</i>  <i>S ist der Zustand, V ist sein Wert, G ist sein Ergebnis und A ist der Schrittwertparameter.</i> <br><br>  Im Rahmen des Verst√§rkungstrainings k√∂nnen Monte-Carlo-Methoden sogar als Erster Besuch oder Jeder Besuch klassifiziert werden.  Kurz gesagt, der Unterschied zwischen beiden besteht darin, wie oft ein Bundesstaat in einer Passage vor dem Monte-Carlo-Update besucht werden kann.  Die Monte-Carlo-Methode f√ºr den ersten Besuch sch√§tzt den Wert aller Zust√§nde als Durchschnittswert der Ergebnisse nach einzelnen Besuchen in jedem Zustand vor Abschluss, w√§hrend die Monte-Carlo-Methode f√ºr jeden Besuch den Durchschnitt der Ergebnisse nach n Besuchen bis zum Abschluss ermittelt.  Wir werden den Monte-Carlo-Erstbesuch in diesem Artikel wegen seiner relativen Einfachheit verwenden. <br><br><h3>  Monte-Carlo-Richtlinienverwaltung </h3><br>  Wenn das Modell die Richtlinie nicht bereitstellen kann, kann Monte Carlo zum Bewerten von Zustandsaktionswerten verwendet werden.  Dies ist n√ºtzlicher als nur die Bedeutung der Zust√§nde, da die Idee der Bedeutung jeder Aktion <i>(q)</i> in einem gegebenen Zustand es dem Agenten erm√∂glicht, automatisch eine Richtlinie aus Beobachtungen in einer unbekannten Umgebung zu formulieren. <br><br>  Genauer gesagt k√∂nnen wir Monte Carlo verwenden, um <i>q (s, a, pi)</i> , das erwartete Ergebnis, wenn wir von Zustand s ausgehen, Aktion a und die nachfolgende Richtlinie <i>Pi</i> zu sch√§tzen.  Die Monte-Carlo-Methoden bleiben unver√§ndert, mit der Ausnahme, dass f√ºr einen bestimmten Zustand eine zus√§tzliche Dimension von Ma√ünahmen ergriffen wird.  Es wird angenommen, dass ein Zustand-Aktion <i>(en, ein)</i> -Paar w√§hrend der Passage besucht wird, wenn Zustand <i>s</i> jemals besucht wird und Aktion <i>a</i> darin ausgef√ºhrt wird.  Ebenso kann die Bewertung von Wertaktionen mit den Ans√§tzen ‚ÄûErster Besuch‚Äú und ‚ÄûJeder Besuch‚Äú durchgef√ºhrt werden. <br><br>  Wie bei der dynamischen Programmierung k√∂nnen wir eine generalisierte Iterationsrichtlinie (GPI) verwenden, um eine Richtlinie aus der Beobachtung von Zustandsaktionswerten zu bilden. <br><br><img src="https://habrastorage.org/webt/wi/pm/p5/wipmp58itbgocsyruulw0ccliay.png"><br><br>  Indem wir die Schritte der Politikevaluierung und -verbesserung abwechseln und Nachforschungen anstellen, um sicherzustellen, dass alle m√∂glichen Ma√ünahmen ergriffen werden, k√∂nnen wir f√ºr jede Bedingung die optimale Politik erzielen.  Beim Monte-Carlo-GPI erfolgt diese Drehung normalerweise nach dem Ende jedes Durchgangs. <br><br><img src="https://habrastorage.org/webt/nf/b4/nw/nfb4nwijndtfex4ytwhdn2ujcyw.png"><br>  <i>Monte Carlo GPI</i> <br><br><h3>  Blackjack-Strategie </h3><br>  Um besser zu verstehen, wie die Monte-Carlo-Methode in der Praxis bei der Bewertung verschiedener Zustandswerte funktioniert, f√ºhren wir das Beispiel eines Blackjack-Spiels Schritt f√ºr Schritt vor.  Lassen Sie uns zun√§chst die Regeln und Bedingungen f√ºr unser Spiel festlegen: <br><br><ul><li>  Wir werden nur gegen den Dealer spielen, es wird keine anderen Spieler geben.  Dadurch k√∂nnen wir die H√§nde des H√§ndlers als Teil der Umwelt betrachten. </li><li>  Der Wert von Karten mit Zahlen, die den Nennwerten entsprechen.  Der Wert der Bildkarten: Bube, K√∂nig und Dame ist 10. Der Wert des Asses kann 1 oder 11 sein, abh√§ngig von der Wahl des Spielers. </li><li>  Beide Seiten erhalten zwei Karten.  Zwei Spielerkarten liegen offen, eine der Karten des Dealers liegt ebenfalls offen. </li><li>  Das Ziel des Spiels ist, dass die Anzahl der Karten auf der Hand &lt;= 21 ist.  Ein Wert gr√∂√üer als 21 ist ein Fehlschlag. Wenn beide Seiten einen Wert von 21 haben, wird das Spiel unentschieden gespielt. </li><li>  Nachdem der Spieler seine Karten und die Karte des ersten Dealers gesehen hat, kann er w√§hlen, ob er eine neue Karte ("noch") oder nicht ("genug") nehmen m√∂chte, bis er mit der Summe der Kartenwerte auf seiner Hand zufrieden ist. </li><li>  Dann zeigt der Dealer seine zweite Karte - wenn der sich ergebende Betrag weniger als 17 betr√§gt, muss er Karten nehmen, bis er 17 Punkte erreicht hat, wonach er die Karte nicht mehr nimmt. </li></ul><br>  Mal sehen, wie die Monte-Carlo-Methode mit diesen Regeln funktioniert. <br><br><h4>  Runde 1. </h4><br>  Du gewinnst insgesamt 19. Aber du versuchst, das Gl√ºck am Schwanz zu fangen, eine Chance zu nutzen, 3 zu bekommen und pleite zu gehen.  Als Sie pleite gingen, hatte der Dealer nur eine offene Karte mit einer Summe von 10. Dies kann wie folgt dargestellt werden: <br><br><img src="https://habrastorage.org/webt/4o/di/sl/4odisljnoapgkqblzsixkr2a4_4.png"><br><br>  Wenn wir pleite gehen, ist unsere Belohnung f√ºr die Runde -1.  Legen wir diesen Wert als Ergebnis der R√ºckgabe des vorletzten Status im folgenden Format fest [Betrag des Agenten, Betrag des H√§ndlers, Ass?]: <br><br><img src="https://habrastorage.org/webt/2q/er/g4/2qerg4mooln9blj9sfqo1ndzxbo.png"><br><br>  Nun, jetzt haben wir kein Gl√ºck mehr.  Gehen wir zu einer anderen Runde √ºber. <br><br><h4>  Runde 2. </h4><br>  Sie geben insgesamt 19 ein. Diesmal entscheiden Sie sich, den Vorgang abzubrechen.  Der Dealer w√§hlt 13, nimmt eine Karte und geht pleite.  Der vorletzte Zustand kann wie folgt beschrieben werden. <br><br><img src="https://habrastorage.org/webt/al/th/dv/althdvyqwkhwqd5cj2vvzcjbiam.png"><br><br>  Beschreiben wir die Bedingungen und Belohnungen, die wir in dieser Runde erhalten haben: <br><br><img src="https://habrastorage.org/webt/rm/mv/n8/rmmvn88mg7za9gyrtvrvohywmwc.png"><br><br>  Mit dem Ende der Passage k√∂nnen wir nun die Werte aller unserer Staaten in dieser Runde anhand der berechneten Ergebnisse aktualisieren.  Bei einem Abzinsungsfaktor von 1 verteilen wir einfach unsere neue Handbelohnung, wie dies bei den vorherigen Zustands√ºberg√§ngen der Fall war.  Da der Zustand <i>V (19, 10, nein)</i> zuvor -1 zur√ºckgegeben hat, berechnen wir den erwarteten R√ºckgabewert und weisen ihn unserem Zustand zu: <br><br><img src="https://habrastorage.org/webt/u0/um/q9/u0umq9dpg6qw420ecz44cqfp9mk.png"><br>  <i>Endzustandswerte zur Demonstration am Beispiel Blackjack</i> . <br><br><h3>  Implementierung </h3><br>  Lassen Sie uns ein Blackjack-Spiel mit der Monte-Carlo-Methode des ersten Besuchs schreiben, um alle m√∂glichen Statuswerte (oder verschiedene verf√ºgbare Kombinationen) im Spiel mit Python herauszufinden.  Unser Ansatz basiert auf dem <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">Ansatz von Sudharsan et.</a>  <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">al.</a>  .  Wie immer finden Sie den gesamten Code aus dem Artikel auf <a href="https://github.com/EXJUSTICE/GradientCrescent)">unserem GitHub</a> . <br><br>  Um die Implementierung zu vereinfachen, werden wir Gym von OpenAI verwenden.  Stellen Sie sich die Umgebung als Schnittstelle zum Starten von Blackjack mit einem Minimum an Code vor. Auf diese Weise k√∂nnen wir uns auf die Implementierung von verst√§rktem Lernen konzentrieren.  Praktischerweise werden alle gesammelten Informationen √ºber die Zust√§nde, Aktionen und Belohnungen in den <i>Beobachtungsvariablen</i> gespeichert, die w√§hrend der aktuellen Spielsitzungen angesammelt werden. <br><br>  Beginnen wir mit dem Import aller Bibliotheken, die wir zum Abrufen und Sammeln unserer Ergebnisse ben√∂tigen. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyplot <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> defaultdict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> functools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> partial %matplotlib inline plt.style.use(<span class="hljs-string"><span class="hljs-string">'ggplot'</span></span>)</code> </pre> <br>  Als n√§chstes initialisieren wir unser <i>Fitnessstudio</i> und definieren eine Richtlinie, die die Aktionen unseres Agenten koordiniert.  Tats√§chlich nehmen wir die Karte solange weiter, bis der Betrag auf der Hand 19 oder mehr erreicht hat. Danach h√∂ren wir auf. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Observation here encompassess all data about state that we need, as well as reactions to it env = gym.make('Blackjack-v0') #Define a policy where we hit until we reach 19. # actions here are 0-stand, 1-hit def sample_policy(observation): score, dealer_score, usable_ace = observation return 0 if score &gt;= 19 else 1</span></span></code> </pre> <br>  Definieren wir eine Methode zum Generieren von Passdaten mithilfe unserer Richtlinie.  Wir speichern Informationen √ºber den Status, die ergriffenen Ma√ünahmen und die Verg√ºtung f√ºr die Ma√ünahme. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate_episode</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># we initialize the list for storing states, actions, and rewards states, actions, rewards = [], [], [] # Initialize the gym environment observation = env.reset() while True: # append the states to the states list states.append(observation) # now, we select an action using our sample_policy function and append the action to actions list action = sample_policy(observation) actions.append(action) # We perform the action in the environment according to our sample_policy, move to the next state observation, reward, done, info = env.step(action) rewards.append(reward) # Break if the state is a terminal state (ie done) if done: break return states, actions, rewards</span></span></code> </pre> <br>  Definieren wir abschlie√üend den ersten Besuch der Monte-Carlo-Vorhersagefunktion.  Zun√§chst initialisieren wir ein leeres W√∂rterbuch, um die aktuellen Statuswerte und ein W√∂rterbuch zu speichern, in dem die Anzahl der Datens√§tze f√ºr jeden Status in verschiedenen Durchl√§ufen gespeichert wird. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">first_visit_mc_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env, n_episodes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, we initialize the empty value table as a dictionary for storing the values of each state value_table = defaultdict(float) N = defaultdict(int)</span></span></code> </pre> <br>  F√ºr jeden Durchgang rufen wir unsere <i>generate_episode-</i> Methode auf, um Informationen zu den Statuswerten und den Belohnungen abzurufen, die nach dem Status eingehen.  Wir initialisieren auch die Variable, um unsere inkrementellen Ergebnisse zu speichern.  Dann erhalten wir die Belohnung und den aktuellen Statuswert f√ºr jeden w√§hrend des Passes besuchten Status und erh√∂hen unsere variablen Ertr√§ge um den Wert der Belohnung pro Schritt. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_episodes): <span class="hljs-comment"><span class="hljs-comment"># Next, we generate the epsiode and store the states and rewards states, _, rewards = generate_episode(policy, env) returns = 0 # Then for each step, we store the rewards to a variable R and states to S, and we calculate for t in range(len(states) ‚Äî 1, -1, -1): R = rewards[t] S = states[t] returns += R # Now to perform first visit MC, we check if the episode is visited for the first time, if yes, #This is the standard Monte Carlo Incremental equation. # NewEstimate = OldEstimate+StepSize(Target-OldEstimate) if S not in states[:t]: N[S] += 1 value_table[S] += (returns ‚Äî value_table[S]) / N[S] return value_table</span></span></code> </pre> <br>  Ich m√∂chte Sie daran erinnern, dass wir seit dem ersten Besuch von Monte Carlo einen Staat in einem Durchgang besuchen.  Aus diesem Grund pr√ºfen wir das Statusw√∂rterbuch unter bestimmten Bedingungen, um festzustellen, ob der Status besucht wurde.  Wenn diese Bedingung erf√ºllt ist, k√∂nnen wir den neuen Wert mithilfe der zuvor definierten Prozedur zum Aktualisieren der Statuswerte mithilfe der Monte-Carlo-Methode berechnen und die Anzahl der Beobachtungen f√ºr diesen Status um 1 erh√∂hen. Anschlie√üend wiederholen wir den Vorgang f√ºr den n√§chsten Durchgang, um schlie√ülich den Durchschnittswert des Ergebnisses zu erhalten . <br><br>  Lassen Sie uns das, was wir haben, durchgehen und die Ergebnisse betrachten! <br><br><pre> <code class="python hljs">value = first_visit_mc_prediction(sample_policy, env, n_episodes=<span class="hljs-number"><span class="hljs-number">500000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): print(value.popitem())</code> </pre> <br><img src="https://habrastorage.org/webt/xc/ow/al/xcowalpr-34tgxtf0gq8zlfllgg.png"><br>  <i>Abschluss einer Stichprobe, die die Zustandswerte verschiedener Kombinationen der H√§nde beim Blackjack zeigt.</i> <br><br>  Wir k√∂nnen weiterhin Monte-Carlo-Beobachtungen f√ºr 5000 P√§sse durchf√ºhren und eine Verteilung der Statuswerte erstellen, die die Werte einer beliebigen Kombination in den H√§nden des Spielers und des Dealers beschreibt. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_blackjack</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(V, ax1, ax2)</span></span></span><span class="hljs-function">:</span></span> player_sum = np.arange(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">21</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) dealer_show = np.arange(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) usable_ace = np.array([<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>]) state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, player <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(player_sum): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j, dealer <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(dealer_show): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k, ace <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(usable_ace): state_values[i, j, k] = V[player, dealer, ace] X, Y = np.meshgrid(player_sum, dealer_show) ax1.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>]) ax2.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ax1, ax2: ax.set_zlim(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) ax.set_ylabel(<span class="hljs-string"><span class="hljs-string">'player sum'</span></span>) ax.set_xlabel(<span class="hljs-string"><span class="hljs-string">'dealer sum'</span></span>) ax.set_zlabel(<span class="hljs-string"><span class="hljs-string">'state-value'</span></span>) fig, axes = pyplot.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>),subplot_kw={<span class="hljs-string"><span class="hljs-string">'projection'</span></span>: <span class="hljs-string"><span class="hljs-string">'3d'</span></span>}) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/o usable ace'</span></span>) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/ usable ace'</span></span>) plot_blackjack(value, axes[<span class="hljs-number"><span class="hljs-number">0</span></span>], axes[<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/db/7t/he/db7thebzqawmtv_jemoprsaeqvg.png"><br>  <i>Visualisierung der Zustandswerte verschiedener Kombinationen beim Blackjack.</i> <br><br>  Fassen wir also zusammen, was wir gelernt haben. <br><br><ul><li>  Auf Stichproben basierende Lernmethoden erm√∂glichen es uns, Zustands- und Aktionszustandswerte ohne √úbergangsdynamik einfach durch Stichproben zu bewerten. </li><li>  Monte-Carlo-Ans√§tze basieren auf einer zuf√§lligen Auswahl des Modells, der Beobachtung der vom Modell zur√ºckgegebenen Belohnungen und der Erfassung von Informationen w√§hrend des normalen Betriebs, um den Durchschnittswert seiner Zust√§nde zu bestimmen. </li><li>  Mit Monte-Carlo-Methoden ist eine verallgemeinerte Iterationsrichtlinie m√∂glich. </li><li>  Der Wert aller m√∂glichen Kombinationen in den H√§nden des Spielers und des Dealers beim Blackjack kann unter Verwendung mehrerer Monte-Carlo-Simulationen gesch√§tzt werden, was den Weg f√ºr optimierte Strategien ebnet. </li></ul><br>  Damit ist die Einf√ºhrung in die Monte-Carlo-Methode abgeschlossen.  In unserem n√§chsten Artikel werden wir uns mit Lehrmethoden der Form Temporal Difference Learning befassen. <br><br><h3>  Quellen: </h3><br>  Sutton et.  al, Verst√§rkungslernen <br>  White et.  al, Fundamentals of Reinforcement Learning, Universit√§t Alberta <br>  Silva et.  al, Reinforcement Learning, UCL <br>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Platt et.</a>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Al, Northeaster University</a> <br><br>  Das ist alles.  Wir sehen uns auf dem <a href="https://otus.pw/Zkti/">Platz</a> ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de477042/">https://habr.com/ru/post/de477042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de477022/index.html">Wie k√∂nnen wir Ihnen helfen? Wie k√∂nnen Sie uns helfen?</a></li>
<li><a href="../de477026/index.html">Siebter j√§hrlicher JetBrains Hackathon</a></li>
<li><a href="../de477032/index.html">Von der Blockchain zur DAG: Zwischenh√§ndler loswerden</a></li>
<li><a href="../de477038/index.html">Die beste Programmiersprache f√ºr Anf√§nger</a></li>
<li><a href="../de477040/index.html">Gartner Chart 2019: Worum geht es bei all diesen Schlagworten?</a></li>
<li><a href="../de477044/index.html">Automatisierung von End-2-End-Tests eines integrierten Informationssystems. Teil 2. Technische</a></li>
<li><a href="../de477046/index.html">.Net Meetup bei der Raiffeisenbank 28/11 + Broadcast</a></li>
<li><a href="../de477048/index.html">Warum hat ein Unternehmen mit einem Kapital von 55 Milliarden US-Dollar daran gedacht, die B√∂rse zu verlassen?</a></li>
<li><a href="../de477050/index.html">Black Friday 2019 f√ºr Video√ºberwachung und Wolken.</a></li>
<li><a href="../de477052/index.html">Reactor, WebFlux, Kotlin Coroutines oder Asynchrony anhand eines einfachen Beispiels</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>