<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌹 😿 🐶 Monte Carlo Blackjack Strategieoptimierung 😐 👴🏼 🤙🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Übersetzung des Artikels wurde speziell für Studierende des Kurses Maschinelles Lernen erstellt . 



 Verstärktes Training eroberte die Welt der ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Monte Carlo Blackjack Strategieoptimierung</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477042/">  <i>Die Übersetzung des Artikels wurde speziell für Studierende des Kurses <a href="https://otus.pw/Zkti/">Maschinelles Lernen erstellt</a> .</i> <br><hr><br><img src="https://habrastorage.org/webt/dj/mh/7h/djmh7hubq1mecnsb-gilezo8qyi.png"><br><br>  Verstärktes Training eroberte die Welt der künstlichen Intelligenz.  Ausgehend von AlphaGo und <a href="https://www.space.com/alphastar-artificial-intelligence-starcraft-2-grandmaster.html">AlphaStar</a> werden immer mehr Aktivitäten, die zuvor vom Menschen dominiert wurden, jetzt von KI-Agenten auf der Grundlage von Verstärkungstraining erobert.  Kurz gesagt, diese Erfolge hängen von der Optimierung der Aktionen des Agenten in einer bestimmten Umgebung ab, um eine maximale Belohnung zu erzielen.  In den letzten Artikeln von <a href="https://medium.com/gradientcrescent">GradientCrescent haben</a> wir uns mit verschiedenen grundlegenden Aspekten des verstärkten Lernens befasst, von den Grundlagen der <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-the-k-bandit-problem-illustrat-940eea430296">Banditensysteme</a> und <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">politischen</a> Ansätzen zur Optimierung des <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">belohnungsbasierten</a> Verhaltens in <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82">Markov-Umgebungen</a> .  All diese Ansätze erforderten eine vollständige Kenntnis unserer Umwelt.  <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310">Die dynamische Programmierung</a> setzt beispielsweise eine vollständige Wahrscheinlichkeitsverteilung aller möglichen Zustandsübergänge voraus.  In der Realität stellen wir jedoch fest, dass die meisten Systeme nicht vollständig interpretiert werden können und dass Wahrscheinlichkeitsverteilungen aufgrund von Komplexität, inhärenter Unsicherheit oder Einschränkungen der Rechenkapazitäten nicht explizit erhalten werden können.  Betrachten Sie als Analogie die Aufgabe des Meteorologen - die Anzahl der Faktoren, die bei der Wettervorhersage eine Rolle spielen, kann so groß sein, dass es unmöglich ist, die Wahrscheinlichkeit genau zu berechnen. <a name="habracut"></a><br><br>  In solchen Fällen sind Lehrmethoden wie Monte Carlo die Lösung.  Der Begriff Monte Carlo wird allgemein verwendet, um einen beliebigen Ansatz zur Schätzung von Zufallsstichproben zu beschreiben.  Mit anderen Worten, wir sagen kein Wissen über unsere Umwelt voraus, sondern lernen aus Erfahrungen, indem wir beispielhafte Folgen von Zuständen, Handlungen und Belohnungen durchlaufen, die durch die Interaktion mit der Umwelt entstehen.  Diese Methoden arbeiten, indem sie die vom Modell während des normalen Betriebs zurückgegebenen Belohnungen direkt beobachten, um den Durchschnittswert seiner Bedingungen zu beurteilen.  Interessanterweise können wir auch ohne Kenntnis der Dynamik der Umgebung (die als Wahrscheinlichkeitsverteilung von Zustandsübergängen betrachtet werden sollte) immer noch ein optimales Verhalten erzielen, um die Belohnungen zu maximieren. <br><br>  Betrachten Sie als Beispiel das Ergebnis eines Würfels mit 12 Würfeln.  Wenn wir diese Würfe als einen einzelnen Zustand betrachten, können wir diese Ergebnisse mitteln, um näher an das tatsächlich vorhergesagte Ergebnis heranzukommen.  Je größer die Stichprobe, desto genauer nähern wir uns dem tatsächlich erwarteten Ergebnis. <br><br><img src="https://habrastorage.org/webt/xr/85/d6/xr85d6ugn6dbszaqh5b-nbq-usq.png"><br>  <i>Die durchschnittlich erwartete Menge von 12 Würfeln für 60 Schüsse beträgt 41,57</i> <br><br>  Diese Art der stichprobenbasierten Bewertung mag dem Leser vertraut erscheinen, da eine solche Stichprobe auch für k-Banditensysteme durchgeführt wird.  Anstatt verschiedene Banditen zu vergleichen, werden Monte-Carlo-Methoden verwendet, um verschiedene Richtlinien in Markov-Umgebungen zu vergleichen und den Wert des Staates zu bestimmen, während eine bestimmte Richtlinie befolgt wird, bis die Arbeit abgeschlossen ist. <br><br><h3>  Monte-Carlo-Schätzung des Zustandswertes </h3><br>  Monte-Carlo-Methoden sind eine Möglichkeit, im Kontext des verstärkenden Lernens die Signifikanz des Zustands eines Modells durch Mittelung der Stichprobenergebnisse zu bewerten.  Aufgrund der Notwendigkeit eines Endzustands sind Monte-Carlo-Methoden von Natur aus auf episodische Umgebungen anwendbar.  Aufgrund dieser Einschränkung werden Monte-Carlo-Methoden normalerweise als "autonom" betrachtet, bei denen alle Aktualisierungen nach Erreichen des Terminalzustands durchgeführt werden.  Es kann eine einfache Analogie zum Herausfinden eines Weges aus einem Labyrinth gegeben werden - ein autonomer Ansatz würde den Agenten dazu zwingen, das Ende zu erreichen, bevor er die gesammelten Zwischenerfahrungen nutzt, um die Zeit zu verkürzen, die er benötigt, um durch das Labyrinth zu gehen.  Andererseits wird der Agent bei der Online-Annäherung sein Verhalten bereits während des Durchgangs des Labyrinths ständig ändern. Vielleicht bemerkt er, dass die grünen Korridore zu Sackgassen führen und entscheidet, sie beispielsweise zu meiden.  In einem der folgenden Artikel werden wir Online-Ansätze diskutieren. <br><br>  Die Monte-Carlo-Methode kann wie folgt formuliert werden: <br><br><img src="https://habrastorage.org/webt/u4/kd/o_/u4kdo_uc3dj64rihmdjhnyj1r7y.png"><br><br>  Um die Funktionsweise der Monte-Carlo-Methode besser zu verstehen, sehen Sie sich das folgende Zustandsübergangsdiagramm an.  Die Belohnung für jeden Zustandsübergang wird in schwarz angezeigt, ein Abzinsungsfaktor von 0,5 wird darauf angewendet.  Lassen Sie uns den tatsächlichen Wert des Zustands beiseite legen und uns auf die Berechnung der Ergebnisse eines Wurfs konzentrieren. <br><br><img src="https://habrastorage.org/webt/oc/he/n9/ochen91kvgbix5pgltzijpbozgg.png"><br>  <i>Zustandsübergangsdiagramm.</i>  <i>Die Statusnummer wird rot angezeigt, das Ergebnis ist schwarz.</i> <br>  Da der Endzustand ein Ergebnis gleich 0 zurückgibt, berechnen wir das Ergebnis jedes Zustands, beginnend mit dem Endzustand (G5).  Bitte beachten Sie, dass wir den Abzinsungsfaktor auf 0,5 eingestellt haben, was zu einer Gewichtung gegenüber den späteren Staaten führt. <br><br><img src="https://habrastorage.org/webt/uw/y6/qq/uwy6qq_amgfxpsyj2_3gpij498o.png"><br><br>  Oder allgemeiner: <br><br><img src="https://habrastorage.org/webt/k7/i4/ua/k7i4ua-g8vaph8w62-rfpc3r8sk.png"><br><br>  Um zu vermeiden, dass alle Ergebnisse in der Liste gespeichert werden, können Sie den Statuswert in der Monte-Carlo-Methode schrittweise aktualisieren, indem Sie eine Gleichung verwenden, die einige Ähnlichkeiten mit der herkömmlichen Gradientenabnahme aufweist: <br><br><img src="https://habrastorage.org/webt/lv/tm/nk/lvtmnki_ff4csblmytygnnxjn6u.png"><br>  <i>Inkrementelle Aktualisierung nach Monte Carlo.</i>  <i>S ist der Zustand, V ist sein Wert, G ist sein Ergebnis und A ist der Schrittwertparameter.</i> <br><br>  Im Rahmen des Verstärkungstrainings können Monte-Carlo-Methoden sogar als Erster Besuch oder Jeder Besuch klassifiziert werden.  Kurz gesagt, der Unterschied zwischen beiden besteht darin, wie oft ein Bundesstaat in einer Passage vor dem Monte-Carlo-Update besucht werden kann.  Die Monte-Carlo-Methode für den ersten Besuch schätzt den Wert aller Zustände als Durchschnittswert der Ergebnisse nach einzelnen Besuchen in jedem Zustand vor Abschluss, während die Monte-Carlo-Methode für jeden Besuch den Durchschnitt der Ergebnisse nach n Besuchen bis zum Abschluss ermittelt.  Wir werden den Monte-Carlo-Erstbesuch in diesem Artikel wegen seiner relativen Einfachheit verwenden. <br><br><h3>  Monte-Carlo-Richtlinienverwaltung </h3><br>  Wenn das Modell die Richtlinie nicht bereitstellen kann, kann Monte Carlo zum Bewerten von Zustandsaktionswerten verwendet werden.  Dies ist nützlicher als nur die Bedeutung der Zustände, da die Idee der Bedeutung jeder Aktion <i>(q)</i> in einem gegebenen Zustand es dem Agenten ermöglicht, automatisch eine Richtlinie aus Beobachtungen in einer unbekannten Umgebung zu formulieren. <br><br>  Genauer gesagt können wir Monte Carlo verwenden, um <i>q (s, a, pi)</i> , das erwartete Ergebnis, wenn wir von Zustand s ausgehen, Aktion a und die nachfolgende Richtlinie <i>Pi</i> zu schätzen.  Die Monte-Carlo-Methoden bleiben unverändert, mit der Ausnahme, dass für einen bestimmten Zustand eine zusätzliche Dimension von Maßnahmen ergriffen wird.  Es wird angenommen, dass ein Zustand-Aktion <i>(en, ein)</i> -Paar während der Passage besucht wird, wenn Zustand <i>s</i> jemals besucht wird und Aktion <i>a</i> darin ausgeführt wird.  Ebenso kann die Bewertung von Wertaktionen mit den Ansätzen „Erster Besuch“ und „Jeder Besuch“ durchgeführt werden. <br><br>  Wie bei der dynamischen Programmierung können wir eine generalisierte Iterationsrichtlinie (GPI) verwenden, um eine Richtlinie aus der Beobachtung von Zustandsaktionswerten zu bilden. <br><br><img src="https://habrastorage.org/webt/wi/pm/p5/wipmp58itbgocsyruulw0ccliay.png"><br><br>  Indem wir die Schritte der Politikevaluierung und -verbesserung abwechseln und Nachforschungen anstellen, um sicherzustellen, dass alle möglichen Maßnahmen ergriffen werden, können wir für jede Bedingung die optimale Politik erzielen.  Beim Monte-Carlo-GPI erfolgt diese Drehung normalerweise nach dem Ende jedes Durchgangs. <br><br><img src="https://habrastorage.org/webt/nf/b4/nw/nfb4nwijndtfex4ytwhdn2ujcyw.png"><br>  <i>Monte Carlo GPI</i> <br><br><h3>  Blackjack-Strategie </h3><br>  Um besser zu verstehen, wie die Monte-Carlo-Methode in der Praxis bei der Bewertung verschiedener Zustandswerte funktioniert, führen wir das Beispiel eines Blackjack-Spiels Schritt für Schritt vor.  Lassen Sie uns zunächst die Regeln und Bedingungen für unser Spiel festlegen: <br><br><ul><li>  Wir werden nur gegen den Dealer spielen, es wird keine anderen Spieler geben.  Dadurch können wir die Hände des Händlers als Teil der Umwelt betrachten. </li><li>  Der Wert von Karten mit Zahlen, die den Nennwerten entsprechen.  Der Wert der Bildkarten: Bube, König und Dame ist 10. Der Wert des Asses kann 1 oder 11 sein, abhängig von der Wahl des Spielers. </li><li>  Beide Seiten erhalten zwei Karten.  Zwei Spielerkarten liegen offen, eine der Karten des Dealers liegt ebenfalls offen. </li><li>  Das Ziel des Spiels ist, dass die Anzahl der Karten auf der Hand &lt;= 21 ist.  Ein Wert größer als 21 ist ein Fehlschlag. Wenn beide Seiten einen Wert von 21 haben, wird das Spiel unentschieden gespielt. </li><li>  Nachdem der Spieler seine Karten und die Karte des ersten Dealers gesehen hat, kann er wählen, ob er eine neue Karte ("noch") oder nicht ("genug") nehmen möchte, bis er mit der Summe der Kartenwerte auf seiner Hand zufrieden ist. </li><li>  Dann zeigt der Dealer seine zweite Karte - wenn der sich ergebende Betrag weniger als 17 beträgt, muss er Karten nehmen, bis er 17 Punkte erreicht hat, wonach er die Karte nicht mehr nimmt. </li></ul><br>  Mal sehen, wie die Monte-Carlo-Methode mit diesen Regeln funktioniert. <br><br><h4>  Runde 1. </h4><br>  Du gewinnst insgesamt 19. Aber du versuchst, das Glück am Schwanz zu fangen, eine Chance zu nutzen, 3 zu bekommen und pleite zu gehen.  Als Sie pleite gingen, hatte der Dealer nur eine offene Karte mit einer Summe von 10. Dies kann wie folgt dargestellt werden: <br><br><img src="https://habrastorage.org/webt/4o/di/sl/4odisljnoapgkqblzsixkr2a4_4.png"><br><br>  Wenn wir pleite gehen, ist unsere Belohnung für die Runde -1.  Legen wir diesen Wert als Ergebnis der Rückgabe des vorletzten Status im folgenden Format fest [Betrag des Agenten, Betrag des Händlers, Ass?]: <br><br><img src="https://habrastorage.org/webt/2q/er/g4/2qerg4mooln9blj9sfqo1ndzxbo.png"><br><br>  Nun, jetzt haben wir kein Glück mehr.  Gehen wir zu einer anderen Runde über. <br><br><h4>  Runde 2. </h4><br>  Sie geben insgesamt 19 ein. Diesmal entscheiden Sie sich, den Vorgang abzubrechen.  Der Dealer wählt 13, nimmt eine Karte und geht pleite.  Der vorletzte Zustand kann wie folgt beschrieben werden. <br><br><img src="https://habrastorage.org/webt/al/th/dv/althdvyqwkhwqd5cj2vvzcjbiam.png"><br><br>  Beschreiben wir die Bedingungen und Belohnungen, die wir in dieser Runde erhalten haben: <br><br><img src="https://habrastorage.org/webt/rm/mv/n8/rmmvn88mg7za9gyrtvrvohywmwc.png"><br><br>  Mit dem Ende der Passage können wir nun die Werte aller unserer Staaten in dieser Runde anhand der berechneten Ergebnisse aktualisieren.  Bei einem Abzinsungsfaktor von 1 verteilen wir einfach unsere neue Handbelohnung, wie dies bei den vorherigen Zustandsübergängen der Fall war.  Da der Zustand <i>V (19, 10, nein)</i> zuvor -1 zurückgegeben hat, berechnen wir den erwarteten Rückgabewert und weisen ihn unserem Zustand zu: <br><br><img src="https://habrastorage.org/webt/u0/um/q9/u0umq9dpg6qw420ecz44cqfp9mk.png"><br>  <i>Endzustandswerte zur Demonstration am Beispiel Blackjack</i> . <br><br><h3>  Implementierung </h3><br>  Lassen Sie uns ein Blackjack-Spiel mit der Monte-Carlo-Methode des ersten Besuchs schreiben, um alle möglichen Statuswerte (oder verschiedene verfügbare Kombinationen) im Spiel mit Python herauszufinden.  Unser Ansatz basiert auf dem <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">Ansatz von Sudharsan et.</a>  <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">al.</a>  .  Wie immer finden Sie den gesamten Code aus dem Artikel auf <a href="https://github.com/EXJUSTICE/GradientCrescent)">unserem GitHub</a> . <br><br>  Um die Implementierung zu vereinfachen, werden wir Gym von OpenAI verwenden.  Stellen Sie sich die Umgebung als Schnittstelle zum Starten von Blackjack mit einem Minimum an Code vor. Auf diese Weise können wir uns auf die Implementierung von verstärktem Lernen konzentrieren.  Praktischerweise werden alle gesammelten Informationen über die Zustände, Aktionen und Belohnungen in den <i>Beobachtungsvariablen</i> gespeichert, die während der aktuellen Spielsitzungen angesammelt werden. <br><br>  Beginnen wir mit dem Import aller Bibliotheken, die wir zum Abrufen und Sammeln unserer Ergebnisse benötigen. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyplot <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> defaultdict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> functools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> partial %matplotlib inline plt.style.use(<span class="hljs-string"><span class="hljs-string">'ggplot'</span></span>)</code> </pre> <br>  Als nächstes initialisieren wir unser <i>Fitnessstudio</i> und definieren eine Richtlinie, die die Aktionen unseres Agenten koordiniert.  Tatsächlich nehmen wir die Karte solange weiter, bis der Betrag auf der Hand 19 oder mehr erreicht hat. Danach hören wir auf. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Observation here encompassess all data about state that we need, as well as reactions to it env = gym.make('Blackjack-v0') #Define a policy where we hit until we reach 19. # actions here are 0-stand, 1-hit def sample_policy(observation): score, dealer_score, usable_ace = observation return 0 if score &gt;= 19 else 1</span></span></code> </pre> <br>  Definieren wir eine Methode zum Generieren von Passdaten mithilfe unserer Richtlinie.  Wir speichern Informationen über den Status, die ergriffenen Maßnahmen und die Vergütung für die Maßnahme. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate_episode</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># we initialize the list for storing states, actions, and rewards states, actions, rewards = [], [], [] # Initialize the gym environment observation = env.reset() while True: # append the states to the states list states.append(observation) # now, we select an action using our sample_policy function and append the action to actions list action = sample_policy(observation) actions.append(action) # We perform the action in the environment according to our sample_policy, move to the next state observation, reward, done, info = env.step(action) rewards.append(reward) # Break if the state is a terminal state (ie done) if done: break return states, actions, rewards</span></span></code> </pre> <br>  Definieren wir abschließend den ersten Besuch der Monte-Carlo-Vorhersagefunktion.  Zunächst initialisieren wir ein leeres Wörterbuch, um die aktuellen Statuswerte und ein Wörterbuch zu speichern, in dem die Anzahl der Datensätze für jeden Status in verschiedenen Durchläufen gespeichert wird. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">first_visit_mc_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env, n_episodes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, we initialize the empty value table as a dictionary for storing the values of each state value_table = defaultdict(float) N = defaultdict(int)</span></span></code> </pre> <br>  Für jeden Durchgang rufen wir unsere <i>generate_episode-</i> Methode auf, um Informationen zu den Statuswerten und den Belohnungen abzurufen, die nach dem Status eingehen.  Wir initialisieren auch die Variable, um unsere inkrementellen Ergebnisse zu speichern.  Dann erhalten wir die Belohnung und den aktuellen Statuswert für jeden während des Passes besuchten Status und erhöhen unsere variablen Erträge um den Wert der Belohnung pro Schritt. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_episodes): <span class="hljs-comment"><span class="hljs-comment"># Next, we generate the epsiode and store the states and rewards states, _, rewards = generate_episode(policy, env) returns = 0 # Then for each step, we store the rewards to a variable R and states to S, and we calculate for t in range(len(states) — 1, -1, -1): R = rewards[t] S = states[t] returns += R # Now to perform first visit MC, we check if the episode is visited for the first time, if yes, #This is the standard Monte Carlo Incremental equation. # NewEstimate = OldEstimate+StepSize(Target-OldEstimate) if S not in states[:t]: N[S] += 1 value_table[S] += (returns — value_table[S]) / N[S] return value_table</span></span></code> </pre> <br>  Ich möchte Sie daran erinnern, dass wir seit dem ersten Besuch von Monte Carlo einen Staat in einem Durchgang besuchen.  Aus diesem Grund prüfen wir das Statuswörterbuch unter bestimmten Bedingungen, um festzustellen, ob der Status besucht wurde.  Wenn diese Bedingung erfüllt ist, können wir den neuen Wert mithilfe der zuvor definierten Prozedur zum Aktualisieren der Statuswerte mithilfe der Monte-Carlo-Methode berechnen und die Anzahl der Beobachtungen für diesen Status um 1 erhöhen. Anschließend wiederholen wir den Vorgang für den nächsten Durchgang, um schließlich den Durchschnittswert des Ergebnisses zu erhalten . <br><br>  Lassen Sie uns das, was wir haben, durchgehen und die Ergebnisse betrachten! <br><br><pre> <code class="python hljs">value = first_visit_mc_prediction(sample_policy, env, n_episodes=<span class="hljs-number"><span class="hljs-number">500000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): print(value.popitem())</code> </pre> <br><img src="https://habrastorage.org/webt/xc/ow/al/xcowalpr-34tgxtf0gq8zlfllgg.png"><br>  <i>Abschluss einer Stichprobe, die die Zustandswerte verschiedener Kombinationen der Hände beim Blackjack zeigt.</i> <br><br>  Wir können weiterhin Monte-Carlo-Beobachtungen für 5000 Pässe durchführen und eine Verteilung der Statuswerte erstellen, die die Werte einer beliebigen Kombination in den Händen des Spielers und des Dealers beschreibt. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_blackjack</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(V, ax1, ax2)</span></span></span><span class="hljs-function">:</span></span> player_sum = np.arange(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">21</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) dealer_show = np.arange(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) usable_ace = np.array([<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>]) state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, player <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(player_sum): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j, dealer <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(dealer_show): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k, ace <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(usable_ace): state_values[i, j, k] = V[player, dealer, ace] X, Y = np.meshgrid(player_sum, dealer_show) ax1.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>]) ax2.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ax1, ax2: ax.set_zlim(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) ax.set_ylabel(<span class="hljs-string"><span class="hljs-string">'player sum'</span></span>) ax.set_xlabel(<span class="hljs-string"><span class="hljs-string">'dealer sum'</span></span>) ax.set_zlabel(<span class="hljs-string"><span class="hljs-string">'state-value'</span></span>) fig, axes = pyplot.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>),subplot_kw={<span class="hljs-string"><span class="hljs-string">'projection'</span></span>: <span class="hljs-string"><span class="hljs-string">'3d'</span></span>}) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/o usable ace'</span></span>) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/ usable ace'</span></span>) plot_blackjack(value, axes[<span class="hljs-number"><span class="hljs-number">0</span></span>], axes[<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/db/7t/he/db7thebzqawmtv_jemoprsaeqvg.png"><br>  <i>Visualisierung der Zustandswerte verschiedener Kombinationen beim Blackjack.</i> <br><br>  Fassen wir also zusammen, was wir gelernt haben. <br><br><ul><li>  Auf Stichproben basierende Lernmethoden ermöglichen es uns, Zustands- und Aktionszustandswerte ohne Übergangsdynamik einfach durch Stichproben zu bewerten. </li><li>  Monte-Carlo-Ansätze basieren auf einer zufälligen Auswahl des Modells, der Beobachtung der vom Modell zurückgegebenen Belohnungen und der Erfassung von Informationen während des normalen Betriebs, um den Durchschnittswert seiner Zustände zu bestimmen. </li><li>  Mit Monte-Carlo-Methoden ist eine verallgemeinerte Iterationsrichtlinie möglich. </li><li>  Der Wert aller möglichen Kombinationen in den Händen des Spielers und des Dealers beim Blackjack kann unter Verwendung mehrerer Monte-Carlo-Simulationen geschätzt werden, was den Weg für optimierte Strategien ebnet. </li></ul><br>  Damit ist die Einführung in die Monte-Carlo-Methode abgeschlossen.  In unserem nächsten Artikel werden wir uns mit Lehrmethoden der Form Temporal Difference Learning befassen. <br><br><h3>  Quellen: </h3><br>  Sutton et.  al, Verstärkungslernen <br>  White et.  al, Fundamentals of Reinforcement Learning, Universität Alberta <br>  Silva et.  al, Reinforcement Learning, UCL <br>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Platt et.</a>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Al, Northeaster University</a> <br><br>  Das ist alles.  Wir sehen uns auf dem <a href="https://otus.pw/Zkti/">Platz</a> ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de477042/">https://habr.com/ru/post/de477042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de477022/index.html">Wie können wir Ihnen helfen? Wie können Sie uns helfen?</a></li>
<li><a href="../de477026/index.html">Siebter jährlicher JetBrains Hackathon</a></li>
<li><a href="../de477032/index.html">Von der Blockchain zur DAG: Zwischenhändler loswerden</a></li>
<li><a href="../de477038/index.html">Die beste Programmiersprache für Anfänger</a></li>
<li><a href="../de477040/index.html">Gartner Chart 2019: Worum geht es bei all diesen Schlagworten?</a></li>
<li><a href="../de477044/index.html">Automatisierung von End-2-End-Tests eines integrierten Informationssystems. Teil 2. Technische</a></li>
<li><a href="../de477046/index.html">.Net Meetup bei der Raiffeisenbank 28/11 + Broadcast</a></li>
<li><a href="../de477048/index.html">Warum hat ein Unternehmen mit einem Kapital von 55 Milliarden US-Dollar daran gedacht, die Börse zu verlassen?</a></li>
<li><a href="../de477050/index.html">Black Friday 2019 für Videoüberwachung und Wolken.</a></li>
<li><a href="../de477052/index.html">Reactor, WebFlux, Kotlin Coroutines oder Asynchrony anhand eines einfachen Beispiels</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>