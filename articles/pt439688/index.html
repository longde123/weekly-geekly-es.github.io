<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ôãÔ∏è üï∫üèæ üëµüèΩ Vis√£o geral das solu√ß√µes de IA e ML em 2018 e previs√µes para 2019: Parte 1 - PNL, Vis√£o por Computador üë©üèΩ‚Äç‚úàÔ∏è üë®üèº‚Äç‚úàÔ∏è üçë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° pessoal! Apresento a voc√™ uma tradu√ß√£o de um artigo do Analytics Vidhya com uma vis√£o geral dos eventos de AI / ML nas tend√™ncias de 2018 e 2019. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Vis√£o geral das solu√ß√µes de IA e ML em 2018 e previs√µes para 2019: Parte 1 - PNL, Vis√£o por Computador</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439688/"><blockquote> Ol√° pessoal!  Apresento a voc√™ uma tradu√ß√£o de um artigo do <i>Analytics Vidhya</i> com uma vis√£o geral dos eventos de AI / ML nas tend√™ncias de 2018 e 2019.  O material √© bastante grande, por isso √© dividido em 2 partes.  Espero que o artigo interesse n√£o apenas especialistas especializados, mas tamb√©m os interessados ‚Äã‚Äãno t√≥pico da IA.  Boa leitura! <br><br><div class="spoiler">  <b class="spoiler_title">Artigo navega√ß√£o</b> <div class="spoiler_text">  <b>Parte 1</b> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Processamento de linguagem natural (PNL)</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tend√™ncias da PNL para 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Vis√£o computacional</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tend√™ncias em vis√£o de m√°quina para 2019</a> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ferramentas e bibliotecas</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tend√™ncias do AutoML para 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizado por Refor√ßo</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tend√™ncias de aprendizado de refor√ßo para 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">IA para bons meninos - movimento em dire√ß√£o √† IA ‚Äú√©tica‚Äù</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tend√™ncias √©ticas na IA para 2019</a> <br></div></div></blockquote><br><h2>  1. Introdu√ß√£o </h2><br>  Nos √∫ltimos anos, os entusiastas da IA ‚Äã‚Äãe os profissionais de aprendizado de m√°quina passaram em busca de um sonho.  Essas tecnologias deixaram de ser nicho, tornaram-se populares e j√° est√£o afetando a vida de milh√µes de pessoas no momento.  Os minist√©rios da IA ‚Äã‚Äãforam criados em diferentes pa√≠ses [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mais detalhes aqui</a> - aprox.  por.] e or√ßamentos s√£o alocados para acompanhar esta corrida. <br><br>  O mesmo vale para profissionais de ci√™ncia de dados.  H√° alguns anos, voc√™ pode se sentir √† vontade conhecendo algumas ferramentas e truques, mas esse tempo j√° passou.  O n√∫mero de eventos recentes em ci√™ncia de dados e a quantidade de conhecimento necess√°ria para acompanhar os tempos nessa √°rea s√£o surpreendentes. <br><br>  Decidi dar um passo atr√°s e analisar os desenvolvimentos em algumas √°reas-chave no campo da intelig√™ncia artificial do ponto de vista dos especialistas em ci√™ncia de dados.  Quais fugas ocorreram?  O que aconteceu em 2018 e o que esperar em 2019?  Leia este artigo para obter respostas! <a name="habracut"></a><br><br>  PS Como em qualquer previs√£o, abaixo est√£o minhas conclus√µes pessoais baseadas em tentativas de combinar fragmentos individuais em todo o cen√°rio.  Se o seu ponto de vista for diferente do meu, ficarei feliz em saber sua opini√£o sobre o que mais pode mudar na ci√™ncia de dados em 2019. <br><br>  As √°reas que abordaremos neste artigo s√£o: <br><br>  - Processo de linguagem natural (PNL) <br>  - Vis√£o computacional <br>  - Ferramentas e bibliotecas <br>  - Aprendizado por Refor√ßo <br>  - Quest√µes √©ticas em IA <br><br><a name="NLP"></a><h2>  Processamento de linguagem natural (PNL) </h2><br>  For√ßar m√°quinas a analisar palavras e frases sempre parecia um sonho.  Existem muitas nuances e recursos em idiomas que √†s vezes s√£o dif√≠ceis de entender, mesmo para as pessoas, mas 2018 foi um verdadeiro ponto de virada para a PNL. <br><br>  Assistimos a um grande avan√ßo ap√≥s o outro: ULMFiT, ELMO, OpenAl Transformer, Google BERT, e esta n√£o √© uma lista completa.  A aplica√ß√£o bem-sucedida do aprendizado de transfer√™ncia (a arte de aplicar modelos pr√©-treinados aos dados) abriu as portas para a PNL em uma variedade de tarefas. <br><blockquote>  Transferir aprendizado - permite adaptar um modelo / sistema pr√©-treinado √† sua tarefa espec√≠fica usando uma quantidade relativamente pequena de dados. </blockquote>  Vejamos alguns desses principais desenvolvimentos com mais detalhes. <br><br><h3>  ULMFiT </h3><br>  Desenvolvido por Sebastian Ruder e Jeremy Howard (fast.ai), o ULMFiT foi a primeira estrutura a receber transfer√™ncia de aprendizado este ano.  Para os n√£o iniciados, o acr√¥nimo ULMFiT significa "Universal Language Model Fine-Tuning".  Jeremy e Sebastian adicionaram, com raz√£o, a palavra "universal" ao ULMFiT - essa estrutura pode ser aplicada a quase todas as tarefas da PNL! <br><br>  O melhor de ULMFiT √© que voc√™ n√£o precisa treinar modelos do zero!  Os pesquisadores j√° fizeram o mais dif√≠cil para voc√™ - participe e inscreva-se em seus projetos.  O ULMFiT superou outros m√©todos em seis tarefas de classifica√ß√£o de texto. <br><br>  Voc√™ pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ler o</a> tutorial de Pratek Joshi [Pateek Joshi - aprox.  trad.] sobre como come√ßar a usar o ULMFiT para qualquer tarefa de classifica√ß√£o de texto. <br><br><h3>  ELMo </h3><br>  Adivinha o que significa a abrevia√ß√£o ELMo?  Acr√¥nimo para Casamentos de modelos de idiomas [anexos de modelos de idiomas - aprox.  trans.].  E o ELMo chamou a aten√ß√£o da comunidade de ML logo ap√≥s o lan√ßamento. <br><br>  O ELMo usa modelos de linguagem para receber anexos para cada palavra e tamb√©m leva em considera√ß√£o o contexto em que a palavra se encaixa em uma frase ou par√°grafo.  O contexto √© um aspecto cr√≠tico da PNL, no qual a maioria dos desenvolvedores falhou anteriormente.  O ELMo usa LSTMs bidirecionais para criar anexos. <br><blockquote>  A mem√≥ria de longo prazo (LSTM) √© um tipo de arquitetura de redes neurais recorrentes proposta em 1997 por Sepp Hochreiter e J√ºrgen Schmidhuber.  Como a maioria das redes neurais recorrentes, uma rede LSTM √© universal no sentido de que, com um n√∫mero suficiente de elementos de rede, pode realizar qualquer c√°lculo de que um computador comum √© capaz, o que requer uma matriz de peso apropriada que possa ser considerada como um programa.  Diferentemente das redes neurais recorrentes tradicionais, a rede LSTM √© bem adaptada para o treinamento nos problemas de classifica√ß√£o, processamento e previs√£o de s√©ries temporais nos casos em que eventos importantes s√£o separados por intervalos de tempo com dura√ß√£o e limites indefinidos. <br><br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">fonte.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Wikipedia</a> </blockquote>  Como o ULMFiT, o ELMo melhora significativamente a produtividade na solu√ß√£o de um grande n√∫mero de tarefas de PNL, como analisar o humor do texto ou responder a perguntas. <br><br><h3>  BERT do Google </h3><br>  Muitos especialistas observam que o lan√ßamento do BERT marcou o in√≠cio de uma nova era na PNL.  Ap√≥s o ULMFiT e o ELMo, o BERT assumiu a lideran√ßa, demonstrando alto desempenho.  Como o an√∫ncio original afirma: "O BERT √© conceitualmente simples e empiricamente poderoso". <br><br>  O BERT mostrou excelentes resultados em 11 tarefas de PNL!  Veja os resultados nos testes do SQuAD: <br><br><img src="https://habrastorage.org/webt/rf/6n/cz/rf6nczjjvbcz1cg4nxfeo-lm7ou.png"><br><br>  Quer experimentar?  Voc√™ pode usar a reimplementa√ß√£o no c√≥digo PyTorch ou TensorFlow do Google e tentar repetir o resultado em sua m√°quina. <br><br><h3>  Facebook PyText </h3><br>  Como o Facebook pode ficar longe desta corrida?  A empresa oferece sua pr√≥pria estrutura de NLP de c√≥digo aberto chamada PyText.  De acordo com um estudo publicado pelo Facebook, o PyText aumentou a precis√£o dos modelos de conversa√ß√£o em 10% e reduziu o tempo de treinamento. <br><br>  O PyText est√° na verdade por tr√°s de v√°rios produtos do Facebook, como o Messenger.  Portanto, trabalhar com ele adicionar√° um bom ponto ao seu portf√≥lio e um conhecimento inestim√°vel que voc√™ certamente obter√°. <br><br>  Voc√™ pode tentar voc√™ mesmo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">baixar o c√≥digo do GitHub</a> . <br><br><h3>  Google duplex </h3><br>  √â dif√≠cil acreditar que voc√™ nunca ouviu falar do Google Duplex.  Aqui est√° uma demonstra√ß√£o que por muito tempo brilhou nas manchetes: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NO0-5MuJvew" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Como este √© um produto do Google, h√° poucas chances de que mais cedo ou mais tarde o c√≥digo seja publicado para todos.  Obviamente, essa demonstra√ß√£o levanta muitas quest√µes: de quest√µes √©ticas a quest√µes de privacidade, mas falaremos sobre isso mais tarde.  Por enquanto, apenas aproveite o qu√£o longe chegamos com a ML nos √∫ltimos anos. <br><br><a name="NLPtrends"></a><h2>  Tend√™ncias da PNL em 2019 </h2><br>  Quem melhor que o pr√≥prio Sebastian Ruder pode dar uma id√©ia de para onde a PNL est√° indo em 2019?  Aqui est√£o suas descobertas: <br><blockquote><ol><li>  O uso de modelos de investimento em idiomas pr√©-treinados ser√° generalizado;  modelos avan√ßados sem suporte ser√£o muito raros. </li><li>  Aparecer√£o visualiza√ß√µes pr√©-treinadas que podem codificar informa√ß√µes especializadas que complementam os anexos do modelo de idioma.  Poderemos agrupar diferentes tipos de apresenta√ß√µes pr√©-treinadas, dependendo dos requisitos da tarefa. </li><li>  Mais trabalhos aparecer√£o no campo de aplicativos multil√≠ngues e modelos multil√≠ngues.  Em particular, contando com a incorpora√ß√£o de palavras entre idiomas, veremos o surgimento de representa√ß√µes interl√≠nguas pr√©-treinadas e profundas. </li></ol></blockquote><a name="cv"></a><h2>  Vis√£o computacional </h2><br><img src="https://habrastorage.org/webt/pu/aj/_c/puaj_c89feaiultos4yynrcj7x4.jpeg"><br><br>  Hoje, a vis√£o computacional √© a √°rea mais popular no campo da aprendizagem profunda.  Parece que os primeiros frutos da tecnologia j√° foram obtidos e estamos no est√°gio de desenvolvimento ativo.  Independentemente da imagem ou do v√≠deo, vemos o surgimento de muitas estruturas e bibliotecas que resolvem facilmente os problemas da vis√£o computacional. <br><br>  Aqui est√° minha lista das melhores solu√ß√µes que podem ser vistas este ano. <br><br><h3>  BigGANs Out </h3><br>  Ian Goodfellow projetou as GANs em 2014, e o conceito gerou uma ampla variedade de aplica√ß√µes.  Ano ap√≥s ano, observamos como o conceito original foi finalizado para uso em casos reais.  Mas uma coisa permaneceu inalterada at√© este ano - as imagens geradas por computador eram facilmente distingu√≠veis.  Uma certa inconsist√™ncia sempre aparecia no quadro, o que tornava a diferen√ßa muito √≥bvia. <br><br>  Nos √∫ltimos meses, surgiram mudan√ßas nessa dire√ß√£o e, com a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cria√ß√£o do BigGAN</a> , esses problemas podem ser resolvidos de uma vez por todas.  Veja as imagens geradas por este m√©todo: <br><br><img src="https://habrastorage.org/webt/mo/w7/ow/mow7owldedw4r1jtwex6wbfwwje.png"><br><br>  Sem um microsc√≥pio, √© dif√≠cil dizer o que h√° de errado com essas imagens.  √â claro que todos decidir√£o por si mesmos, mas n√£o h√° d√∫vida de que o GAN muda a maneira como percebemos as imagens digitais (e o v√≠deo). <br><br>  Para refer√™ncia: esses modelos foram treinados primeiro no conjunto de dados ImageNet e depois no JFT-300M para demonstrar que esses modelos s√£o bem transferidos de um conjunto de dados para outro.  Aqui est√° um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para uma p√°gina</a> da lista de correspond√™ncia da GAN explicando como visualizar e entender a GAN. <br><br><h3>  Model Fast.ai treinado no ImageNet em 18 minutos </h3><br>  Esta √© uma implementa√ß√£o muito legal.  Existe uma cren√ßa generalizada de que, para executar tarefas de aprendizado profundo, voc√™ precisar√° de terabytes de dados e grandes recursos de computa√ß√£o.  O mesmo vale para o treinamento do modelo a partir do zero nos dados do ImageNet.  Muitos de n√≥s pensamos da mesma maneira antes de algumas pessoas no fast.ai n√£o conseguirem provar o contr√°rio de todos. <br><br>  O modelo deles deu 93% de precis√£o com impressionantes 18 minutos.  O hardware usado, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descrito</a> em detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">em seu blog</a> , consistia em 16 inst√¢ncias p√∫blicas da nuvem da AWS, cada uma com 8 GPUs NVIDIA V100.  Eles criaram um algoritmo usando as bibliotecas fast.ai e PyTorch. <br><br>  O custo total da montagem foi de apenas US $ 40!  Jeremy descreveu suas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">abordagens e m√©todos</a> em mais detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> .  Esta √© uma vit√≥ria comum! <br><br><h3>  vid2vid da NVIDIA </h3><br>  Nos √∫ltimos 5 anos, o processamento de imagens fez grandes progressos, mas e o v√≠deo?  Os m√©todos de convers√£o de um quadro est√°tico para um din√¢mico se mostraram um pouco mais complicados do que o esperado.  Voc√™ pode tirar uma sequ√™ncia de quadros de um v√≠deo e prever o que acontecer√° no pr√≥ximo quadro?  Tais estudos foram feitos antes, mas as publica√ß√µes eram vagas, na melhor das hip√≥teses. <br><br><img src="https://habrastorage.org/webt/hz/ox/hj/hzoxhjbehlnlzl8ivc-bgiz0vh0.png"><br><br>  A NVIDIA decidiu tornar sua decis√£o publicamente dispon√≠vel no in√≠cio deste ano [2018 - aprox.  por.], que foi avaliada positivamente pela sociedade.  O objetivo do vid2vid √© derivar uma fun√ß√£o de exibi√ß√£o de um determinado v√≠deo de entrada para criar um v√≠deo de sa√≠da que transmita o conte√∫do do v√≠deo de entrada com uma precis√£o incr√≠vel. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/S1OwOd-war8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Voc√™ pode tentar a implementa√ß√£o no PyTorch, leve-o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ao GitHub aqui</a> . <br><br><a name="cvtrends"></a><h2>  Tend√™ncias de vis√£o de m√°quina para 2019 </h2><br>  Como mencionei anteriormente, em 2019 √© mais prov√°vel que vejamos o desenvolvimento das tend√™ncias de 2018, em vez de novos avan√ßos: carros aut√¥nomos, algoritmos de reconhecimento de rosto, realidade virtual e muito mais.  Voc√™ pode discordar de mim se tiver um ponto de vista ou acr√©scimo diferente, compartilh√°-lo conosco, o que mais podemos esperar em 2019? <br><br>  A quest√£o dos drones, na pend√™ncia da aprova√ß√£o de pol√≠ticos e do governo, pode finalmente ter um sinal verde nos Estados Unidos (a √çndia est√° muito atrasada nesse assunto).  Pessoalmente, eu gostaria que mais pesquisas fossem feitas em cen√°rios do mundo real.  Confer√™ncias como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CVPR</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ICML fornecem uma</a> boa cobertura dos √∫ltimos desenvolvimentos nesta √°rea, mas a proximidade dos projetos da realidade n√£o √© muito clara. <br><br>  ‚ÄúResposta visual √†s perguntas‚Äù e ‚Äúsistemas de di√°logo visual‚Äù podem finalmente sair com uma estreia muito esperada.  Esses sistemas n√£o t√™m capacidade de generaliza√ß√£o, mas espera-se que em breve veremos uma abordagem multimodal integrada. <br><br><img src="https://habrastorage.org/webt/s5/bn/uy/s5bnuydmsc8hf37vm26icbmwrgc.jpeg"><br><br>  O autotreinamento veio √† tona este ano.  Aposto que no pr√≥ximo ano ele ser√° aplicado em um n√∫mero muito maior de estudos.  Essa √© uma dire√ß√£o muito legal: os sinais s√£o determinados diretamente a partir dos dados de entrada, em vez de perder tempo marcando manualmente as imagens.  Vamos manter os dedos cruzados! <br><br><h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Leia mais: Parte 2 - Ferramentas e bibliotecas, AutoML, Aprendizado por refor√ßo, √âtica na IA</a> </h4></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt439688/">https://habr.com/ru/post/pt439688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt439676/index.html">Reagir a integra√ß√£o nativa e C ++ para iOS e Android</a></li>
<li><a href="../pt439678/index.html">Enviar para o desafio F # aplicado</a></li>
<li><a href="../pt439680/index.html">Cerca de 50% dos russos est√£o dispostos a vender seus dados pessoais</a></li>
<li><a href="../pt439682/index.html">Treinamento Cisco 200-125 CCNA v3.0. Cisco Certified Network Specialist (CCNA). Dia 4. Dispositivos de gateway</a></li>
<li><a href="../pt439684/index.html">Inscreva-se no Desafio F # aplicado</a></li>
<li><a href="../pt439690/index.html">Compara√ß√£o do desempenho da m√°quina virtual de 6 plataformas em nuvem: Selectel, MCS, I. Nuvem, Google Cloud, AWS e Azure</a></li>
<li><a href="../pt439692/index.html">AT&T processada por mudar o √≠cone da rede de 4G para 5G E</a></li>
<li><a href="../pt439694/index.html">Tecido inteligente que responde a mudan√ßas de temperatura corporal</a></li>
<li><a href="../pt439696/index.html">Na crista de uma onda, ou "eu quero mainstream" - mas vale a pena?</a></li>
<li><a href="../pt439698/index.html">Introdu√ß√£o √† programa√ß√£o: um simples jogo de tiro em 3D do zero no fim de semana, parte 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>