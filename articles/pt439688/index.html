<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>♋️ 🕺🏾 👵🏽 Visão geral das soluções de IA e ML em 2018 e previsões para 2019: Parte 1 - PNL, Visão por Computador 👩🏽‍✈️ 👨🏼‍✈️ 🍑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Olá pessoal! Apresento a você uma tradução de um artigo do Analytics Vidhya com uma visão geral dos eventos de AI / ML nas tendências de 2018 e 2019. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Visão geral das soluções de IA e ML em 2018 e previsões para 2019: Parte 1 - PNL, Visão por Computador</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439688/"><blockquote> Olá pessoal!  Apresento a você uma tradução de um artigo do <i>Analytics Vidhya</i> com uma visão geral dos eventos de AI / ML nas tendências de 2018 e 2019.  O material é bastante grande, por isso é dividido em 2 partes.  Espero que o artigo interesse não apenas especialistas especializados, mas também os interessados ​​no tópico da IA.  Boa leitura! <br><br><div class="spoiler">  <b class="spoiler_title">Artigo navegação</b> <div class="spoiler_text">  <b>Parte 1</b> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Processamento de linguagem natural (PNL)</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tendências da PNL para 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Visão computacional</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tendências em visão de máquina para 2019</a> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ferramentas e bibliotecas</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tendências do AutoML para 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizado por Reforço</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tendências de aprendizado de reforço para 2019</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">IA para bons meninos - movimento em direção à IA “ética”</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tendências éticas na IA para 2019</a> <br></div></div></blockquote><br><h2>  1. Introdução </h2><br>  Nos últimos anos, os entusiastas da IA ​​e os profissionais de aprendizado de máquina passaram em busca de um sonho.  Essas tecnologias deixaram de ser nicho, tornaram-se populares e já estão afetando a vida de milhões de pessoas no momento.  Os ministérios da IA ​​foram criados em diferentes países [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mais detalhes aqui</a> - aprox.  por.] e orçamentos são alocados para acompanhar esta corrida. <br><br>  O mesmo vale para profissionais de ciência de dados.  Há alguns anos, você pode se sentir à vontade conhecendo algumas ferramentas e truques, mas esse tempo já passou.  O número de eventos recentes em ciência de dados e a quantidade de conhecimento necessária para acompanhar os tempos nessa área são surpreendentes. <br><br>  Decidi dar um passo atrás e analisar os desenvolvimentos em algumas áreas-chave no campo da inteligência artificial do ponto de vista dos especialistas em ciência de dados.  Quais fugas ocorreram?  O que aconteceu em 2018 e o que esperar em 2019?  Leia este artigo para obter respostas! <a name="habracut"></a><br><br>  PS Como em qualquer previsão, abaixo estão minhas conclusões pessoais baseadas em tentativas de combinar fragmentos individuais em todo o cenário.  Se o seu ponto de vista for diferente do meu, ficarei feliz em saber sua opinião sobre o que mais pode mudar na ciência de dados em 2019. <br><br>  As áreas que abordaremos neste artigo são: <br><br>  - Processo de linguagem natural (PNL) <br>  - Visão computacional <br>  - Ferramentas e bibliotecas <br>  - Aprendizado por Reforço <br>  - Questões éticas em IA <br><br><a name="NLP"></a><h2>  Processamento de linguagem natural (PNL) </h2><br>  Forçar máquinas a analisar palavras e frases sempre parecia um sonho.  Existem muitas nuances e recursos em idiomas que às vezes são difíceis de entender, mesmo para as pessoas, mas 2018 foi um verdadeiro ponto de virada para a PNL. <br><br>  Assistimos a um grande avanço após o outro: ULMFiT, ELMO, OpenAl Transformer, Google BERT, e esta não é uma lista completa.  A aplicação bem-sucedida do aprendizado de transferência (a arte de aplicar modelos pré-treinados aos dados) abriu as portas para a PNL em uma variedade de tarefas. <br><blockquote>  Transferir aprendizado - permite adaptar um modelo / sistema pré-treinado à sua tarefa específica usando uma quantidade relativamente pequena de dados. </blockquote>  Vejamos alguns desses principais desenvolvimentos com mais detalhes. <br><br><h3>  ULMFiT </h3><br>  Desenvolvido por Sebastian Ruder e Jeremy Howard (fast.ai), o ULMFiT foi a primeira estrutura a receber transferência de aprendizado este ano.  Para os não iniciados, o acrônimo ULMFiT significa "Universal Language Model Fine-Tuning".  Jeremy e Sebastian adicionaram, com razão, a palavra "universal" ao ULMFiT - essa estrutura pode ser aplicada a quase todas as tarefas da PNL! <br><br>  O melhor de ULMFiT é que você não precisa treinar modelos do zero!  Os pesquisadores já fizeram o mais difícil para você - participe e inscreva-se em seus projetos.  O ULMFiT superou outros métodos em seis tarefas de classificação de texto. <br><br>  Você pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ler o</a> tutorial de Pratek Joshi [Pateek Joshi - aprox.  trad.] sobre como começar a usar o ULMFiT para qualquer tarefa de classificação de texto. <br><br><h3>  ELMo </h3><br>  Adivinha o que significa a abreviação ELMo?  Acrônimo para Casamentos de modelos de idiomas [anexos de modelos de idiomas - aprox.  trans.].  E o ELMo chamou a atenção da comunidade de ML logo após o lançamento. <br><br>  O ELMo usa modelos de linguagem para receber anexos para cada palavra e também leva em consideração o contexto em que a palavra se encaixa em uma frase ou parágrafo.  O contexto é um aspecto crítico da PNL, no qual a maioria dos desenvolvedores falhou anteriormente.  O ELMo usa LSTMs bidirecionais para criar anexos. <br><blockquote>  A memória de longo prazo (LSTM) é um tipo de arquitetura de redes neurais recorrentes proposta em 1997 por Sepp Hochreiter e Jürgen Schmidhuber.  Como a maioria das redes neurais recorrentes, uma rede LSTM é universal no sentido de que, com um número suficiente de elementos de rede, pode realizar qualquer cálculo de que um computador comum é capaz, o que requer uma matriz de peso apropriada que possa ser considerada como um programa.  Diferentemente das redes neurais recorrentes tradicionais, a rede LSTM é bem adaptada para o treinamento nos problemas de classificação, processamento e previsão de séries temporais nos casos em que eventos importantes são separados por intervalos de tempo com duração e limites indefinidos. <br><br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">fonte.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Wikipedia</a> </blockquote>  Como o ULMFiT, o ELMo melhora significativamente a produtividade na solução de um grande número de tarefas de PNL, como analisar o humor do texto ou responder a perguntas. <br><br><h3>  BERT do Google </h3><br>  Muitos especialistas observam que o lançamento do BERT marcou o início de uma nova era na PNL.  Após o ULMFiT e o ELMo, o BERT assumiu a liderança, demonstrando alto desempenho.  Como o anúncio original afirma: "O BERT é conceitualmente simples e empiricamente poderoso". <br><br>  O BERT mostrou excelentes resultados em 11 tarefas de PNL!  Veja os resultados nos testes do SQuAD: <br><br><img src="https://habrastorage.org/webt/rf/6n/cz/rf6nczjjvbcz1cg4nxfeo-lm7ou.png"><br><br>  Quer experimentar?  Você pode usar a reimplementação no código PyTorch ou TensorFlow do Google e tentar repetir o resultado em sua máquina. <br><br><h3>  Facebook PyText </h3><br>  Como o Facebook pode ficar longe desta corrida?  A empresa oferece sua própria estrutura de NLP de código aberto chamada PyText.  De acordo com um estudo publicado pelo Facebook, o PyText aumentou a precisão dos modelos de conversação em 10% e reduziu o tempo de treinamento. <br><br>  O PyText está na verdade por trás de vários produtos do Facebook, como o Messenger.  Portanto, trabalhar com ele adicionará um bom ponto ao seu portfólio e um conhecimento inestimável que você certamente obterá. <br><br>  Você pode tentar você mesmo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">baixar o código do GitHub</a> . <br><br><h3>  Google duplex </h3><br>  É difícil acreditar que você nunca ouviu falar do Google Duplex.  Aqui está uma demonstração que por muito tempo brilhou nas manchetes: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NO0-5MuJvew" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Como este é um produto do Google, há poucas chances de que mais cedo ou mais tarde o código seja publicado para todos.  Obviamente, essa demonstração levanta muitas questões: de questões éticas a questões de privacidade, mas falaremos sobre isso mais tarde.  Por enquanto, apenas aproveite o quão longe chegamos com a ML nos últimos anos. <br><br><a name="NLPtrends"></a><h2>  Tendências da PNL em 2019 </h2><br>  Quem melhor que o próprio Sebastian Ruder pode dar uma idéia de para onde a PNL está indo em 2019?  Aqui estão suas descobertas: <br><blockquote><ol><li>  O uso de modelos de investimento em idiomas pré-treinados será generalizado;  modelos avançados sem suporte serão muito raros. </li><li>  Aparecerão visualizações pré-treinadas que podem codificar informações especializadas que complementam os anexos do modelo de idioma.  Poderemos agrupar diferentes tipos de apresentações pré-treinadas, dependendo dos requisitos da tarefa. </li><li>  Mais trabalhos aparecerão no campo de aplicativos multilíngues e modelos multilíngues.  Em particular, contando com a incorporação de palavras entre idiomas, veremos o surgimento de representações interlínguas pré-treinadas e profundas. </li></ol></blockquote><a name="cv"></a><h2>  Visão computacional </h2><br><img src="https://habrastorage.org/webt/pu/aj/_c/puaj_c89feaiultos4yynrcj7x4.jpeg"><br><br>  Hoje, a visão computacional é a área mais popular no campo da aprendizagem profunda.  Parece que os primeiros frutos da tecnologia já foram obtidos e estamos no estágio de desenvolvimento ativo.  Independentemente da imagem ou do vídeo, vemos o surgimento de muitas estruturas e bibliotecas que resolvem facilmente os problemas da visão computacional. <br><br>  Aqui está minha lista das melhores soluções que podem ser vistas este ano. <br><br><h3>  BigGANs Out </h3><br>  Ian Goodfellow projetou as GANs em 2014, e o conceito gerou uma ampla variedade de aplicações.  Ano após ano, observamos como o conceito original foi finalizado para uso em casos reais.  Mas uma coisa permaneceu inalterada até este ano - as imagens geradas por computador eram facilmente distinguíveis.  Uma certa inconsistência sempre aparecia no quadro, o que tornava a diferença muito óbvia. <br><br>  Nos últimos meses, surgiram mudanças nessa direção e, com a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">criação do BigGAN</a> , esses problemas podem ser resolvidos de uma vez por todas.  Veja as imagens geradas por este método: <br><br><img src="https://habrastorage.org/webt/mo/w7/ow/mow7owldedw4r1jtwex6wbfwwje.png"><br><br>  Sem um microscópio, é difícil dizer o que há de errado com essas imagens.  É claro que todos decidirão por si mesmos, mas não há dúvida de que o GAN muda a maneira como percebemos as imagens digitais (e o vídeo). <br><br>  Para referência: esses modelos foram treinados primeiro no conjunto de dados ImageNet e depois no JFT-300M para demonstrar que esses modelos são bem transferidos de um conjunto de dados para outro.  Aqui está um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link para uma página</a> da lista de correspondência da GAN explicando como visualizar e entender a GAN. <br><br><h3>  Model Fast.ai treinado no ImageNet em 18 minutos </h3><br>  Esta é uma implementação muito legal.  Existe uma crença generalizada de que, para executar tarefas de aprendizado profundo, você precisará de terabytes de dados e grandes recursos de computação.  O mesmo vale para o treinamento do modelo a partir do zero nos dados do ImageNet.  Muitos de nós pensamos da mesma maneira antes de algumas pessoas no fast.ai não conseguirem provar o contrário de todos. <br><br>  O modelo deles deu 93% de precisão com impressionantes 18 minutos.  O hardware usado, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descrito</a> em detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">em seu blog</a> , consistia em 16 instâncias públicas da nuvem da AWS, cada uma com 8 GPUs NVIDIA V100.  Eles criaram um algoritmo usando as bibliotecas fast.ai e PyTorch. <br><br>  O custo total da montagem foi de apenas US $ 40!  Jeremy descreveu suas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">abordagens e métodos</a> em mais detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> .  Esta é uma vitória comum! <br><br><h3>  vid2vid da NVIDIA </h3><br>  Nos últimos 5 anos, o processamento de imagens fez grandes progressos, mas e o vídeo?  Os métodos de conversão de um quadro estático para um dinâmico se mostraram um pouco mais complicados do que o esperado.  Você pode tirar uma sequência de quadros de um vídeo e prever o que acontecerá no próximo quadro?  Tais estudos foram feitos antes, mas as publicações eram vagas, na melhor das hipóteses. <br><br><img src="https://habrastorage.org/webt/hz/ox/hj/hzoxhjbehlnlzl8ivc-bgiz0vh0.png"><br><br>  A NVIDIA decidiu tornar sua decisão publicamente disponível no início deste ano [2018 - aprox.  por.], que foi avaliada positivamente pela sociedade.  O objetivo do vid2vid é derivar uma função de exibição de um determinado vídeo de entrada para criar um vídeo de saída que transmita o conteúdo do vídeo de entrada com uma precisão incrível. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/S1OwOd-war8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Você pode tentar a implementação no PyTorch, leve-o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ao GitHub aqui</a> . <br><br><a name="cvtrends"></a><h2>  Tendências de visão de máquina para 2019 </h2><br>  Como mencionei anteriormente, em 2019 é mais provável que vejamos o desenvolvimento das tendências de 2018, em vez de novos avanços: carros autônomos, algoritmos de reconhecimento de rosto, realidade virtual e muito mais.  Você pode discordar de mim se tiver um ponto de vista ou acréscimo diferente, compartilhá-lo conosco, o que mais podemos esperar em 2019? <br><br>  A questão dos drones, na pendência da aprovação de políticos e do governo, pode finalmente ter um sinal verde nos Estados Unidos (a Índia está muito atrasada nesse assunto).  Pessoalmente, eu gostaria que mais pesquisas fossem feitas em cenários do mundo real.  Conferências como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CVPR</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ICML fornecem uma</a> boa cobertura dos últimos desenvolvimentos nesta área, mas a proximidade dos projetos da realidade não é muito clara. <br><br>  “Resposta visual às perguntas” e “sistemas de diálogo visual” podem finalmente sair com uma estreia muito esperada.  Esses sistemas não têm capacidade de generalização, mas espera-se que em breve veremos uma abordagem multimodal integrada. <br><br><img src="https://habrastorage.org/webt/s5/bn/uy/s5bnuydmsc8hf37vm26icbmwrgc.jpeg"><br><br>  O autotreinamento veio à tona este ano.  Aposto que no próximo ano ele será aplicado em um número muito maior de estudos.  Essa é uma direção muito legal: os sinais são determinados diretamente a partir dos dados de entrada, em vez de perder tempo marcando manualmente as imagens.  Vamos manter os dedos cruzados! <br><br><h4>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Leia mais: Parte 2 - Ferramentas e bibliotecas, AutoML, Aprendizado por reforço, Ética na IA</a> </h4></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt439688/">https://habr.com/ru/post/pt439688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt439676/index.html">Reagir a integração nativa e C ++ para iOS e Android</a></li>
<li><a href="../pt439678/index.html">Enviar para o desafio F # aplicado</a></li>
<li><a href="../pt439680/index.html">Cerca de 50% dos russos estão dispostos a vender seus dados pessoais</a></li>
<li><a href="../pt439682/index.html">Treinamento Cisco 200-125 CCNA v3.0. Cisco Certified Network Specialist (CCNA). Dia 4. Dispositivos de gateway</a></li>
<li><a href="../pt439684/index.html">Inscreva-se no Desafio F # aplicado</a></li>
<li><a href="../pt439690/index.html">Comparação do desempenho da máquina virtual de 6 plataformas em nuvem: Selectel, MCS, I. Nuvem, Google Cloud, AWS e Azure</a></li>
<li><a href="../pt439692/index.html">AT&T processada por mudar o ícone da rede de 4G para 5G E</a></li>
<li><a href="../pt439694/index.html">Tecido inteligente que responde a mudanças de temperatura corporal</a></li>
<li><a href="../pt439696/index.html">Na crista de uma onda, ou "eu quero mainstream" - mas vale a pena?</a></li>
<li><a href="../pt439698/index.html">Introdução à programação: um simples jogo de tiro em 3D do zero no fim de semana, parte 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>