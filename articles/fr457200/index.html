<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÉ üé∞ ‚úçüèΩ Un robot de dessin pour r√©aliser des sc√®nes de tous les jours et m√™me des histoires üí¥ üòØ üëáüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Si on vous demandait de dessiner une image de plusieurs personnes en tenue de ski, debout dans la neige, il y a de fortes chances que vous commenciez ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Un robot de dessin pour r√©aliser des sc√®nes de tous les jours et m√™me des histoires</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/microsoft/blog/457200/"><p> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/657/93f/e0f/65793fe0f0e42b382c46426806424e36.png" alt="Dessin de bot" width="1024" height="414"></a> </p><br><p>  Si on vous demandait de dessiner une image de plusieurs personnes en tenue de ski, debout dans la neige, il y a de fortes chances que vous commenciez par un contour de trois ou quatre personnes raisonnablement positionn√©es au centre de la toile, puis esquissez dans les skis sous leur les pieds.  Bien que cela n'ait pas √©t√© sp√©cifi√©, vous pourriez d√©cider d'ajouter un sac √† dos √† chacun des skieurs pour empanner avec les attentes de ce que les skieurs porteraient.  Enfin, vous rempliriez soigneusement les d√©tails, peignant peut-√™tre leurs v√™tements en bleu, des √©charpes en rose, le tout sur un fond blanc, rendant ces personnes plus r√©alistes et s'assurant que leur environnement correspond √† la description.  Enfin, pour rendre la sc√®ne plus vivante, vous pouvez m√™me esquisser des pierres brunes qui d√©passent dans la neige pour sugg√©rer que ces skieurs sont dans les montagnes. </p><br><p>  Maintenant, il y a un bot qui peut faire tout √ßa. </p><a name="habracut"></a><br><p>  La nouvelle technologie d'IA en cours de d√©veloppement chez Microsoft Research AI peut comprendre une description en langage naturel, esquisser une disposition de l'image, synth√©tiser l'image, puis affiner les d√©tails en fonction de la disposition et des mots individuels fournis.  En d'autres termes, ce bot peut g√©n√©rer des images √† partir de descriptions textuelles de sous-titres de sc√®nes quotidiennes.  Ce m√©canisme d√©lib√©r√© a produit une am√©lioration significative de la qualit√© d'image g√©n√©r√©e par rapport √† la technique de pointe pr√©c√©dente pour la g√©n√©ration de texte en image pour des sc√®nes quotidiennes compliqu√©es, selon les r√©sultats des tests standard de l'industrie rapport√©s dans ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Object-driven Text- to-Image Synthesis via Adversarial Training</a> ¬ª, qui sera publi√© ce mois-ci √† Long Beach, en Californie, lors de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Conf√©rence IEEE 2019 sur la vision par ordinateur et la reconnaissance des formes</a> (CVPR 2019).  Il s'agit d'un projet de collaboration entre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pengchuan Zhang</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Qiuyuan Huang</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jianfeng Gao</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Microsoft Research AI</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lei Zhang</a> de Microsoft, Xiaodong He de JD AI Research et Wenbo Li et Siwei Lyu de l'Universit√© d'Albany, SUNY (tandis que Wenbo Li travaillait comme stagiaire chez Microsoft Research AI). </p><br><p>  Il existe deux principaux d√©fis intrins√®ques au probl√®me des robots de dessin bas√©s sur la description.  La premi√®re est que de nombreux types d'objets peuvent appara√Ætre dans les sc√®nes de tous les jours et que le bot devrait √™tre capable de les comprendre et de les dessiner tous.  Les m√©thodes pr√©c√©dentes de g√©n√©ration de texte en image utilisent des paires image-l√©gende qui ne fournissent qu'un signal de supervision √† grain tr√®s grossier pour g√©n√©rer des objets individuels, ce qui limite leur qualit√© de g√©n√©ration d'objet.  Dans cette nouvelle technologie, les chercheurs utilisent l'ensemble de donn√©es COCO qui contient des √©tiquettes et des cartes de segmentation pour 1,5 million d'instances d'objets √† travers 80 classes d'objets communes, permettant au bot d'apprendre √† la fois le concept et l'apparence de ces objets.  Ce signal supervis√© √† grain fin pour la g√©n√©ration d'objets am√©liore consid√©rablement la qualit√© de g√©n√©ration de ces classes d'objets courantes. </p><br><p>  Le deuxi√®me d√©fi r√©side dans la compr√©hension et la g√©n√©ration des relations entre plusieurs objets dans une m√™me sc√®ne.  Un grand succ√®s a √©t√© obtenu en g√©n√©rant des images qui ne contiennent qu'un seul objet principal pour plusieurs domaines sp√©cifiques, tels que les visages, les oiseaux et les objets communs.  Cependant, la g√©n√©ration de sc√®nes plus complexes contenant plusieurs objets avec des relations s√©mantiquement significatives entre ces objets reste un d√©fi important dans la technologie de g√©n√©ration de texte en image.  Ce nouveau robot de dessin a appris √† g√©n√©rer la disposition des objets √† partir de mod√®les de co-occurrence dans l'ensemble de donn√©es COCO pour ensuite g√©n√©rer une image conditionn√©e sur la disposition pr√©-g√©n√©r√©e. </p><br><h3>  G√©n√©ration d'images attentives orient√©es objet </h3><br><p>  Au c≈ìur du bot de dessin de Microsoft Research AI se trouve une technologie connue sous le nom de Generative Adversarial Network, ou GAN.  Le GAN se compose de deux mod√®les d'apprentissage automatique - un g√©n√©rateur qui g√©n√®re des images √† partir de descriptions textuelles et un discriminateur qui utilise des descriptions textuelles pour juger de l'authenticit√© des images g√©n√©r√©es.  Le g√©n√©rateur tente d'obtenir de fausses images devant le discriminateur;  le discriminateur, d'autre part, ne veut jamais √™tre dupe.  En travaillant ensemble, le discriminateur pousse le g√©n√©rateur vers la perfection. </p><br><p>  Le robot de dessin a √©t√© form√© sur un ensemble de donn√©es de 100 000 images, chacune avec des √©tiquettes d'objets saillants et des cartes de segmentation et cinq l√©gendes diff√©rentes, permettant aux mod√®les de concevoir des objets individuels et des relations s√©mantiques entre les objets.  Le GAN, par exemple, apprend √† quoi devrait ressembler un chien en comparant des images avec et sans descriptions de chiens. </p><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/27e/b5f/e0a/27eb5fe0ab38f35180ba6558f15f0456.png" alt="Figure 1: une sc√®ne complexe avec plusieurs objets et relations." width="273" height="272"></a> <br><p>  Figure 1: une sc√®ne complexe avec plusieurs objets et relations. </p><br><p>  Les GAN fonctionnent bien lors de la g√©n√©ration d'images contenant un seul objet saillant, comme un visage humain, des oiseaux ou des chiens, mais la qualit√© stagne avec des sc√®nes quotidiennes plus complexes, une telle sc√®ne d√©crite comme ¬´Une femme portant un casque monte √† cheval¬ª (voir la figure 1.) En effet, ces sc√®nes contiennent plusieurs objets (femme, casque, cheval) et de riches relations s√©mantiques entre elles (femme portant un casque, femme chevauchant un cheval).  Le bot doit d'abord comprendre ces concepts et les placer dans l'image avec une mise en page significative.  Apr√®s cela, un signal plus supervis√© capable d'enseigner la g√©n√©ration d'objet et la g√©n√©ration de mise en page est requis pour remplir cette t√¢che de compr√©hension du langage et de g√©n√©ration d'image. </p><br><p>  Au fur et √† mesure que les humains dessinent ces sc√®nes compliqu√©es, nous d√©cidons d'abord des principaux objets √† dessiner et faisons une mise en page en pla√ßant des bo√Ætes englobantes pour ces objets sur la toile.  Ensuite, nous nous concentrons sur chaque objet, en v√©rifiant √† plusieurs reprises les mots correspondants qui d√©crivent cet objet.  Pour capturer ce trait humain, les chercheurs ont cr√©√© ce qu'ils ont appel√© un GAN attentif orient√© objet, ou ObjGAN, pour mod√©liser math√©matiquement le comportement humain de l'attention centr√©e sur l'objet.  ObjGAN fait cela en d√©composant le texte d'entr√©e en mots individuels et en faisant correspondre ces mots √† des objets sp√©cifiques de l'image. </p><br><p>  Les humains v√©rifient g√©n√©ralement deux aspects pour affiner le dessin: le r√©alisme des objets individuels et la qualit√© des correctifs d'image.  ObjGAN imite √©galement ce comportement en introduisant deux discriminateurs - un discriminateur par objet et un discriminateur par patch.  Le discriminateur par objet essaie de d√©terminer si l'objet g√©n√©r√© est r√©aliste ou non et si l'objet est coh√©rent avec la description de la phrase.  Le discriminateur par patch essaie de d√©terminer si ce patch est r√©aliste ou non et si ce patch est coh√©rent avec la description de la phrase. </p><br><h3>  Travaux connexes: visualisation de l'histoire </h3><br><p>  Des mod√®les de g√©n√©ration de texte en image √† la pointe de la technologie peuvent g√©n√©rer des images d'oiseaux r√©alistes sur la base d'une description en une seule phrase.  Cependant, la g√©n√©ration de texte en image peut aller bien au-del√† de la synth√®se d'une seule image bas√©e sur une phrase.  Dans ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">StoryGAN: A Sequential Conditional GAN ‚Äã‚Äãfor Story Visualization</a> ¬ª, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jianfeng Gao</a> de Microsoft Research, avec Zhe Gan, Jingjing Liu et Yu Cheng de Microsoft Dynamics 365 AI Research, Yitong Li, David Carlson et Lawrence Carin de Duke University, Yelong Shen de Tencent AI Research et Yuexin Wu de l'Universit√© Carnegie Mellon vont plus loin et proposent une nouvelle t√¢che, appel√©e Story Visualization.  √âtant donn√© un paragraphe de plusieurs phrases, une histoire compl√®te peut √™tre visualis√©e, g√©n√©rant une s√©quence d'images, une pour chaque phrase.  C'est une t√¢che difficile, car le robot de dessin n'est pas seulement n√©cessaire pour imaginer un sc√©nario qui correspond √† l'histoire, mod√©liser les interactions entre les diff√©rents personnages apparaissant dans l'histoire, mais il doit √©galement √™tre en mesure de maintenir la coh√©rence globale entre les sc√®nes et les personnages dynamiques.  Ce d√©fi n'a √©t√© relev√© par aucune m√©thode de g√©n√©ration d'images ou de vid√©os. </p><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/04c/5c0/39c/04c5c039ca8dc39e2f47ffabb9437db7.png" alt="Figure 2: Visualisation de l'histoire vs g√©n√©ration d'image simple." width="1024" height="454"></a> <br><p>  Figure 2: Visualisation de l'histoire vs  g√©n√©ration d'image simple. </p><br><p>  Les chercheurs ont mis au point un nouveau mod√®le de g√©n√©ration de s√©quence histoire-image, StoryGAN, bas√© sur le cadre s√©quentiel GAN ‚Äã‚Äãconditionnel.  Ce mod√®le est unique en ce qu'il se compose d'un encodeur de contexte profond qui suit dynamiquement le flux de l'histoire, et de deux discriminateurs au niveau de l'histoire et de l'image pour am√©liorer la qualit√© de l'image et la coh√©rence des s√©quences g√©n√©r√©es.  StoryGAN peut √©galement √™tre naturellement √©tendu pour l'√©dition d'image interactive, o√π une image d'entr√©e peut √™tre √©dit√©e s√©quentiellement sur la base des instructions de texte.  Dans ce cas, une s√©quence d'instructions utilisateur servira d'entr√©e ¬´histoire¬ª.  En cons√©quence, les chercheurs ont modifi√© les ensembles de donn√©es existants pour cr√©er les ensembles de donn√©es CLEVR-SV et Pororo-SV, comme le montre la figure 2. </p><br><h3>  Applications pratiques - une histoire vraie </h3><br><p>  La technologie de g√©n√©ration de texte en image pourrait trouver des applications pratiques agissant comme une sorte d'assistant de croquis pour les peintres et les d√©corateurs d'int√©rieur, ou comme un outil pour la retouche photo √† commande vocale.  Avec plus de puissance de calcul, les chercheurs imaginent la technologie g√©n√©rant des films d'animation bas√©s sur des sc√©narios, augmentant le travail des cin√©astes d'animation en supprimant une partie du travail manuel impliqu√©. </p><br><p>  Pour l'instant, les images g√©n√©r√©es sont encore loin d'√™tre photo-r√©alistes.  Les objets individuels r√©v√®lent presque toujours des d√©fauts, tels que des visages flous et / ou des bus aux formes d√©form√©es.  Ces d√©fauts indiquent clairement qu'un ordinateur, et non un √™tre humain, a cr√©√© les images.  N√©anmoins, la qualit√© des images ObjGAN est nettement meilleure que les images GAN les meilleures de leur cat√©gorie pr√©c√©dente et sert de jalon sur la voie vers une intelligence g√©n√©rique de type humain qui augmente les capacit√©s humaines. </p><br><p>  Pour que les IA et les humains partagent le m√™me monde, chacun doit avoir un moyen d'interagir avec l'autre.  Le langage et la vision sont les deux modalit√©s les plus importantes pour que les humains et les machines interagissent.  La g√©n√©ration de texte en image est une t√¢che importante qui fait progresser la recherche en intelligence multimodale en vision du langage. </p><br><p>  Les chercheurs qui ont cr√©√© ce travail passionnant ont h√¢te de partager ces r√©sultats avec les participants au CVPR de Long Beach et d'entendre ce que vous pensez.  En attendant, n'h√©sitez pas √† consulter leur code open-source pour <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ObjGAN</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">StoryGAN</a> sur GitHub </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr457200/">https://habr.com/ru/post/fr457200/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr457190/index.html">Nous traitons les affaires gr√¢ce √† la mise en ≈ìuvre de syst√®mes CRM</a></li>
<li><a href="../fr457192/index.html">Airbus prend de nouveaux sommets avec la r√©alit√© mixte de Microsoft</a></li>
<li><a href="../fr457194/index.html">Airbus atteint de nouveaux sommets avec l'aide de la technologie de r√©alit√© mixte de Microsoft</a></li>
<li><a href="../fr457196/index.html">Petty little joy # 5: Dynaconf - gestion des param√®tres dans le projet</a></li>
<li><a href="../fr457198/index.html">Le r√©seau neuronal a appris √† dessiner des sc√®nes complexes √† partir d'une description textuelle</a></li>
<li><a href="../fr457202/index.html">Comment nous choisissons les id√©es pour le d√©veloppement de nos produits: le vendeur doit pouvoir entendre ...</a></li>
<li><a href="../fr457204/index.html">Windows PowerShell et longs chemins</a></li>
<li><a href="../fr457206/index.html">SQL Index Manager - une longue histoire sur SQL Server, les fouilles et la maintenance des index</a></li>
<li><a href="../fr457208/index.html">G√©n√©ration dynamique de robots.txt pour les sites ASP.NET Core en fonction de l'environnement</a></li>
<li><a href="../fr457210/index.html">Stockez des ressources statiques sur votre h√©bergement</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>