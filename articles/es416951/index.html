<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëàüèª üíÖüèæ üéõÔ∏è Clusters de Kubernetes en servicio VPC üë®üèª‚Äçüíª ü§¥üèΩ üî¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hemos agregado la capacidad de lanzar Kubernetes de manera conveniente en el servicio Virtual Private Cloud en el modo de prueba beta inicial. 


 Est...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Clusters de Kubernetes en servicio VPC</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/selectel/blog/416951/"><img src="https://habrastorage.org/webt/bc/b6/cy/bcb6cykv49fwewsnhfr-7ny-0uc.png"><br><p><br>  Hemos agregado la capacidad de lanzar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">Kubernetes de manera</a> conveniente en el servicio <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener noreferrer">Virtual Private Cloud</a> en el modo de prueba beta inicial. </p><br><p>  Esta funcionalidad ser√° √∫til para los usuarios que necesitan una administraci√≥n conveniente de una gran cantidad de aplicaciones que se ejecutan como contenedores.  Kubernetes ofrece herramientas para escalar, autocuraci√≥n, equilibrio de carga para contenedores que se ejecutan dentro de un cl√∫ster. </p><br><p>  Dado que el servicio <strong>Virtual Private Cloud</strong> se basa en OpenStack, utilizamos uno de sus componentes: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">OpenStack Magnum</a> .  Le permite crear r√°pidamente grupos privados de Kubernetes con el n√∫mero deseado de nodos. </p><br><p>  Actualmente, cualquier usuario de nuestro servicio puede crear varios cl√∫steres independientes en su proyecto.  Como nodos de cl√∫ster, se utilizar√°n m√°quinas virtuales, cuya configuraci√≥n se puede seleccionar y cambiar. </p><br><p>  En este art√≠culo, hablaremos sobre los objetos principales del cl√∫ster de Kubernetes y usaremos los ejemplos para ver el proceso de creaci√≥n de un cl√∫ster con OpenStack Magnum. </p><a name="habracut"></a><br><h2>  Crear y administrar un cl√∫ster de Kubernetes </h2><br><p>  Actualmente, la creaci√≥n de un cl√∫ster de Kubernetes solo es posible a trav√©s de las utilidades de consola o la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">API OpenStack</a> en las zonas de disponibilidad <strong>ru-1a</strong> y <strong>ru-1b</strong> (San Petersburgo). </p><br><p>  Para comenzar, necesitar√°: </p><br><ul><li>  Cree uno nuevo o use un proyecto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener noreferrer">VPC</a> existente; </li><li>  Cree un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener noreferrer">usuario con una clave SSH</a> ; </li><li>  Agregue un usuario al proyecto creado en la p√°gina de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener noreferrer">administraci√≥n</a> del proyecto; </li><li>  Vaya al proyecto y obtenga el archivo de acceso en la pesta√±a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener noreferrer">Acceso</a> ; </li><li>  Instale el cliente de consola <strong>openstack</strong> con la <strong>biblioteca python-magnumclient</strong> ; </li><li>  Instale el cliente de consola <strong>kubectl</strong> . </li></ul><br><p>  Para instalar el cliente de consola <strong>openstack</strong> , puede usar las instrucciones <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener noreferrer">en el enlace</a> , sin embargo, debe tener en cuenta que para este cliente tambi√©n necesitar√° instalar la biblioteca <strong>python-magnumclient</strong> para admitir la creaci√≥n de cl√∫steres de Kubernetes. </p><br><p>  El conjunto completo de comandos para instalar un cliente OpenStack con el complemento requerido para los sistemas operativos de la familia Ubuntu / Debian: </p><br><pre><code class="bash hljs">$ sudo apt update $ sudo apt -y install curl python-pip python-dev python3-dev git libxml2-dev libxslt1-dev python-openssl python3-openssl python-pyasn1 libffi-dev libssl-dev build-essential $ sudo pip install -UI pbr setuptools pytz $ sudo pip install -UI git+https://github.com/openstack/python-openstackclient $ sudo pip install -UI git+https://github.com/openstack/python-magnumclient</code> </pre> <br><p>  El conjunto completo de comandos para instalar un cliente OpenStack con el complemento requerido para los sistemas operativos de la familia Fedora / CentOS: </p><br><pre> <code class="bash hljs">$ sudo yum -y install python-pip gcc libffi-devel python-devel libxslt-devel openssl-devel git libffi-devel $ sudo pip install -UI pbr setuptools pytz $ sudo pip install -UI git+https://github.com/openstack/python-openstackclient $ sudo pip install -UI git+https://github.com/openstack/python-magnumclient</code> </pre> <br><p>  Para administrar objetos Kubernetes, necesita el cliente de consola <strong>kubectl</strong> .  Los m√©todos de instalaci√≥n para varios sistemas operativos se describen en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener noreferrer">la documentaci√≥n oficial</a> . </p><br><p>  Para crear un cl√∫ster, deber√° crear o usar los existentes: </p><br><ul><li>  <strong>Plantilla de</strong> cl√∫ster; </li><li>  Un conjunto de par√°metros para la CPU y la RAM de las m√°quinas virtuales ( <strong>sabor</strong> ). </li></ul><br><p>  Puede crear plantillas de cl√∫ster y probarlas usted mismo, o usar plantillas p√∫blicas creadas previamente. </p><br><p>  Tambi√©n deber√° determinar la zona de disponibilidad, el tipo de discos para su cl√∫ster y la cantidad de nodos.  Vale la pena considerar que todav√≠a no admitimos la posibilidad de crear un cl√∫ster en varias zonas.  Puede elegir cualquier tipo de unidad de red (r√°pida, universal o b√°sica). <br>  Puede obtener m√°s informaci√≥n sobre los tipos de unidades en nuestra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener noreferrer">base de conocimientos</a> . </p><br><p>  El n√∫mero de nodos puede ser diferente para los roles <strong>maestro</strong> y <strong>minion</strong> .  En los nodos que desempe√±an el rol maestro, se lanzar√°n los elementos de control del cl√∫ster: <strong>controlador-administrador</strong> , <strong>planificador</strong> , <strong>api</strong> .  En los otros nodos, se <strong>lanzar√°n los</strong> servicios <strong>kubelet</strong> , <strong>kube-proxy</strong> y todos los contenedores de aplicaciones.  Puede obtener m√°s informaci√≥n sobre los componentes que se ejecutan en los nodos del cl√∫ster en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">documentaci√≥n oficial</a> . </p><br><p>  Para acceder a los nodos a trav√©s de SSH, deber√° utilizar la clave SSH creada anteriormente.  Los comandos de muestra usar√°n una clave llamada <strong>ssh-test</strong> . </p><br><p>  Utilizaremos la plantilla y el sabor del cl√∫ster p√∫blico, el tipo de disco r√°pido y la zona de disponibilidad <strong>ru-1b</strong> . <br>  En nuestro cl√∫ster, se lanzar√°n inicialmente 2 nodos maestros y 3 nodos minion. </p><br><p>  Para verificar estos par√°metros, utilizamos los comandos openstackclient y el archivo de acceso descargado ( <strong>rc.sh</strong> ): </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#          . $ source rc.sh #  ,         $ openstack flavor show BL1.2-4096 -c ram -c vcpus +-------+-------+ | Field | Value | +-------+-------+ | ram | 4096 | | vcpus | 2 | +-------+-------+ #       ru-1b $ openstack volume type show fast.ru-1b -c name +-------+------------+ | Field | Value | +-------+------------+ | name | fast.ru-1b | +-------+------------+ #    Kubernetes $ openstack coe cluster template list -c name +---------------------------------------+ | name | +---------------------------------------+ | kubernetes-nofloatingips-ru-1b-v1.9.3 | | kubernetes-nofloatingips-ru-1b-v1.9.6 | | kubernetes-nofloatingips-ru-1b-v1.9.9 | | kubernetes-floatingips-ru-1b-v1.9.3 | | kubernetes-floatingips-ru-1b-v1.9.6 | | kubernetes-floatingips-ru-1b-v1.9.9 | | kubernetes-nofloatingips-ru-1a-v1.9.3 | | kubernetes-nofloatingips-ru-1a-v1.9.6 | | kubernetes-nofloatingips-ru-1a-v1.9.9 | | kubernetes-floatingips-ru-1a-v1.9.3 | | kubernetes-floatingips-ru-1a-v1.9.6 | | kubernetes-floatingips-ru-1a-v1.9.9 | +---------------------------------------+</span></span></code> </pre> <br><p>  Por ejemplo, elegiremos la segunda plantilla de cl√∫ster; las direcciones flotantes de acceso p√∫blico para cada uno de los nodos no se crear√°n a partir de ella.  No los necesitaremos. </p><br><pre> <code class="plaintext hljs">#   Kubernetes   test-cluster #   keypair   ,   $ openstack coe cluster create \ --cluster-template kubernetes-nofloatingips-ru-1b-v1.9.9 \ --master-count 2 \ --node-count 3 \ --keypair ssh-test \ --master-flavor BL1.2-4096 \ --flavor BL1.2-4096 \ test-cluster</code> </pre> <br><p>  <em>Tenga en cuenta que hemos elegido la misma configuraci√≥n para diferentes nodos (par√°metros maestro-sabor y sabor), puede elegir diferentes conjuntos de configuraci√≥n seg√∫n los requisitos del cl√∫ster.</em>  <em>Su cambio es posible despu√©s de su creaci√≥n.</em> </p><br><p>  Tambi√©n vale la pena considerar que al crear un cl√∫ster con varios nodos maestros, se crear√° autom√°ticamente un equilibrador de carga para acceder a la API de Kubernetes. </p><br><p>  Despu√©s de unos minutos, aparecer√° un cl√∫ster de Kubernetes en su proyecto.  En el panel de control del proyecto, ver√° nuevas m√°quinas virtuales, discos y objetos de red. </p><br><p>  Puede verificar el estado de su cl√∫ster a trav√©s de openstackclient: </p><br><pre> <code class="bash hljs">openstack coe cluster list -c name -c status +--------------+--------------------+ | name | status | +--------------+--------------------+ | <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster | CREATE_IN_PROGRESS | +--------------+--------------------+</code> </pre> <br><p>  Despu√©s de que el cl√∫ster ingrese al estado CREATE_COMPLETE, puede administrar sus objetos a trav√©s de la utilidad kubectl descargando el archivo de configuraci√≥n utilizando los siguientes comandos: </p><br><pre> <code class="bash hljs">$ mkdir -p ~/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster $ openstack coe cluster config <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster --dir ~/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster</code> </pre> <br><p>  Despu√©s de eso, puede trabajar con el cl√∫ster utilizando la utilidad kubectl: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=~/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster/config $ kubectl get pods --all-namespaces -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase NAME STATUS coredns-785dcf9c58-6gnfp Running heapster-6846cdc674-rm4k6 Running kube-dns-autoscaler-6b94f7bbf8-x5clt Running kubernetes-dashboard-747575c864-wlg6p Running monitoring-grafana-84b4596dd7-zf5rx Running monitoring-influxdb-c8486fc95-bqqb6 Running node-exporter-test-cluster-robvp4cvwpt7-minion-0 Running</code> </pre> <br><p>  Si es necesario, puede aumentar o disminuir el n√∫mero de nodos de s√∫bditos en el cl√∫ster a trav√©s de openstackclient pasando el nuevo valor de cuenta_nodo: </p><br><pre> <code class="bash hljs">$ openstack coe cluster update <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster replace node_count=4</code> </pre> <br><h2>  Objetos clave de cl√∫ster de Kubernetes </h2><br><h3>  Vainas </h3><br><p>  Aunque Kubernetes gestiona un conjunto de contenedores, la entidad subyacente que gestiona Kubernetes no es un contenedor, sino un <strong>Pod</strong> . </p><br><p>  Pod es un conjunto de espacios de nombres de kernel de Linux y configuraciones de pila de red que le permiten ensamblar un conjunto de contenedores en una sola entidad. <br>  La mayor√≠a de las veces, un contenedor con la aplicaci√≥n se inicia dentro de un Pod separado. <br>  Si es necesario, puede ejecutar varios contenedores dentro de un Pod, esto puede ser √∫til cuando necesita proporcionar acceso de un contenedor a otro a trav√©s de la interfaz de red localhost, o por alguna otra raz√≥n ejecutar varios contenedores en el mismo host. <br>  Todos los contenedores que se ejecutan en el mismo Pod tendr√°n un nombre de host, direcci√≥n IP, tabla de enrutamiento y discos. </p><br><p>  Vale la pena se√±alar que al escalar el n√∫mero de instancias de su aplicaci√≥n dentro de Kubernetes, es necesario aumentar el n√∫mero de Pods, y no el n√∫mero de contenedores en un Pod espec√≠fico. <br>  M√°s detalles en la documentaci√≥n oficial de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">Pods</a> . </p><br><p>  Por ejemplo, cree el Pod m√°s simple con Nginx usando la descripci√≥n en formato yaml: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-basic.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80</span></span></code> </pre> <br><p>  Para crear un Pod, podemos usar la utilidad <strong>kubectl</strong> . <br>  Agregamos todos los ejemplos presentados en el art√≠culo a nuestro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">grupo Github</a> , por lo que no puede crear archivos en su computadora, pero use la URL del archivo desde el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">repositorio</a> p√∫blico: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/pods/nginx-basic.yaml</code> </pre> <br><p>  Despu√©s de la creaci√≥n, podemos solicitar informaci√≥n completa sobre el estado del Pod mediante el comando kubectl describe: </p><br><pre> <code class="bash hljs">$ kubectl describe pod nginx Name: nginx Namespace: default Node: <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0/10.0.0.5 Start Time: Sun, 17 Jun 2018 12:29:03 +0000 Labels: &lt;none&gt; Annotations: &lt;none&gt; Status: Running IP: 10.100.88.9 Containers: nginx: Container ID: docker://6ca6383b66686c05c61c1f690737110e0f8994eda393f44a7ebfbbf2b2026267 Image: library/nginx:1.14-alpine Image ID: docker-pullable://docker.io/nginx@sha256:944b79ca7dbe456ce72e73e70816c1990e39967c8f010349a388c00b77ec519c Port: 80/TCP Host Port: 0/TCP State: Running Started: Sun, 17 Jun 2018 12:29:16 +0000 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rp5ls (ro) Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-rp5ls: Type: Secret (a volume populated by a Secret) SecretName: default-token-rp5ls Optional: <span class="hljs-literal"><span class="hljs-literal">false</span></span> QoS Class: BestEffort Node-Selectors: &lt;none&gt; Tolerations: &lt;none&gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 52s default-scheduler Successfully assigned nginx to <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Normal SuccessfulMountVolume 51s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 MountVolume.SetUp succeeded <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> volume <span class="hljs-string"><span class="hljs-string">"default-token-rp5ls"</span></span> Normal Pulling 50s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 pulling image <span class="hljs-string"><span class="hljs-string">"library/nginx:1.14-alpine"</span></span> Normal Pulled 39s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Successfully pulled image <span class="hljs-string"><span class="hljs-string">"library/nginx:1.14-alpine"</span></span> Normal Created 39s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Created container Normal Started 39s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Started container</code> </pre> <br><p>  Como puede ver, Pod comenz√≥ en un nodo llamado test-cluster-nd5c5y6lsfxb-minion-0 y recibi√≥ una direcci√≥n IP interna de 10.100.88.9. </p><br><p>  Desde la secci√≥n Eventos, puede ver los principales eventos de inicio: seleccionar un nodo para iniciar y descargar la imagen. </p><br><p>  Podemos ingresar al Pod y verificar el estado de los procesos dentro del contenedor: </p><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it nginx sh ps aux PID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 7 nginx 0:00 nginx: worker process 20 root 0:00 sh 24 root 0:00 ps aux <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre> <br><p>  Debe tenerse en cuenta que la direcci√≥n IP 10.100.88.9 no estar√° disponible para otras aplicaciones dentro y fuera del cl√∫ster de Kubernetes, el acceso al Nginx en ejecuci√≥n solo ser√° posible desde el propio Pod: </p><br><pre> <code class="bash hljs">$ ping -c 1 10.100.88.9 PING 10.100.88.9 (10.100.88.9): 56 data bytes --- 10.100.88.9 ping statistics --- 1 packets transmitted, 0 packets received, 100% packet loss $ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> nginx -- ping -c1 10.100.88.9 PING 10.100.88.9 (10.100.88.9): 56 data bytes 64 bytes from 10.100.88.9: seq=0 ttl=64 time=0.075 ms --- 10.100.88.9 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.075/0.075/0.075 ms</code> </pre> <br><p>  Adem√°s del hecho de que la direcci√≥n IP especificada solo es accesible desde el contenedor, tampoco es permanente.  Esto significa que si este Pod se recrea, puede obtener una direcci√≥n IP diferente. </p><br><p>  Para resolver estos problemas, puede usar un objeto llamado Servicio. </p><br><h3>  Servicios </h3><br><p>  El servicio le permite asignar direcciones IP permanentes para Pods, proporcionarles acceso desde redes externas y equilibrar solicitudes entre Pods. <br>  Puede obtener m√°s informaci√≥n sobre el Servicio en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">documentaci√≥n oficial</a> . </p><br><p>  Por ejemplo, necesitamos eliminar el Pod en ejecuci√≥n: </p><br><pre> <code class="bash hljs">$ kubectl delete pod nginx</code> </pre> <br><p>  Agregue a la descripci√≥n del Pod una etiqueta (Etiqueta), que se requiere para el Servicio: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-labeled.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: app: webservice spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80</span></span></code> </pre> <br><p>  Tambi√©n necesitaremos una descripci√≥n del servicio: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-nodeport.yaml apiVersion: v1 kind: Service metadata: name: nginx-nodeport labels: app: webservice spec: type: NodePort ports: - port: 80 nodePort: 30001 protocol: TCP selector: app: webservice</span></span></code> </pre> <br><p>  Crear pod y servicio: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/pods/nginx-labeled.yaml \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/services/nginx-nodeport.yaml</code> </pre> <br><p>  Como el Servicio creado es del tipo NodePort, el puerto 30001 indicado por nosotros en todas las interfaces de red se abrir√° en todos los nodos del cl√∫ster. <br>  Esto significa que si agregamos una direcci√≥n IP externa a cualquier nodo, podemos acceder al Pod en ejecuci√≥n con Nginx desde una red externa. </p><br><p>  Para no usar las direcciones externas de los nodos del cl√∫ster para acceder al Servicio, podemos usar el tipo LoadBalancer en lugar de NodePort. <br>  Necesitaremos una nueva descripci√≥n del servicio: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-loadbalancer.yaml apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer labels: app: webservice spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: app: webservice</span></span></code> </pre> <br><p>  Elimine el servicio actual y aplique la nueva descripci√≥n: </p><br><pre> <code class="bash hljs">$ kubectl delete service nginx-service $ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/services/nginx-loadbalancer.yaml</code> </pre> <br><p>  Despu√©s de iniciar el servicio, Nginx estar√° disponible en el puerto TCP 80 desde una red externa, y no ser√° necesario asignar y usar direcciones externas para los nodos del cl√∫ster.  El servicio de tipo LoadBalancer asignar√° autom√°ticamente una nueva direcci√≥n externa a su proyecto VPC y comenzar√° a usarlo. </p><br><p>  Puede obtener informaci√≥n sobre la direcci√≥n externa resaltada usando kubectl: </p><br><pre> <code class="bash hljs">$ kubectl get service nginx-service -o=custom-columns=IP:status.loadBalancer.ingress[0].ip IP xxx.xxx.xxx.xxx</code> </pre> <br><p>  En nuestros ejemplos, solo se lanz√≥ un Pod con Nginx.  Para escalar la aplicaci√≥n a m√°s Pods, podemos usar la implementaci√≥n. </p><br><h3>  Implementaciones </h3><br><p>  La implementaci√≥n es la esencia del cl√∫ster de Kubernetes, que le permite escalar Pods y actualizar o deshacer versiones para una gran cantidad de Pods. <br>  En lugar de Implementaci√≥n, tambi√©n puede usar el objeto ReplicaSet, pero no lo tocaremos en nuestros ejemplos. <br>  Puede obtener m√°s informaci√≥n sobre la implementaci√≥n en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">documentaci√≥n oficial</a> . </p><br><p>  Nuevamente necesitaremos eliminar Pod (no necesitamos eliminar el Servicio): </p><br><pre> <code class="bash hljs">$ kubectl delete pod nginx</code> </pre> <br><p>  Agregue la siguiente descripci√≥n de implementaci√≥n: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-1.14.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 10 selector: matchLabels: app: webservice minReadySeconds: 10 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: webservice spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80</span></span></code> </pre> <br><p>  Crea la implementaci√≥n especificada: </p><br><pre> <code class="bash hljs">$ kubectl create -f \ https://raw.githubusercontent.com/selectel/kubernetes-examples/master/deployments/nginx-1.14.yaml</code> </pre> <br><p>  Elegimos 10 para el par√°metro de r√©plicas, por lo que se crear√°n 10 Pods con la aplicaci√≥n Nginx en nuestro cl√∫ster: </p><br><pre> <code class="bash hljs">$ kubectl get pods --selector app=webservice NAME READY STATUS RESTARTS AGE nginx-deployment-54bfdc4489-42rrb 1/1 Running 0 4m nginx-deployment-54bfdc4489-5lvtc 1/1 Running 0 4m nginx-deployment-54bfdc4489-g7rk2 1/1 Running 0 4m nginx-deployment-54bfdc4489-h5rxp 1/1 Running 0 4m nginx-deployment-54bfdc4489-l9l2d 1/1 Running 0 4m nginx-deployment-54bfdc4489-pjpvg 1/1 Running 0 4m nginx-deployment-54bfdc4489-q8dnp 1/1 Running 0 4m nginx-deployment-54bfdc4489-s4wzf 1/1 Running 0 4m nginx-deployment-54bfdc4489-tfxf9 1/1 Running 0 4m nginx-deployment-54bfdc4489-xjzb5 1/1 Running 0 4m</code> </pre> <br><p>  Puede acceder a la aplicaci√≥n en ejecuci√≥n desde la red externa utilizando el Servicio creado en la secci√≥n anterior.  El servicio equilibrar√° autom√°ticamente las solicitudes de la red externa entre 10 instancias de Nginx. </p><br><p>  Si es necesario, podemos actualizar la versi√≥n de Nginx.  Actualice la descripci√≥n de implementaci√≥n cambiando la versi√≥n de la imagen de 1.14-alpine a 1.15-alpine: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-1.15.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 10 selector: matchLabels: app: webservice minReadySeconds: 10 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: webservice spec: containers: - name: nginx image: library/nginx:1.15-alpine # &lt;-- changed ports: - containerPort: 80</span></span></code> </pre> <br><p>  Para comenzar el proceso de actualizaci√≥n de Pods, utilizamos el comando kubectl.  Preste atenci√≥n al argumento --grabar, nos es √∫til para la posterior reversi√≥n conveniente de la versi√≥n de Nginx: </p><br><pre> <code class="bash hljs">$ kubectl apply -f \ https://raw.githubusercontent.com/selectel/kubernetes-examples/master/deployments/nginx-1.15.yaml \ --record</code> </pre> <br><p>  Puede controlar el progreso de la actualizaci√≥n con el siguiente comando: </p><br><pre> <code class="bash hljs">$ kubectl rollout status deployment nginx-deployment Waiting <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rollout to finish: 4 out of 10 new replicas have been updated...</code> </pre> <br><p>  Kubernetes esperar√° 10 segundos despu√©s de una actualizaci√≥n exitosa de un Pod, ya que especificamos un valor de 10 para el par√°metro minReadySeconds en la descripci√≥n de Implementaci√≥n. </p><br><p>  Una vez completada la actualizaci√≥n, todos los Pods para la implementaci√≥n entrar√°n en estado activo: </p><br><pre> <code class="bash hljs">$ kubectl get deployment --selector app=webservice NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 10 10 10 10 23m</code> </pre> <br><p>  Podemos revertir la versi√≥n de la aplicaci√≥n si algo sali√≥ mal.  Para hacer esto, debemos seleccionar la revisi√≥n de implementaci√≥n deseada: </p><br><pre> <code class="bash hljs">$ kubectl rollout <span class="hljs-built_in"><span class="hljs-built_in">history</span></span> deployment nginx-deployment deployments <span class="hljs-string"><span class="hljs-string">"nginx-deployment"</span></span> REVISION CHANGE-CAUSE 1 &lt;none&gt; 2 kubectl apply --filename=https://raw.githubusercontent.com/selectel/kubernetes-examples/master/deployments/nginx-1.15.yaml --record=<span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Hay 2 revisiones en la salida del comando: la primera es la creaci√≥n inicial de la implementaci√≥n, la segunda es una actualizaci√≥n.  Como utilizamos el argumento --record al actualizar, vemos el comando que cre√≥ la segunda revisi√≥n de Implementaci√≥n. </p><br><p>  Para revertir la versi√≥n, use el siguiente comando: </p><br><pre> <code class="bash hljs">$ kubectl rollout undo deployment nginx-deployment --to-revision=1</code> </pre> <br><p>  De manera similar con la actualizaci√≥n, podemos monitorear la reversi√≥n de la versi√≥n usando el comando: </p><br><pre> <code class="bash hljs">$ kubectl rollout status deployment nginx-deployment Waiting <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rollout to finish: 6 out of 10 new replicas have been updated‚Ä¶</code> </pre> <br><p>  En todos nuestros ejemplos, utilizamos contenedores sin un almac√©n de datos persistente.  En la siguiente secci√≥n lo arreglaremos. </p><br><h2>  Almacenamiento de datos </h2><br><p>  Por defecto, todos los datos de los contenedores que se ejecutan dentro de Pods son ef√≠meros y se perder√°n cuando Pods se bloquee. </p><br><p>  Puede usar el objeto PersistentVolumeClaim para ejecutar Pods con un almac√©n de datos persistente. </p><br><p>  Crear un objeto de este tipo en un cl√∫ster es muy simple: solo agregue su descripci√≥n, similar a la forma en que creamos Pod, Servicio o Implementaci√≥n en las secciones anteriores. </p><br><p>  Se puede encontrar m√°s informaci√≥n en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">documentaci√≥n oficial</a> . </p><br><p>  Descripci√≥n de ejemplo de PersistentVolumeClaim creando un disco de 10GB: </p><br><pre> <code class="bash hljs">apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pv-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi</code> </pre> <br><p>  Podemos conectarlo como un disco a nuestro Pod actualizando la descripci√≥n de Pod con Nginx creado anteriormente: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-with-volume.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80 volumeMounts: - mountPath: "/var/www/html" name: data volumes: - name: data persistentVolumeClaim: claimName: my-pv-claim</span></span></code> </pre> <br><p>  Sin embargo, para que se cree el disco, deber√° especificar las propiedades del disco creado en forma de StorageClass.  En el servicio "Virtual Private Cloud", puede utilizar unidades de red de tipos r√°pidos, universales y b√°sicos como almacenamiento permanente de datos de Kubernetes Pod. </p><br><p>  Por ejemplo, para crear una StorageClass que le permita usar discos r√°pidos en la zona de disponibilidad ru-1b, necesita la siguiente descripci√≥n: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># fast.ru-1b.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast.ru-1b annotations: storageclass.beta.kubernetes.io/is-default-class: "true" provisioner: kubernetes.io/cinder parameters: type: fast.ru-1b availability: ru-1b</span></span></code> </pre> <br><p>  Antes de crear los objetos especificados, elimine la implementaci√≥n creada anteriormente: </p><br><pre> <code class="bash hljs">$ kubectl delete deployment nginx-deployment</code> </pre> <br><p>  En primer lugar, creemos una StorageClass, para que se convierta en la clase predeterminada, y PersistentVolumeClaim creada m√°s tarde la usar√°: </p><br><pre> <code class="bash hljs">$ kubectl create -f \ https://raw.githubusercontent.com/selectel/kubernetes-examples/master/storageclasses/fast.ru-1b.yaml</code> </pre> <br><p>  Crear PersistentVolumeClaim y Pod: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/persistentvolumeclaims/my-pv-claim.yaml \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/pods/nginx-with-volume.yaml</code> </pre> <br><p>  Despu√©s de eso, se crear√° autom√°ticamente un disco en su proyecto que se conectar√° a uno de los nodos minion del cl√∫ster.  Cuando cae, el disco cambiar√° autom√°ticamente a otro nodo. </p><br><p>  Podemos ver el disco dentro del contenedor con Nginx: </p><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it nginx sh mount | grep <span class="hljs-string"><span class="hljs-string">"/var/www/html"</span></span> /dev/sdc on /var/www/html <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> ext4 (rw,seclabel,relatime,data=ordered) <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre> <br><p>  Puede conectar la unidad a la implementaci√≥n.  Un ejemplo correspondiente se puede encontrar en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">documentaci√≥n oficial</a> . </p><br><h2>  Panel de control de Kubernetes </h2><br><p>  Puede usar el tablero incorporado de Kubernetes para ver el estado de los objetos del cl√∫ster y su administraci√≥n. </p><br><p>  Para acceder a todas las funciones del panel, deber√° crear una cuenta con la funci√≥n de administrador en su cl√∫ster. </p><br><p>  Para hacer esto, necesitamos una descripci√≥n de la cuenta: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># admin-user.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system</span></span></code> </pre> <br><p>  Y descripci√≥n del rol: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cluster-admin.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system</span></span></code> </pre> <br><p>  Crea los objetos especificados: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/accounts/admin-user.yaml \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/clusterrolebindings/cluster-admin.yaml</code> </pre> <br><p>  A continuaci√≥n, deber√° averiguar el valor del token generado para esta cuenta. <br>  Para hacer esto, busque el objeto secreto correspondiente en el cl√∫ster: </p><br><pre> <code class="bash hljs">$ kubectl get secret --namespace=kube-system | grep <span class="hljs-string"><span class="hljs-string">"admin-user-token"</span></span> admin-user-token-bkfhb kubernetes.io/service-account-token 3 22m</code> </pre> <br><p>  Y mire el valor del token del Secreto encontrado con el nombre admin-user-token-bkfhb: </p><br><pre> <code class="bash hljs">$ kubectl describe secret admin-user-token-bkfhb --namespace=kube-system | grep <span class="hljs-string"><span class="hljs-string">"token:"</span></span> token: XXXXXX...</code> </pre> <br><p>  En respuesta, recibir√° el valor del token, gu√°rdelo, nos ser√° √∫til en el futuro. <br>  Para obtener detalles sobre el control de acceso para los objetos de Kubernetes, consulte la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">documentaci√≥n oficial</a> . </p><br><p>  En el caso de que haya creado un cl√∫ster a partir de una plantilla p√∫blica, Pod y Servicio ya existen en √©l para garantizar el funcionamiento del panel: </p><br><pre> <code class="bash hljs">$ kubectl get svc kubernetes-dashboard --namespace=kube-system 206ms Tue Jun 19 14:35:19 2018 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard ClusterIP 10.254.122.245 &lt;none&gt; 443/TCP 2d $ kubectl get pod --namespace=kube-system --selector k8s-app=kubernetes-dashboard 119ms Tue Jun 19 14:36:48 2018 NAME READY STATUS RESTARTS AGE kubernetes-dashboard-747575c864-jpxvt 1/1 Running 0 2d</code> </pre> <br><p>  Dado que el servicio es del tipo ClusterIP, estar√° disponible solo desde el propio cl√∫ster. <br>  Puede acceder al panel desde su computadora en funcionamiento con el archivo de configuraci√≥n del cl√∫ster utilizando el comando kubectl: </p><br><pre> <code class="bash hljs">$ kubectl proxy Starting to serve on 127.0.0.1:8001</code> </pre> <br><p>  Pruebe el proxy abriendo la direcci√≥n especificada en el navegador: </p><br><img src="https://habrastorage.org/webt/lc/zm/k1/lczmk1ud4tjatfu3lthvkrcu66e.png"><br><p>  Si ve una respuesta similar a la captura de pantalla, puede ir a la pantalla del panel de control con la siguiente direcci√≥n: </p><br><pre> <code class="bash hljs">http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</code> </pre> <br><p>  Al revisarlo, deber√≠a ver la pantalla de inicio de sesi√≥n en el panel: </p><br><img src="https://habrastorage.org/webt/60/fj/p5/60fjp5-2xjyn_p5mz-jh_ozwzxe.png"><br><p>  Deber√° especificar el token recibido anteriormente.  Despu√©s de iniciar sesi√≥n, puede usar el panel de control: </p><br><img src="https://habrastorage.org/webt/38/ry/dr/38rydrraxndpo0hyqhnm-k1nb_k.png"><br><p>  Puede conocer todas las caracter√≠sticas del panel de control en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener noreferrer">documentaci√≥n oficial</a> . </p><br><h2>  Supervisi√≥n de objetos de Kubernetes </h2><br><p>  Si usa la plantilla de cl√∫ster p√∫blico, ejecutar√° autom√°ticamente los componentes para recopilar y mostrar m√©tricas: Prometheus y Grafana. </p><br><p>  De manera similar al panel de control, ClusterIP se instala como el tipo de servicio; el acceso a √©l solo es posible desde el cl√∫ster o mediante el proxy kubectl.  Puede acceder a Grafana desde la computadora de su trabajo en la siguiente direcci√≥n: </p><br><pre> <code class="bash hljs">http://127.0.0.1:8001/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana:80</code> </pre> <br><img src="https://habrastorage.org/webt/p3/7e/go/p37egoksdoz8bvp8tsq2fgpsuru.png"><br><h2>  Conclusi√≥n </h2><br><p>  En este art√≠culo, examinamos los objetos de Kubernetes m√°s utilizados y observamos ejemplos de c√≥mo iniciar y administrar un cl√∫ster con OpenStack Magnum. </p><br><p>  En un futuro cercano, ser√° posible usar las √∫ltimas versiones de Kubernetes, y la administraci√≥n de cl√∫steres estar√° disponible a trav√©s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">del panel de control</a> . </p><br><p>  Estaremos encantados si utiliza nuestro servicio en modo de prueba y proporciona comentarios. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es416951/">https://habr.com/ru/post/es416951/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es416941/index.html">Teor√≠a de la felicidad. Introducci√≥n a la merfolog√≠a.</a></li>
<li><a href="../es416943/index.html">Materiales √∫tiles para dise√±ar interfaces de voz.</a></li>
<li><a href="../es416945/index.html">Como hicimos BelAZ. Parte 1 - Hierro</a></li>
<li><a href="../es416947/index.html">Juega antes de los Juegos Ol√≠mpicos: los deportes electr√≥nicos se vuelven oficiales</a></li>
<li><a href="../es416949/index.html">La actualizaci√≥n a gran escala del Sr. Steven para instalar una red de caza cuatro veces mayor se ha completado</a></li>
<li><a href="../es416953/index.html">Crea un sombreador de agua de dibujos animados para la web. Parte 1</a></li>
<li><a href="../es416955/index.html">Peque√±os trucos con Elasticsearch</a></li>
<li><a href="../es416957/index.html">¬øQu√© m√°quina l√°ser comprar? Revisi√≥n confiable de la m√°quina l√°ser Raylogic 11G</a></li>
<li><a href="../es416959/index.html">Apple presenta la nueva funci√≥n antirrobo de iOS</a></li>
<li><a href="../es416961/index.html">Resoluci√≥n autom√°tica de conflictos mediante transformaciones operativas.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>