<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§¶ üóº üîß Se crea la base para una teor√≠a generalizada de las redes neuronales. üë≥üèø üòØ üëåüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Las enormes capacidades de las redes neuronales son a veces comparables a su imprevisibilidad. Ahora los matem√°ticos comienzan a comprender c√≥mo la fo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Se crea la base para una teor√≠a generalizada de las redes neuronales.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442574/"><h3>  Las enormes capacidades de las redes neuronales son a veces comparables a su imprevisibilidad.  Ahora los matem√°ticos comienzan a comprender c√≥mo la forma de una red neuronal afecta su trabajo. </h3><br><br><img src="https://habrastorage.org/getpro/habr/post_images/856/cbb/518/856cbb5185fda3edec1e6c1096d9226b.jpg"><br><br>  Cuando dise√±amos un rascacielos, esperamos que al final cumpla con todas las especificaciones: que la torre pueda soportar tal peso, as√≠ como un terremoto de cierta fuerza. <br><br>  Sin embargo, una de las tecnolog√≠as m√°s importantes del mundo moderno, de hecho, dise√±amos a ciegas.  Jugamos con diferentes esquemas, diferentes configuraciones, pero hasta que comencemos una ejecuci√≥n de prueba del sistema, realmente no tenemos idea de qu√© puede hacer o d√≥nde se negar√° a funcionar. <br><a name="habracut"></a><br>  Se trata de la tecnolog√≠a de redes neuronales que subyace a los sistemas de inteligencia artificial modernos m√°s avanzados.  Las redes neuronales se est√°n moviendo gradualmente hacia las √°reas m√°s b√°sicas de la sociedad: determinan lo que aprendemos sobre el mundo a trav√©s de las noticias en las redes sociales, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ayudan a los</a> m√©dicos a hacer un diagn√≥stico e incluso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">afectan</a> si un criminal es enviado a prisi√≥n. <br><br>  Y "la mejor descripci√≥n de lo que sabemos es decir que pr√°cticamente no sabemos nada acerca de c√≥mo funcionan realmente las redes neuronales, y cu√°l deber√≠a ser la teor√≠a que las describe", dijo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Boris Ganin</a> , matem√°tico de la Universidad de Texas, y un especialista invitado en Facebook AI Research que estudia redes neuronales. <br><br>  Compara la situaci√≥n con el desarrollo de otra tecnolog√≠a revolucionaria: una m√°quina de vapor.  Inicialmente, las m√°quinas de vapor solo pod√≠an bombear agua.  Luego sirvieron como motores para locomotoras de vapor, y hoy las redes neuronales probablemente han alcanzado aproximadamente el mismo nivel.  Los cient√≠ficos y matem√°ticos desarrollaron una teor√≠a de la termodin√°mica que les permiti√≥ comprender qu√© est√° sucediendo exactamente dentro de cualquier motor.  Y al final, ese conocimiento nos trajo al espacio. <br><br>  "Al principio hubo grandes logros de ingenier√≠a, luego grandes trenes, y luego se necesit√≥ una comprensi√≥n te√≥rica para pasar de esto a los cohetes", dijo Ganin. <br><br>  En la creciente comunidad de desarrolladores de redes neuronales, hay un peque√±o grupo de investigadores con un sesgo matem√°tico que intenta crear una teor√≠a de redes neuronales que pueda explicar c√≥mo funcionan y garantizar que, despu√©s de crear una red neuronal de cierta configuraci√≥n, pueda realizar ciertas tareas. <br><br>  Si bien el trabajo se encuentra en una etapa temprana, pero durante el a√±o pasado, los investigadores ya han publicado varios art√≠culos cient√≠ficos que describen en detalle la relaci√≥n entre la forma y el funcionamiento de las redes neuronales.  El trabajo describe las redes neuronales en su totalidad, hasta sus cimientos.  Ella demuestra que mucho antes de confirmar la capacidad de las redes neuronales para conducir autom√≥viles, es necesario demostrar su capacidad para multiplicar n√∫meros. <br><br><h2>  La mejor receta para el cerebro. </h2><br>  Las redes neuronales se esfuerzan por imitar el cerebro humano, y una forma de describir su trabajo es decir que fusiona peque√±as abstracciones en otras m√°s grandes.  Desde este punto de vista, la complejidad de los pensamientos se mide por la cantidad de peque√±as abstracciones que subyacen en ellas y la cantidad de combinaciones de abstracciones de bajo nivel en abstracciones de alto nivel, en tareas como estudiar las diferencias entre perros y p√°jaros. <br><br>  "Si una persona aprende a reconocer a un perro, entonces aprende a reconocer algo peludo en cuatro patas", dijo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Maitra Ragu</a> , una estudiante graduada en ciencias de la computaci√≥n en la Universidad de Cornell, miembro del equipo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Google Brain</a> .  "Idealmente, nos gustar√≠a que nuestras redes neuronales hicieran algo similar". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a42/4d2/17d/a424d217dd912526caaa61b33e30e36f.jpg"><br>  <i>Maitra Ragu</i> <br><br>  La abstracci√≥n se origina en el cerebro humano de forma natural.  Las redes neuronales tienen que funcionar para esto.  Las redes neuronales, como el cerebro, est√°n formadas por bloques de construcci√≥n llamados "neuronas", conectadas entre s√≠ de varias maneras.  Al mismo tiempo, las neuronas de la red neuronal, aunque est√°n creadas a imagen de las neuronas cerebrales, no intentan imitarlas por completo.  Cada neurona puede representar un atributo o una combinaci√≥n de atributos que la red neuronal considera en cada nivel de abstracci√≥n. <br><br>  Los ingenieros pueden elegir entre muchas opciones para combinar estas neuronas.  Deben decidir cu√°ntas capas de neuronas debe tener una red neuronal (es decir, determinar su "profundidad").  Considere, por ejemplo, una red neuronal que reconoce im√°genes.  La imagen se incluye en la primera capa del sistema.  En la siguiente capa, la red puede tener neuronas que simplemente reconocen los bordes de la imagen.  La siguiente capa combina las l√≠neas y define las curvas.  El siguiente combina las curvas en formas y texturas, y el √∫ltimo procesa las formas y texturas para tomar una decisi√≥n sobre lo que est√° mirando: ¬°el mamut peludo! <br><br>  ‚ÄúLa idea es que cada capa combine varios aspectos de la anterior.  Un c√≠rculo es una curva en muchos lugares, una curva es una l√≠nea en muchos lugares ‚Äù, dice <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">David Rolnik</a> , matem√°tico de la Universidad de Pennsylvania. <br><br>  Los ingenieros tambi√©n tienen que elegir el "ancho" de cada capa, que corresponde al n√∫mero de caracter√≠sticas diferentes que la red considera en cada nivel de abstracci√≥n.  En el caso del reconocimiento de im√°genes, el ancho de las capas corresponder√° al n√∫mero de tipos de l√≠neas, curvas o formas que la red neuronal considerar√° en cada nivel. <br><br>  Adem√°s de la profundidad y el ancho de la red neuronal, hay una opci√≥n del m√©todo para conectar las neuronas en las capas y entre ellas, y una opci√≥n de pesos para cada una de las conexiones. <br><br>  Si planea completar una tarea espec√≠fica, ¬øc√≥mo sabe qu√© arquitectura de red neuronal puede realizarla de la mejor manera?  Hay reglas de muestra bastante generales.  Para problemas con el reconocimiento de im√°genes, los programadores suelen usar redes neuronales "convolucionales", el sistema de enlaces entre capas en el que se repite de capa a capa.  Para procesar un lenguaje natural (reconocimiento de voz o generaci√≥n de lenguaje), los programadores han descubierto que las redes neuronales recurrentes son las m√°s adecuadas.  Las neuronas en ellos pueden conectarse con neuronas no solo de las capas vecinas. <br><br>  Sin embargo, fuera de estos principios generales, los programadores en su mayor√≠a deben confiar en la evidencia experimental: simplemente ejecutan 1,000 redes neuronales diferentes y ven cu√°l funciona mejor. <br><br>  "En la pr√°ctica, estas elecciones a menudo se toman por prueba y error", dijo Ganin.  "Esta es una forma bastante complicada, ya que hay infinitas elecciones y nadie sabe cu√°l ser√° la mejor". <br><br>  La mejor opci√≥n ser√≠a confiar menos en el m√©todo de prueba y error, y m√°s en la comprensi√≥n preexistente de lo que puede brindarle una arquitectura de red neuronal en particular.  Varios art√≠culos cient√≠ficos publicados recientemente han avanzado esta √°rea en esta direcci√≥n. <br><br>  ‚ÄúEste trabajo tiene como objetivo crear algo as√≠ como un libro de recetas para dise√±ar una red neuronal adecuada.  Si sabes lo que quieres lograr con √©l, puedes elegir la receta correcta ‚Äù, dijo Rolnik. <br><br><h2>  Lazo oveja roja </h2><br>  Una de las primeras garant√≠as te√≥ricas de la arquitectura de redes neuronales apareci√≥ hace tres d√©cadas.  En 1989, un inform√°tico demostr√≥ que si una red neuronal tiene solo una capa computacional, en la que puede haber un n√∫mero ilimitado de neuronas y un n√∫mero ilimitado de conexiones entre ellas, entonces la red neuronal podr√° realizar cualquier tarea. <br><br>  Esta fue una declaraci√≥n m√°s o menos general, que result√≥ ser bastante intuitiva y no particularmente √∫til.  Esto es lo mismo que decir que si puede definir un n√∫mero ilimitado de l√≠neas en una imagen, puede distinguir todos los objetos con una sola capa.  En principio, esto puede cumplirse, pero trate de ponerlo en pr√°ctica. <br><br>  Hoy en d√≠a, los investigadores llaman a estas redes anchas y planas "expresivas" porque, en teor√≠a, pueden cubrir un conjunto m√°s rico de relaciones entre los posibles datos de entrada (como una imagen) y salida (como la descripci√≥n de una imagen).  Al mismo tiempo, es extremadamente dif√≠cil entrenar estas redes, es decir, es pr√°cticamente imposible hacer que realmente entreguen estos datos.  Tambi√©n requieren m√°s potencia inform√°tica que cualquier computadora. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1fa/99c/58e/1fa99c58e5bf806edb24c0c47a735a75.jpg"><br>  <i>Boris Ganin</i> <br><br>  Recientemente, los investigadores han estado tratando de comprender hasta qu√© punto uno puede llevar las redes neuronales yendo en la direcci√≥n opuesta, haci√©ndolas m√°s estrechas (menos neuronas por capa) y m√°s profundas (m√°s capas).  Es posible que pueda reconocer solo 100 l√≠neas diferentes, pero con las conexiones necesarias para convertir 100 de estas l√≠neas en 50 curvas que se pueden combinar en 10 formas diferentes, puede obtener todos los bloques de construcci√≥n necesarios para reconocer la mayor√≠a de los objetos. <br><br>  En el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">trabajo</a> que completaron el a√±o pasado, Rolnik y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Max Tegmark</a> del MIT demostraron que al aumentar la profundidad y disminuir el ancho, es posible realizar las mismas tareas con un n√∫mero exponencialmente menor de neuronas.  Demostraron que si la situaci√≥n que simula tiene 100 variables de entrada, puede obtener la misma confiabilidad usando <sup>2,100</sup> neuronas en una capa o 2,10 neuronas en dos capas.  Descubrieron que hab√≠a ventajas en tomar partes peque√±as y combinarlas en niveles m√°s altos de abstracci√≥n, en lugar de tratar de cubrir todos los niveles de abstracci√≥n a la vez. <br><br>  "El concepto de la profundidad de la red neuronal est√° conectado con la posibilidad de expresar algo complejo mediante la realizaci√≥n de muchos pasos simples", dijo Rolnik.  "Parece una l√≠nea de montaje". <br><br>  Rolnik y Tegmark demostraron la utilidad de la profundidad al obligar a las redes neuronales a realizar una tarea simple: multiplicar funciones polin√≥micas.  (Estas son ecuaciones con variables elevadas a grados naturales, por ejemplo, y = x <sup>3</sup> + 1).  Entrenaron las redes, mostr√°ndoles ejemplos de ecuaciones y los resultados de su multiplicaci√≥n.  Luego le dijeron a las redes neuronales que calcularan el resultado de la multiplicaci√≥n de ecuaciones que no hab√≠an visto antes.  Las redes neuronales m√°s profundas aprendieron c√≥mo hacer esto con mucho menos neuronas que las peque√±as. <br><br>  Y aunque es poco probable que la multiplicaci√≥n altere nuestro mundo, Rolnik dice que en el trabajo se describi√≥ una idea importante: "Si una red neuronal poco profunda ni siquiera puede multiplicarse, no debes confiar en ella con otra cosa". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/58d/189/b73/58d189b7307053cde31c537aa8bc5d33.jpg"><br>  <i>David Rolnik</i> <br><br>  Otros investigadores est√°n investigando el tema del ancho m√≠nimo suficiente.  A finales de septiembre, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Jesse Johnson</a> , anteriormente matem√°tico de la Universidad de Oklahoma y ahora investigador que trabaja para la compa√±√≠a farmac√©utica Sanofi, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">demostr√≥</a> que en alg√∫n momento ninguna profundidad podr√≠a compensar la falta de ancho. <br><br>  Para que esto tenga sentido, imagine los corderos en el campo, pero que sean corderos del punk rock: la lana de cada uno de ellos estar√° pintada en uno de varios colores.  La red neuronal debe dibujar un borde alrededor de todas las ovejas del mismo color.  En esencia, esta tarea es similar a la clasificaci√≥n de im√°genes: una red neuronal tiene un conjunto de im√°genes (que representa como puntos en un espacio multidimensional) y necesita agrupar otras similares. <br><br>  Johnson demostr√≥ que una red neuronal no har√° frente a esta tarea si el ancho de las capas es menor o igual a la cantidad de datos de entrada.  Cada una de nuestras ovejas se puede describir con dos datos de entrada: las coordenadas de su ubicaci√≥n en el campo, x e y.  Luego, la red neuronal marca cada oveja con color y dibuja un borde alrededor de las ovejas del mismo color.  En este caso, para resolver el problema necesita al menos tres neuronas por capa. <br><br>  M√°s espec√≠ficamente, Johnson demostr√≥ que si la relaci√≥n del ancho al n√∫mero de variables no es suficiente, la red neuronal no podr√° dibujar bucles cerrados, y una red neuronal tendr√≠a que dibujar ese bucle si, por ejemplo, todas las ovejas rojas se hubieran acumulado en el medio del pasto.  "Si ninguna de las capas es m√°s gruesa que la cantidad de mediciones de entrada, la funci√≥n no puede crear algunas formas, independientemente de la cantidad de capas", dijo Johnson. <br><br>  Tal trabajo comienza a construir el n√∫cleo de la teor√≠a de las redes neuronales.  Hasta ahora, los investigadores solo pueden hacer las declaraciones m√°s simples con respecto a la relaci√≥n entre arquitectura y funcionalidad, y estas declaraciones son muy pocas en comparaci√≥n con la cantidad de tareas resueltas por las redes neuronales. <br><br>  Entonces, aunque la teor√≠a de las redes neuronales no podr√° cambiar el proceso de su dise√±o en el futuro cercano, se est√°n creando planos para una nueva teor√≠a de c√≥mo se entrenan las computadoras, y sus consecuencias ser√°n a√∫n m√°s fuertes que una persona que salga al espacio. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/442574/">https://habr.com/ru/post/442574/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../442562/index.html">C√≥mo enfriar equipos en un centro de datos: tres nuevas tecnolog√≠as</a></li>
<li><a href="../442566/index.html">Igual que en la luna: ingenier√≠a inversa de un m√≥dulo de amplificador operacional h√≠brido</a></li>
<li><a href="../442568/index.html">Semana de la seguridad 10: Vulnerabilidades del controlador NVIDIA</a></li>
<li><a href="../442570/index.html">Sigma gobierna. Arte o nuevo est√°ndar para SOC</a></li>
<li><a href="../442572/index.html">Usar la herramienta de configuraci√≥n de Datapath</a></li>
<li><a href="../442576/index.html">Larga vida a los overclockers: c√≥mo la refrigeraci√≥n l√≠quida comenz√≥ a dominar en los centros de datos</a></li>
<li><a href="../442578/index.html">Lanzamiento de Linux 5.0</a></li>
<li><a href="../442580/index.html">Ingenier√≠a inversa de formato binario utilizando archivos Korg .SNG como ejemplo</a></li>
<li><a href="../442582/index.html">C√≥mo tratamos de mobbing</a></li>
<li><a href="../442584/index.html">Documentos sobre el edificio: peque√±as alegr√≠as de la automatizaci√≥n en el ejemplo de la Torre Oscura</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>