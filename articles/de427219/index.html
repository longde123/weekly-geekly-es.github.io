<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåπ üé≤ ‚è±Ô∏è Nutzloses, nicht blockierendes Messaging in MPI: Lichtanalyse und Tutorial f√ºr diejenigen, die ein wenig "im Thema" sind üí± üõ•Ô∏è üèØ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Zuletzt musste ich eine weitere triviale Trainingsaufgabe von meinem Lehrer l√∂sen. Als ich es jedoch l√∂ste, gelang es mir, die Aufmerksamkeit auf Ding...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nutzloses, nicht blockierendes Messaging in MPI: Lichtanalyse und Tutorial f√ºr diejenigen, die ein wenig "im Thema" sind</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427219/">  Zuletzt musste ich eine weitere triviale Trainingsaufgabe von meinem Lehrer l√∂sen.  Als ich es jedoch l√∂ste, gelang es mir, die Aufmerksamkeit auf Dinge zu lenken, an die ich vorher √ºberhaupt nicht gedacht hatte, vielleicht haben Sie auch nicht dar√ºber nachgedacht.  Dieser Artikel wird eher f√ºr Studenten und f√ºr alle n√ºtzlich sein, die ihre Reise in die Welt der parallelen Programmierung mit MPI beginnen. <br><br><img src="https://habrastorage.org/webt/v0/ik/2-/v0ik2-rxhe1gexlrsumqpwhslbu.jpeg"><br><br><h2>  Unser "Gegeben:" </h2><br>  Die Essenz unserer im Wesentlichen rechnerischen Aufgabe besteht also darin, zu vergleichen, wie oft ein Programm, das nicht blockierende, verz√∂gerte Punkt-zu-Punkt-√úbertragungen verwendet, schneller ist als das Programm, das blockierende Punkt-zu-Punkt-√úbertragungen verwendet.  Wir werden Messungen f√ºr Eingabearrays der Dimensionen 64, 256, 1024, 4096, 8192, 16384, 65536, 262144, 1048576, 4194304, 16777216, 33554432 durchf√ºhren.  Standardm√§√üig wird vorgeschlagen, es durch vier Prozesse zu l√∂sen.  Und hier ist in der Tat, was wir betrachten werden: <br><br><a name="habracut"></a><img src="https://habrastorage.org/webt/rt/vc/vo/rtvcvob3gfonidkax7qtskxfbc0.png"><cut></cut><br><br>  Am Ausgang sollten wir drei Vektoren erhalten: Y1, Y2 und Y3, die der Nullprozess sammelt.  Ich werde das Ganze auf meinem System testen, das auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem Intel-Prozessor</a> mit 16 GB RAM basiert.  F√ºr die Entwicklung von Programmen verwenden wir die Implementierung des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI-</a> Standards <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aus Microsoft Version 9.0.1</a> (zum Zeitpunkt des Schreibens ist dies relevant), Visual Studio Community 2017 und nicht Fortran. <br><br><h2>  Materiel </h2><br>  Ich m√∂chte nicht im Detail beschreiben, wie die verwendeten MPI-Funktionen funktionieren. Sie k√∂nnen sich jederzeit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Dokumentation dazu ansehen</a> , daher werde ich nur einen kurzen √úberblick dar√ºber geben, was wir verwenden werden. <br><br><h4>  Austausch blockieren </h4><br>  <b>Zum Blockieren von Punkt-zu-Punkt-Nachrichten verwenden wir die folgenden Funktionen:</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Send</a> - implementiert das Blockieren des Sendens von Nachrichten, d. H.  Nach dem Aufrufen der Funktion wird der Prozess blockiert, bis die an ihn gesendeten Daten aus seinem Speicher in den internen MPI-Systempuffer geschrieben wurden. Danach arbeitet der Prozess weiter. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Recv</a> - f√ºhrt einen blockierenden Nachrichtenempfang durch, d.h.  Nach dem Aufruf der Funktion wird der Prozess blockiert, bis Daten aus dem Sendeprozess eintreffen und diese Daten von der MPI-Umgebung vollst√§ndig in den Puffer des Empfangsprozesses geschrieben werden. <br><br><h4>  Aufgeschobener nicht blockierender Austausch </h4><br>  <b>F√ºr verz√∂gertes, nicht blockierendes Punkt-zu-Punkt-Messaging verwenden wir die folgenden Funktionen:</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Send_init</a> - bereitet im Hintergrund die Umgebung f√ºr das Senden von Daten vor, die in Zukunft auftreten werden, und keine Sperren; <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Recv_init</a> - Diese Funktion funktioniert √§hnlich wie die vorherige, nur diesmal, um Daten zu empfangen. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Start</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Startet</a> den Prozess des Empfangens oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sendens</a> einer Nachricht und wird auch im Hintergrund von a.k.a.  ohne zu blockieren; <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Wait</a> - wird verwendet, um zu pr√ºfen und gegebenenfalls auf den Abschluss des Sendens oder Empfangens einer Nachricht zu warten, blockiert jedoch nur den Prozess, falls erforderlich (wenn die Daten "nicht gesendet" oder "nicht empfangen" sind).  Ein Prozess m√∂chte beispielsweise Daten verwenden, die ihn noch nicht erreicht haben - nicht gut. Daher f√ºgen wir MPI_Wait vor der Stelle ein, an der diese Daten ben√∂tigt werden (wir f√ºgen sie auch dann ein, wenn lediglich die Gefahr einer Datenbesch√§digung besteht).  Ein weiteres Beispiel ist, dass der Prozess die Hintergrunddaten√ºbertragung gestartet hat und nach dem Start der Daten√ºbertragung sofort begonnen hat, diese Daten irgendwie zu √§ndern - nicht gut. Deshalb f√ºgen wir MPI_Wait vor der Stelle im Programm ein, an der diese Daten ge√§ndert werden sollen (hier f√ºgen wir sie auch ein, selbst wenn Es besteht lediglich die Gefahr einer Datenkorruption. <br><br>  Somit ist <i>semantisch die</i> Reihenfolge der Anrufe mit einem verz√∂gerten nicht blockierenden Austausch wie folgt: <br><br><ol><li>  MPI_Send_init / MPI_Recv_init - Vorbereitung der Umgebung f√ºr den Empfang oder das Senden </li><li>  MPI_Start - Startet den Empfangs- / Sendevorgang </li><li>  MPI_Wait - Wir nennen das Risiko einer Besch√§digung (einschlie√ülich "Unterschreiben" und "Unterberichterstattung") von gesendeten oder empfangenen Daten </li></ol><br>  Ich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">habe</a> in meinen Testprogrammen auch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Startall</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Waitall</a> verwendet. Ihre Bedeutung ist im Grunde dieselbe wie bei MPI_Start bzw. MPI_Wait, nur dass sie mit mehreren Paketen und / oder √úbertragungen arbeiten.  Dies ist jedoch nicht die gesamte Liste der Start- und Wartefunktionen. Es gibt mehrere weitere Funktionen zur √úberpr√ºfung der Vollst√§ndigkeit von Vorg√§ngen. <br><br><h2>  Prozess√ºbergreifende Architektur </h2><br>  Aus Gr√ºnden der √úbersichtlichkeit erstellen wir ein Diagramm zur Durchf√ºhrung von Berechnungen nach vier Prozessen.  In diesem Fall sollte versucht werden, alle Vektorarithmetikoperationen relativ gleichm√§√üig √ºber die Prozesse zu verteilen.  Folgendes habe ich bekommen: <br><br><img src="https://habrastorage.org/webt/bq/_b/q4/bq_bq43yrgkgjayi8p5uy0g9ckk.png"><br><br>  Sehen Sie diese Arrays T0-T2?  Dies sind Puffer zum Speichern von Zwischenergebnissen von Operationen.  In einem Diagramm befindet sich beim Senden von Nachrichten von einem Prozess zu einem anderen am Anfang des Pfeils der Name des Arrays, dessen Daten √ºbertragen werden, und am Ende des Pfeils das Array, das diese Daten empf√§ngt. <br><br>  Wann haben wir endlich die Fragen beantwortet: <br><br><ol><li>  Was f√ºr ein Problem l√∂sen wir? </li><li>  Mit welchen Tools werden wir es l√∂sen? </li><li>  Wie werden wir es l√∂sen? </li></ol><br>  Es bleibt nur zu l√∂sen ... <br><br><h2>  Unsere "L√∂sung": </h2><br>  Als n√§chstes werde ich die Codes der beiden oben diskutierten Programme vorstellen, aber zun√§chst werde ich einige weitere Erkl√§rungen geben, was und wie. <br><br>  Ich habe alle vektorarithmetischen Operationen in separaten Prozeduren (add, sub, mul, div) ausgef√ºhrt, um die Lesbarkeit des Codes zu verbessern.  Alle Eingabearrays werden gem√§√ü den Formeln initialisiert, die ich <i>fast</i> zuf√§llig angegeben habe.  Da der Nullprozess die Arbeitsergebnisse aller anderen Prozesse sammelt, arbeitet er am l√§ngsten. Daher ist es logisch, die Arbeitszeit gleich der Laufzeit des Programms zu betrachten (wie wir uns erinnern, sind wir interessiert an: Arithmetik + Messaging) im ersten und zweiten Fall.  Wir werden die Zeitintervalle mit der Funktion <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Wtime</a> messen und gleichzeitig habe ich beschlossen, die Aufl√∂sung der Uhren, die ich dort habe, mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MPI_Wtick anzuzeigen</a> (irgendwo in meiner Seele hoffe ich, dass sie in mein unver√§nderliches TSC passen. In diesem Fall bin ich sogar bereit, ihnen den Fehler zu verzeihen verbunden mit der Zeit, zu der die Funktion MPI_Wtime genannt wurde).  Also werden wir alles zusammenstellen, was ich oben geschrieben habe, und gem√§√ü der Grafik werden wir endlich diese Programme entwickeln (und nat√ºrlich auch debuggen). <br><br><hr><br>  Wen interessiert es, den Code zu sehen: <br><br><div class="spoiler">  <b class="spoiler_title">Programm mit blockierenden Daten√ºbertragungen</b> <div class="spoiler_text"><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;fstream&gt; #include &lt;mpi.h&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main(int argc, char **argv) { if (argc &lt; 2) { return 1; } int n = atoi(argv[1]); int rank; double start_time, end_time; MPI_Status status; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; double *D = new double[n]; double *E = new double[n]; double *G = new double[n]; double *T0 = new double[n]; double *T1 = new double[n]; double *T2 = new double[n]; for (int i = 0; i &lt; n; i++) { A[i] = double (2 * i + 1); B[i] = double(2 * i); C[i] = double(0.003 * (i + 1)); D[i] = A[i] * 0.001; E[i] = B[i]; G[i] = C[i]; } cout.setf(ios::fixed); cout &lt;&lt; fixed &lt;&lt; setprecision(9); MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); if (rank == 0) { start_time = MPI_Wtime(); sub(A, B, T0, n); MPI_Send(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); div(T0, G, T1, n); MPI_Recv(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); add(T1, T2, T0, n); mul(T0, T1, T2, n); MPI_Recv(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); add(T0, T2, T1, n); MPI_Recv(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); MPI_Recv(T2, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); end_time = MPI_Wtime(); cout &lt;&lt; "Clock resolution: " &lt;&lt; MPI_Wtick() &lt;&lt; " secs" &lt;&lt; endl; cout &lt;&lt; "Thread " &lt;&lt; rank &lt;&lt; " execution time: " &lt;&lt; end_time - start_time &lt;&lt; endl; } if (rank == 1) { add(C, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); mul(T1, G, T2, n); add(T2, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); sub(T1, T0, T2, n); MPI_Recv(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); add(T0, T2, T1, n); MPI_Send(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); } if (rank == 2) { mul(C, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); MPI_Recv(T2, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); MPI_Send(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); add(T1, T2, T0, n); mul(T0, G, T1, n); MPI_Recv(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); mul(T1, T2, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;status); mul(T0, T1, T2, n); MPI_Send(T2, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); } if (rank == 3) { mul(E, D, T0, n); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); sub(T0, B, T1, n); mul(T1, T1, T2, n); sub(T1, G, T0, n); mul(T0, T2, T1, n); MPI_Send(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); } MPI_Finalize(); delete[] A; delete[] B; delete[] C; delete[] D; delete[] E; delete[] G; delete[] T0; delete[] T1; delete[] T2; return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Programm mit verz√∂gerten nicht blockierenden Daten√ºbertragungen</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;fstream&gt; #include &lt;mpi.h&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main(int argc, char **argv) { if (argc &lt; 2) { return 1; } int n = atoi(argv[1]); int rank; double start_time, end_time; MPI_Request request[7]; MPI_Status statuses[4]; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; double *D = new double[n]; double *E = new double[n]; double *G = new double[n]; double *T0 = new double[n]; double *T1 = new double[n]; double *T2 = new double[n]; for (int i = 0; i &lt; n; i++) { A[i] = double(2 * i + 1); B[i] = double(2 * i); C[i] = double(0.003 * (i + 1)); D[i] = A[i] * 0.001; E[i] = B[i]; G[i] = C[i]; } cout.setf(ios::fixed); cout &lt;&lt; fixed &lt;&lt; setprecision(9); MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); if (rank == 0) { start_time = MPI_Wtime(); MPI_Send_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Send_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[6]);// MPI_Start(&amp;request[2]); sub(A, B, T0, n); MPI_Startall(2, &amp;request[0]); div(T0, G, T1, n); MPI_Waitall(3, &amp;request[0], statuses); add(T1, T2, T0, n); mul(T0, T1, T2, n); MPI_Startall(2, &amp;request[3]); MPI_Wait(&amp;request[3], &amp;statuses[0]); add(T0, T2, T1, n); MPI_Startall(2, &amp;request[5]); MPI_Wait(&amp;request[4], &amp;statuses[0]); MPI_Waitall(2, &amp;request[5], statuses); end_time = MPI_Wtime(); cout &lt;&lt; "Clock resolution: " &lt;&lt; MPI_Wtick() &lt;&lt; " secs" &lt;&lt; endl; cout &lt;&lt; "Thread " &lt;&lt; rank &lt;&lt; " execution time: " &lt;&lt; end_time - start_time &lt;&lt; endl; } if (rank == 1) { MPI_Recv_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Send_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Recv_init(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Send_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Start(&amp;request[0]); add(C, C, T0, n); MPI_Start(&amp;request[1]); MPI_Wait(&amp;request[0], &amp;statuses[0]); mul(T1, G, T2, n); MPI_Start(&amp;request[2]); MPI_Wait(&amp;request[1], &amp;statuses[0]); add(T2, C, T0, n); MPI_Start(&amp;request[3]); MPI_Wait(&amp;request[2], &amp;statuses[0]); sub(T1, T0, T2, n); MPI_Wait(&amp;request[3], &amp;statuses[0]); MPI_Start(&amp;request[4]); MPI_Wait(&amp;request[4], &amp;statuses[0]); add(T0, T2, T1, n); MPI_Start(&amp;request[5]); MPI_Wait(&amp;request[5], &amp;statuses[0]); } if (rank == 2) { MPI_Recv_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Send_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Send_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Recv_init(T1, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Send_init(T2, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[6]);// MPI_Startall(2, &amp;request[0]); mul(C, C, T0, n); MPI_Startall(2, &amp;request[2]); MPI_Waitall(4, &amp;request[0], statuses); add(T1, T2, T0, n); MPI_Start(&amp;request[4]); mul(T0, G, T1, n); MPI_Wait(&amp;request[4], &amp;statuses[0]); mul(T1, T2, T0, n); MPI_Start(&amp;request[5]); MPI_Wait(&amp;request[5], &amp;statuses[0]); mul(T0, T1, T2, n); MPI_Start(&amp;request[6]); MPI_Wait(&amp;request[6], &amp;statuses[0]); } if (rank == 3) { MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[0]); MPI_Send_init(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[1]); mul(E, D, T0, n); MPI_Start(&amp;request[0]); sub(T0, B, T1, n); mul(T1, T1, T2, n); MPI_Wait(&amp;request[0], &amp;statuses[0]); sub(T1, G, T0, n); mul(T0, T2, T1, n); MPI_Start(&amp;request[1]); MPI_Wait(&amp;request[1], &amp;statuses[0]); } MPI_Finalize(); delete[] A; delete[] B; delete[] C; delete[] D; delete[] E; delete[] G; delete[] T0; delete[] T1; delete[] T2; return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre></div></div><br><hr><br><h2>  Testen und Analysieren </h2><br>  Lassen Sie uns unsere Programme f√ºr Arrays unterschiedlicher Gr√∂√üe ausf√ºhren und sehen, was passiert.  Die Testergebnisse sind in der Tabelle zusammengefasst, in deren letzter Spalte wir den Beschleunigungskoeffizienten berechnen und schreiben, den wir wie folgt definieren: K <sub>accele</sub> = T <sub>ex.</sub>  <sub>nicht blockieren.</sub>  / T- <sub>Block.</sub> <br><br><img src="https://habrastorage.org/webt/ba/hg/zv/bahgzvsgz67vnkfsji-swsyy_cg.png"><br><br>  Wenn Sie sich diese Tabelle etwas genauer als gew√∂hnlich ansehen, werden Sie feststellen, dass mit zunehmender Anzahl verarbeiteter Elemente der Beschleunigungskoeffizient folgenderma√üen abnimmt: <br><br><img src="https://habrastorage.org/webt/qq/to/hv/qqtohvv7azbc05r6x4mwtfbbxfe.png"><br><br>  Versuchen wir herauszufinden, was los ist.  Zu diesem Zweck schlage ich vor, ein kleines Testprogramm zu schreiben, das die Zeit jeder Vektorarithmetikoperation misst und die Ergebnisse sorgf√§ltig auf eine normale Textdatei reduziert. <br><br><hr><br>  Hier in der Tat das Programm selbst: <br><br><div class="spoiler">  <b class="spoiler_title">Zeitmessung</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;Windows.h&gt; #include &lt;fstream&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main() { struct res { double add; double sub; double mul; double div; }; int i, j, k, n, loop; LARGE_INTEGER start_time, end_time, freq; ofstream fout("test_measuring.txt"); int N[12] = { 64, 256, 1024, 4096, 8192, 16384, 65536, 262144, 1048576, 4194304, 16777216, 33554432 }; SetConsoleOutputCP(1251); cout &lt;&lt; "   loop: "; cin &gt;&gt; loop; fout &lt;&lt; setiosflags(ios::fixed) &lt;&lt; setiosflags(ios::right) &lt;&lt; setprecision(9); fout &lt;&lt; " : " &lt;&lt; loop &lt;&lt; endl; fout &lt;&lt; setw(10) &lt;&lt; "\n " &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; setw(30) &lt;&lt; ".  (c)" &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; endl; QueryPerformanceFrequency(&amp;freq); cout &lt;&lt; "\n : " &lt;&lt; freq.QuadPart &lt;&lt; " " &lt;&lt; endl; for (k = 0; k &lt; sizeof(N) / sizeof(int); k++) { res output = {}; n = N[k]; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; for (i = 0; i &lt; n; i++) { A[i] = 2.0 * i; B[i] = 2.0 * i + 1; C[i] = 0; } for (j = 0; j &lt; loop; j++) { QueryPerformanceCounter(&amp;start_time); add(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.add += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); sub(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.sub += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); mul(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.mul += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); div(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.div += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); } fout &lt;&lt; setw(10) &lt;&lt; n &lt;&lt; setw(30) &lt;&lt; output.add / loop &lt;&lt; setw(30) &lt;&lt; output.sub / loop &lt;&lt; setw(30) &lt;&lt; output.mul / loop &lt;&lt; setw(30) &lt;&lt; output.div / loop &lt;&lt; endl; delete[] A; delete[] B; delete[] C; } fout.close(); cout &lt;&lt; endl; system("pause"); return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre></div></div><br><hr><br>  Beim Start werden Sie aufgefordert, die Anzahl der Messzyklen einzugeben, die ich f√ºr 10.000 Zyklen getestet habe.  Am Ausgang erhalten wir das durchschnittliche Ergebnis f√ºr jede Operation: <br><br><img src="https://habrastorage.org/webt/iz/0g/bs/iz0gbs8ilynlmbchxtga_0at61s.png"><br><br>  Um die Zeit zu messen, habe ich den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√ºbergeordneten QueryPerformanceCounter verwendet</a> .  Ich empfehle dringend, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diese FAQ zu</a> lesen, damit die meisten Fragen zur Zeitmessung mit dieser Funktion von selbst verschwinden.  Nach meinen Beobachtungen klammert es sich an die TSC (aber theoretisch mag es nicht daf√ºr sein), gibt aber laut Hilfe die aktuelle Anzahl der Ticks des Z√§hlers zur√ºck.  Tatsache ist jedoch, dass mein Z√§hler das Zeitintervall von 32 ns physisch nicht messen kann (siehe die erste Zeile der Ergebnistabelle).  Dieses Ergebnis ist darauf zur√ºckzuf√ºhren, dass zwischen den beiden Aufrufen des QueryPerformanceCounter 0 Ticks oder 1 Ticks vergehen. F√ºr die erste Zeile in der Tabelle k√∂nnen wir nur den Schluss ziehen, dass ungef√§hr ein Drittel der 10.000 Ergebnisse 1 Tick entspricht.  <i>Die Daten in dieser Tabelle f√ºr 64, 256 und sogar f√ºr 1024 Elemente sind also ungef√§hr.</i>  Lassen Sie uns nun eines der Programme √∂ffnen und berechnen, wie viele Gesamtoperationen jedes Typs insgesamt auftreten. Traditionell werden wir alles gem√§√ü der folgenden Tabelle "verteilen": <br><br><img src="https://habrastorage.org/webt/et/vo/w2/etvow2fj-vxgtusc9aez__9egxq.png"><br><br>  Schlie√ülich kennen wir die Zeit jeder Vektorarithmetikoperation und wie viel Zeit sie in unserem Programm hat. Versuchen Sie herauszufinden, wie viel Zeit f√ºr diese Operationen in parallelen Programmen aufgewendet wird und wie viel Zeit f√ºr das Blockieren und den verz√∂gerten nicht blockierenden Datenaustausch zwischen Prozessen aufgewendet wird. Aus Gr√ºnden der Klarheit werden wir dies auf reduzieren Tabelle: <br><br><img src="https://habrastorage.org/webt/no/k2/-0/nok2-0p1kpw8g1ahveqog4q9lzi.png"><br><br>  Basierend auf den Ergebnissen der Daten erstellen wir ein Diagramm mit drei Funktionen: Die erste beschreibt die √Ñnderung der Zeit, die zum Blockieren von √úbertragungen zwischen Prozessen aufgewendet wird, anhand der Anzahl der Elemente von Arrays, die zweite beschreibt die √Ñnderung der Zeit, die f√ºr verz√∂gerte nicht blockierende √úbertragungen zwischen Prozessen aufgewendet wird, anhand der Anzahl der Elemente in Arrays, und die dritte beschreibt die √Ñnderung der Zeit. ausgegeben f√ºr arithmetische Operationen, aus der Anzahl der Elemente von Arrays: <br><br><img src="https://habrastorage.org/webt/4e/w6/5h/4ew65htbb-s3vh7anlo0txafxpg.png"><br><br>  Wie Sie bereits bemerkt haben, ist die vertikale Skalierung des Diagramms logarithmisch, es ist ein notwendiges Ma√ü, weil  Die Streuung der Zeiten ist zu gro√ü und auf einem normalen Diagramm w√§re nichts sichtbar gewesen.  Achten Sie auf die Funktion der Abh√§ngigkeit der f√ºr die Arithmetik aufgewendeten Zeit von der Anzahl der Elemente, sie √ºberholt die beiden anderen Funktionen sicher um etwa 1 Million Elemente.  Die Sache ist, dass es im Unendlichen schneller w√§chst als seine beiden Gegner.  Daher wird mit zunehmender Anzahl verarbeiteter Elemente die Laufzeit von Programmen immer mehr durch Arithmetik als durch √úbertragungen bestimmt.  Angenommen, Sie haben die Anzahl der √úbertragungen zwischen Prozessen erh√∂ht. Konzeptionell sehen Sie nur, dass der Moment, in dem die Rechenfunktion die beiden anderen √ºberholt, sp√§ter eintritt. <br><br><h2>  Zusammenfassung </h2><br>  Wenn Sie also die L√§nge der Arrays weiter erh√∂hen, werden Sie zu dem Schluss kommen, dass ein Programm mit verz√∂gerten nicht blockierenden √úbertragungen nur geringf√ºgig schneller ist als das Programm, das den blockierenden Austausch verwendet.  Wenn Sie die L√§nge der Arrays auf unendlich einstellen (oder nur sehr lange Arrays verwenden), wird die Betriebszeit Ihres Programms durch Berechnungen zu 100% bestimmt, und der Beschleunigungskoeffizient tendiert sicher zu 1. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de427219/">https://habr.com/ru/post/de427219/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de427207/index.html">Rockstar-Mitarbeiter setzen sich nach Kritik an 100-Stunden-Arbeitswochen f√ºr das Unternehmen ein</a></li>
<li><a href="../de427209/index.html">GeoPuzzle - mach die Welt St√ºck f√ºr St√ºck</a></li>
<li><a href="../de427211/index.html">Electron ist ein Flash f√ºr den Desktop</a></li>
<li><a href="../de427215/index.html">Microservices m√ºssen wachsen, nicht damit beginnen</a></li>
<li><a href="../de427217/index.html">Leistungsanalyse von WSGI-Servern: Teil Zwei</a></li>
<li><a href="../de427221/index.html">Was ich auf dem Weg zu meinem Traum von k√ºnstlicher Intelligenz realisiert habe</a></li>
<li><a href="../de427223/index.html">Was ist die Verantwortung des Hauptentwicklers</a></li>
<li><a href="../de427225/index.html">Oracle Database 18c XE ver√∂ffentlicht</a></li>
<li><a href="../de427227/index.html">Wie wir ein Brettspiel mit Fernbedienung gemacht haben - Teil 2</a></li>
<li><a href="../de427229/index.html">4 Jahre Game Project Management Programm</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>