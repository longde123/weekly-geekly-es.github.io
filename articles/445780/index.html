<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äçüåæ üêó üê∑ M√©todos de conjunto. Extracto del libro. üö¥üèº üíí üë©üèΩ‚Äçüåæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola, Khabrozhiteli, hemos entregado a la imprenta un nuevo libro "Machine Learning: Algorithms for Business" . Aqu√≠ hay un extracto sobre los m√©todos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>M√©todos de conjunto. Extracto del libro.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/445780/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/vk/fr/zn/vkfrzn9ctkjsd2wjx8puqifp980.jpeg" alt="imagen"></a> <br><br>  Hola, Khabrozhiteli, hemos entregado a la imprenta un nuevo libro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Machine Learning: Algorithms for Business"</a> .  Aqu√≠ hay un extracto sobre los m√©todos de conjunto, su prop√≥sito es explicar qu√© los hace efectivos y c√≥mo evitar errores comunes que conducen a su mal uso en las finanzas. <br><a name="habracut"></a><br><h3>  6.2.  Tres fuentes de error. </h3><br>  Los modelos MO suelen sufrir tres <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">errores</a> . <br><br>  1. Sesgo: este error es causado por suposiciones poco realistas.  Cuando el sesgo es alto, esto significa que el algoritmo MO no pudo reconocer las relaciones importantes entre los rasgos y los resultados.  En esta situaci√≥n, se dice que el algoritmo est√° "no aprobado". <br><br>  2. Dispersi√≥n: este error es causado por la sensibilidad a peque√±os cambios en el subconjunto de entrenamiento.  Cuando la varianza es alta, esto significa que el algoritmo se vuelve a alinear con el subconjunto de entrenamiento y, por lo tanto, incluso los cambios m√≠nimos en el subconjunto de entrenamiento pueden producir predicciones terriblemente diferentes.  En lugar de modelar patrones generales en un subconjunto de entrenamiento, el algoritmo toma por error el ruido de la se√±al. <br><br>  3. Ruido: este error es causado por la dispersi√≥n de los valores observados, como cambios impredecibles o errores de medici√≥n.  Este es un error fatal que no puede ser explicado por ning√∫n modelo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bc/8g/ag/bc8gaguh6e1w07vd3aoqcqcz4m0.png" alt="imagen"></div><br>  Un m√©todo de conjunto es un m√©todo que combina muchos estudiantes d√©biles, que se basan en el mismo algoritmo de aprendizaje, con el objetivo de crear un estudiante (m√°s fuerte) cuyo rendimiento sea mejor que cualquiera de los estudiantes individuales.  Las t√©cnicas de conjunto ayudan a reducir el sesgo y / o la dispersi√≥n. <br><br><h3>  6.3.  Agregaci√≥n Bootstrap </h3><br>  El ensacado (agregaci√≥n), o agregaci√≥n de muestras de bootstrap, es una forma efectiva de reducir la variaci√≥n en los pron√≥sticos.  Funciona de la siguiente manera: primero, es necesario generar N subconjuntos de datos de entrenamiento usando muestreo aleatorio con retorno.  En segundo lugar, ajuste N evaluadores, uno para cada subconjunto de entrenamiento.  Estos evaluadores se ajustan independientemente uno del otro, por lo tanto, los modelos se pueden ajustar en paralelo.  En tercer lugar, el pron√≥stico conjunto es una media aritm√©tica simple de pron√≥sticos individuales de N modelos.  En el caso de las variables categ√≥ricas, la probabilidad de que una observaci√≥n pertenezca a una clase est√° determinada por la proporci√≥n de evaluadores que clasifican esta observaci√≥n como miembro de esta clase (por mayor√≠a de votos, es decir, por mayor√≠a de votos).  Cuando el tasador base puede hacer predicciones con la probabilidad de predicci√≥n, el clasificador en bolsas puede obtener el valor promedio de las probabilidades. <br><br>  Si utiliza la clase BaggingClassifier de la biblioteca sklearn para calcular la precisi√≥n sin paquetes, entonces debe tener en cuenta este defecto: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/scikit-learn/scikitlearn/issues/8933</a> .  Una soluci√≥n alternativa es cambiar el nombre de las etiquetas en un orden secuencial entero. <br><br><h3>  6.3.1.  Reducci√≥n de la dispersi√≥n </h3><br>  La principal ventaja del embolsado es que reduce la variaci√≥n de las previsiones, lo que ayuda a resolver el problema del sobreajuste.  La varianza en la predicci√≥n en bolsa (œÜi [c]) es una funci√≥n del n√∫mero de evaluadores en bolsa (N), la varianza promedio de la predicci√≥n realizada por un evaluador (œÉÃÑ) y la correlaci√≥n promedio entre sus predicciones (œÅÃÑ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pq/ly/9i/pqly9iyqdqu5m8ty8zahuoeomve.png" alt="imagen"></div><br>  el arranque secuencial (cap√≠tulo 4) consiste en tomar muestras lo m√°s independientes posible, reduciendo as√≠ œÅÃÑ, lo que deber√≠a reducir la dispersi√≥n de clasificadores en bolsas.  En la fig.  6.1, trazamos el diagrama de desviaci√≥n est√°ndar de la predicci√≥n en bolsas en funci√≥n de N ‚àà [5, 30], œÅÃÑ ‚àà [0, 1] y œÉÃÑ = 1. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/na/bm/8z/nabm8zbq6st62mg82lmhac45y0i.png" alt="imagen"></div><br><h3>  6.3.2.  Precisi√≥n mejorada </h3><br>  Considere un clasificador en bolsas, que hace predicciones sobre k clases por mayor√≠a de votos entre N clasificadores independientes.  Podemos designar predicciones como {0,1}, donde 1 significa predicci√≥n correcta.  La precisi√≥n del clasificador es la probabilidad p de marcar la predicci√≥n como 1. En promedio, obtenemos predicciones de Np marcadas como 1 con una varianza de Np (1 - p).  El voto mayoritario hace la predicci√≥n correcta cuando se observa la clase m√°s predecible.  Por ejemplo, para N = 10 yk = 3, el clasificador en bolsas hizo la predicci√≥n correcta cuando se observ√≥ <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g-/oq/oi/g-oqoilmsmjgpaukoouor90ndbo.png" alt="imagen"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wx/wk/cl/wxwkcl97h4cnn1n-yx8ljmdj14g.png" alt="imagen"></div><br>  Listado 6.1.  La correcci√≥n del clasificador en bolsas <br><br><pre><code class="plaintext hljs">from scipy.misc import comb N,p,k=100,1./3,3. p_=0 for i in xrange(0,int(N/k)+1): p_+=comb(N,i)*p**i*(1-p)**(Ni) print p,1-p_</code> </pre> <br>  Este es un argumento fuerte a favor de empacar cualquier clasificador en el caso general, cuando las capacidades computacionales lo permiten.  Sin embargo, a diferencia del aumento, el embolsado no puede mejorar la precisi√≥n de los clasificadores d√©biles: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/fz/i8/9nfzi86m4gvaqru13iicjf1q0bg.png" alt="imagen"></div><br>  Para un an√°lisis detallado de este tema, se aconseja al lector que recurra al teorema del jurado de Condorcet.  Aunque este teorema se obtuvo con el prop√≥sito de votar por mayor√≠a en la ciencia pol√≠tica, el problema abordado por este teorema tiene caracter√≠sticas comunes con el descrito anteriormente. <br><br><h3>  6.3.3.  Redundancia de observaciones </h3><br>  En el Cap√≠tulo 4, examinamos una de las razones por las que las observaciones financieras no pueden considerarse igualmente distribuidas y mutuamente independientes.  Las observaciones excesivas tienen dos efectos perjudiciales sobre el embolsado.  En primer lugar, es m√°s probable que las muestras tomadas con retorno sean casi id√©nticas, incluso si no tienen observaciones comunes.  Lo hace <img src="https://habrastorage.org/webt/sx/i4/u0/sxi4u0afpswnvbe5nzxexnp_k80.png" alt="imagen">  y el embolsado no reducir√° la variaci√≥n, independientemente de N. Por ejemplo, si cada caso en t est√° marcado de acuerdo con un rendimiento financiero entre t y t + 100, entonces debemos seleccionar el 1% de los casos por tasador en bolsa, pero no m√°s.  En el cap√≠tulo 4, secci√≥n 4.5, se recomiendan tres soluciones alternativas, una de las cuales fue establecer max_samples = out ['tW']. Mean () en la implementaci√≥n de la clase de clasificador empaquetado en la biblioteca sklearn.  Otra (mejor) soluci√≥n fue la aplicaci√≥n del m√©todo de selecci√≥n secuencial de bootstrap. <br><br>  El segundo efecto perjudicial de la redundancia de observaci√≥n es que se inflar√° la precisi√≥n del paquete adicional.  Esto se debe al hecho de que el muestreo aleatorio con muestreo regresa a las muestras del subconjunto de entrenamiento que son muy similares a las que est√°n fuera del paquete.  En este caso, la validaci√≥n cruzada de bloques k estratificada correcta sin barajar antes de dividir mostrar√° una precisi√≥n mucho menor en el subconjunto de prueba que la que se evalu√≥ fuera del paquete.  Por esta raz√≥n, cuando se utiliza esta clase de biblioteca sklearn, se recomienda establecer KFold estratificado (n_splits = k, shuffle = False), verificar el clasificador empaquetado e ignorar los resultados de la precisi√≥n sin paquetes.  Una k baja es preferible a una k alta, ya que la divisi√≥n excesiva volver√° a colocar patrones en el subconjunto de prueba que son demasiado similares a los utilizados en el subconjunto de entrenamiento. <br><br><h3>  6.4.  Bosque al azar </h3><br>  Los √°rboles de decisi√≥n son bien conocidos porque tienden a ajustarse en exceso, lo que aumenta la variaci√≥n de los pron√≥sticos.  Para abordar este problema, se desarroll√≥ un m√©todo de bosque aleatorio (RF) para generar pron√≥sticos de conjunto con menor varianza. <br><br>  Un bosque aleatorio tiene algunas similitudes comunes con el embolsado en el sentido de capacitar a evaluadores individuales de forma independiente en subconjuntos de datos de arranque.  Una diferencia clave del ensacado es que se construye un segundo nivel de aleatoriedad en los bosques aleatorios: durante la optimizaci√≥n de cada fragmentaci√≥n nodal, solo se evaluar√° una submuestra aleatoria (sin retorno) de los atributos a fin de relacionar a√∫n m√°s a los evaluadores. <br><br>  Al igual que el ensacado, un bosque aleatorio reduce la variaci√≥n de los pron√≥sticos sin sobreajustar (recu√©rdelo hasta).  La segunda ventaja es que un bosque aleatorio eval√∫a la importancia de los atributos, que discutiremos en detalle en el Cap√≠tulo 8. La tercera ventaja es que un bosque aleatorio proporciona estimaciones de precisi√≥n fuera del paquete, sin embargo, en aplicaciones financieras es probable que se inflen (como se describe en Secci√≥n 6.3.3).  Pero al igual que el ensacado, un bosque aleatorio no necesariamente exhibir√° un sesgo menor que los √°rboles de decisi√≥n individuales. <br><br>  Si una gran cantidad de muestras es redundante (no distribuidas de manera equitativa e independientes entre s√≠), todav√≠a habr√° un nuevo ajuste: el muestreo aleatorio con retorno generar√° una gran cantidad de √°rboles casi id√©nticos (), donde cada √°rbol de decisi√≥n est√° sobreajustado (un inconveniente debido a que los √°rboles de decisi√≥n son notorios) .  A diferencia del ensacado, un bosque aleatorio siempre establece el tama√±o de las muestras de arranque de acuerdo con el tama√±o del subconjunto de datos de entrenamiento.  Veamos c√≥mo podemos resolver este problema de reajustar bosques aleatorios en la biblioteca sklearn.  Con fines ilustrativos, me referir√© a las clases de la biblioteca sklearn;  sin embargo, estas soluciones se pueden aplicar a cualquier implementaci√≥n: <br><br>  1. Establezca el par√°metro max_features en un valor m√°s bajo para lograr una discrepancia entre los √°rboles. <br><br>  2. Parada temprana: establezca el par√°metro de regularizaci√≥n min_weight_fraction_leaf en un valor suficientemente grande (por ejemplo, 5%) para que la precisi√≥n fuera del paquete converja con la correcci√≥n fuera de la muestra (bloque k). <br><br>  3. Use el evaluador BaggingClassifier en el evaluador base DecisionTreeClassifier, donde max_samples se establece en unicidad promedio (avgU) entre muestras. <br><br><ul><li>  clf = DecisionTreeClassifier (criterio = 'entrop√≠a', max_features = 'auto', class_weight = 'balanceado') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  4. Utilice el evaluador BaggingClassifier en el evaluador RandomForestClassifier base, donde max_samples se establece en unicidad promedio (avgU) entre muestras. <br><br><ul><li>  clf = RandomForestClassifier (n_estimators = 1, criterio = 'entrop√≠a', bootstrap = False, class_weight = 'balance_subsample') </li><li>  bc = BaggingClassifier (base_estimator = clf, n_estimators = 1000, max_samples = avgU, max_features = 1.) </li></ul><br>  5. Modifique la clase de bosque aleatorio para reemplazar los bootstraps est√°ndar con bootstraps secuenciales. <br><br>  Para resumir, el Listado 6.2 muestra tres formas alternativas de configurar un bosque aleatorio usando diferentes clases. <br><br>  Listado 6.2.  Tres formas de configurar un bosque aleatorio <br><br><pre> <code class="plaintext hljs">clf0=RandomForestClassifier(n_estimators=1000, class_weight='balanced_ subsample', criterion='entropy') clf1=DecisionTreeClassifier(criterion='entropy', max_features='auto', class_weight='balanced') clf1=BaggingClassifier(base_estimator=clf1, n_estimators=1000, max_samples=avgU) clf2=RandomForestClassifier(n_estimators=1, criterion='entropy', bootstrap=False, class_weight='balanced_subsample') clf2=BaggingClassifier(base_estimator=clf2, n_estimators=1000, max_samples=avgU, max_features=1.)</code> </pre> <br>  Al ajustar los √°rboles de decisi√≥n, la rotaci√≥n del espacio de caracter√≠sticas en la direcci√≥n que coincide con los ejes, como regla, reduce el n√∫mero de niveles necesarios para el √°rbol.  Por esta raz√≥n, le sugiero que ajuste un √°rbol aleatorio en el PCA de atributos, ya que esto puede acelerar los c√°lculos y reducir ligeramente el reajuste (m√°s sobre esto en el Cap√≠tulo 8).  Adem√°s, como se describe en el Cap√≠tulo 4, Secci√≥n 4.8, el argumento class_weight = 'balance_subsample' ayudar√° a evitar que los √°rboles clasifiquen err√≥neamente las clases minoritarias. <br><br><h3>  6.5.  Impulsar </h3><br>  Kearns y Valiant [1989] estuvieron entre los primeros en preguntar si los valuadores d√©biles podr√≠an combinarse para lograr la realizaci√≥n de un valorizador altamente preciso.  Poco despu√©s, Schapire [1990] mostr√≥ una respuesta afirmativa a esta pregunta utilizando un procedimiento que hoy llamamos impulso (impulso, impulso, amplificaci√≥n).  En t√©rminos generales, funciona de la siguiente manera: primero, generar un subconjunto de entrenamiento mediante selecci√≥n aleatoria con retorno de acuerdo con ciertos pesos de muestra (inicializados por pesos uniformes).  En segundo lugar, ajuste un evaluador utilizando este subconjunto de capacitaci√≥n.  En tercer lugar, si un solo evaluador logra una precisi√≥n que excede el umbral de aceptabilidad (por ejemplo, en un clasificador binario es del 50% para que el clasificador funcione mejor que la adivinaci√≥n aleatoria), entonces el evaluador permanece, de lo contrario se descarta.  Cuarto, otorgue m√°s peso a las observaciones clasificadas incorrectamente y menos peso a las observaciones clasificadas correctamente.  Quinto, repita los pasos anteriores hasta que se reciban N tasadores.  Sexto, el pron√≥stico conjunto es el promedio ponderado de los pron√≥sticos individuales de N modelos, donde los pesos est√°n determinados por la precisi√≥n de los evaluadores individuales.  Hay una serie de algoritmos potenciados, de los cuales el refuerzo adaptativo AdaBoost es uno de los m√°s populares (Geron [2017]).  La Figura 6.3 resume el flujo de decisiones en la implementaci√≥n est√°ndar del algoritmo AdaBoost. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1d/u-/cr/1du-crm0gpkjgvy7lk45jj9f9ig.png" alt="imagen"></div><br><h3>  6.6.  Bagging vs aumento de finanzas </h3><br>  De la descripci√≥n anterior, varios aspectos hacen que el impulso sea completamente diferente del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">embolsado</a> : <br><br><ul><li>  El ajuste de clasificadores individuales se lleva a cabo secuencialmente. </li><li>  Los clasificadores pobres son rechazados. </li><li>  En cada iteraci√≥n, las observaciones se ponderan de manera diferente. </li></ul><br>  El pron√≥stico del conjunto es el promedio ponderado de estudiantes individuales. <br><br>  La principal ventaja de impulsar es que reduce tanto la varianza como el sesgo en los pron√≥sticos.  No obstante, la correcci√≥n de sesgo se produce debido a un mayor riesgo de sobreajuste.  Se puede argumentar que en las aplicaciones financieras, el embolsado suele ser preferible a impulsar.  El embolsado resuelve el problema de sobreajuste, mientras que el refuerzo resuelve el problema de sobreajuste.  El sobreajuste es a menudo un problema m√°s serio que el subadaptado, ya que ajustar demasiado el algoritmo MO a los datos financieros no es en absoluto dif√≠cil debido a la baja relaci√≥n se√±al / ruido.  Adem√°s, el embolsado se puede paralelizar, mientras que el aumento generalmente requiere una ejecuci√≥n secuencial. <br><br><h3>  6.7.  Embolsado para escalabilidad </h3><br>  Como sabe, algunos algoritmos MO populares no se escalan muy bien seg√∫n el tama√±o de la muestra.  El m√©todo de m√°quinas de vectores de soporte (SVM) es un excelente ejemplo.  Si intenta ajustar el evaluador SVM a m√°s de un mill√≥n de observaciones, puede llevar mucho tiempo hasta que el algoritmo converja.  E incluso despu√©s de converger, no hay garant√≠a de que la soluci√≥n sea un √≥ptimo global o que no se realinee. <br><br>  Un enfoque pr√°ctico es construir un algoritmo en bolsas donde el evaluador base pertenezca a una clase que no escala bien con el tama√±o de la muestra, como SVM.  Al definir este tasador b√°sico, introducimos una condici√≥n estricta para una parada temprana.  Por ejemplo, en la implementaci√≥n de m√°quinas de vectores de soporte (SVM) en la biblioteca sklearn, puede establecer un valor bajo para el par√°metro max_iter, por ejemplo, iteraciones 1E5.  El valor predeterminado es max_iter = -1, que le dice al evaluador que contin√∫e iterando hasta que los errores caigan por debajo del nivel de tolerancia.  Por otro lado, puede aumentar el nivel de tolerancia con el par√°metro tol, que por defecto es tol = iE-3.  Cualquiera de estas dos opciones llevar√° a una parada anticipada.  Puede detener otros algoritmos antes de tiempo utilizando par√°metros equivalentes, como el n√∫mero de niveles en un bosque aleatorio (max_depth) o la fracci√≥n m√≠nima ponderada de la suma total de pesos (todas las muestras de entrada) necesarias para estar en un nodo hoja (min_weight_fraction_leaf). <br><br>  Dado que los algoritmos empaquetados pueden ser paralelos, transformamos una tarea secuencial grande en una serie de tareas m√°s peque√±as que se ejecutan simult√°neamente.  Por supuesto, una parada temprana aumentar√° la variaci√≥n de los resultados de los evaluadores de bases individuales;  sin embargo, este aumento puede ser m√°s que compensado por la disminuci√≥n de la varianza asociada con el algoritmo empaquetado.  Puede controlar esta reducci√≥n agregando nuevos valoradores de base independientes.  Utilizado de esta manera, el ensacado le permite obtener estimaciones r√°pidas y s√≥lidas en conjuntos de datos muy grandes. <br><br>  ¬ªSe puede encontrar m√°s informaci√≥n sobre el libro en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el sitio web del editor</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Contenidos</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Extracto</a> <br><br>  25% de descuento para libros de reserva Khabrozhiteley en un cup√≥n - <b>Machine Learning</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/445780/">https://habr.com/ru/post/445780/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445762/index.html">¬øEs l√≥gica la matem√°tica o por qu√© las teor√≠as axiom√°ticas son parad√≥jicas?</a></li>
<li><a href="../445764/index.html">Mi forma de crear componentes maestros en la Figura</a></li>
<li><a href="../445766/index.html">Sobre el centro de datos con toda honestidad: c√≥mo resolvimos el problema del polvo en las salas de servidores del centro de datos</a></li>
<li><a href="../445772/index.html">Sistema de pago r√°pido o lo imposible es posible</a></li>
<li><a href="../445778/index.html">10 nuevos cursos gratuitos sobre servicios cognitivos y Azure</a></li>
<li><a href="../445782/index.html">Una selecci√≥n de destornilladores y herramientas m√∫ltiples inusuales desde Leatherman hasta Xiaomi</a></li>
<li><a href="../445784/index.html">Crecimiento profesional de los empleados: qu√© es y por qu√© es necesario: nos comunicamos con Dodo Pizza, Icons8 y Evil Martians</a></li>
<li><a href="../445786/index.html">Criptograf√≠a en Java. Clase KeyStore</a></li>
<li><a href="../445788/index.html">Videovigilancia en la nube h√°galo usted mismo: nuevas caracter√≠sticas de Ivideon Web SDK</a></li>
<li><a href="../445792/index.html">C√≥mo desarrollamos documentaci√≥n en un proyecto abierto de Embox</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>