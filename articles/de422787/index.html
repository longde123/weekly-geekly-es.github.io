<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõåüèº üë®üèæ‚Äçü§ù‚Äçüë®üèº üíß Wesentliche √Ñnderungen in f√ºhrenden Chip-Architekturen ü§æ üö• üèáüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Durch die Einf√ºhrung von AI auf Chipebene k√∂nnen Sie mehr Daten lokal verarbeiten, da eine Erh√∂hung der Anzahl der Ger√§te nicht mehr den gleichen Effe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wesentliche √Ñnderungen in f√ºhrenden Chip-Architekturen</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422787/"><h4>  <font color="gray">Durch die Einf√ºhrung von AI auf Chipebene k√∂nnen Sie mehr Daten lokal verarbeiten, da eine Erh√∂hung der Anzahl der Ger√§te nicht mehr den gleichen Effekt erzielt</font> </h4><br>  Chiphersteller arbeiten an neuen Architekturen, die die pro Watt und Zyklus verarbeitete Datenmenge erheblich erh√∂hen.  Der Grundstein f√ºr eine der gr√∂√üten Revolutionen in der Chiparchitektur der letzten Jahrzehnte ist gelegt. <br><br>  Alle gro√üen Hersteller von Chips und Systemen √§ndern die Entwicklungsrichtung.  Sie traten in das Rennen der Architekturen ein, das einen Paradigmenwechsel in allem erm√∂glicht: von Lese- und Schreibmethoden √ºber das Ged√§chtnis bis hin zu ihrer Verarbeitung und letztendlich dem Layout verschiedener Elemente auf einem Chip.  Obwohl die Miniaturisierung fortgesetzt wird, setzt niemand auf Skalierung, um mit dem explosiven Wachstum von Daten von Sensoren und der Erh√∂hung des Verkehrsaufkommens zwischen Maschinen fertig zu werden. <br><a name="habracut"></a><br>  Unter den √Ñnderungen in den neuen Architekturen: <br><br><ul><li>  Neue Methoden zur Verarbeitung einer gr√∂√üeren Datenmenge in einem Taktzyklus, manchmal mit geringerer Genauigkeit oder nach Priorit√§t bestimmter Vorg√§nge, je nach Anwendung. </li><li>  Neue Speicherarchitekturen, die die Art und Weise √§ndern, wie wir Daten speichern, lesen, schreiben und darauf zugreifen. </li><li>  Spezialisierte Verarbeitungsmodule im gesamten System in der N√§he des Speichers.  Anstelle eines Zentralprozessors werden je nach Datentyp und Anwendung Beschleuniger ausgew√§hlt. </li><li>  Auf dem Gebiet der KI wird daran gearbeitet, verschiedene Datentypen in Form von Vorlagen zu kombinieren, wodurch die Datendichte effektiv erh√∂ht und Diskrepanzen zwischen verschiedenen Typen minimiert werden. </li><li>  Jetzt ist das Layout im Geh√§use der Hauptbestandteil der Architektur, wobei immer mehr auf die einfache √Ñnderung dieser Designs geachtet wird. </li></ul><br>  "Es gibt verschiedene Trends, die den technologischen Fortschritt beeinflussen", sagte Stephen Wu, ein angesehener Rambus-Ingenieur.  - In Rechenzentren k√∂nnen Sie Hardware und Software optimal nutzen.  Aus diesem Blickwinkel betrachten die Eigent√ºmer von Rechenzentren die Wirtschaft.  Etwas Neues einzuf√ºhren ist teuer.  Da sich jedoch die Engp√§sse √§ndern, werden spezielle Chips f√ºr eine effizientere Datenverarbeitung eingef√ºhrt.  Und wenn Sie den Datenfluss zu E / A und Speicher reduzieren, kann dies gro√üe Auswirkungen haben. ‚Äú <br><br>  Die √Ñnderungen sind am Rande der Computerinfrastruktur, dh bei den Endsensoren, offensichtlicher.  Die Hersteller erkannten pl√∂tzlich, dass Dutzende Milliarden Ger√§te zu viele Daten generieren w√ºrden: Ein solches Volumen konnte nicht zur Verarbeitung in die Cloud gesendet werden.  Die Verarbeitung all dieser Daten am Rande f√ºhrt jedoch zu anderen Problemen: Sie erfordern erhebliche Leistungsverbesserungen, ohne den Stromverbrauch signifikant zu erh√∂hen. <br><br>  "Es gibt einen neuen Trend zu geringerer Genauigkeit", sagte Robert Ober, Teslas f√ºhrender Plattformarchitekt bei Nvidia.  - Dies sind nicht nur Rechenzyklen.  Dies ist ein intensiveres Packen von Daten in den Speicher, wo das Format von 16-Bit-Befehlen verwendet wird. ‚Äú <br><br>  Aubert glaubt, dass Sie dank einer Reihe von Architekturoptimierungen in absehbarer Zukunft die Verarbeitungsgeschwindigkeit alle paar Jahre verdoppeln k√∂nnen.  "Wir werden eine dramatische Steigerung der Produktivit√§t sehen", sagte er.  - Daf√ºr m√ºssen Sie drei Dinge tun.  Der erste ist das Rechnen.  Der zweite ist die Erinnerung.  Der dritte Bereich ist die Hostbandbreite und die E / A-Bandbreite.  Es muss noch viel Arbeit geleistet werden, um den Speicher und den Netzwerkstapel zu optimieren. ‚Äú <br><br>  Es wird bereits etwas implementiert.  In einer Pr√§sentation auf der Hot Chips-Konferenz 2018 wies Jeff Rupley, leitender Architekt im Austin Research Center von Samsung, auf einige wichtige architektonische √Ñnderungen am M3-Prozessor hin.  Eine enth√§lt mehr Anweisungen pro Schlag - sechs statt vier im letzten M2-Chip.  Zus√§tzlich wurde eine Verzweigungsvorhersage in neuronalen Netzen implementiert und die Befehlswarteschlange wurde verdoppelt. <br><br>  Solche √Ñnderungen verlagern den Innovationspunkt von der direkten Herstellung von Mikroschaltungen auf Architektur und Design einerseits und auf die Anordnung von Elementen auf der anderen Seite der Produktionskette.  Obwohl die Innovationen in technologischen Prozessen fortgesetzt werden, ist es nur auf Kosten dieser unglaublich schwierig, in jedem neuen Chipmodell eine Steigerung der Produktivit√§t und Leistung um 15 bis 20% zu erreichen - und dies reicht nicht aus, um das schnelle Wachstum des Datenvolumens zu bew√§ltigen. <br><br>  "√Ñnderungen finden exponentiell statt", sagte Victor Pan, Pr√§sident und CEO von Xilinx, in einer Rede auf der Hot Chips-Konferenz. "Jedes Jahr werden 10 Zettabyte [10 <sup>21</sup> Byte] Daten generiert, die meisten davon in unstrukturierter Form." <br><br><h1>  Neue Ans√§tze zur Erinnerung </h1><br>  Das Arbeiten mit so vielen Daten erfordert ein Umdenken jeder Komponente im System, von den Datenverarbeitungsmethoden bis zu ihrer Speicherung. <br><br>  "Es gab viele Versuche, neue Speicherarchitekturen zu erstellen", sagte Carlos Machin, Senior Innovation Director bei eSilicon EMEA.  - Das Problem ist, dass Sie alle Zeilen lesen und jeweils ein Bit ausw√§hlen m√ºssen.  Eine M√∂glichkeit besteht darin, einen Speicher zu erstellen, der von links nach rechts sowie von oben nach unten gelesen werden kann.  Sie k√∂nnen noch weiter gehen und dem Speicher Berechnungen hinzuf√ºgen. ‚Äú <br><br>  Diese √Ñnderungen umfassen das √Ñndern der Methoden zum Lesen des Speichers, der Position und des Typs der Verarbeitungselemente sowie die Einf√ºhrung von AI, um die Speicherung, Verarbeitung und Bewegung von Daten im gesamten System zu priorisieren. <br><br>  ‚ÄûWas ist, wenn wir bei sp√§rlichen Daten jeweils nur ein Byte aus diesem Array lesen k√∂nnen - oder vielleicht acht aufeinanderfolgende Bytes aus demselben Bytepfad, ohne Energie f√ºr andere Bytes oder Bytepfade zu verschwenden, an denen wir nicht interessiert sind ?  "Fragt Mark Greenberg, Cadence Product Marketing Director."  - In Zukunft ist dies m√∂glich.  Wenn Sie sich beispielsweise die Architektur von HBM2 ansehen, ist der Stapel in 16 virtuellen Kan√§len mit jeweils 64 Bit organisiert, und Sie ben√∂tigen nur 4 aufeinanderfolgende 64-Bit-W√∂rter, um auf einen virtuellen Kanal zuzugreifen.  Somit ist es m√∂glich, Datenfelder mit einer Breite von 1024 Bit zu erstellen, horizontal zu schreiben, aber vier 64-Bit-W√∂rter gleichzeitig vertikal zu lesen. " <br><br>  Das Ged√§chtnis ist eine der Hauptkomponenten der von Neumann-Architektur, aber jetzt ist es auch eine der Hauptarenen f√ºr Experimente geworden.  "Der Hauptfeind sind virtuelle Speichersysteme, bei denen Daten auf unnat√ºrlichere Weise verschoben werden", sagte Dan Bouvier, Chefarchitekt f√ºr Kundenprodukte bei AMD.  - Dies ist eine Sendung.  Daran sind wir im Bereich Grafik gew√∂hnt.  Wenn wir jedoch die Konflikte in der DRAM-Speicherbank l√∂sen, erhalten wir ein viel effizienteres Streaming.  Dann kann eine separate GPU DRAM im Bereich von 90% Wirkungsgrad verwenden, was sehr gut ist.  Wenn Sie das Streaming jedoch ohne Unterbrechungen einrichten, fallen CPU und APU ebenfalls in den Wirkungsgradbereich von 80% bis 85%. ‚Äú <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6af/814/e8c/6af814e8c5d0a2f4b9274d165aa8f622.png"><br>  <i><font color="gray">Abb.</font></i>  <i><font color="gray">1. Architektur von Neumann.</font></i>  <i><font color="gray">Quelle: Halbleitertechnik</font></i> <br><br>  IBM entwickelt eine andere Art von Speicherarchitektur, bei der es sich im Wesentlichen um eine aktualisierte Version der Festplattenaggregation handelt.  Das Ziel ist, dass das System anstelle eines einzelnen Laufwerks beliebig jeden verf√ºgbaren Speicher √ºber einen Connector verwenden kann, den Jeff Stucheli, ein IBM-Hardwarearchitekt, als "Swiss Army Knife" zum Verbinden von Elementen bezeichnet.  Der Vorteil des Ansatzes besteht darin, dass Sie verschiedene Datentypen mischen und abgleichen k√∂nnen. <br><br>  ‚ÄûDer Prozessor wird zum Zentrum einer Hochleistungssignalschnittstelle‚Äú, sagt Stucelli.  "Wenn Sie die Mikroarchitektur √§ndern, f√ºhrt der Kern mehr Operationen pro Zyklus mit derselben Frequenz aus." <br><br>  Konnektivit√§t und Durchsatz sollten die Verarbeitung eines radikal erh√∂hten Volumens generierter Daten sicherstellen.  "Die Hauptengp√§sse liegen jetzt in den Datenbewegungsorten", sagte Wu von Rambus.  "Die Branche hat gro√üartige Arbeit geleistet und die Geschwindigkeit der Datenverarbeitung erh√∂ht."  Wenn Sie jedoch Daten oder spezielle Datenvorlagen erwarten, m√ºssen Sie den Speicher schneller ausf√ºhren.  Wenn Sie sich also DRAM und NVM ansehen, h√§ngt die Leistung vom Verkehrsmuster ab.  Wenn die Daten gestreamt werden, bietet der Speicher eine sehr gute Leistung.  Wenn die Daten jedoch in zuf√§lligen Tropfen vorliegen, sind sie weniger effizient.  Und egal was Sie tun, mit zunehmender Lautst√§rke m√ºssen Sie es immer noch schneller machen. ‚Äú <br><br><h1>  Mehr Computer, weniger Verkehr. </h1><br>  Das Problem wird durch die Tatsache versch√§rft, dass es verschiedene Arten von Daten gibt, die von Ger√§ten am Rand mit unterschiedlichen Frequenzen und Geschwindigkeiten erzeugt werden.  Damit sich diese Daten frei zwischen verschiedenen Verarbeitungsmodulen bewegen k√∂nnen, muss die Verwaltung wesentlich effizienter werden als in der Vergangenheit. <br><br>  ‚ÄûEs gibt vier Hauptkonfigurationen: Viele-zu-Viele-Speichersubsysteme, E / A mit geringem Stromverbrauch sowie Netz- und Ringtopologien‚Äú, sagt Charlie Janak, Vorsitzender und CEO von Arteris IP.  - Sie k√∂nnen alle vier auf einem Chip platzieren, was bei den wichtigsten IoT-Chips der Fall ist.  Oder Sie k√∂nnen HBM-Subsysteme mit hohem Durchsatz hinzuf√ºgen.  Die Komplexit√§t ist jedoch enorm, da einige dieser Workloads sehr spezifisch sind und der Chip verschiedene Arbeitsaufgaben hat.  Wenn Sie sich einige dieser Mikrochips ansehen, erhalten sie riesige Datenmengen.  Dies ist in Systemen wie Autoradar und Lidar der Fall.  Sie k√∂nnen ohne einige erweiterte Verbindungen nicht existieren. ‚Äú <br><br>  Die Aufgabe besteht darin, die Datenbewegung zu minimieren und gleichzeitig den Datenfluss bei Bedarf zu maximieren - und irgendwie ein Gleichgewicht zwischen lokaler und zentraler Verarbeitung zu finden, ohne den Energieverbrauch unn√∂tig zu erh√∂hen. <br><br>  "Einerseits ist dies ein Bandbreitenproblem", sagte Rajesh Ramanujam, Produktmarketingmanager bei NetSpeed ‚Äã‚ÄãSystems.  - Sie m√∂chten den Datenverkehr so ‚Äã‚Äãweit wie m√∂glich reduzieren, um Daten n√§her an den Prozessor zu √ºbertragen.  Wenn Sie die Daten dennoch verschieben m√ºssen, ist es ratsam, sie so weit wie m√∂glich zu komprimieren.  Aber nichts existiert f√ºr sich.  Alles muss auf Systemebene geplant werden.  Bei jedem Schritt m√ºssen mehrere voneinander abh√§ngige Achsen ber√ºcksichtigt werden.  Sie bestimmen, ob Sie Speicher auf herk√∂mmliche Weise zum Lesen und Schreiben verwenden oder ob Sie neue Technologien verwenden.  In einigen F√§llen m√ºssen Sie m√∂glicherweise die Art und Weise √§ndern, in der Sie die Daten selbst speichern.  Wenn Sie eine h√∂here Leistung ben√∂tigen, bedeutet dies normalerweise eine Vergr√∂√üerung der Chipfl√§che, was sich auf die W√§rmeableitung auswirkt.  Unter Ber√ºcksichtigung der funktionalen Sicherheit kann eine Daten√ºberlastung nicht mehr zugelassen werden. ‚Äú <br><br>  Aus diesem Grund wird der Datenverarbeitung am Rand und der Kanalbandbreite von verschiedenen Datenverarbeitungsmodulen so viel Aufmerksamkeit geschenkt.  Wenn Sie jedoch unterschiedliche Architekturen entwickeln, ist es sehr unterschiedlich, wie und wo diese Datenverarbeitung implementiert wird. <br><br>  Zum Beispiel f√ºhrte Marvell einen SSD-Controller mit integrierter KI ein, um die hohe Rechenlast am Rande zu bew√§ltigen.  Die AI-Engine kann f√ºr Analysen direkt im SSD-Laufwerk verwendet werden. <br><br>  "Sie k√∂nnen Modelle direkt in die Hardware laden und die Hardware-Verarbeitung auf dem SSD-Controller durchf√ºhren", sagte Ned Varnitsa, Chefingenieur von Marvell.  - Heute macht es den Server in der Cloud (Host).  Wenn jedoch jede Festplatte Daten an die Cloud sendet, wird eine gro√üe Menge an Netzwerkverkehr erzeugt.  Es ist besser, die Verarbeitung am Rand durchzuf√ºhren, und der Host gibt nur einen Befehl aus, bei dem es sich nur um Metadaten handelt.  Je mehr Laufwerke Sie haben, desto mehr Rechenleistung.  Dies ist ein gro√üer Vorteil des reduzierten Verkehrsaufkommens. ‚Äú <br><br>  Dieser Ansatz ist besonders interessant, da er sich je nach Anwendung an unterschiedliche Daten anpasst.  Der Host kann also eine Aufgabe generieren und zur Verarbeitung an das Speicherger√§t senden. Danach werden nur noch Metadaten oder Berechnungsergebnisse zur√ºckgesendet.  In einem anderen Szenario kann ein Speicherger√§t Daten speichern, vorverarbeiten und Metadaten, Tags und Indizes generieren, die dann vom Host nach Bedarf f√ºr die weitere Analyse abgerufen werden. <br><br>  Dies ist eine der m√∂glichen Optionen.  Es gibt andere.  Rupli von Samsung betonte, wie wichtig es ist, Redewendungen zu verarbeiten und zusammenzuf√ºhren, mit denen zwei Anweisungen dekodiert und zu einer Operation kombiniert werden k√∂nnen. <br><br><h1>  AI befasst sich mit Kontrolle und Optimierung </h1><br>  Auf allen Optimierungsebenen wird k√ºnstliche Intelligenz eingesetzt - dies ist eines der wirklich neuen Elemente in der Chiparchitektur.  Anstatt dem Betriebssystem und der Middleware die Verwaltung von Funktionen zu erm√∂glichen, wird diese √úberwachungsfunktion auf den Chip, zwischen den Chips und auf Systemebene verteilt.  In einigen F√§llen werden neuronale Hardware-Netzwerke eingef√ºhrt. <br><br>  ‚ÄûEs geht nicht so sehr darum, mehr zusammen zu packen, sondern die traditionelle Architektur zu √§ndern‚Äú, sagt Mike Gianfanya, Vice President Marketing bei eSilicon.  - Mithilfe von KI und maschinellem Lernen k√∂nnen Sie Elemente im gesamten System verteilen und so eine effizientere Verarbeitung mit Prognosen erzielen.  Oder Sie k√∂nnen separate Chips verwenden, die unabh√§ngig voneinander im System oder im Modul funktionieren. ‚Äú <br><br>  ARM hat seinen ersten Chip f√ºr maschinelles Lernen entwickelt, den es sp√§ter in diesem Jahr f√ºr mehrere M√§rkte ver√∂ffentlichen will.  "Dies ist ein neuer Prozessortyp", sagte Ian Bratt, Honoured Engineer von ARM.  - Es enth√§lt einen grundlegenden Block - es ist eine Computer-Engine sowie eine MAC-Engine, eine DMA-Engine mit einem Steuermodul und einem Broadcast-Netzwerk.  Insgesamt gibt es 16 Rechenkerne, die mit der 7-nm-Prozesstechnologie hergestellt wurden und 4 TeraOps mit einer Frequenz von 1 GHz erzeugen. ‚Äú <br><br>  Da ARM mit einem Partner-√ñkosystem zusammenarbeitet, ist sein Chip vielseitiger und anpassbarer als andere AI / ML-Chips, die entwickelt werden.  Anstelle einer monolithischen Struktur wird die Verarbeitung nach Funktionen getrennt, sodass jedes Computermodul auf einer separaten Feature-Map arbeitet.  Bratt identifizierte vier Hauptbestandteile: statische Planung, effizientes Falten, Verengungsmechanismen und programmierte Anpassung an zuk√ºnftige Design√§nderungen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fbe/7e4/ab1/fbe7e4ab11731ad3dafebc992b9c7bf2.png"><br>  <i><font color="gray">Abb.</font></i>  <i><font color="gray">2. ARM-Prozessor-ML-Architektur.</font></i>  <i><font color="gray">Quelle: ARM / Hot Chips</font></i> <br><br>  In der Zwischenzeit entschied sich Nvidia f√ºr eine andere Taktik: die Schaffung einer speziellen Deep-Learning-Engine neben der GPU, um die Bild- und Videoverarbeitung zu optimieren. <br><br><h1>  Fazit </h1><br>  Mit einigen oder allen dieser Ans√§tze erwarten die Chiphersteller, dass sich die Leistung alle paar Jahre verdoppelt, um mit dem explosiven Datenwachstum Schritt zu halten und gleichzeitig im engen Rahmen der Energieverbrauchsbudgets zu bleiben.  Dies ist jedoch nicht nur mehr Computer.  Dies ist eine √Ñnderung in der Chip- und Systemdesignplattform, wenn das wachsende Datenvolumen anstelle von Hardware- und Softwareeinschr√§nkungen zum Hauptfaktor wird. <br><br>  "Als Computer in Unternehmen auftauchten, schien es vielen, dass sich die Welt um uns herum beschleunigt hatte", sagte Aart de Gues, Vorsitzender und CEO von Synopsys.  - Sie haben Papierst√ºcke mit Stapel B√ºcher abgerechnet.  Das Hauptbuch hat sich in einen Stapel Lochkarten zum Drucken und Rechnen verwandelt.  Eine enorme Ver√§nderung ist eingetreten, und wir sehen es wieder.  Mit dem mentalen Aufkommen einfacher Computer hat sich der Algorithmus der Aktionen nicht ge√§ndert: Sie k√∂nnen jeden Schritt verfolgen.  Aber jetzt passiert etwas anderes, das zu einer neuen Beschleunigung f√ºhren k√∂nnte.  Es ist wie auf einem landwirtschaftlichen Feld, nur an einem bestimmten Tag, an dem die Temperatur das gew√ºnschte Niveau erreicht, zu gie√üen und eine bestimmte Art von D√ºnger aufzutragen.  Diese Verwendung von maschinellem Lernen ist eine Optimierung, die in der Vergangenheit nicht offensichtlich war. ‚Äú <br><br>  Mit dieser Einsch√§tzung ist er nicht allein.  "Die neuen Architekturen werden √ºbernommen", sagte Wally Raines, Pr√§sident und CEO von Mentor, Siemens Business.  - Sie werden entworfen.  Maschinelles Lernen wird in vielen oder den meisten F√§llen eingesetzt, da Ihr Gehirn aus eigenen Erfahrungen lernt.  Ich habe 20 oder mehr Unternehmen besucht, die spezialisierte KI-Prozessoren der einen oder anderen Art entwickeln, und jeder von ihnen hat seine eigene kleine Nische.  Sie werden ihre Anwendung jedoch zunehmend in bestimmten Anwendungen sehen und sie werden die traditionelle von Neumann-Architektur erg√§nzen.  Neuromorphes Computing wird zum Mainstream.  Dies ist ein gro√üer Schritt in Bezug auf Recheneffizienz und Kostenreduzierung.  Mobile Ger√§te und Sensoren werden die Arbeit erledigen, die Server heute leisten. ‚Äú </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422787/">https://habr.com/ru/post/de422787/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422775/index.html">DEFCON-Konferenz 22. Andrew "Zoz" Brooks. Nicht vermasseln! Teil 1</a></li>
<li><a href="../de422777/index.html">Eine einfache Einf√ºhrung in ALU f√ºr neuronale Netze: Erkl√§rung, physikalische Bedeutung und Implementierung</a></li>
<li><a href="../de422781/index.html">Fintech Digest: SWIFT wird weiterhin in der Russischen F√∂deration arbeiten. Mit VISA k√∂nnen Sie Geld per Telefonnummer und teuren biometrischen Daten √ºberweisen</a></li>
<li><a href="../de422783/index.html">Besser, schneller, leistungsf√§higer: Styled-Components v4</a></li>
<li><a href="../de422785/index.html">Werksdigitalisierung: Ein Blick nach vorne</a></li>
<li><a href="../de422789/index.html">@ Pythonetc Aug 2018</a></li>
<li><a href="../de422791/index.html">Wie man NICHT Englisch lernt: H√§ufige Fehler</a></li>
<li><a href="../de422793/index.html">DEFCON-Konferenz 22. Andrew "Zoz" Brooks. Nicht vermasseln! Teil 2</a></li>
<li><a href="../de422795/index.html">Technologie und Wirtschaft: Ein neues Modell der Zusammenarbeit mit Zyxel in Russland</a></li>
<li><a href="../de422797/index.html">Wie wir aus einer normalen IP-Kamera einen kleinen Cloud-Videorecorder gemacht haben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>