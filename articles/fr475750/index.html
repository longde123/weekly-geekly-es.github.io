<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤴🏽 🗣️ 👨🏼‍⚕️ Transfert de connaissances et traduction automatique de neurones en pratique 📄 🎆 🙇🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La traduction automatique neuronale (NMT) se développe très rapidement. Aujourd'hui, pour assembler votre traducteur, vous n'avez pas besoin d'avoir d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Transfert de connaissances et traduction automatique de neurones en pratique</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475750/"> La traduction automatique neuronale (NMT) se développe très rapidement.  Aujourd'hui, pour assembler votre traducteur, vous n'avez pas besoin d'avoir deux formations supérieures.  Mais pour former le modèle, vous avez besoin d'un grand corpus parallèle (un corpus dans lequel la traduction dans la langue source est associée à la phrase).  En pratique, nous parlons d'au moins un million de paires de phrases.  Il existe même un vaste domaine distinct du FMI qui explore les méthodes d'enseignement des paires de langues avec une petite quantité de données sous forme électronique (English Low Resource NMT). <br><br>  Nous collectons le corps tchouvacho-russe et en même temps nous examinons ce qui peut être fait avec le volume de données disponible.  Dans cet exemple, un cas de 90 000 paires d'offres a été utilisé.  Le meilleur résultat pour le moment a été donné par la méthode de transfert des connaissances (Eng. Transfer Learning), et il sera discuté dans l'article.  Le but de l'article est de donner un exemple pratique de mise en œuvre qui pourrait être facilement reproduit. <a name="habracut"></a><br><br>  Le plan de formation est le suivant.  Nous devons prendre un grand bâtiment (parent), former un modèle neuronal dessus, puis former notre modèle fille.  De plus, la langue cible de la traduction sera la même: le russe.  Intuitivement, cela peut être comparé à l'apprentissage d'une deuxième langue.  Il est plus facile d'apprendre, en connaissant une langue étrangère.  Cela ressemble également à l'étude d'une zone étroite d'une langue étrangère, par exemple, la terminologie médicale de la langue anglaise: vous devez d'abord apprendre l'anglais en général. <br><br>  En tant que corps parental, ils ont essayé de prendre 1 million de paires de phrases <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">du corps parallèle anglais-russe</a> et 1 million <a href="" rel="nofollow">du corps kazakh-russe</a> .  Il y a 5 millions de phrases dans les données kazakhes.  Parmi ceux-ci, seuls ceux dont le coefficient de conformité (troisième colonne) était supérieur à 2. La version kazakhe a donné des résultats légèrement meilleurs.  Il semble intuitivement que cela soit compréhensible, car les langues tchouvache et kazakhe se ressemblent davantage.  Mais en fait, cela n'est pas prouvé, et dépend également grandement de la qualité du boîtier.  Plus de détails sur la sélection de l'organe parental peuvent être trouvés <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">dans cet article</a> .  À propos du corps subsidiaire de 90 000 paires d'offres, vous pouvez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">trouver et demander des exemples de données ici.</a> <br><br>  Passons maintenant au code.  Si vous ne disposez pas de votre propre carte graphique rapide, vous pouvez entraîner le modèle sur le site <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">Colab</a> .  Pour la formation, nous avons utilisé la bibliothèque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">Sockeye</a> .  Il est supposé que Python3 est déjà installé. <br><br><pre><code class="bash hljs">pip install sockeye</code> </pre> <br>  Vous devrez peut-être également bricoler séparément avec <a href="" rel="nofollow">MXNet</a> , qui est chargé de travailler avec la carte vidéo.  Colab a besoin d'une installation de bibliothèque supplémentaire <br><br><pre> <code class="bash hljs">pip install mxnet-cu100mkl</code> </pre> <br>  Concernant les réseaux de neurones, il est généralement admis qu'il leur suffit de fournir les données telles quelles, et ils le découvriront.  Mais en réalité ce n'est pas toujours le cas.  Dans notre cas, le corps doit donc être prétraité.  Tout d'abord, nous le symbolisons afin qu'il soit plus facile pour les modèles de comprendre que «chat!» Et «chat» sont à peu près la même chose.  Par exemple, juste un tokenizer python fera l'affaire. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> WordPunctTokenizer <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tokenize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(src_filename, new_filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(src_filename, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> src_file: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(new_filename, <span class="hljs-string"><span class="hljs-string">"w"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> new_file: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> src_file: new_file.write(<span class="hljs-string"><span class="hljs-string">"%s"</span></span> % <span class="hljs-string"><span class="hljs-string">' '</span></span>.join(WordPunctTokenizer().tokenize(line))) new_file.write(<span class="hljs-string"><span class="hljs-string">"\n"</span></span>)</code> </pre> <br>  En conséquence, nous alimentons des paires de phrases du formulaire <br><br><pre> <code class="xml hljs">  ӗ  ҫ ӳ ӑӑ. ӑ ӑ ӑ ӑӗ, ӑ ӑӑӗ,   ӑ  ӗӗ -ӑ ӗӗҫ,  ҫӗ ӗ ӗҫ ӑӑ ӑӑ, ҫ ӗ ӗ   ӑ ӑ ӑӑ ӑ .</code> </pre> <br>  et <br><br><pre> <code class="xml hljs">     .  , ,       , ,    ,        .</code> </pre> <br>  Le résultat est les offres tokenisées suivantes: <br><br><pre> <code class="xml hljs">  ӗ  ҫ ӳ ӑӑ . ӑ ӑ ӑ ӑӗ , ӑ ӑӑӗ ,   ӑ  ӗӗ  - ӑ ӗӗҫ ,  ҫӗ ӗ ӗҫ ӑӑ ӑӑ , ҫ ӗ ӗ   ӑ ӑ ӑӑ ӑ  .</code> </pre> <br>  et en russe <br><br><pre> <code class="xml hljs">      .   ,  ,        ,  ,     ,         .</code> </pre> <br>  Dans notre cas, nous aurons besoin des dictionnaires combinés des cas parent et enfant, nous allons donc créer des fichiers communs: <br><br><pre> <code class="bash hljs">cp kk.parent.train.tok kkchv.all.train.tok cat chv.child.train.tok &gt;&gt; kk.parent.train.tok cp ru.parent.train.tok ru.all.train.tok cat ru.child.train.tok &gt;&gt; ru.all.train.tok</code> </pre> <br>  puisque la formation continue du modèle enfant a lieu sur le même dictionnaire. <br><br>  Maintenant une petite mais importante digression.  En MP, les phrases sont divisées en atomes sous forme de mots, puis opèrent sur des phrases comme des séquences de mots.  Mais cela ne suffit généralement pas, car une énorme queue est formée à partir des mots qui se produisent une fois dans le corpus.  Construire un modèle probabiliste pour eux est difficile.  Cela est particulièrement vrai pour les langues à morphologie développée (cas, sexe, nombre).  Le russe et le tchouvache ne sont que de telles langues.  Mais il y a une solution.  Vous pouvez diviser la phrase en un niveau inférieur, en sous-mots.  Nous avons utilisé le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">codage par paire d'octets.</a> <br><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rsennrich/subword-nmt.git</code> </pre> <br>  Nous obtenons approximativement de telles séquences de sous-mots <br><br><pre> <code class="xml hljs">@@   ӗ  ҫ ӳ@@  ӑӑ . @@ ӑ ӑ ӑ @@ ӑӗ , ӑ ӑӑ@@ ӗ ,   ӑ@@  @@  ӗӗ  - ӑ@@  ӗ@@ ӗҫ ,  ҫӗ@@  ӗ ӗҫ@@ @@  ӑӑ ӑӑ , ҫ@@ @@ @@  ӗ ӗ @@ @@  @@  ӑ ӑ ӑӑ ӑ  .</code> </pre> <br>  et <br><br><pre> <code class="xml hljs">@@    @@  @@   . @@  @@ @@ @@ @@  , @@  , @@ @@  @@    @@  @@ @@  @@ @@ @@ @@  ,  ,  @@  @@ @@ @@  @@ @@ @@  ,       @@ @@  @@ @@ @@  .</code> </pre> <br>  On peut voir que les affixes se distinguent bien des mots: pas @@ depuis longtemps et bon @@ ça. <br>  Pour ce faire, préparez les dictionnaires bpe <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py --input kkchv.all.train.tok ru.all.train.tok -s 10000 -o bpe.codes --write-vocabulary bpe.vocab.kkchv bpe.vocab.ru</code> </pre> <br>  Et appliquez-les aux jetons, par exemple: <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.kkchv --vocabulary-threshold 50 &lt; kkchv.all.train.tok &gt; kkchv.all.train.bpe !python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.ru --vocabulary-threshold 50 &lt; ru.all.train.tok &gt; ru.all.train.bpe</code> </pre> <br>  Par analogie, vous devez faire pour tous les fichiers: formation, validation et tester les modèles parent et enfant. <br><br>  Nous passons maintenant directement à la formation du modèle neuronal.  Vous devez d'abord préparer des dictionnaires de modèles généraux: <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.all.train.bpe -t ru.all.train.bpe -o kkru_all_data</code> </pre> <br>  Ensuite, entraînez le modèle parent.  Un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">exemple simple est décrit</a> plus en détail <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">sur la page Sockeye.</a>  Techniquement, le processus comprend deux étapes: préparer les données à l'aide de dictionnaires de modèles créés précédemment <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.parent.train.bpe -t ru.parent.train.bpe -o kkru_parent_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  et l'apprentissage lui-même <br><br><pre> <code class="bash hljs">python -m sockeye.train -d kkru_parent_data -vs kk.parent.dev.bpe -vt ru.parent.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o kkru_parent_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10</code> </pre> <br>  La formation dans les installations de Colab prend environ une journée.  Une fois la formation du modèle terminée, vous pouvez la traduire avec <br><br><pre> <code class="bash hljs">python -m sockeye.translate --input kk.parent.test.bpe -m kkru_parent_model --output ru.parent.test_kkru_parent.bpe</code> </pre> <br>  Former le modèle enfant <br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s chv.child.train.bpe -t ru.child.train.bpe -o chvru_child_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  Le code de démarrage de la formation ressemble à ceci: <br><br><pre> <code class="bash hljs">python -m sockeye.train -d chvru_child_data -vs chv.child.dev.bpe -vt ru.child.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o ruchv_150K_skv_dev19_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10 --config kkru_parent_model/args.yaml --params kkru_parent_model/params.best</code> </pre> <br>  Des paramètres sont ajoutés qui indiquent que la configuration et les poids du modèle parent doivent être utilisés comme point de départ.  Détails <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow">dans l'exemple avec recyclage du saumon rouge</a> .  L'apprentissage d'un modèle enfant converge en environ 12 heures. <br><br>  Pour résumer, comparez les résultats.  Le modèle de traduction automatique habituel a donné une qualité de 24,96 BLEU, tandis que le modèle de transfert de connaissances était de 32,38 BLEU.  La différence est également visible visuellement sur des exemples de traductions.  Par conséquent, pendant que nous continuons à assembler le boîtier, nous utiliserons ce modèle. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr475750/">https://habr.com/ru/post/fr475750/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr475738/index.html">5 cours gratuits pour les administrateurs informatiques Microsoft</a></li>
<li><a href="../fr475740/index.html">Le livre "Programmation de l'Olympiade" est sorti</a></li>
<li><a href="../fr475742/index.html">Cas de RetouchMe: ce que nous avons tiré de la localisation de l'application en 35 langues</a></li>
<li><a href="../fr475744/index.html">Modèle d'administrateur système à quatre niveaux</a></li>
<li><a href="../fr475746/index.html">Anatomie des systèmes acoustiques: Cermets et composites - À propos des diffuseurs audio de moniteur</a></li>
<li><a href="../fr475754/index.html">Short sur Scrum</a></li>
<li><a href="../fr475756/index.html">Food Design Digest octobre 2019</a></li>
<li><a href="../fr475758/index.html">Niveaux d'abonnement renouvelables automatiquement dans l'application iOS</a></li>
<li><a href="../fr475760/index.html">Développeurs juniors - pourquoi nous les embauchons et comment nous travaillons avec eux</a></li>
<li><a href="../fr475762/index.html">Un curriculum vitae avec une photo vole dans une urne. Caractéristiques de la recherche d'emploi aux États-Unis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>