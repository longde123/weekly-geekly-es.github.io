<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏽 🎪 🖐🏿 Microsoft ML Spark: une extension Spark qui rend SparkML plus humain et LightGBM en bonus ↔️ 👩🏽‍🎓 🗜️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Beaucoup de ceux qui ont travaillé avec Spark ML savent que certaines des choses qu'ils ont faites là-bas "ne sont pas entièrement réussies". 
 ou pas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Microsoft ML Spark: une extension Spark qui rend SparkML plus humain et LightGBM en bonus</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/raiffeisenbank/blog/456668/"><p>  Beaucoup de ceux qui ont travaillé avec Spark ML savent que certaines des choses qu'ils ont faites là-bas "ne sont pas entièrement réussies". <br>  ou pas du tout fait.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La position des développeurs Spark est</a> que SparkML est la plate-forme de base, et toutes les extensions doivent être des packages séparés.  Mais ce n'est pas toujours pratique, car le Data Scientist et les analystes veulent travailler avec des outils familiers (Jupter, Zeppelin), où il y a la plupart de ce qui est nécessaire.  Ils ne veulent pas collecter des fichiers JAR de 500 mégaoctets avec maven-assembly ou télécharger des dépendances entre leurs mains et les ajouter aux paramètres de démarrage de Spark.  Un travail plus fin avec les systèmes de construction de projets JVM peut nécessiter beaucoup d'efforts supplémentaires de la part des analystes et des DataScientists habitués à Jupyter / Zeppelin.  Demander à DevOps et aux administrateurs de cluster de mettre un paquet de packages sur les nœuds de calcul est clairement une mauvaise idée.  Quiconque a écrit des extensions pour SparkML par lui-même sait combien de difficultés cachées il y a avec les classes et méthodes importantes (qui pour une raison quelconque sont privées [ml]), les restrictions sur les types de paramètres stockés, etc. </p><br><p>  Et il semble que maintenant, avec la bibliothèque MMLSpark, la vie sera un peu plus facile, et le seuil pour entrer dans le machine learning évolutif avec SparkML et Scala est légèrement plus bas. </p><a name="habracut"></a><br><h2 id="vvedenie">  Présentation </h2><br><p> En raison d'un certain nombre de difficultés, ainsi que d'un ensemble clairsemé de méthodes et de solutions prêtes à l'emploi dans SparkML, de nombreuses entreprises écrivent leurs extensions pour Spark.  Un exemple est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PravdaML</a> , qui est en cours de développement à Odnoklassniki et qui, à en juger par une évaluation rapide de ce qui se trouve sur GitHub, semble très prometteur.  Malheureusement, la plupart de ces solutions sont généralement fermées ou ouvertes, mais n'ont pas la possibilité d'installer via Maven / sbt et la documentation de l'API, ce qui rend très difficile de travailler avec elles. </p><br><p>  Aujourd'hui, nous regardons la bibliothèque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MMLSpark</a> . </p><br><p>  Nous considérerons, comme d'habitude, l'exemple de la tâche de classement des passagers du Titanic.  Le but est de montrer autant de fonctionnalités de la bibliothèque MMLSpark que possible, pas <del>  assommer SOTA sur ImageNet </del>  montrer un apprentissage machine cool.  Alors le Titanic fera l'affaire. </p><br><p><img src="https://habrastorage.org/webt/rg/bt/sf/rgbtsf7j0ovmfa5lpjkfpfapzti.jpeg"></p><br><p>  La bibliothèque elle-même possède une API native pour Scala ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> ), Python API ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> ) et, à en juger par certains endroits du référentiel GitHub, elle aura bientôt une API pour R. </p><br><p>  Il existe de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bons exemples d'ordinateurs portables dans le</a> projet GitHub <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">(PySpark + Jupyter)</a> , mais nous irons dans l'autre sens.  Comme l'écrivait Dmitry Bugaychenko, si vous développez pour Spark, c'est-à-dire que vous avez toutes les raisons d'utiliser Scala pour cela, de plus, Scala vous permet de définir de manière beaucoup plus efficace et plus flexible vos propres Transformer et Estimator pour les incorporer dans SparkML Pipeline, mais la lenteur avec laquelle Numpy fonctionne Le code / pandas en UDF (appelé sur les exécutables de la JVM) a déjà été beaucoup écrit. </p><br><h2 id="kratko-ob-ustanovke">  Dossier d'installation </h2><br><p>  L'ordinateur portable entier est disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  Pour fonctionner avec le Titanic, l'image Zeppelin Docker s'exécutant localement sur un ordinateur portable avec des paramètres par défaut est suffisante pour les yeux.  Docker peut être trouvé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  La bibliothèque MMLSpark n'est pas dans Maven Central, mais dans des spark-packages, et pour l'ajouter à Zeppelin, vous devez exécuter le bloc suivant au début de l'ordinateur portable: </p><br><pre><code class="plaintext hljs">%spark.dep z.addRepo("bintray.com").url("http://dl.bintray.com/spark-packages/maven/") z.load("Azure:mmlspark:0.17")</code> </pre> <br><p>  Il vaut la peine de dire que la bibliothèque a une excellente compatibilité descendante: contrairement, par exemple, au XGBoost4j-spark, qui nécessite un minimum de Spark 2.3+, cette chose est entrée dans Spark 2.2.1, fourni avec l'image Zeppelin Docker, et toutes les difficultés Je ne l'ai pas remarqué. </p><br><p>  <strong>Remarque: la</strong> plupart de la bibliothèque MMLSpark est dédiée à l'inférence de grilles sur le cluster, pour lesquelles CNTK est présent (qui, à en juger par la documentation, devrait lire des modèles cntk prêts à l'emploi) et un énorme bloc OpenCV.  Nous allons nous concentrer sur des tâches plus banales et essayer de «simuler» le cas lorsque nous avons d'énormes tableaux de données tabulaires qui se trouvent dans HDFS sous la forme de .csv, de tableaux ou dans un autre format.  Nous devons donc les pré-traiter et construire un modèle, alors que ces données ne rentrent pas dans la mémoire d'une machine.  Par conséquent, nous effectuerons toutes les actions sur le cluster. </p><br><h2 id="chtenie-i-razvedochnyy-analiz">  Lecture et analyse de l'intelligence </h2><br><p>  En général, Spark + Zeppelin n'est pas mal du tout et peut faire face à la tâche EDA, mais nous allons essayer d'étendre leurs capacités.  Tout d'abord, nous importons les classes dont nous avons besoin: </p><br><ul><li>  Tout de spark.sql.types pour déclarer un schéma et lire les données correctement </li><li>  Tout depuis spark.sql.functions pour accéder aux colonnes et utiliser les fonctions intégrées </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">com.microsoft.ml.spark.SummarizeData</a> , qui peut être appelé un analogue de pandas.DataFrame.describe </li></ul><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">SummarizeData</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql.functions._ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql.types._</code> </pre> <br><p>  Nous lisons notre dossier: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> titanicSchema = <span class="hljs-type"><span class="hljs-type">StructType</span></span>( <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Passanger"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"PClass"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Name"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Age"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"SibSp"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Parch"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Ticket"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Fare"</span></span>, <span class="hljs-type"><span class="hljs-type">FloatType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Embarked"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">Nil</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = spark .read .schema(titanicSchema) .option(<span class="hljs-string"><span class="hljs-string">"header"</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) .csv(<span class="hljs-string"><span class="hljs-string">"/mountV/titanic/train.csv"</span></span>)</code> </pre> <br><p>  Et maintenant, regardons les données elles-mêmes, ainsi que leur taille: </p><br><pre> <code class="scala hljs">println(<span class="hljs-string"><span class="hljs-string">s"Train shape is: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${train.count}</span></span></span><span class="hljs-string"> x </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${train.columns.length}</span></span></span><span class="hljs-string">"</span></span>) train.limit(<span class="hljs-number"><span class="hljs-number">5</span></span>).createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"trainHead"</span></span>)</code> </pre> <br><p>  <strong>Remarque:</strong> Il n'est vraiment pas nécessaire d'utiliser createOrReplaceTempView lorsque vous pouvez simplement écrire .show (5).  Mais show a un problème: lorsque les données sont "larges", alors la représentation textuelle de la plaque "flotte", et rien ne devient clair du tout. </p><br><p>  Obtenez la taille de nos données: La <code>Train shape is: 891 x 12</code> <br>  Et maintenant, dans la cellule sql, nous pouvons regarder les 5 premières lignes: </p><br><pre> <code class="sql hljs">%sql <span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> trainHead</code> </pre> <br><p><img src="https://habrastorage.org/webt/qe/jc/wj/qejcwjvi65jp6xucnorcdfdmxo4.png"></p><br><p>  Eh bien, voyons le résumé sur notre table: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SummarizeData</span></span>() .setBasic(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setCounts(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setPercentiles(<span class="hljs-literal"><span class="hljs-literal">false</span></span>) .setSample(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setErrorThreshold(<span class="hljs-number"><span class="hljs-number">0.25</span></span>) .transform(train) .createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"summary"</span></span>)</code> </pre> <br><p>  La classe SummarizeData présente plusieurs avantages par rapport au simple Dataset.describe, car elle vous permet de compter le nombre de valeurs manquantes et uniques, et vous permet également de spécifier la précision du calcul des quantiles.  Cela peut être critique pour les très gros volumes de données. <br><img src="https://habrastorage.org/webt/91/g4/8c/91g48c0oaqiuz8pjgklo38jaiis.png"></p><br><div class="spoiler">  <b class="spoiler_title">Quelques réflexions personnelles</b> <div class="spoiler_text"><p>  En général, il me semblait personnellement que Odnoklassniki dans PravdaML avait une meilleure implémentation de l'analogue SummarizeData.  Microsoft a opté pour la simplicité et utilise <code>org.apache.spark.sql.functions</code> , c'est juste que tout est commodément emballé dans une seule classe.  Pour Odnoklassniki, cela est implémenté via leur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>VectorStatCollector</code></a> , qui nécessite un code un peu plus complexe lors de l'appel (vous devez d'abord ajouter toutes les fonctionnalités à un vecteur) et peut nécessiter des opérations supplémentaires (par exemple, <code>VectorAssembler</code> refuse généralement de digérer <code>DecimalType</code> ).  Mais j'ai une hypothèse basée sur mon expérience avec Spark que SummarizeData de MMLSpark pourrait planter avec des erreurs comme <code>StackOverflow</code> dans <code>org.apache.spark.sql.catalyst</code> s'il y a <strong>vraiment beaucoup de</strong> colonnes, et le graphique de calcul n'est pas petit au moment où il démarre ( bien que spécialement pour ces fans de "l'extrême" dans Spark 2.4, ils ont ajouté la possibilité de réduire l'optimiseur de graphique <code>Catalyst</code> ).  Eh bien, il semble qu'avec un <strong>très grand</strong> nombre de colonnes, la version de Microsoft sera plus lente.  Mais cela, bien sûr, doit être vérifié séparément. </p></div></div><br><h2 id="ochistka-dannyh">  Nettoyage des données </h2><br><p>  Dans le Titanic, tout est comme d'habitude - un tas de colonnes de chaînes ont des valeurs manquantes.  Et une sorte de dévers dans les données (il semble que cette version particulière des données ne soit pas très spécifique) - 25 lignes à partir des valeurs manquantes.  Tout d'abord, corrigez ceci: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> trainFiltered = train.filter(!(isnan(col(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>)) || isnull(col(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>))))</code> </pre> <br><h3 id="obrabotka-strokovyh-dannyh">  Traitement des données de chaîne </h3><br><p>  Pour autant que je m'en souvienne, les attributs sortis des champs <code>Name</code> et <code>Cabin</code> étaient les meilleurs apportés dans le Titanic.  Vous pouvez en fournir beaucoup, mais nous nous limiterons à quelques-uns, juste pour ne pas donner d'exemples de presque le même code. </p><br><p>  Habituellement, il est pratique d'utiliser des expressions régulières pour de telles choses. <br>  Mais nous voulons dans ce cas: </p><br><ul><li>  tout a été distribué, les données ont été traitées au même endroit; </li><li>  tout a été conçu en tant que classes SpakrML Transformer ou Spark ML Estimator, afin que plus tard, il puisse être assemblé dans Pipeline. </li></ul><br><p>  <strong>Remarque:</strong> Pipeline, tout d'abord, nous garantit que nous appliquons toujours les mêmes transformations au train et au test, et nous permet également de détecter l'erreur de «regarder vers l'avenir» dans la validation croisée.  Et cela nous donne également des capacités simples pour enregistrer, charger et prévoir à l'aide de notre pipeline. </p><br><p>  SparkML a une classe «presque universelle» pour de telles tâches - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SQLTranformer</a> , mais l'écriture en SQL est clairement pire que l'écriture en Scala, ne serait-ce que parce qu'il est possible d'attraper la syntaxe ou des erreurs typiques lors de la compilation et de la coloration syntaxique dans Idea.  Et ici MMLSpark vient à notre aide, où un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">UDFTransformer</a> vraiment universel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">est implémenté</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span></code> </pre> <br><p>  Pour commencer, nous allons créer notre fonction de transformation, qui est très simple à la limite, mais notre objectif est maintenant de montrer le processus de création de UDFTransformer.  En principe, sur la base d'exemples aussi simples, n'importe qui peut ajouter de la logique à n'importe quel niveau de complexité. </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> miss = <span class="hljs-string"><span class="hljs-string">".*miss\\..*"</span></span>.r <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> mr = <span class="hljs-string"><span class="hljs-string">".*mr\\..*"</span></span>.r <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> mrs = <span class="hljs-string"><span class="hljs-string">".*mrs\\..*"</span></span>.r <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> master = <span class="hljs-string"><span class="hljs-string">".*master.*"</span></span>.r <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convertNames</span></span></span></span>(input: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-type"><span class="hljs-type">Option</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>] = { <span class="hljs-type"><span class="hljs-type">Option</span></span>(input).map(x =&gt; { x.toLowerCase <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> miss() =&gt; <span class="hljs-string"><span class="hljs-string">"Miss"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> mr() =&gt; <span class="hljs-string"><span class="hljs-string">"Mr"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> mrs() =&gt; <span class="hljs-string"><span class="hljs-string">"Mrs"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> master() =&gt; <span class="hljs-string"><span class="hljs-string">"Master"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> _ =&gt; <span class="hljs-string"><span class="hljs-string">"Unknown"</span></span> } }) }</code> </pre> <br><p>  (Vous pouvez immédiatement voir à quel point Scala est pratique pour travailler avec des valeurs manquantes qui, soit dit en passant, sont non seulement <code>null</code> , mais aussi <code>Double.NaN</code> , mais il y a <del>  une telle blague </del>  une chose aussi rare que les omissions dans les variables <code>BooleanType</code> , etc.) </p><br><p>  <code>UserDefinedFunction</code> maintenant notre <code>UserDefinedFunction</code> et créez immédiatement un <code>Transformer</code> basé sur lui: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> nameTransformUDF = udf(convertNames _) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> nameTransformer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span>() .setUDF(nameTransformUDF) .setInputCol(<span class="hljs-string"><span class="hljs-string">"Name"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"NameType"</span></span>)</code> </pre> <br><p>  <strong>Remarque:</strong> Dans un ordinateur portable Zeppelin, c'est toujours la même chose, mais quand tout est réuni plus tard dans le code de production, il est important que tous les UDF soient dans des classes ou des objets qui sont <code>extends Serializable</code> .  La chose évidente que vous pouvez parfois oublier puis vous plonger pendant longtemps est ce qui ne va pas avec la lecture des longues traces de pile des erreurs Spark. </p><br><p>  Maintenant, nous avons encore le champ <code>Cabin</code> .  Examinons-le de plus près: <br><img src="https://habrastorage.org/webt/1m/sn/n2/1msnn2zqeyerzpysj2ma3bwzuda.png"></p><br><p>  On voit qu'il y a beaucoup de valeurs manquantes, il y a des lettres, des chiffres, différentes combinaisons, etc.  Prenons le nombre de cabines (s'il y en a plusieurs), ainsi que les nombres - ils ont probablement une sorte de logique, par exemple, si la numérotation se fait à une extrémité du navire, alors les cabines à l'avant avaient moins de chance.  Nous allons également créer des fonctions, puis en nous basant sur <code>UDFTransformer</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getCabinsCount</span></span></span></span>(input: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-type"><span class="hljs-type">Int</span></span> = { <span class="hljs-type"><span class="hljs-type">Option</span></span>(input) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">Some</span></span>(x) =&gt; x.split(<span class="hljs-string"><span class="hljs-string">" "</span></span>).length <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">None</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">-1</span></span> } } <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numPattern = <span class="hljs-string"><span class="hljs-string">"([az])([0-9]+)"</span></span>.r <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getNumbersFromCabin</span></span></span></span>(input: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-type"><span class="hljs-type">Int</span></span> = { <span class="hljs-type"><span class="hljs-type">Option</span></span>(input) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">Some</span></span>(x) =&gt; { x.split(<span class="hljs-string"><span class="hljs-string">" "</span></span>)(<span class="hljs-number"><span class="hljs-number">0</span></span>).toLowerCase <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> numPattern(sym, num) =&gt; <span class="hljs-type"><span class="hljs-type">Integer</span></span>.parseInt(num) <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> _ =&gt; <span class="hljs-number"><span class="hljs-number">-1</span></span> } } <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">None</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">-2</span></span> } } <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cabinsCountUDF = udf(getCabinsCount _) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numbersFromCabinUDF = udf(getNumbersFromCabin _) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cabinsCountTransformer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span>() .setInputCol(<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"CabinCount"</span></span>) .setUDF(cabinsCountUDF) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numbersFromCabinTransformer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span>() .setInputCol(<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"CabinNumber"</span></span>) .setUDF(numbersFromCabinUDF)</code> </pre> <br><p>  Commençons maintenant par les valeurs manquantes, à savoir l'âge.  Tout d'abord, profitons des capacités de visualisation de Zeppelin: </p><br><p><img src="https://habrastorage.org/webt/h-/cj/l6/h-cjl6tzbakgmqxc1w-gt_kgwk0.png"></p><br><p>  Et voyez comment les valeurs manquantes gâchent tout.  Il serait logique de les remplacer par un milieu (ou médiane), mais notre objectif est de considérer toutes les fonctionnalités de la bibliothèque MMLSpark.  Par conséquent, nous écrirons notre propre <code>Estimator</code> , qui considérera les groupes / moyennes sur l'échantillon de formation et les remplacera par les lacunes correspondantes. </p><br><p>  Nous aurons besoin de: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql.{<span class="hljs-type"><span class="hljs-type">Dataset</span></span>, <span class="hljs-type"><span class="hljs-type">DataFrame</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.{<span class="hljs-type"><span class="hljs-type">Estimator</span></span>, <span class="hljs-type"><span class="hljs-type">Model</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.param.{<span class="hljs-type"><span class="hljs-type">Param</span></span>, <span class="hljs-type"><span class="hljs-type">ParamMap</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.util.<span class="hljs-type"><span class="hljs-type">Identifiable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.util.<span class="hljs-type"><span class="hljs-type">DefaultParamsWritable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.{<span class="hljs-type"><span class="hljs-type">HasInputCol</span></span>, <span class="hljs-type"><span class="hljs-type">HasOutputCol</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">ConstructorWritable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">ConstructorReadable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">Wrappable</span></span></code> </pre> <br><p>  Prenons attention à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>ConstructorWritable</code></a> , qui simplifie considérablement la vie.  Si notre <code>Model</code> est un modèle «entraîné» renvoyé par la méthode <code>fit(),</code> , qui est entièrement déterminé par son constructeur (et c'est probablement 99% des cas), alors nous ne pouvons pas du tout écrire la sérialisation avec nos mains.  Cela simplifie et accélère considérablement le développement, élimine les erreurs et abaisse également le seuil d'entrée pour DataScientist et les analystes qui ne sont généralement pas des programmeurs professionnels. </p><br><p>  Définissez notre classe <code>Estimator</code> .  En fait, la chose la plus importante ici est la méthode d' <code>fit</code> , les autres sont des points techniques: </p><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GroupImputerEstimator</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">override val uid: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span></span><span class="hljs-class">) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Estimator</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">] </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">HasInputCol</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">HasOutputCol</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Wrappable</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">DefaultParamsWritable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">this</span></span></span></span>() = <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>(<span class="hljs-type"><span class="hljs-type">Identifiable</span></span>.randomUID(<span class="hljs-string"><span class="hljs-string">"GroupImputer"</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> groupCol: <span class="hljs-type"><span class="hljs-type">Param</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>] = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Param</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>]( <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>, <span class="hljs-string"><span class="hljs-string">"groupCol"</span></span>, <span class="hljs-string"><span class="hljs-string">"Groupping column"</span></span> ) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">setGroupCol</span></span></span></span>(v: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">type</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.set(groupCol, v) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getGroupCol</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">String</span></span> = $(groupCol) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span></span>(dataset: <span class="hljs-type"><span class="hljs-type">Dataset</span></span>[_]): <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span> = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> meanDF = dataset .toDF .groupBy($(groupCol)) .agg(mean(col($(inputCol))).alias(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>)) .select(col($(groupCol)), col(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>( uid, meanDF, getInputCol, getOutputCol, getGroupCol ) } <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transformSchema</span></span></span></span>(schema: <span class="hljs-type"><span class="hljs-type">StructType</span></span>): <span class="hljs-type"><span class="hljs-type">StructType</span></span> = schema .add( <span class="hljs-type"><span class="hljs-type">StructField</span></span>( $(outputCol), schema.filter(x =&gt; x.name == $(inputCol))(<span class="hljs-number"><span class="hljs-number">0</span></span>).dataType ) ) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copy</span></span></span></span>(extra: <span class="hljs-type"><span class="hljs-type">ParamMap</span></span>): <span class="hljs-type"><span class="hljs-type">Estimator</span></span>[<span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>] = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> to = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerEstimator</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.uid) copyValues(to, extra).asInstanceOf[<span class="hljs-type"><span class="hljs-type">GroupImputerEstimator</span></span>] } }</code> </pre> <br><p>  <strong>Remarque:</strong> Je n'ai pas utilisé defaultCopy, car lorsque j'ai appelé, pour une raison quelconque, il a juré que je n'avais pas de constructeur. \ &lt;init&gt; (java.lang.String), bien qu'il semble que cela n'aurait pas dû se produire.  Dans tous les cas, l'implémentation de la <code>copy</code> facile. </p><br><p>  Vous devez maintenant implémenter <code>Model</code> - une classe qui décrit le modèle entraîné et implémente la méthode de <code>transform</code> .  Nous allons le construire sur la base de la fonction <code>coalesce</code> intégrée dans <code>org.apache.spark.sql.functions</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GroupImputerModel</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params"> val uid: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val meanDF: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">DataFrame</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val inputCol: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val outputCol: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val groupCol: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params"> </span></span></span><span class="hljs-class">) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Model</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">] </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ConstructorWritable</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> ttag: <span class="hljs-type"><span class="hljs-type">TypeTag</span></span>[<span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>] = typeTag[<span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">objectsToSave</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">List</span></span>[<span class="hljs-type"><span class="hljs-type">Any</span></span>] = <span class="hljs-type"><span class="hljs-type">List</span></span>(uid, meanDF, inputCol, outputCol, groupCol) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copy</span></span></span></span>(extra: <span class="hljs-type"><span class="hljs-type">ParamMap</span></span>): <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>(uid, meanDF, inputCol, outputCol, groupCol) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transform</span></span></span></span>(dataset: <span class="hljs-type"><span class="hljs-type">Dataset</span></span>[_]): <span class="hljs-type"><span class="hljs-type">DataFrame</span></span> = { dataset .toDF .join(meanDF, <span class="hljs-type"><span class="hljs-type">Seq</span></span>(groupCol), <span class="hljs-string"><span class="hljs-string">"left"</span></span>) .withColumn( outputCol, coalesce(col(inputCol), col(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>)) .cast(<span class="hljs-type"><span class="hljs-type">IntegerType</span></span>)) .drop(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>) } <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transformSchema</span></span></span><span class="hljs-function"> </span></span>(schema: <span class="hljs-type"><span class="hljs-type">StructType</span></span>): <span class="hljs-type"><span class="hljs-type">StructType</span></span> = schema .add( <span class="hljs-type"><span class="hljs-type">StructField</span></span>(outputCol, schema.filter(x =&gt; x.name == inputCol)(<span class="hljs-number"><span class="hljs-number">0</span></span>).dataType) ) }</code> </pre> <br><p>  Le dernier objet que nous devons déclarer est un <code>Reader</code> , que nous implémentons à l'aide de la classe MMLSpark <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ConstructorReadable</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GroupImputerModel</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ConstructorReadable</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">]</span></span></code> </pre> <br><h2 id="sozdanie-pipeline">  Création de pipeline </h2><br><p>  Dans Pipeline, je voudrais montrer à la fois les classes SparkML habituelles et la chose incroyablement pratique de MMLSpark - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MultiColumnAdapter</a> , qui vous permet d'appliquer des transformateurs SparkML à plusieurs colonnes à la fois (pour référence, par exemple, StringIndexer et OneHotEncoder prennent exactement une colonne à l'entrée, ce qui les transforme annonce dans la douleur): </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.feature.{<span class="hljs-type"><span class="hljs-type">StringIndexer</span></span>, <span class="hljs-type"><span class="hljs-type">VectorAssembler</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.<span class="hljs-type"><span class="hljs-type">Pipeline</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.{<span class="hljs-type"><span class="hljs-type">MultiColumnAdapter</span></span>, <span class="hljs-type"><span class="hljs-type">LightGBMClassifier</span></span>}</code> </pre> <br><p>  Tout d'abord, nous allons déclarer quelles colonnes nous avons: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> catCols = <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>, <span class="hljs-string"><span class="hljs-string">"Embarked"</span></span>, <span class="hljs-string"><span class="hljs-string">"NameType"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numCols = <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-string"><span class="hljs-string">"PClass"</span></span>, <span class="hljs-string"><span class="hljs-string">"AgeNoMissings"</span></span>, <span class="hljs-string"><span class="hljs-string">"SibSp"</span></span>, <span class="hljs-string"><span class="hljs-string">"Parch"</span></span>, <span class="hljs-string"><span class="hljs-string">"CabinCount"</span></span>, <span class="hljs-string"><span class="hljs-string">"CabinNumber"</span></span>)</code> </pre> <br><p>  Créez maintenant un encodeur de chaînes: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> stringEncoder = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">MultiColumnAdapter</span></span>() .setBaseStage(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">StringIndexer</span></span>().setHandleInvalid(<span class="hljs-string"><span class="hljs-string">"keep"</span></span>)) .setInputCols(catCols) .setOutputCols(catCols.map(x =&gt; x + <span class="hljs-string"><span class="hljs-string">"_freqEncoded"</span></span>))</code> </pre> <br><p>  <strong>Remarque:</strong> contrairement à scikit-learn dans SparkML, <code>StringIndexer</code> fonctionne sur le principe de l'encodeur de fréquence, et il peut être utilisé pour spécifier une relation d'ordre (c'est-à-dire catégorie 0 &lt;catégorie 1, et cela a du sens) - cette approche fonctionne souvent bien pour arbres décisifs. </p><br><p>  <code>Imputer</code> notre <code>Imputer</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> missingImputer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerEstimator</span></span>() .setInputCol(<span class="hljs-string"><span class="hljs-string">"Age"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"AgeNoMissings"</span></span>) .setGroupCol(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>)</code> </pre> <br><p>  Et <code>VectorAssembler</code> , puisque les classificateurs SparkML sont plus à l'aise de travailler avec <code>VectorType</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> assembler = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">VectorAssembler</span></span>() .setInputCols(stringEncoder.getOutputCols ++ numCols) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>)</code> </pre> <br><p>  Nous allons maintenant utiliser le boost de gradient fourni avec MMLSpark - LightGBM, qui est inclus dans le "Big Three" des meilleures implémentations de cet algorithme avec XGBoost et CatBoost.  Il fonctionne beaucoup plus rapidement, mieux et plus stable que l'implémentation GBM de SparkML (même en tenant compte du fait que le port JVM est toujours en développement actif): </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> catColIndices = <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> lgbClf = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LightGBMClassifier</span></span>() .setFeaturesCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>) .setLabelCol(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>) .setProbabilityCol(<span class="hljs-string"><span class="hljs-string">"predictedProb"</span></span>) .setPredictionCol(<span class="hljs-string"><span class="hljs-string">"predictedLabel"</span></span>) .setRawPredictionCol(<span class="hljs-string"><span class="hljs-string">"rawPrediction"</span></span>) .setIsUnbalance(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setCategoricalSlotIndexes(catColIndices) .setObjective(<span class="hljs-string"><span class="hljs-string">"binary"</span></span>)</code> </pre> <br><p>  <strong>Remarque:</strong> LightGBM prend en charge le travail avec des variables catégorielles (presque comme catboost), nous lui avons donc indiqué à l'avance où se trouvent les attributs de catégorie dans notre vecteur, et lui-même saura quoi faire avec eux et comment les coder. </p><br><div class="spoiler">  <b class="spoiler_title">En savoir plus sur les fonctionnalités LightGBM pour Spark</b> <div class="spoiler_text"><ul><li>  Sur les nœuds exécutant RadHat LightGBM, toute version, à l'exception de la toute dernière, plantera du fait qu'il <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">n'aime pas la</a> version <code>glibc</code> .  Cela a été résolu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">récemment</a> , cependant, lors de l'installation via Maven, MMLSpark extrait l'avant-dernière version de LightGBM lors de l'installation via Maven, vous devez donc ajouter la dépendance de la dernière version sur RadHat avec vos mains. </li><li>  LightGBM dans son travail crée une socket sur le pilote pour la communication avec les dirigeants, et il le fait en utilisant le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>new java.net.ServerSocket(0)</code></a> , et donc un port aléatoire des ports éphémères du système d'exploitation est utilisé.  Si la plage de ports éphémères diffère de la plage de ports ouverts par le pare-feu, alors <del>  peut brûler beaucoup </del>  Vous pouvez obtenir un effet intéressant lorsque LightGBM fonctionne parfois (lorsque j'ai choisi un bon port), et parfois non.  Et il y aura des erreurs comme <code>ConnectionTimeOut</code> , qui peuvent également indiquer, par exemple, l'option lorsque GC se bloque sur des cadres ou quelque chose comme ça.  En général, ne répétez pas mes erreurs. </li></ul></div></div><br><p>  Enfin, déclarons notre Pipeline: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pipeline = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>() .setStages( <span class="hljs-type"><span class="hljs-type">Array</span></span>( missingImputer, nameTransformer, cabinsCountTransformer, numbersFromCabinTransformer, stringEncoder, assembler, lgbClf ) )</code> </pre> <br><h2 id="obuchenie">  La formation </h2><br><p>  Nous allons briser notre ensemble de formation en un train et un test et vérifier notre pipeline.  Ici, vous pouvez simplement évaluer la commodité du pipeline, car il est complètement indépendant de la partition et garantit que nous appliquerons les mêmes transformations pour former et tester, et tous les paramètres de transformation seront appris sur le train: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> <span class="hljs-type"><span class="hljs-type">Array</span></span>(trainDF, testDF) = trainFiltered.randomSplit(<span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0.8</span></span>, <span class="hljs-number"><span class="hljs-number">0.2</span></span>)) println(<span class="hljs-string"><span class="hljs-string">s"Train rows: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${trainDF.count}</span></span></span><span class="hljs-string">\nTest rows: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${testDF.count}</span></span></span><span class="hljs-string">"</span></span>) <span class="hljs-comment"><span class="hljs-comment">// Train rows: 708 // Test rows: 158 val predictions = pipeline .fit(trainDF) .transform(testDF)</span></span></code> </pre> <br><p>  Pour un calcul pratique des métriques, nous utiliserons une autre classe de MMLSpark - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ComputeModelStatistics</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">ComputeModelStatistics</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.metrics.<span class="hljs-type"><span class="hljs-type">MetricConstants</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> modelEvaluator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ComputeModelStatistics</span></span>() .setLabelCol(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>) .setScoresCol(<span class="hljs-string"><span class="hljs-string">"predictedProb"</span></span>) .setScoredLabelsCol(<span class="hljs-string"><span class="hljs-string">"predictedLabel"</span></span>) .setEvaluationMetric(<span class="hljs-type"><span class="hljs-type">MetricConstants</span></span>.<span class="hljs-type"><span class="hljs-type">ClassificationMetrics</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/we/rc/ba/wercbac58qpt0hsygugtqqkhc3u.png"></p><br><p>  Pas mal, étant donné que nous n'avons pas modifié les paramètres par défaut. </p><br><h2 id="podbor-giperparametrov">  Sélection d'hyperparamètres </h2><br><p>  Pour sélectionner des hyperparamètres dans MMLSpark, il y a une chose sympa distincte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>TuneHyperparameters</code></a> , qui implémente une recherche aléatoire sur la grille.  Cependant, malheureusement, il ne prend pas encore en charge <code>Pipeline</code> , nous allons donc utiliser le SparkML <code>CrossValidator</code> habituel: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.tuning.{<span class="hljs-type"><span class="hljs-type">ParamGridBuilder</span></span>, <span class="hljs-type"><span class="hljs-type">CrossValidator</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.evaluation.<span class="hljs-type"><span class="hljs-type">BinaryClassificationEvaluator</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> paramSpace = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ParamGridBuilder</span></span>() .addGrid(lgbClf.maxDepth, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) .addGrid(lgbClf.learningRate, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) .addGrid(lgbClf.numIterations, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>)) .build println(<span class="hljs-string"><span class="hljs-string">s"Size of ParamsGrid: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${paramSpace.size}</span></span></span><span class="hljs-string">"</span></span>) <span class="hljs-comment"><span class="hljs-comment">// Size of ParamsGrid: 8 val crossValidator = new CrossValidator() .setEstimator(pipeline) .setEstimatorParamMaps(paramSpace) .setNumFolds(3) .setSeed(42L) .setEvaluator( new BinaryClassificationEvaluator() .setMetricName("areaUnderROC") .setLabelCol("Survived") .setRawPredictionCol("rawPrediction") ) val bestModel = crossValidator .fit(trainFiltered)</span></span></code> </pre> <br><p>  Malheureusement, je n'ai pas trouvé de moyen pratique de voir les résultats avec les paramètres sur lesquels ils ont été obtenus.  Par conséquent, il est nécessaire d'utiliser des conceptions "monstrueuses": </p><br><pre> <code class="scala hljs">crossValidator .getEstimatorParamMaps .zip(bestModel.avgMetrics) .foreach(x =&gt; { println( <span class="hljs-string"><span class="hljs-string">"\n"</span></span> + x._1 .toSeq .foldLeft(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">StringBuilder</span></span>())( (a, b) =&gt; a .append(<span class="hljs-string"><span class="hljs-string">s"\n\t</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${b.param.name}</span></span></span><span class="hljs-string"> : </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${b.value}</span></span></span><span class="hljs-string">"</span></span>)) .toString + <span class="hljs-string"><span class="hljs-string">s"\n\tMetric: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${x._2}</span></span></span><span class="hljs-string">"</span></span> ) })</code> </pre> <br><p>  Ce qui nous donne quelque chose comme ça: <br><img src="https://habrastorage.org/webt/xl/li/o7/xllio7tbr67gq_wqolrqwzkzswu.png"></p><br><p>  Nous avons obtenu le meilleur résultat en réduisant la vitesse d'apprentissage et en augmentant la profondeur des arbres.  Sur cette base, il serait possible d'ajuster l'espace de recherche et d'arriver à un résultat encore plus optimal, mais nous n'avons tout simplement pas un tel objectif. </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  En fait, alors que MMLSpark a la version 0.17 et contient toujours des bogues distincts.  Mais de toutes les extensions Spark que j'ai vues, MMLSpark a à mon avis la documentation la plus complète et le processus d'installation et de mise en œuvre le plus compréhensible.  Microsoft ne l'a pas encore vraiment promu, il n'y avait qu'un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">rapport sur les Databricks</a> , mais il s'agissait davantage de DeepLearning, et non des choses de routine sur lesquelles j'ai écrit. </p><br><p>  Personnellement, dans nos tâches, cette bibliothèque m'a beaucoup aidé, me permettant de parcourir un peu moins la jungle des sources Spark et de ne pas utiliser la réflexion pour accéder aux méthodes privées [ml], et un de mes collègues a trouvé la bibliothèque presque par accident.  Dans le même temps, en raison du fait que la bibliothèque est en développement actif, la structure du fichier source <del>  bouillie pleine </del>  quelque peu déroutant.  Eh bien, en raison du fait qu'il n'y a pas d'exemples spéciaux ou d'autre documentation (à l'exception de scaladoc nu), au début, je devais explorer la source tout le temps. </p><br><p>  Par conséquent, j'espère vraiment que ce mini-tutoriel (malgré toute son évidence et sa simplicité) sera utile à quelqu'un et permettra d'économiser beaucoup de temps et d'efforts! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr456668/">https://habr.com/ru/post/fr456668/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr456656/index.html">Leçons sur SDL 2: Leçon 4 - Etirement PNG</a></li>
<li><a href="../fr456658/index.html">Growth up: comment nous évaluons les compétences en équipe</a></li>
<li><a href="../fr456662/index.html">Comment économiser de l'argent sur un thérapeute en utilisant le développement piloté par les tests</a></li>
<li><a href="../fr456664/index.html">Qui poursuivre lorsqu'un robot perd votre argent</a></li>
<li><a href="../fr456666/index.html">WebTotem ou comment nous voulons rendre Internet plus sûr</a></li>
<li><a href="../fr456670/index.html">À propos de la méthode d'authentification très espion</a></li>
<li><a href="../fr456672/index.html">Recettes Nginx: notifications asynchrones de PostgreSQL vers websocket</a></li>
<li><a href="../fr456674/index.html">De nouvelles opportunités de promotion sur Facebook que vous ne connaissiez pas</a></li>
<li><a href="../fr456676/index.html">Connexion à une application PHP distribuée</a></li>
<li><a href="../fr456680/index.html">Huit lois de dénomination dans la conception UX (partie 2)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>