<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👁‍🗨 🌑 🤶🏾 Apprentissage profond. Apprentissage fédéré 🎱 ⛵️ 🧘🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut, habrozhiteli! Nous avons récemment remis le livre à Andrew W. Trask, jetant les bases d'une maîtrise accrue des technologies d'apprentissage en...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apprentissage profond. Apprentissage fédéré</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/458800/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/xs/t-/oc/xst-oc7auy1he8nhwbj7bbkhxwk.jpeg" align="left" alt="image"></a>  Salut, habrozhiteli!  Nous avons récemment remis le livre à Andrew W. Trask, jetant les bases d'une maîtrise accrue des technologies d'apprentissage en profondeur.  Il commence par une description des bases des réseaux de neurones, puis examine en détail les couches et architectures supplémentaires. <br><br>  Nous vous proposons une revue du passage "Federated Learning" <br><br>  L'idée de l'apprentissage fédéré est née du fait que de nombreuses données contenant des informations utiles pour résoudre les problèmes (par exemple, pour le diagnostic du cancer par IRM) sont difficiles à obtenir en quantité suffisante pour enseigner un puissant modèle d'apprentissage en profondeur.  En plus des informations utiles nécessaires à la formation du modèle, les ensembles de données contiennent également d'autres informations qui ne sont pas pertinentes pour la tâche à accomplir, mais leur divulgation à quelqu'un pourrait potentiellement être nuisible. <br><br>  L'apprentissage fédéré est une technique pour enfermer un modèle dans un environnement sécurisé et l'enseigner sans déplacer les données n'importe où.  Prenons un exemple. <br><a name="habracut"></a><br><pre><code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Counter <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecsnp.random.seed(<span class="hljs-number"><span class="hljs-number">12345</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> codecs.open(<span class="hljs-string"><span class="hljs-string">'spam.txt'</span></span>,<span class="hljs-string"><span class="hljs-string">"r"</span></span>,encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>,errors=<span class="hljs-string"><span class="hljs-string">'ignore'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: ←     http:<span class="hljs-comment"><span class="hljs-comment">//www2.aueb.gr/users/ion/data/enron-spam/ raw = f.readlines() vocab, spam, ham = (set(["&lt;unk&gt;"]), list(), list()) for row in raw: spam.append(set(row[:-2].split(" "))) for word in spam[-1]: vocab.add(word) with codecs.open('ham.txt',"r",encoding='utf-8',errors='ignore') as f: raw = f.readlines() for row in raw: ham.append(set(row[:-2].split(" "))) for word in ham[-1]: vocab.add(word) vocab, w2i = (list(vocab), {}) for i,w in enumerate(vocab): w2i[w] = i def to_indices(input, l=500): indices = list() for line in input: if(len(line) &lt; l): line = list(line) + ["&lt;unk&gt;"] * (l - len(line)) idxs = list() for word in line: idxs.append(w2i[word]) indices.append(idxs) return indices</span></span></code> </pre> <br><h3>  Apprendre à détecter le spam. </h3><br>  <b>Disons que nous devons former un modèle pour détecter le spam des e-mails des gens</b> <br><br>  Dans ce cas, nous parlons de la classification des e-mails.  Nous formerons notre premier modèle sur un ensemble de données public appelé Enron.  Il s'agit d'un énorme corpus d'e-mails publiés lors des audiences d'Enron (désormais un organe d'analyse standard des e-mails).  Un fait intéressant: je connaissais des gens qui, par la nature de leurs activités, devaient lire / commenter cet ensemble de données, et ils notent que les gens se sont envoyés dans ces lettres une variété d'informations (souvent très personnelles).  Mais comme ce corps a été rendu public lors du procès, il peut désormais être utilisé sans restriction. <br><br>  Le code de la section précédente et de cette section implémente uniquement les opérations préparatoires.  Les fichiers d'entrée (ham.txt et spam.txt) sont disponibles sur la page Web du livre: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">www.manning.com/books/grokking-deep-learning</a> et dans le référentiel GitHub: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">github.com/iamtrask/Grokking-Deep-Learning</a> .  Nous devons le pré-traiter afin de le préparer pour le transfert vers la classe Embedding du chapitre 13, où nous avons créé notre cadre d'apprentissage en profondeur.  Comme précédemment, tous les mots de ce corpus sont convertis en listes d'index.  De plus, nous apportons toutes les lettres à la même longueur de 500 mots, soit en les réduisant, soit en ajoutant des jetons.  Grâce à cela, nous obtenons un ensemble de données rectangulaire. <br><br><pre> <code class="javascript hljs">spam_idx = to_indices(spam) ham_idx = to_indices(ham) train_spam_idx = spam_idx[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">-1000</span></span>] train_ham_idx = ham_idx[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">-1000</span></span>] test_spam_idx = spam_idx[<span class="hljs-number"><span class="hljs-number">-1000</span></span>:] test_ham_idx = ham_idx[<span class="hljs-number"><span class="hljs-number">-1000</span></span>:] train_data = list() train_target = list() test_data = list() test_target = list() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(max(len(train_spam_idx),len(train_ham_idx))): train_data.append(train_spam_idx[i%len(train_spam_idx)]) train_target.append([<span class="hljs-number"><span class="hljs-number">1</span></span>]) train_data.append(train_ham_idx[i%len(train_ham_idx)]) train_target.append([<span class="hljs-number"><span class="hljs-number">0</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(max(len(test_spam_idx),len(test_ham_idx))): test_data.append(test_spam_idx[i%len(test_spam_idx)]) test_target.append([<span class="hljs-number"><span class="hljs-number">1</span></span>]) test_data.append(test_ham_idx[i%len(test_ham_idx)]) test_target.append([<span class="hljs-number"><span class="hljs-number">0</span></span>]) def train(model, input_data, target_data, batch_size=<span class="hljs-number"><span class="hljs-number">500</span></span>, iterations=<span class="hljs-number"><span class="hljs-number">5</span></span>): n_batches = int(len(input_data) / batch_size) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> iter <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(iterations): iter_loss = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_batches): #         model.weight.data[w2i[<span class="hljs-string"><span class="hljs-string">'&lt;unk&gt;'</span></span>]] *= <span class="hljs-number"><span class="hljs-number">0</span></span> input = Tensor(input_data[b_i*bs:(b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>)*bs], autograd=True) target = Tensor(target_data[b_i*bs:(b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>)*bs], autograd=True) pred = model.forward(input).sum(<span class="hljs-number"><span class="hljs-number">1</span></span>).sigmoid() loss = criterion.forward(pred,target) loss.backward() optim.step() iter_loss += loss.data[<span class="hljs-number"><span class="hljs-number">0</span></span>] / bs sys.stdout.write(<span class="hljs-string"><span class="hljs-string">"\r\tLoss:"</span></span> + str(iter_loss / (b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>))) print() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model def test(model, test_input, test_output): model.weight.data[w2i[<span class="hljs-string"><span class="hljs-string">'&lt;unk&gt;'</span></span>]] *= <span class="hljs-number"><span class="hljs-number">0</span></span> input = Tensor(test_input, autograd=True) target = Tensor(test_output, autograd=True) pred = model.forward(input).sum(<span class="hljs-number"><span class="hljs-number">1</span></span>).sigmoid() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ((pred.data &gt; <span class="hljs-number"><span class="hljs-number">0.5</span></span>) == target.data).mean()</code> </pre> <br>  Après avoir défini les fonctions auxiliaires train () et test (), nous pouvons initialiser le réseau neuronal et le former en écrivant quelques lignes de code.  Après trois itérations, le réseau est capable de classer l'ensemble de données de contrôle avec une précision de 99,45% (l'ensemble de données de contrôle est bien équilibré, donc ce résultat peut être considéré comme excellent): <br><br><pre> <code class="javascript hljs">model = Embedding(vocab_size=len(vocab), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.weight.data *= <span class="hljs-number"><span class="hljs-number">0</span></span> criterion = MSELoss() optim = SGD(parameters=model.get_parameters(), alpha=<span class="hljs-number"><span class="hljs-number">0.01</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">3</span></span>): model = train(model, train_data, train_target, iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"% Correct on Test Set: "</span></span> + \ str(test(model, test_data, test_target)*<span class="hljs-number"><span class="hljs-number">100</span></span>)) ______________________________________________________________________________ Loss:<span class="hljs-number"><span class="hljs-number">0.037140416860871446</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">98.65</span></span> Loss:<span class="hljs-number"><span class="hljs-number">0.011258669226059114</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">99.15</span></span> Loss:<span class="hljs-number"><span class="hljs-number">0.008068268387986223</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">99.45</span></span></code> </pre> <br><h3>  Rendons le modèle fédéral </h3><br>  <b>Ci-dessus, l'apprentissage en profondeur le plus courant a été effectué.</b>  <b>Maintenant, ajoutez de la confidentialité</b> <br><br>  Dans la section précédente, nous avons implémenté un exemple d'analyse d'e-mails.  Maintenant, mettez tous les e-mails en un seul endroit.  Il s'agit d'une bonne vieille méthode de travail (qui est encore largement utilisée dans le monde).  Pour commencer, nous imiterons l'environnement de l'enseignement fédéral, dans lequel il existe plusieurs collections de lettres différentes: <br><br><pre> <code class="javascript hljs">bob = (train_data[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1000</span></span>], train_target[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1000</span></span>]) alice = (train_data[<span class="hljs-number"><span class="hljs-number">1000</span></span>:<span class="hljs-number"><span class="hljs-number">2000</span></span>], train_target[<span class="hljs-number"><span class="hljs-number">1000</span></span>:<span class="hljs-number"><span class="hljs-number">2000</span></span>]) sue = (train_data[<span class="hljs-number"><span class="hljs-number">2000</span></span>:], train_target[<span class="hljs-number"><span class="hljs-number">2000</span></span>:])</code> </pre> <br>  Rien de compliqué pour l'instant.  Nous pouvons maintenant effectuer la même procédure de formation qu'auparavant, mais déjà sur trois ensembles de données distincts.  Après chaque itération, nous allons faire la moyenne des valeurs dans les modèles de Bob, Alice et Sue et évaluer les résultats.  Veuillez noter que certaines méthodes d'apprentissage fédérées impliquent la combinaison après chaque package (ou collection de packages);  J'ai décidé de garder le code aussi simple que possible: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">3</span></span>): print(<span class="hljs-string"><span class="hljs-string">"Starting Training Round..."</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\tStep 1: send the model to Bob"</span></span>) bob_model = train(copy.deepcopy(model), bob[<span class="hljs-number"><span class="hljs-number">0</span></span>], bob[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tStep 2: send the model to Alice"</span></span>) alice_model = train(copy.deepcopy(model), alice[<span class="hljs-number"><span class="hljs-number">0</span></span>], alice[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tStep 3: Send the model to Sue"</span></span>) sue_model = train(copy.deepcopy(model), sue[<span class="hljs-number"><span class="hljs-number">0</span></span>], sue[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tAverage Everyone's New Models"</span></span>) model.weight.data = (bob_model.weight.data + \ alice_model.weight.data + \ sue_model.weight.data)/<span class="hljs-number"><span class="hljs-number">3</span></span> print(<span class="hljs-string"><span class="hljs-string">"\t% Correct on Test Set: "</span></span> + \ str(test(model, test_data, test_target)*<span class="hljs-number"><span class="hljs-number">100</span></span>)) print(<span class="hljs-string"><span class="hljs-string">"\nRepeat!!\n"</span></span>)</code> </pre> <br><br>  Ci-dessous un extrait avec les résultats.  Ce modèle a atteint presque le même niveau de précision que le précédent, et théoriquement nous n'avions pas accès aux données d'entraînement - ou pas?  Quoi qu'il en soit, mais chaque personne change le modèle dans le processus d'apprentissage, non?  Ne pouvons-nous pas vraiment tirer quelque chose de leurs ensembles de données? <br><br><pre> <code class="javascript hljs">Starting Training Round... Step <span class="hljs-number"><span class="hljs-number">1</span></span>: send the model to Bob Loss:<span class="hljs-number"><span class="hljs-number">0.21908166249699718</span></span> ...... Step <span class="hljs-number"><span class="hljs-number">3</span></span>: Send the model to Sue Loss:<span class="hljs-number"><span class="hljs-number">0.015368461608470256</span></span> Average Everyone<span class="hljs-string"><span class="hljs-string">'s New Models % Correct on Test Set: 98.8</span></span></code> </pre> <br><h3>  Pirater un modèle fédéré </h3><br>  <b>Regardons un exemple simple de comment extraire des informations d'un ensemble de données de formation.</b> <br><br>  L'apprentissage fédéré souffre de deux gros problèmes, particulièrement difficiles à résoudre, lorsque chaque personne ne dispose que de quelques exemples de formation: rapidité et confidentialité.  Il s'avère que si quelqu'un n'a que quelques exemples de formation (ou si le modèle qui vous a été envoyé a été formé avec seulement quelques exemples: un module de formation), vous pouvez toujours en apprendre beaucoup sur les données source.  Si vous imaginez que vous avez 10000 personnes (et que tout le monde a une très petite quantité de données), vous passerez la plupart du temps à envoyer le modèle d'avant en arrière et pas tellement de formation (surtout si le modèle est très grand). <br><br>  Mais n'allons pas de l'avant.  Voyons ce que vous pouvez découvrir après que l'utilisateur a mis à jour les poids sur un package: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> copy bobs_email = [<span class="hljs-string"><span class="hljs-string">"my"</span></span>, <span class="hljs-string"><span class="hljs-string">"computer"</span></span>, <span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"is"</span></span>, <span class="hljs-string"><span class="hljs-string">"pizza"</span></span>] bob_input = np.array([[w2i[x] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> bobs_email]]) bob_target = np.array([[<span class="hljs-number"><span class="hljs-number">0</span></span>]]) model = Embedding(vocab_size=len(vocab), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.weight.data *= <span class="hljs-number"><span class="hljs-number">0</span></span> bobs_model = train(copy.deepcopy(model), bob_input, bob_target, iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Bob crée et forme le modèle par e-mail dans sa boîte de réception.  Mais il se trouve qu'il a enregistré son mot de passe en s'envoyant une lettre avec le texte: "Le mot de passe de mon ordinateur est pizza."  Naive Bob!  Après avoir vu quels poids ont changé, nous pouvons comprendre le dictionnaire (et comprendre le sens) de la lettre de Bob: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, v <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(bobs_model.weight.data - model.weight.data): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(v != <span class="hljs-number"><span class="hljs-number">0</span></span>): print(vocab[i])</code> </pre> <br>  D'une manière si simple, nous avons découvert le mot de passe top secret de Bob (et peut-être ses préférences culinaires).  Et que faire?  Comment faire confiance à l'apprentissage fédéré s'il est si facile de savoir quelles données de formation ont provoqué le changement de poids? <br><br><pre> <code class="javascript hljs">is pizza computer password my</code> </pre> <br>  »Plus d'informations sur le livre sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site Web de l'éditeur</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Contenu</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Extrait</a> <br><br>  30% de réduction pour les livres de précommande Habrozhiteli sur un coupon - <b>Grokking Deep Learning</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr458800/">https://habr.com/ru/post/fr458800/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr458790/index.html">Introduction à CatBoost. Rapport Yandex</a></li>
<li><a href="../fr458792/index.html">Employés «brûlés»: y a-t-il une issue?</a></li>
<li><a href="../fr458794/index.html">Réunion des analystes d'affaires à Redmadrobot le 18 juillet</a></li>
<li><a href="../fr458796/index.html">Comment préparer votre site à de lourdes charges de travail: 5 conseils pratiques et outils utiles</a></li>
<li><a href="../fr458798/index.html">Nutrient Bot ou comment je veux prendre du pain avec des entraîneurs de fitness</a></li>
<li><a href="../fr458804/index.html">Recueil d'articles sur l'apprentissage automatique et l'intelligence artificielle</a></li>
<li><a href="../fr458808/index.html">Rapport Habr post-mortem: un journal est tombé</a></li>
<li><a href="../fr458810/index.html">Corel et Parallels vendus au groupe d'investissement KKR des États-Unis</a></li>
<li><a href="../fr458812/index.html">JVM TI: comment créer un plugin pour une machine virtuelle</a></li>
<li><a href="../fr458814/index.html">Lancement d'un site pour un produit avec une demande informée</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>