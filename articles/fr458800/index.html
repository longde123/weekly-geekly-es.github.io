<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÅ‚Äçüó® üåë ü§∂üèæ Apprentissage profond. Apprentissage f√©d√©r√© üé± ‚õµÔ∏è üßòüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut, habrozhiteli! Nous avons r√©cemment remis le livre √† Andrew W. Trask, jetant les bases d'une ma√Ætrise accrue des technologies d'apprentissage en...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apprentissage profond. Apprentissage f√©d√©r√©</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/458800/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/xs/t-/oc/xst-oc7auy1he8nhwbj7bbkhxwk.jpeg" align="left" alt="image"></a>  Salut, habrozhiteli!  Nous avons r√©cemment remis le livre √† Andrew W. Trask, jetant les bases d'une ma√Ætrise accrue des technologies d'apprentissage en profondeur.  Il commence par une description des bases des r√©seaux de neurones, puis examine en d√©tail les couches et architectures suppl√©mentaires. <br><br>  Nous vous proposons une revue du passage "Federated Learning" <br><br>  L'id√©e de l'apprentissage f√©d√©r√© est n√©e du fait que de nombreuses donn√©es contenant des informations utiles pour r√©soudre les probl√®mes (par exemple, pour le diagnostic du cancer par IRM) sont difficiles √† obtenir en quantit√© suffisante pour enseigner un puissant mod√®le d'apprentissage en profondeur.  En plus des informations utiles n√©cessaires √† la formation du mod√®le, les ensembles de donn√©es contiennent √©galement d'autres informations qui ne sont pas pertinentes pour la t√¢che √† accomplir, mais leur divulgation √† quelqu'un pourrait potentiellement √™tre nuisible. <br><br>  L'apprentissage f√©d√©r√© est une technique pour enfermer un mod√®le dans un environnement s√©curis√© et l'enseigner sans d√©placer les donn√©es n'importe o√π.  Prenons un exemple. <br><a name="habracut"></a><br><pre><code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Counter <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecsnp.random.seed(<span class="hljs-number"><span class="hljs-number">12345</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> codecs.open(<span class="hljs-string"><span class="hljs-string">'spam.txt'</span></span>,<span class="hljs-string"><span class="hljs-string">"r"</span></span>,encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>,errors=<span class="hljs-string"><span class="hljs-string">'ignore'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: ‚Üê     http:<span class="hljs-comment"><span class="hljs-comment">//www2.aueb.gr/users/ion/data/enron-spam/ raw = f.readlines() vocab, spam, ham = (set(["&lt;unk&gt;"]), list(), list()) for row in raw: spam.append(set(row[:-2].split(" "))) for word in spam[-1]: vocab.add(word) with codecs.open('ham.txt',"r",encoding='utf-8',errors='ignore') as f: raw = f.readlines() for row in raw: ham.append(set(row[:-2].split(" "))) for word in ham[-1]: vocab.add(word) vocab, w2i = (list(vocab), {}) for i,w in enumerate(vocab): w2i[w] = i def to_indices(input, l=500): indices = list() for line in input: if(len(line) &lt; l): line = list(line) + ["&lt;unk&gt;"] * (l - len(line)) idxs = list() for word in line: idxs.append(w2i[word]) indices.append(idxs) return indices</span></span></code> </pre> <br><h3>  Apprendre √† d√©tecter le spam. </h3><br>  <b>Disons que nous devons former un mod√®le pour d√©tecter le spam des e-mails des gens</b> <br><br>  Dans ce cas, nous parlons de la classification des e-mails.  Nous formerons notre premier mod√®le sur un ensemble de donn√©es public appel√© Enron.  Il s'agit d'un √©norme corpus d'e-mails publi√©s lors des audiences d'Enron (d√©sormais un organe d'analyse standard des e-mails).  Un fait int√©ressant: je connaissais des gens qui, par la nature de leurs activit√©s, devaient lire / commenter cet ensemble de donn√©es, et ils notent que les gens se sont envoy√©s dans ces lettres une vari√©t√© d'informations (souvent tr√®s personnelles).  Mais comme ce corps a √©t√© rendu public lors du proc√®s, il peut d√©sormais √™tre utilis√© sans restriction. <br><br>  Le code de la section pr√©c√©dente et de cette section impl√©mente uniquement les op√©rations pr√©paratoires.  Les fichiers d'entr√©e (ham.txt et spam.txt) sont disponibles sur la page Web du livre: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">www.manning.com/books/grokking-deep-learning</a> et dans le r√©f√©rentiel GitHub: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">github.com/iamtrask/Grokking-Deep-Learning</a> .  Nous devons le pr√©-traiter afin de le pr√©parer pour le transfert vers la classe Embedding du chapitre 13, o√π nous avons cr√©√© notre cadre d'apprentissage en profondeur.  Comme pr√©c√©demment, tous les mots de ce corpus sont convertis en listes d'index.  De plus, nous apportons toutes les lettres √† la m√™me longueur de 500 mots, soit en les r√©duisant, soit en ajoutant des jetons.  Gr√¢ce √† cela, nous obtenons un ensemble de donn√©es rectangulaire. <br><br><pre> <code class="javascript hljs">spam_idx = to_indices(spam) ham_idx = to_indices(ham) train_spam_idx = spam_idx[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">-1000</span></span>] train_ham_idx = ham_idx[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">-1000</span></span>] test_spam_idx = spam_idx[<span class="hljs-number"><span class="hljs-number">-1000</span></span>:] test_ham_idx = ham_idx[<span class="hljs-number"><span class="hljs-number">-1000</span></span>:] train_data = list() train_target = list() test_data = list() test_target = list() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(max(len(train_spam_idx),len(train_ham_idx))): train_data.append(train_spam_idx[i%len(train_spam_idx)]) train_target.append([<span class="hljs-number"><span class="hljs-number">1</span></span>]) train_data.append(train_ham_idx[i%len(train_ham_idx)]) train_target.append([<span class="hljs-number"><span class="hljs-number">0</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(max(len(test_spam_idx),len(test_ham_idx))): test_data.append(test_spam_idx[i%len(test_spam_idx)]) test_target.append([<span class="hljs-number"><span class="hljs-number">1</span></span>]) test_data.append(test_ham_idx[i%len(test_ham_idx)]) test_target.append([<span class="hljs-number"><span class="hljs-number">0</span></span>]) def train(model, input_data, target_data, batch_size=<span class="hljs-number"><span class="hljs-number">500</span></span>, iterations=<span class="hljs-number"><span class="hljs-number">5</span></span>): n_batches = int(len(input_data) / batch_size) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> iter <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(iterations): iter_loss = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_batches): #         model.weight.data[w2i[<span class="hljs-string"><span class="hljs-string">'&lt;unk&gt;'</span></span>]] *= <span class="hljs-number"><span class="hljs-number">0</span></span> input = Tensor(input_data[b_i*bs:(b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>)*bs], autograd=True) target = Tensor(target_data[b_i*bs:(b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>)*bs], autograd=True) pred = model.forward(input).sum(<span class="hljs-number"><span class="hljs-number">1</span></span>).sigmoid() loss = criterion.forward(pred,target) loss.backward() optim.step() iter_loss += loss.data[<span class="hljs-number"><span class="hljs-number">0</span></span>] / bs sys.stdout.write(<span class="hljs-string"><span class="hljs-string">"\r\tLoss:"</span></span> + str(iter_loss / (b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>))) print() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model def test(model, test_input, test_output): model.weight.data[w2i[<span class="hljs-string"><span class="hljs-string">'&lt;unk&gt;'</span></span>]] *= <span class="hljs-number"><span class="hljs-number">0</span></span> input = Tensor(test_input, autograd=True) target = Tensor(test_output, autograd=True) pred = model.forward(input).sum(<span class="hljs-number"><span class="hljs-number">1</span></span>).sigmoid() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ((pred.data &gt; <span class="hljs-number"><span class="hljs-number">0.5</span></span>) == target.data).mean()</code> </pre> <br>  Apr√®s avoir d√©fini les fonctions auxiliaires train () et test (), nous pouvons initialiser le r√©seau neuronal et le former en √©crivant quelques lignes de code.  Apr√®s trois it√©rations, le r√©seau est capable de classer l'ensemble de donn√©es de contr√¥le avec une pr√©cision de 99,45% (l'ensemble de donn√©es de contr√¥le est bien √©quilibr√©, donc ce r√©sultat peut √™tre consid√©r√© comme excellent): <br><br><pre> <code class="javascript hljs">model = Embedding(vocab_size=len(vocab), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.weight.data *= <span class="hljs-number"><span class="hljs-number">0</span></span> criterion = MSELoss() optim = SGD(parameters=model.get_parameters(), alpha=<span class="hljs-number"><span class="hljs-number">0.01</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">3</span></span>): model = train(model, train_data, train_target, iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"% Correct on Test Set: "</span></span> + \ str(test(model, test_data, test_target)*<span class="hljs-number"><span class="hljs-number">100</span></span>)) ______________________________________________________________________________ Loss:<span class="hljs-number"><span class="hljs-number">0.037140416860871446</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">98.65</span></span> Loss:<span class="hljs-number"><span class="hljs-number">0.011258669226059114</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">99.15</span></span> Loss:<span class="hljs-number"><span class="hljs-number">0.008068268387986223</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">99.45</span></span></code> </pre> <br><h3>  Rendons le mod√®le f√©d√©ral </h3><br>  <b>Ci-dessus, l'apprentissage en profondeur le plus courant a √©t√© effectu√©.</b>  <b>Maintenant, ajoutez de la confidentialit√©</b> <br><br>  Dans la section pr√©c√©dente, nous avons impl√©ment√© un exemple d'analyse d'e-mails.  Maintenant, mettez tous les e-mails en un seul endroit.  Il s'agit d'une bonne vieille m√©thode de travail (qui est encore largement utilis√©e dans le monde).  Pour commencer, nous imiterons l'environnement de l'enseignement f√©d√©ral, dans lequel il existe plusieurs collections de lettres diff√©rentes: <br><br><pre> <code class="javascript hljs">bob = (train_data[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1000</span></span>], train_target[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1000</span></span>]) alice = (train_data[<span class="hljs-number"><span class="hljs-number">1000</span></span>:<span class="hljs-number"><span class="hljs-number">2000</span></span>], train_target[<span class="hljs-number"><span class="hljs-number">1000</span></span>:<span class="hljs-number"><span class="hljs-number">2000</span></span>]) sue = (train_data[<span class="hljs-number"><span class="hljs-number">2000</span></span>:], train_target[<span class="hljs-number"><span class="hljs-number">2000</span></span>:])</code> </pre> <br>  Rien de compliqu√© pour l'instant.  Nous pouvons maintenant effectuer la m√™me proc√©dure de formation qu'auparavant, mais d√©j√† sur trois ensembles de donn√©es distincts.  Apr√®s chaque it√©ration, nous allons faire la moyenne des valeurs dans les mod√®les de Bob, Alice et Sue et √©valuer les r√©sultats.  Veuillez noter que certaines m√©thodes d'apprentissage f√©d√©r√©es impliquent la combinaison apr√®s chaque package (ou collection de packages);  J'ai d√©cid√© de garder le code aussi simple que possible: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">3</span></span>): print(<span class="hljs-string"><span class="hljs-string">"Starting Training Round..."</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\tStep 1: send the model to Bob"</span></span>) bob_model = train(copy.deepcopy(model), bob[<span class="hljs-number"><span class="hljs-number">0</span></span>], bob[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tStep 2: send the model to Alice"</span></span>) alice_model = train(copy.deepcopy(model), alice[<span class="hljs-number"><span class="hljs-number">0</span></span>], alice[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tStep 3: Send the model to Sue"</span></span>) sue_model = train(copy.deepcopy(model), sue[<span class="hljs-number"><span class="hljs-number">0</span></span>], sue[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tAverage Everyone's New Models"</span></span>) model.weight.data = (bob_model.weight.data + \ alice_model.weight.data + \ sue_model.weight.data)/<span class="hljs-number"><span class="hljs-number">3</span></span> print(<span class="hljs-string"><span class="hljs-string">"\t% Correct on Test Set: "</span></span> + \ str(test(model, test_data, test_target)*<span class="hljs-number"><span class="hljs-number">100</span></span>)) print(<span class="hljs-string"><span class="hljs-string">"\nRepeat!!\n"</span></span>)</code> </pre> <br><br>  Ci-dessous un extrait avec les r√©sultats.  Ce mod√®le a atteint presque le m√™me niveau de pr√©cision que le pr√©c√©dent, et th√©oriquement nous n'avions pas acc√®s aux donn√©es d'entra√Ænement - ou pas?  Quoi qu'il en soit, mais chaque personne change le mod√®le dans le processus d'apprentissage, non?  Ne pouvons-nous pas vraiment tirer quelque chose de leurs ensembles de donn√©es? <br><br><pre> <code class="javascript hljs">Starting Training Round... Step <span class="hljs-number"><span class="hljs-number">1</span></span>: send the model to Bob Loss:<span class="hljs-number"><span class="hljs-number">0.21908166249699718</span></span> ...... Step <span class="hljs-number"><span class="hljs-number">3</span></span>: Send the model to Sue Loss:<span class="hljs-number"><span class="hljs-number">0.015368461608470256</span></span> Average Everyone<span class="hljs-string"><span class="hljs-string">'s New Models % Correct on Test Set: 98.8</span></span></code> </pre> <br><h3>  Pirater un mod√®le f√©d√©r√© </h3><br>  <b>Regardons un exemple simple de comment extraire des informations d'un ensemble de donn√©es de formation.</b> <br><br>  L'apprentissage f√©d√©r√© souffre de deux gros probl√®mes, particuli√®rement difficiles √† r√©soudre, lorsque chaque personne ne dispose que de quelques exemples de formation: rapidit√© et confidentialit√©.  Il s'av√®re que si quelqu'un n'a que quelques exemples de formation (ou si le mod√®le qui vous a √©t√© envoy√© a √©t√© form√© avec seulement quelques exemples: un module de formation), vous pouvez toujours en apprendre beaucoup sur les donn√©es source.  Si vous imaginez que vous avez 10000 personnes (et que tout le monde a une tr√®s petite quantit√© de donn√©es), vous passerez la plupart du temps √† envoyer le mod√®le d'avant en arri√®re et pas tellement de formation (surtout si le mod√®le est tr√®s grand). <br><br>  Mais n'allons pas de l'avant.  Voyons ce que vous pouvez d√©couvrir apr√®s que l'utilisateur a mis √† jour les poids sur un package: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> copy bobs_email = [<span class="hljs-string"><span class="hljs-string">"my"</span></span>, <span class="hljs-string"><span class="hljs-string">"computer"</span></span>, <span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"is"</span></span>, <span class="hljs-string"><span class="hljs-string">"pizza"</span></span>] bob_input = np.array([[w2i[x] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> bobs_email]]) bob_target = np.array([[<span class="hljs-number"><span class="hljs-number">0</span></span>]]) model = Embedding(vocab_size=len(vocab), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.weight.data *= <span class="hljs-number"><span class="hljs-number">0</span></span> bobs_model = train(copy.deepcopy(model), bob_input, bob_target, iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Bob cr√©e et forme le mod√®le par e-mail dans sa bo√Æte de r√©ception.  Mais il se trouve qu'il a enregistr√© son mot de passe en s'envoyant une lettre avec le texte: "Le mot de passe de mon ordinateur est pizza."  Naive Bob!  Apr√®s avoir vu quels poids ont chang√©, nous pouvons comprendre le dictionnaire (et comprendre le sens) de la lettre de Bob: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, v <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(bobs_model.weight.data - model.weight.data): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(v != <span class="hljs-number"><span class="hljs-number">0</span></span>): print(vocab[i])</code> </pre> <br>  D'une mani√®re si simple, nous avons d√©couvert le mot de passe top secret de Bob (et peut-√™tre ses pr√©f√©rences culinaires).  Et que faire?  Comment faire confiance √† l'apprentissage f√©d√©r√© s'il est si facile de savoir quelles donn√©es de formation ont provoqu√© le changement de poids? <br><br><pre> <code class="javascript hljs">is pizza computer password my</code> </pre> <br>  ¬ªPlus d'informations sur le livre sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site Web de l'√©diteur</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Contenu</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Extrait</a> <br><br>  30% de r√©duction pour les livres de pr√©commande Habrozhiteli sur un coupon - <b>Grokking Deep Learning</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr458800/">https://habr.com/ru/post/fr458800/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr458790/index.html">Introduction √† CatBoost. Rapport Yandex</a></li>
<li><a href="../fr458792/index.html">Employ√©s ¬´br√ªl√©s¬ª: y a-t-il une issue?</a></li>
<li><a href="../fr458794/index.html">R√©union des analystes d'affaires √† Redmadrobot le 18 juillet</a></li>
<li><a href="../fr458796/index.html">Comment pr√©parer votre site √† de lourdes charges de travail: 5 conseils pratiques et outils utiles</a></li>
<li><a href="../fr458798/index.html">Nutrient Bot ou comment je veux prendre du pain avec des entra√Æneurs de fitness</a></li>
<li><a href="../fr458804/index.html">Recueil d'articles sur l'apprentissage automatique et l'intelligence artificielle</a></li>
<li><a href="../fr458808/index.html">Rapport Habr post-mortem: un journal est tomb√©</a></li>
<li><a href="../fr458810/index.html">Corel et Parallels vendus au groupe d'investissement KKR des √âtats-Unis</a></li>
<li><a href="../fr458812/index.html">JVM TI: comment cr√©er un plugin pour une machine virtuelle</a></li>
<li><a href="../fr458814/index.html">Lancement d'un site pour un produit avec une demande inform√©e</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>