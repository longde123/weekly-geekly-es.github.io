<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äçüç≥ üßÄ üëø Wir arbeiten mit neuronalen Netzen: eine Checkliste zum Debuggen ‚úäüèø üÜó üé•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Software-Code f√ºr maschinelles Lernen ist oft komplex und ziemlich verwirrend. Das Erkennen und Beseitigen von Fehlern ist eine ressourcenintensive Au...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wir arbeiten mit neuronalen Netzen: eine Checkliste zum Debuggen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/skillbox/blog/444684/"><img src="https://habrastorage.org/getpro/habr/post_images/84a/7dc/4b8/84a7dc4b81799226681bb176f1c518b0.png" alt="Bild"><br><br>  Software-Code f√ºr maschinelles Lernen ist oft komplex und ziemlich verwirrend.  Das Erkennen und Beseitigen von Fehlern ist eine ressourcenintensive Aufgabe.  Selbst die einfachsten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">direkt verbundenen neuronalen Netze</a> erfordern einen ernsthaften Ansatz f√ºr die Netzwerkarchitektur, die Initialisierung von Gewichten und die Netzwerkoptimierung.  Ein kleiner Fehler kann zu unangenehmen Problemen f√ºhren. <br><br>  Dieser Artikel befasst sich mit dem Debugging-Algorithmus Ihrer neuronalen Netze. <br><a name="habracut"></a><br><blockquote>  <b>Skillbox empfiehlt:</b> Ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-Entwickler von Grund auf</a> . <br><br>  <b>Wir erinnern Sie daran:</b> <i>F√ºr alle Leser von ‚ÄûHabr‚Äú - ein Rabatt von 10.000 Rubel bei der Anmeldung f√ºr einen Skillbox-Kurs mit dem Promo-Code ‚ÄûHabr‚Äú.</i> </blockquote><cut></cut><br><h3>  Der Algorithmus besteht aus f√ºnf Stufen: </h3><br><ul><li>  einfacher Start; </li><li>  Best√§tigung von Verlusten; </li><li>  √úberpr√ºfung von Zwischenergebnissen und Verbindungen; </li><li>  Diagnose von Parametern; </li><li>  Arbeitskontrolle. </li></ul><br>  Wenn Ihnen etwas interessanter erscheint als der Rest, k√∂nnen Sie direkt zu diesen Abschnitten gehen. <br><br><h3>  Einfacher Start </h3><br>  Ein neuronales Netzwerk mit komplexer Architektur, Regularisierung und einem Lerngeschwindigkeitsplaner ist schwieriger zu deb√ºtieren als ein regul√§res Netzwerk.  Wir sind hier etwas knifflig, da das Element selbst einen indirekten Bezug zum Debuggen hat, aber dies ist immer noch eine wichtige Empfehlung. <br><br>  Ein einfacher Anfang besteht darin, ein vereinfachtes Modell zu erstellen und es an einem Datensatz (Punkt) zu trainieren. <br><br>  <b>Zuerst erstellen wir ein vereinfachtes Modell</b> <br><br>  Erstellen Sie f√ºr einen schnellen Start ein kleines Netzwerk mit einer einzelnen verborgenen Ebene und √ºberpr√ºfen Sie, ob alles ordnungsgem√§√ü funktioniert.  Dann komplizieren wir das Modell schrittweise, √ºberpr√ºfen jeden neuen Aspekt seiner Struktur (zus√§tzliche Ebene, Parameter usw.) und fahren fort. <br><br>  <b>Wir trainieren das Modell an einem einzigen Datensatz (Punkt)</b> <br><br>  Als schnellen Test f√ºr den Zustand Ihres Projekts k√∂nnen Sie ein oder zwei Datenpunkte f√ºr das Training verwenden, um zu √ºberpr√ºfen, ob das System ordnungsgem√§√ü funktioniert.  Das neuronale Netzwerk sollte eine 100% ige Genauigkeit des Trainings und der √úberpr√ºfung aufweisen.  Ist dies nicht der Fall, ist entweder das Modell zu klein oder Sie haben bereits einen Fehler. <br><br>  Auch wenn alles in Ordnung ist, bereiten Sie das Modell f√ºr den Durchgang einer oder mehrerer Epochen vor, bevor Sie fortfahren. <br><br><h3>  Verlustsch√§tzung </h3><br>  Die Verlustsch√§tzung ist der Hauptweg, um die Modellleistung zu verfeinern.  Sie m√ºssen sicherstellen, dass der Verlust der Aufgabe entspricht und die Verlustfunktionen auf der richtigen Skala bewertet werden.  Wenn Sie mehr als eine Verlustart verwenden, stellen Sie sicher, dass alle in derselben Reihenfolge und korrekt skaliert sind. <br><br>  Es ist wichtig, auf die anf√§nglichen Verluste zu achten.  √úberpr√ºfen Sie, wie nahe das tats√§chliche Ergebnis am erwarteten Ergebnis liegt, wenn das Modell mit einer zuf√§lligen Annahme gestartet wurde.  Die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit von Andrei Karpati schl√§gt Folgendes vor</a> : ‚ÄûStellen Sie sicher, dass Sie das erwartete Ergebnis erhalten, wenn Sie mit einer kleinen Anzahl von Parametern arbeiten.  Es ist besser, den Datenverlust sofort zu √ºberpr√ºfen (wobei der Regularisierungsgrad auf Null gesetzt ist).  F√ºr CIFAR-10 mit dem Softmax-Klassifikator erwarten wir beispielsweise einen Anfangsverlust von 2,302, da die erwartete diffuse Wahrscheinlichkeit f√ºr jede Klasse 0,1 betr√§gt (da es 10 Klassen gibt) und der Verlust von Softmax die negative logarithmische Wahrscheinlichkeit der richtigen Klasse ist als - ln (0,1) = 2,302. <br><br>  F√ºr ein bin√§res Beispiel wird eine √§hnliche Berechnung einfach f√ºr jede der Klassen durchgef√ºhrt.  Hier sind zum Beispiel die Daten: 20% Nullen und 80% Einsen.  Der erwartete Anfangsverlust betr√§gt bis zu ‚Äì0,2 ln (0,5) ‚Äì0,8 ln (0,5) = 0,693147.  Wenn das Ergebnis gr√∂√üer als 1 ist, kann dies darauf hinweisen, dass die Gewichte des neuronalen Netzwerks nicht richtig ausgeglichen sind oder die Daten nicht normalisiert sind. <br><br><h3>  Zwischenergebnisse und Verbindungen pr√ºfen </h3><br>  Um ein neuronales Netzwerk zu debuggen, ist es notwendig, die Dynamik von Prozessen innerhalb des Netzwerks und die Rolle einzelner Zwischenschichten zu verstehen, da diese miteinander verbunden sind.  Hier sind einige h√§ufige Fehler, auf die Sie sto√üen k√∂nnten: <br><br><ul><li>  Falsche Ausdr√ºcke f√ºr Verlaufsaktualisierungen </li><li>  Gewichtsaktualisierungen gelten nicht; </li><li>  verschwindende oder explodierende Farbverl√§ufe. </li></ul><br>  Wenn die Gradientenwerte Null sind, bedeutet dies, dass die Lerngeschwindigkeit im Optimierer zu langsam ist oder dass Sie auf einen falschen Ausdruck gesto√üen sind, um den Gradienten zu aktualisieren. <br><br>  Dar√ºber hinaus m√ºssen die Werte der Aktivierungsfunktionen, Gewichte und Aktualisierungen der einzelnen Ebenen √ºberwacht werden.  Beispielsweise sollte der Wert von Parameteraktualisierungen (Gewichte und Offsets) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1-e3 sein</a> . <br><br>  Es gibt ein Ph√§nomen namens "Sterbendes ReLU" oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Verschwindendes Gradientenproblem",</a> wenn ReLU-Neuronen nach dem Untersuchen des gro√üen negativen Vorspannungswerts f√ºr ihre Gewichte Null ausgeben.  Diese Neuronen werden an keiner Datenstelle wieder aktiviert. <br><br>  Sie k√∂nnen Gradiententests verwenden, um diese Fehler zu erkennen, indem Sie den Gradienten mithilfe eines numerischen Ansatzes approximieren.  Wenn es nahe an den berechneten Gradienten liegt, wurde die R√ºckausbreitung korrekt implementiert.  Um eine Verlaufspr√ºfung zu erstellen, lesen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> diese gro√üartigen CS231-Ressourcen sowie das Tutorial von Andrew Nga zu diesem Thema. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fayzan Sheikh</a> weist auf drei Hauptmethoden zur Visualisierung eines neuronalen Netzwerks hin: <br><br><ul><li>  Vorl√§ufig - einfache Methoden, die uns die allgemeine Struktur des trainierten Modells zeigen.  Sie umfassen die Ausgabe von Formularen oder Filtern einzelner Schichten des neuronalen Netzwerks und von Parametern in jeder Schicht. </li><li>  Basierend auf der Aktivierung.  In ihnen entschl√ºsseln wir die Aktivierung einzelner Neuronen oder Gruppen von Neuronen, um ihre Funktionen zu verstehen. </li><li>  Gradientenbasiert.  Diese Methoden neigen dazu, die Gradienten zu manipulieren, die sich beim Training des Modells aus der Passage hin und her bilden (einschlie√ülich Signifikanzkarten und Klassenaktivierungskarten). </li></ul><br>  Es gibt verschiedene n√ºtzliche Tools zur Visualisierung der Aktivierungen und Verbindungen einzelner Ebenen, z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">ConX</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Tensorboard</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d86/8ff/f33/d868fff33e5b7cb69563bfee29b9db08.png"><br><br><h3>  Parameterdiagnose </h3><br>  Neuronale Netze haben viele Parameter, die miteinander interagieren, was die Optimierung erschwert.  Tats√§chlich ist dieser Abschnitt Gegenstand aktiver Forschung durch Spezialisten, daher sollten die folgenden Vorschl√§ge nur als Ratschl√§ge betrachtet werden, auf denen Sie aufbauen k√∂nnen. <br><br>  Paketgr√∂√üe (Stapelgr√∂√üe) - Wenn die Paketgr√∂√üe gro√ü genug sein soll, um genaue Sch√§tzungen des Fehlergradienten zu erhalten, aber klein genug, damit der stochastische Gradientenabstieg (SGD) Ihr Netzwerk rationalisieren kann.  Die geringe Gr√∂√üe der Pakete wird zu einer schnellen Konvergenz aufgrund von Rauschen im Lernprozess und in Zukunft zu Optimierungsschwierigkeiten f√ºhren.  Dies wird <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> ausf√ºhrlicher beschrieben. <br><br>  <b>Lerngeschwindigkeit</b> - Zu langsam f√ºhrt zu einer langsamen Konvergenz oder dem Risiko, in lokalen Tiefs zu stecken.  Gleichzeitig f√ºhrt eine hohe Lerngeschwindigkeit zu einer Diskrepanz bei der Optimierung, da Sie das Risiko eingehen, durch die Tiefe zu "springen", aber gleichzeitig einen engen Teil der Verlustfunktion.  Versuchen Sie, die Geschwindigkeitsplanung zu verwenden, um sie w√§hrend des Trainings des neuronalen Netzwerks zu reduzieren.  CS231n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hat einen gro√üen Abschnitt zu diesem Thema</a> . <br><br>  <b>Gradienten-Clipping</b> - Trimmen der Gradienten von Parametern w√§hrend der R√ºckausbreitung auf den Maximalwert oder die Grenznorm.  N√ºtzlich zum L√∂sen von Problemen mit explodierenden Verl√§ufen, die im dritten Absatz auftreten k√∂nnen. <br><br>  <b>Batch-Normalisierung</b> - wird verwendet, um die Eingabedaten jeder Schicht zu normalisieren, wodurch das Problem der internen kovarianten Verschiebung gel√∂st werden kann.  Wenn Sie Dropout und Batch Norma zusammen verwenden, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">lesen Sie diesen Artikel</a> . <br><br>  <b>Stochastic Gradient Descent (SGD)</b> - Es gibt verschiedene Arten von SGD, die Impuls, adaptive Lerngeschwindigkeiten und die Nesterov-Methode verwenden.  Gleichzeitig hat keiner von ihnen einen klaren Vorteil sowohl in Bezug auf die Trainingseffizienz als auch in Bezug auf die Verallgemeinerung ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Details hier</a> ). <br><br>  <b>Regularisierung</b> - ist f√ºr die Erstellung eines verallgemeinerten Modells von entscheidender Bedeutung, da dadurch die Komplexit√§t des Modells oder extreme Parameterwerte beeintr√§chtigt werden.  Dies ist eine M√∂glichkeit, die Varianz des Modells zu verringern, ohne seine Verschiebung signifikant zu erh√∂hen.  Weitere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationen hier</a> . <br><br>  Um alles selbst zu bewerten, m√ºssen Sie die Regularisierung deaktivieren und den Gradienten des Datenverlusts selbst √ºberpr√ºfen. <br><br>  <b>Dropout</b> ist eine weitere M√∂glichkeit, Ihr Netzwerk zu optimieren, um eine √úberlastung zu vermeiden.  W√§hrend des Trainings tritt ein Verlust nur auf, indem die Aktivit√§t des Neurons mit einer bestimmten Wahrscheinlichkeit p (Hyperparameter) aufrechterhalten oder im umgekehrten Fall auf Null gesetzt wird.  Infolgedessen muss das Netzwerk f√ºr jede Trainingspartei eine andere Teilmenge von Parametern verwenden, wodurch die √Ñnderungen bestimmter Parameter, die dominant werden, verringert werden. <br><br>  Wichtig: Wenn Sie sowohl die Ausfall- als auch die Chargennormalisierung verwenden, achten Sie auf die Reihenfolge dieser Vorg√§nge oder sogar auf deren gemeinsame Verwendung.  All dies wird noch aktiv diskutiert und erg√§nzt.  Hier sind zwei wichtige Diskussionen zu diesem Thema <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zu Stackoverflow</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arxiv</a> . <br><br><h3>  Arbeitskontrolle </h3><br>  Es geht darum, Workflows und Experimente zu dokumentieren.  Wenn Sie nichts dokumentieren, k√∂nnen Sie beispielsweise vergessen, welche Trainingsgeschwindigkeit oder welches Klassengewicht verwendet wird.  Dank der Steuerung k√∂nnen Sie fr√ºhere Experimente einfach anzeigen und reproduzieren.  Dies reduziert die Anzahl der doppelten Experimente. <br><br>  Richtig, manuelle Dokumentation kann bei viel Arbeit eine Herausforderung sein.  Hier helfen Tools wie Comet.ml dabei, Datens√§tze, Code√§nderungen, Versuchsverlauf und Produktionsmodelle automatisch zu protokollieren, einschlie√ülich wichtiger Informationen zu Ihrem Modell (Hyperparameter, Modellleistungsindikatoren und Umgebungsinformationen). <br><br>  Ein neuronales Netzwerk kann sehr empfindlich auf kleine √Ñnderungen reagieren, was zu einer Verringerung der Modellleistung f√ºhrt.  Das Verfolgen und Dokumentieren von Arbeiten ist der erste Schritt zur Standardisierung Ihrer Umgebung und Modellierung. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0e0/52c/de4/0e052cde4414da934bdde111cc0f0bf5.png"><br><br>  Ich hoffe, dass dieser Beitrag der Ausgangspunkt sein kann, von dem aus Sie mit dem Debuggen Ihres neuronalen Netzwerks beginnen. <br><blockquote>  <b>Skillbox empfiehlt:</b> <br><br><ul><li>  Zweij√§hriger Praktikumskurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Ich bin ein PRO-Webentwickler</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"</a> </li><li>  Online-Kurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"C # Entwickler mit 0"</a> . </li><li>  Praktischer Jahreskurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"PHP-Entwickler von 0 bis PRO"</a> . <br></li></ul></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de444684/">https://habr.com/ru/post/de444684/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de444672/index.html">Roskachestvo pr√§sentierte eine Bewertung der in Russland erh√§ltlichen kabelgebundenen und kabellosen Kopfh√∂rer</a></li>
<li><a href="../de444674/index.html">Sony Xperia 10 und Xperia 10 Plus - Breitbild-Smartphones</a></li>
<li><a href="../de444676/index.html">CRM-Bewertung, Tops, Bewertungen - l√ºgen sie alle?</a></li>
<li><a href="../de444678/index.html">Verf√ºgbarkeitstag: 12. April, normaler Flug</a></li>
<li><a href="../de444682/index.html">Sony- und Nintendo-Aktien st√ºrzen ab, nachdem Video-Streaming f√ºr Gamer von Google gestartet wurde</a></li>
<li><a href="../de444686/index.html">Waves Smart Assets: Schwarzwei√ülisten, Intervallhandel</a></li>
<li><a href="../de444688/index.html">Bitte h√∂ren Sie auf, mit Eloquent √ºber die Repository-Vorlage zu sprechen</a></li>
<li><a href="../de444690/index.html">Wie Uber-Forscher menschliches Verhaltenswissen anwenden und skalieren</a></li>
<li><a href="../de444692/index.html">MOSDROID # 16 Schwefel bei Redmadrobot</a></li>
<li><a href="../de444694/index.html">Wie wir den Abfluss vorhergesagt haben, n√§hern wir uns ihm als Naturkatastrophe</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>