<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÑ üëÇüèª üèçÔ∏è Clusters Kubernetes no servi√ßo VPC üßìüèº üëãüèª ü¶å</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Adicionamos a capacidade de iniciar convenientemente o Kubernetes no servi√ßo Virtual Private Cloud no modo de teste beta inicial. 


 Essa funcionalid...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Clusters Kubernetes no servi√ßo VPC</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/selectel/blog/416951/"><img src="https://habrastorage.org/webt/bc/b6/cy/bcb6cykv49fwewsnhfr-7ny-0uc.png"><br><p><br>  Adicionamos a capacidade de iniciar convenientemente o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">Kubernetes</a> no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener noreferrer">servi√ßo Virtual Private Cloud</a> no modo de teste beta inicial. </p><br><p>  Essa funcionalidade ser√° √∫til para usu√°rios que precisam de gerenciamento conveniente de um grande n√∫mero de aplicativos em execu√ß√£o como cont√™ineres.  O Kubernetes oferece ferramentas para dimensionamento, autocorre√ß√£o e balanceamento de carga para cont√™ineres executados dentro de um cluster. </p><br><p>  Como o <strong>servi√ßo de nuvem privada virtual</strong> √© baseado no OpenStack, usamos um de seus componentes - o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">OpenStack Magnum</a> .  Permite criar rapidamente clusters Kubernetes privados com o n√∫mero desejado de n√≥s. </p><br><p>  Atualmente, qualquer usu√°rio do nosso servi√ßo pode criar v√°rios clusters independentes em seu projeto.  Como n√≥s de cluster, ser√£o usadas m√°quinas virtuais, cuja configura√ß√£o pode ser selecionada e alterada. </p><br><p>  Neste artigo, falaremos sobre os principais objetos do cluster Kubernetes e usaremos os exemplos para analisar o processo de cria√ß√£o de um cluster usando o OpenStack Magnum. </p><a name="habracut"></a><br><h2>  Criar e gerenciar um cluster Kubernetes </h2><br><p>  Atualmente, a cria√ß√£o de um cluster Kubernetes √© poss√≠vel apenas por meio de utilit√°rios do console ou da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">API OpenStack</a> nas zonas de disponibilidade <strong>ru-1a</strong> e <strong>ru-1b</strong> (S√£o Petersburgo). </p><br><p>  Para come√ßar, voc√™ precisar√° de: </p><br><ul><li>  Crie um novo ou use um projeto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener noreferrer">VPC</a> existente; </li><li>  Crie um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener noreferrer">usu√°rio com uma chave SSH</a> ; </li><li>  Adicione um usu√°rio ao projeto criado na p√°gina de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener noreferrer">gerenciamento de</a> projetos; </li><li>  V√° para o projeto e obtenha o arquivo de acesso na guia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener noreferrer">Acesso</a> ; </li><li>  Instale o cliente do console <strong>openstack com a biblioteca python-magnumclient</strong> ; </li><li>  Instale o cliente do console <strong>kubectl</strong> . </li></ul><br><p>  Para instalar o cliente do console <strong>openstack</strong> , <strong>voc√™</strong> pode usar as instru√ß√µes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener noreferrer">no link</a> ; no entanto, lembre-se de que, para esse cliente, voc√™ tamb√©m precisar√° instalar a biblioteca <strong>python-magnumclient</strong> para suportar a cria√ß√£o de clusters Kubernetes. </p><br><p>  O conjunto completo de comandos para instalar um cliente openstack com o plug-in necess√°rio para sistemas operacionais da fam√≠lia Ubuntu / Debian: </p><br><pre><code class="bash hljs">$ sudo apt update $ sudo apt -y install curl python-pip python-dev python3-dev git libxml2-dev libxslt1-dev python-openssl python3-openssl python-pyasn1 libffi-dev libssl-dev build-essential $ sudo pip install -UI pbr setuptools pytz $ sudo pip install -UI git+https://github.com/openstack/python-openstackclient $ sudo pip install -UI git+https://github.com/openstack/python-magnumclient</code> </pre> <br><p>  O conjunto completo de comandos para instalar um cliente openstack com o plug-in necess√°rio para sistemas operacionais da fam√≠lia Fedora / CentOS: </p><br><pre> <code class="bash hljs">$ sudo yum -y install python-pip gcc libffi-devel python-devel libxslt-devel openssl-devel git libffi-devel $ sudo pip install -UI pbr setuptools pytz $ sudo pip install -UI git+https://github.com/openstack/python-openstackclient $ sudo pip install -UI git+https://github.com/openstack/python-magnumclient</code> </pre> <br><p>  Para gerenciar objetos Kubernetes, voc√™ precisa do cliente do console <strong>kubectl</strong> .  Os m√©todos de instala√ß√£o para v√°rios sistemas operacionais s√£o descritos na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener noreferrer">documenta√ß√£o oficial</a> . </p><br><p>  Para criar um cluster, voc√™ precisar√° criar ou usar os existentes: </p><br><ul><li>  <strong>Modelo de</strong> cluster; </li><li>  Um conjunto de par√¢metros para a CPU e RAM de m√°quinas virtuais ( <strong>sabor</strong> ). </li></ul><br><p>  Voc√™ pode criar um modelo de cluster e criar um sabor pr√≥prio ou usar modelos p√∫blicos pr√©-criados. </p><br><p>  Voc√™ tamb√©m precisar√° determinar a zona de disponibilidade, o tipo de discos para o cluster e o n√∫mero de n√≥s.  Vale considerar que ainda n√£o suportamos a possibilidade de criar um cluster em v√°rias zonas.  Voc√™ pode escolher qualquer tipo de unidade de rede (r√°pida, universal ou b√°sica). <br>  Voc√™ pode aprender mais sobre os tipos de unidades em nossa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener noreferrer">base de conhecimento</a> . </p><br><p>  O n√∫mero de n√≥s pode ser diferente para fun√ß√µes <strong>mestre</strong> e <strong>subordinado</strong> .  Nos n√≥s que executam a fun√ß√£o principal, os elementos de controle do cluster ser√£o ativados - <strong>controller-manager</strong> , <strong>scheduler</strong> , <strong>api</strong> .  Nos outros n√≥s, os servi√ßos <strong>kubelet</strong> , <strong>kube-proxy</strong> e todos os cont√™ineres de aplicativos ser√£o lan√ßados.  Voc√™ pode aprender mais sobre os componentes que s√£o executados nos n√≥s do cluster na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">documenta√ß√£o oficial</a> . </p><br><p>  Para acessar n√≥s via SSH, voc√™ precisar√° usar a chave SSH criada anteriormente.  Os comandos de amostra usar√£o uma chave chamada <strong>ssh-test</strong> . </p><br><p>  Usaremos o modelo e o sabor do cluster p√∫blico, o tipo de disco r√°pido e a zona de disponibilidade do <strong>ru-1b</strong> . <br>  Em nosso cluster, 2 n√≥s principais e 3 n√≥s minion ser√£o inicializados inicialmente. </p><br><p>  Para verificar esses par√¢metros, usamos os comandos openstackclient e o arquivo de acesso baixado ( <strong>rc.sh</strong> ): </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#          . $ source rc.sh #  ,         $ openstack flavor show BL1.2-4096 -c ram -c vcpus +-------+-------+ | Field | Value | +-------+-------+ | ram | 4096 | | vcpus | 2 | +-------+-------+ #       ru-1b $ openstack volume type show fast.ru-1b -c name +-------+------------+ | Field | Value | +-------+------------+ | name | fast.ru-1b | +-------+------------+ #    Kubernetes $ openstack coe cluster template list -c name +---------------------------------------+ | name | +---------------------------------------+ | kubernetes-nofloatingips-ru-1b-v1.9.3 | | kubernetes-nofloatingips-ru-1b-v1.9.6 | | kubernetes-nofloatingips-ru-1b-v1.9.9 | | kubernetes-floatingips-ru-1b-v1.9.3 | | kubernetes-floatingips-ru-1b-v1.9.6 | | kubernetes-floatingips-ru-1b-v1.9.9 | | kubernetes-nofloatingips-ru-1a-v1.9.3 | | kubernetes-nofloatingips-ru-1a-v1.9.6 | | kubernetes-nofloatingips-ru-1a-v1.9.9 | | kubernetes-floatingips-ru-1a-v1.9.3 | | kubernetes-floatingips-ru-1a-v1.9.6 | | kubernetes-floatingips-ru-1a-v1.9.9 | +---------------------------------------+</span></span></code> </pre> <br><p>  Por exemplo, escolheremos o segundo modelo de cluster; endere√ßos flutuantes publicamente acess√≠veis para cada um dos n√≥s n√£o ser√£o criados a partir dele.  N√≥s n√£o precisaremos deles. </p><br><pre> <code class="plaintext hljs">#   Kubernetes   test-cluster #   keypair   ,   $ openstack coe cluster create \ --cluster-template kubernetes-nofloatingips-ru-1b-v1.9.9 \ --master-count 2 \ --node-count 3 \ --keypair ssh-test \ --master-flavor BL1.2-4096 \ --flavor BL1.2-4096 \ test-cluster</code> </pre> <br><p>  <em>Observe que n√≥s escolhemos a mesma configura√ß√£o para n√≥s diferentes (par√¢metros de sabor principal e sabor), voc√™ pode escolher diferentes conjuntos de configura√ß√µes, dependendo dos requisitos do cluster.</em>  <em>Sua mudan√ßa √© poss√≠vel ap√≥s a sua cria√ß√£o.</em> </p><br><p>  Tamb√©m vale a pena considerar que, ao criar um cluster com v√°rios n√≥s principais, um balanceador de carga ser√° criado automaticamente para acessar a API do Kubernetes. </p><br><p>  Ap√≥s alguns minutos, um cluster Kubernetes aparecer√° no seu projeto.  No painel de controle do projeto, voc√™ ver√° novas m√°quinas virtuais, discos e objetos de rede. </p><br><p>  Voc√™ pode verificar o status do seu cluster atrav√©s do openstackclient: </p><br><pre> <code class="bash hljs">openstack coe cluster list -c name -c status +--------------+--------------------+ | name | status | +--------------+--------------------+ | <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster | CREATE_IN_PROGRESS | +--------------+--------------------+</code> </pre> <br><p>  Depois que o cluster entra no estado CREATE_COMPLETE, voc√™ pode gerenciar seus objetos atrav√©s do utilit√°rio kubectl baixando o arquivo de configura√ß√£o usando os seguintes comandos: </p><br><pre> <code class="bash hljs">$ mkdir -p ~/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster $ openstack coe cluster config <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster --dir ~/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster</code> </pre> <br><p>  Depois disso, voc√™ pode trabalhar com o cluster usando o utilit√°rio kubectl: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=~/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster/config $ kubectl get pods --all-namespaces -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase NAME STATUS coredns-785dcf9c58-6gnfp Running heapster-6846cdc674-rm4k6 Running kube-dns-autoscaler-6b94f7bbf8-x5clt Running kubernetes-dashboard-747575c864-wlg6p Running monitoring-grafana-84b4596dd7-zf5rx Running monitoring-influxdb-c8486fc95-bqqb6 Running node-exporter-test-cluster-robvp4cvwpt7-minion-0 Running</code> </pre> <br><p>  Se necess√°rio, voc√™ pode aumentar ou diminuir o n√∫mero de n√≥s minion no cluster por meio do openstackclient passando o novo valor node_count: </p><br><pre> <code class="bash hljs">$ openstack coe cluster update <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster replace node_count=4</code> </pre> <br><h2>  Principais objetos de cluster do Kubernetes </h2><br><h3>  Pods </h3><br><p>  Embora o Kubernetes gerencie um conjunto de cont√™ineres, a entidade subjacente que o Kubernetes gerencia n√£o √© um cont√™iner, mas um <strong>Pod</strong> . </p><br><p>  Pod √© um conjunto de namespaces do kernel do Linux e configura√ß√µes de pilha de rede que permitem montar um conjunto de cont√™ineres em uma √∫nica entidade. <br>  Na maioria das vezes, um cont√™iner com o aplicativo √© iniciado dentro de um Pod separado. <br>  Se necess√°rio, voc√™ pode executar v√°rios cont√™ineres dentro de um Pod, isso pode ser √∫til quando voc√™ precisar fornecer acesso de um cont√™iner para outro por meio da interface de rede localhost ou, por algum outro motivo, executar v√°rios cont√™ineres no mesmo host. <br>  Todos os cont√™ineres em execu√ß√£o no mesmo Pod ter√£o um nome de host, endere√ßo IP, tabela de roteamento e discos. </p><br><p>  √â importante notar que, ao dimensionar o n√∫mero de inst√¢ncias do seu aplicativo no Kubernetes, √© necess√°rio aumentar o n√∫mero de Pods, e n√£o o n√∫mero de cont√™ineres em um Pod espec√≠fico. <br>  Mais detalhes na documenta√ß√£o oficial dos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">pods</a> . </p><br><p>  Por exemplo, vamos criar o Pod mais simples com o Nginx usando a descri√ß√£o no formato yaml: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-basic.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80</span></span></code> </pre> <br><p>  Para criar um Pod, podemos usar o utilit√°rio <strong>kubectl</strong> . <br>  Adicionamos todos os exemplos apresentados no artigo ao nosso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">grupo Github</a> , para que voc√™ n√£o possa criar arquivos no seu computador, mas use o URL do arquivo do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">reposit√≥rio</a> p√∫blico: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/pods/nginx-basic.yaml</code> </pre> <br><p>  Ap√≥s a cria√ß√£o, podemos solicitar informa√ß√µes completas sobre o status do Pod usando o comando kubectl description: </p><br><pre> <code class="bash hljs">$ kubectl describe pod nginx Name: nginx Namespace: default Node: <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0/10.0.0.5 Start Time: Sun, 17 Jun 2018 12:29:03 +0000 Labels: &lt;none&gt; Annotations: &lt;none&gt; Status: Running IP: 10.100.88.9 Containers: nginx: Container ID: docker://6ca6383b66686c05c61c1f690737110e0f8994eda393f44a7ebfbbf2b2026267 Image: library/nginx:1.14-alpine Image ID: docker-pullable://docker.io/nginx@sha256:944b79ca7dbe456ce72e73e70816c1990e39967c8f010349a388c00b77ec519c Port: 80/TCP Host Port: 0/TCP State: Running Started: Sun, 17 Jun 2018 12:29:16 +0000 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rp5ls (ro) Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-rp5ls: Type: Secret (a volume populated by a Secret) SecretName: default-token-rp5ls Optional: <span class="hljs-literal"><span class="hljs-literal">false</span></span> QoS Class: BestEffort Node-Selectors: &lt;none&gt; Tolerations: &lt;none&gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 52s default-scheduler Successfully assigned nginx to <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Normal SuccessfulMountVolume 51s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 MountVolume.SetUp succeeded <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> volume <span class="hljs-string"><span class="hljs-string">"default-token-rp5ls"</span></span> Normal Pulling 50s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 pulling image <span class="hljs-string"><span class="hljs-string">"library/nginx:1.14-alpine"</span></span> Normal Pulled 39s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Successfully pulled image <span class="hljs-string"><span class="hljs-string">"library/nginx:1.14-alpine"</span></span> Normal Created 39s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Created container Normal Started 39s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Started container</code> </pre> <br><p>  Como voc√™ pode ver, o Pod foi iniciado em um n√≥ chamado test-cluster-nd5c5y6lsfxb-minion-0 e recebeu um endere√ßo IP interno de 10.100.88.9. </p><br><p>  Na se√ß√£o Eventos, voc√™ pode ver os principais eventos de inicializa√ß√£o - selecionando um n√≥ para iniciar e fazer o download da imagem. </p><br><p>  Podemos entrar no Pod e verificar o status dos processos dentro do cont√™iner: </p><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it nginx sh ps aux PID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 7 nginx 0:00 nginx: worker process 20 root 0:00 sh 24 root 0:00 ps aux <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre> <br><p>  Deve-se ter em mente que o endere√ßo IP 10.100.88.9 n√£o estar√° dispon√≠vel para outros aplicativos dentro e fora do cluster Kubernetes; o acesso ao Nginx em execu√ß√£o ser√° poss√≠vel apenas dentro do pr√≥prio Pod: </p><br><pre> <code class="bash hljs">$ ping -c 1 10.100.88.9 PING 10.100.88.9 (10.100.88.9): 56 data bytes --- 10.100.88.9 ping statistics --- 1 packets transmitted, 0 packets received, 100% packet loss $ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> nginx -- ping -c1 10.100.88.9 PING 10.100.88.9 (10.100.88.9): 56 data bytes 64 bytes from 10.100.88.9: seq=0 ttl=64 time=0.075 ms --- 10.100.88.9 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.075/0.075/0.075 ms</code> </pre> <br><p>  Al√©m do fato de o endere√ßo IP especificado estar acess√≠vel apenas no cont√™iner, ele tamb√©m n√£o √© permanente.  Isso significa que, se este Pod for recriado, ele poder√° obter um endere√ßo IP diferente. </p><br><p>  Para resolver esses problemas, voc√™ pode usar um objeto chamado Servi√ßo. </p><br><h3>  Servi√ßos </h3><br><p>  O servi√ßo permite atribuir endere√ßos IP permanentes para os Pods, fornecer acesso a partir de redes externas e equilibrar solicita√ß√µes entre os Pods. <br>  Voc√™ pode aprender mais sobre o servi√ßo na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">documenta√ß√£o oficial</a> . </p><br><p>  Por exemplo, precisamos remover o Pod em execu√ß√£o: </p><br><pre> <code class="bash hljs">$ kubectl delete pod nginx</code> </pre> <br><p>  Adicione √† descri√ß√£o do Pod um r√≥tulo (Label), necess√°rio para o Servi√ßo: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-labeled.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: app: webservice spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80</span></span></code> </pre> <br><p>  Tamb√©m precisaremos de uma descri√ß√£o do Servi√ßo: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-nodeport.yaml apiVersion: v1 kind: Service metadata: name: nginx-nodeport labels: app: webservice spec: type: NodePort ports: - port: 80 nodePort: 30001 protocol: TCP selector: app: webservice</span></span></code> </pre> <br><p>  Crie Pod e Servi√ßo: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/pods/nginx-labeled.yaml \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/services/nginx-nodeport.yaml</code> </pre> <br><p>  Como o Servi√ßo criado √© do tipo NodePort, a porta 30001 indicada por n√≥s em todas as interfaces de rede ser√° aberta em todos os n√≥s do cluster. <br>  Isso significa que, se adicionarmos um endere√ßo IP externo a qualquer n√≥, podemos acessar o Pod em execu√ß√£o com o Nginx a partir de uma rede externa. </p><br><p>  Para n√£o usar os endere√ßos externos dos n√≥s do cluster para acessar o Servi√ßo, podemos usar o tipo LoadBalancer em vez de NodePort. <br>  Vamos precisar de uma nova descri√ß√£o do servi√ßo: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-loadbalancer.yaml apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer labels: app: webservice spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: app: webservice</span></span></code> </pre> <br><p>  Exclua o servi√ßo atual e aplique a nova descri√ß√£o: </p><br><pre> <code class="bash hljs">$ kubectl delete service nginx-service $ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/services/nginx-loadbalancer.yaml</code> </pre> <br><p>  Depois de iniciar o servi√ßo, o Nginx estar√° dispon√≠vel na porta TCP 80 a partir de uma rede externa e n√£o ser√° necess√°rio atribuir e usar endere√ßos externos para os n√≥s do cluster.  O servi√ßo do tipo LoadBalancer alocar√° automaticamente um novo endere√ßo externo para o seu projeto VPC e come√ßar√° a us√°-lo. </p><br><p>  Voc√™ pode obter informa√ß√µes sobre o endere√ßo externo destacado usando o kubectl: </p><br><pre> <code class="bash hljs">$ kubectl get service nginx-service -o=custom-columns=IP:status.loadBalancer.ingress[0].ip IP xxx.xxx.xxx.xxx</code> </pre> <br><p>  Nos nossos exemplos, apenas um Pod foi lan√ßado com o Nginx.  Para dimensionar o aplicativo para mais pods, podemos usar a implanta√ß√£o. </p><br><h3>  Implanta√ß√µes </h3><br><p>  A implanta√ß√£o √© a ess√™ncia do cluster Kubernetes, que permite dimensionar os Pods e atualizar ou reverter convenientemente as vers√µes para um grande n√∫mero de Pods. <br>  Em vez de Implanta√ß√£o, voc√™ tamb√©m pode usar o objeto ReplicaSet, mas n√£o o tocaremos em nossos exemplos. <br>  Voc√™ pode aprender mais sobre implanta√ß√£o na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">documenta√ß√£o oficial</a> . </p><br><p>  Novamente, precisamos remover o Pod (n√£o precisamos remover o Servi√ßo): </p><br><pre> <code class="bash hljs">$ kubectl delete pod nginx</code> </pre> <br><p>  Adicione a seguinte descri√ß√£o de implanta√ß√£o: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-1.14.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 10 selector: matchLabels: app: webservice minReadySeconds: 10 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: webservice spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80</span></span></code> </pre> <br><p>  Crie a implanta√ß√£o especificada: </p><br><pre> <code class="bash hljs">$ kubectl create -f \ https://raw.githubusercontent.com/selectel/kubernetes-examples/master/deployments/nginx-1.14.yaml</code> </pre> <br><p>  Escolhemos 10 para o par√¢metro replicas, para que 10 Pods com o aplicativo Nginx sejam criados em nosso cluster: </p><br><pre> <code class="bash hljs">$ kubectl get pods --selector app=webservice NAME READY STATUS RESTARTS AGE nginx-deployment-54bfdc4489-42rrb 1/1 Running 0 4m nginx-deployment-54bfdc4489-5lvtc 1/1 Running 0 4m nginx-deployment-54bfdc4489-g7rk2 1/1 Running 0 4m nginx-deployment-54bfdc4489-h5rxp 1/1 Running 0 4m nginx-deployment-54bfdc4489-l9l2d 1/1 Running 0 4m nginx-deployment-54bfdc4489-pjpvg 1/1 Running 0 4m nginx-deployment-54bfdc4489-q8dnp 1/1 Running 0 4m nginx-deployment-54bfdc4489-s4wzf 1/1 Running 0 4m nginx-deployment-54bfdc4489-tfxf9 1/1 Running 0 4m nginx-deployment-54bfdc4489-xjzb5 1/1 Running 0 4m</code> </pre> <br><p>  Voc√™ pode acessar o aplicativo em execu√ß√£o na rede externa usando o Servi√ßo criado na se√ß√£o anterior.  O servi√ßo equilibrar√° automaticamente as solicita√ß√µes da rede externa entre 10 inst√¢ncias do Nginx. </p><br><p>  Se necess√°rio, podemos atualizar a vers√£o do Nginx.  Atualize a descri√ß√£o de Implanta√ß√£o, alterando a vers√£o da imagem de 1,14-alpino para 1,15-alpino: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-1.15.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 10 selector: matchLabels: app: webservice minReadySeconds: 10 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: webservice spec: containers: - name: nginx image: library/nginx:1.15-alpine # &lt;-- changed ports: - containerPort: 80</span></span></code> </pre> <br><p>  Para iniciar o processo de atualiza√ß√£o dos Pods, usamos o comando kubectl.  Preste aten√ß√£o ao argumento --record, √© √∫til para a conveniente revers√£o subsequente da vers√£o do Nginx: </p><br><pre> <code class="bash hljs">$ kubectl apply -f \ https://raw.githubusercontent.com/selectel/kubernetes-examples/master/deployments/nginx-1.15.yaml \ --record</code> </pre> <br><p>  Voc√™ pode monitorar o andamento da atualiza√ß√£o usando o seguinte comando: </p><br><pre> <code class="bash hljs">$ kubectl rollout status deployment nginx-deployment Waiting <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rollout to finish: 4 out of 10 new replicas have been updated...</code> </pre> <br><p>  O Kubernetes aguardar√° 10 segundos ap√≥s uma atualiza√ß√£o bem-sucedida de um Pod, pois especificamos o valor 10 para o par√¢metro minReadySeconds na descri√ß√£o da Implanta√ß√£o. </p><br><p>  Ap√≥s a conclus√£o da atualiza√ß√£o, todos os Pods for Deployment entrar√£o em um estado ativo: </p><br><pre> <code class="bash hljs">$ kubectl get deployment --selector app=webservice NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 10 10 10 10 23m</code> </pre> <br><p>  Podemos reverter a vers√£o do aplicativo se algo der errado.  Para fazer isso, precisamos selecionar a revis√£o de implanta√ß√£o desejada: </p><br><pre> <code class="bash hljs">$ kubectl rollout <span class="hljs-built_in"><span class="hljs-built_in">history</span></span> deployment nginx-deployment deployments <span class="hljs-string"><span class="hljs-string">"nginx-deployment"</span></span> REVISION CHANGE-CAUSE 1 &lt;none&gt; 2 kubectl apply --filename=https://raw.githubusercontent.com/selectel/kubernetes-examples/master/deployments/nginx-1.15.yaml --record=<span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Existem 2 revis√µes na sa√≠da do comando - a primeira √© a cria√ß√£o inicial do Deployment, a segunda √© uma atualiza√ß√£o.  Como usamos o argumento --record ao atualizar, vemos o comando que criou a segunda revis√£o do Deployment. </p><br><p>  Para reverter a vers√£o, use o seguinte comando: </p><br><pre> <code class="bash hljs">$ kubectl rollout undo deployment nginx-deployment --to-revision=1</code> </pre> <br><p>  Da mesma forma com a atualiza√ß√£o, podemos monitorar a revers√£o da vers√£o usando o comando: </p><br><pre> <code class="bash hljs">$ kubectl rollout status deployment nginx-deployment Waiting <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rollout to finish: 6 out of 10 new replicas have been updated‚Ä¶</code> </pre> <br><p>  Em todos os nossos exemplos, usamos cont√™ineres sem um armazenamento de dados persistente.  Na pr√≥xima se√ß√£o, vamos corrigi-lo. </p><br><h2>  Armazenamento de dados </h2><br><p>  Por padr√£o, todos os dados dos cont√™ineres em execu√ß√£o nos Pods s√£o ef√™meros e ser√£o perdidos quando os Pods falham. </p><br><p>  Voc√™ pode usar o objeto PersistentVolumeClaim para executar Pods com um data warehouse persistente. </p><br><p>  Criar um objeto desse tipo em um cluster √© muito simples - basta adicionar sua descri√ß√£o, semelhante √† maneira como criamos Pod, Servi√ßo ou Implanta√ß√£o nas se√ß√µes anteriores. </p><br><p>  Mais informa√ß√µes podem ser encontradas na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">documenta√ß√£o oficial</a> . </p><br><p>  Descri√ß√£o de exemplo de PersistentVolumeClaim criando um disco de 10 GB: </p><br><pre> <code class="bash hljs">apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pv-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi</code> </pre> <br><p>  Podemos conect√°-lo como um disco ao nosso Pod atualizando a descri√ß√£o do Pod com o Nginx criado anteriormente: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-with-volume.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80 volumeMounts: - mountPath: "/var/www/html" name: data volumes: - name: data persistentVolumeClaim: claimName: my-pv-claim</span></span></code> </pre> <br><p>  No entanto, para que o disco seja criado, voc√™ precisar√° especificar as propriedades do disco criado na forma de StorageClass.  No servi√ßo "Nuvem privada virtual", voc√™ pode usar unidades de rede de tipos r√°pido, universal e b√°sico como armazenamento permanente de dados do Kubernetes Pod. </p><br><p>  Por exemplo, para criar um StorageClass que permita o uso de discos r√°pidos na zona de disponibilidade do ru-1b, voc√™ precisa da seguinte descri√ß√£o: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># fast.ru-1b.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast.ru-1b annotations: storageclass.beta.kubernetes.io/is-default-class: "true" provisioner: kubernetes.io/cinder parameters: type: fast.ru-1b availability: ru-1b</span></span></code> </pre> <br><p>  Antes de criar os objetos especificados, exclua a Implanta√ß√£o criada anteriormente: </p><br><pre> <code class="bash hljs">$ kubectl delete deployment nginx-deployment</code> </pre> <br><p>  Primeiro, vamos criar uma StorageClass, para que ela se torne a classe padr√£o, e o PersistentVolumeClaim criado posteriormente a usar√°: </p><br><pre> <code class="bash hljs">$ kubectl create -f \ https://raw.githubusercontent.com/selectel/kubernetes-examples/master/storageclasses/fast.ru-1b.yaml</code> </pre> <br><p>  Crie PersistentVolumeClaim e Pod: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/persistentvolumeclaims/my-pv-claim.yaml \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/pods/nginx-with-volume.yaml</code> </pre> <br><p>  Depois disso, um disco ser√° criado automaticamente em seu projeto, que ser√° conectado a um dos n√≥s subordinados do cluster.  Quando cai, o disco muda automaticamente para outro n√≥. </p><br><p>  Podemos ver o disco dentro do cont√™iner com o Nginx: </p><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it nginx sh mount | grep <span class="hljs-string"><span class="hljs-string">"/var/www/html"</span></span> /dev/sdc on /var/www/html <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> ext4 (rw,seclabel,relatime,data=ordered) <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre> <br><p>  Voc√™ pode conectar a unidade ao Deployment.  Um exemplo correspondente pode ser encontrado na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">documenta√ß√£o oficial</a> . </p><br><h2>  Painel de Controle Kubernetes </h2><br><p>  Voc√™ pode usar o painel interno do pr√≥prio Kubernetes para exibir o status dos objetos de cluster e seu gerenciamento. </p><br><p>  Para acessar todos os recursos do painel, voc√™ precisar√° criar uma conta com a fun√ß√£o de administrador em seu cluster. </p><br><p>  Para fazer isso, precisamos de uma descri√ß√£o da conta: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># admin-user.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system</span></span></code> </pre> <br><p>  E descri√ß√£o da fun√ß√£o: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cluster-admin.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system</span></span></code> </pre> <br><p>  Crie os objetos especificados: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/accounts/admin-user.yaml \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/clusterrolebindings/cluster-admin.yaml</code> </pre> <br><p>  Em seguida, voc√™ precisar√° descobrir o valor do token gerado para esta conta. <br>  Para fazer isso, localize o objeto Secreto correspondente no cluster: </p><br><pre> <code class="bash hljs">$ kubectl get secret --namespace=kube-system | grep <span class="hljs-string"><span class="hljs-string">"admin-user-token"</span></span> admin-user-token-bkfhb kubernetes.io/service-account-token 3 22m</code> </pre> <br><p>  E observe o valor do token do Segredo encontrado com o nome admin-user-token-bkfhb: </p><br><pre> <code class="bash hljs">$ kubectl describe secret admin-user-token-bkfhb --namespace=kube-system | grep <span class="hljs-string"><span class="hljs-string">"token:"</span></span> token: XXXXXX...</code> </pre> <br><p>  Em resposta, voc√™ receber√° o valor do token, salve-o, ser√° √∫til para n√≥s no futuro. <br>  Para detalhes de controle de acesso para objetos Kubernetes, consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">documenta√ß√£o oficial</a> . </p><br><p>  Caso voc√™ tenha criado um cluster a partir de um modelo p√∫blico, o Pod e o Servi√ßo j√° existem nele para garantir a opera√ß√£o do painel: </p><br><pre> <code class="bash hljs">$ kubectl get svc kubernetes-dashboard --namespace=kube-system 206ms Tue Jun 19 14:35:19 2018 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard ClusterIP 10.254.122.245 &lt;none&gt; 443/TCP 2d $ kubectl get pod --namespace=kube-system --selector k8s-app=kubernetes-dashboard 119ms Tue Jun 19 14:36:48 2018 NAME READY STATUS RESTARTS AGE kubernetes-dashboard-747575c864-jpxvt 1/1 Running 0 2d</code> </pre> <br><p>  Como o Servi√ßo √© do tipo ClusterIP, ele estar√° dispon√≠vel apenas no pr√≥prio cluster. <br>  Voc√™ pode acessar o painel do seu computador em funcionamento com o arquivo de configura√ß√£o do cluster usando o comando kubectl: </p><br><pre> <code class="bash hljs">$ kubectl proxy Starting to serve on 127.0.0.1:8001</code> </pre> <br><p>  Teste o proxy abrindo o endere√ßo especificado no navegador: </p><br><img src="https://habrastorage.org/webt/lc/zm/k1/lczmk1ud4tjatfu3lthvkrcu66e.png"><br><p>  Se voc√™ encontrar uma resposta semelhante √† captura de tela, poder√° ir para a tela do painel de controle usando o seguinte endere√ßo: </p><br><pre> <code class="bash hljs">http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</code> </pre> <br><p>  Passando por isso, voc√™ deve ver a tela de login no painel: </p><br><img src="https://habrastorage.org/webt/60/fj/p5/60fjp5-2xjyn_p5mz-jh_ozwzxe.png"><br><p>  Voc√™ precisar√° especificar o token recebido anteriormente.  Ap√≥s o login, voc√™ pode usar o painel de controle: </p><br><img src="https://habrastorage.org/webt/38/ry/dr/38rydrraxndpo0hyqhnm-k1nb_k.png"><br><p>  Voc√™ pode descobrir todos os recursos do painel de controle na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="nofollow noopener noreferrer">documenta√ß√£o oficial</a> . </p><br><h2>  Monitorando objetos Kubernetes </h2><br><p>  Se voc√™ usar o modelo de cluster p√∫blico, executar√° automaticamente os componentes para coletar e exibir m√©tricas - Prometheus e Grafana. </p><br><p>  Da mesma forma que no painel de controle, o ClusterIP √© instalado como o tipo de Servi√ßo; o acesso a ele √© poss√≠vel apenas dentro do cluster ou atrav√©s do proxy kubectl.  Voc√™ pode acessar o Grafana no seu computador de trabalho no seguinte endere√ßo: </p><br><pre> <code class="bash hljs">http://127.0.0.1:8001/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana:80</code> </pre> <br><img src="https://habrastorage.org/webt/p3/7e/go/p37egoksdoz8bvp8tsq2fgpsuru.png"><br><h2>  Conclus√£o </h2><br><p>  Neste artigo, examinamos os objetos Kubernetes mais usados ‚Äã‚Äãe analisamos exemplos de como iniciar e gerenciar um cluster usando o OpenStack Magnum. </p><br><p>  Num futuro pr√≥ximo, ser√° poss√≠vel usar as vers√µes mais recentes do Kubernetes, e o gerenciamento de cluster estar√° dispon√≠vel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no painel de controle</a> . </p><br><p>  Ficaremos contentes se voc√™ usar nosso servi√ßo no modo de teste e fornecer feedback. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt416951/">https://habr.com/ru/post/pt416951/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt416941/index.html">Teoria da felicidade. Introdu√ß√£o √† Merfologia</a></li>
<li><a href="../pt416943/index.html">Materiais √∫teis para projetar interfaces de voz</a></li>
<li><a href="../pt416945/index.html">Como fizemos o BelAZ. Parte 1 - Ferro</a></li>
<li><a href="../pt416947/index.html">Jogue o jogo antes das Olimp√≠adas: eSports se torna oficial</a></li>
<li><a href="../pt416949/index.html">A atualiza√ß√£o em larga escala do Sr. Steven para instalar uma rede de ca√ßa quatro vezes maior foi conclu√≠da</a></li>
<li><a href="../pt416953/index.html">Crie um sombreador de √°gua de desenho animado para a web. Parte 1</a></li>
<li><a href="../pt416955/index.html">Pequenos truques com Elasticsearch</a></li>
<li><a href="../pt416957/index.html">Qual m√°quina a laser comprar? Revis√£o confi√°vel da m√°quina a laser Raylogic 11G</a></li>
<li><a href="../pt416959/index.html">Apple apresenta novo recurso anti-roubo para iOS</a></li>
<li><a href="../pt416961/index.html">Resolu√ß√£o autom√°tica de conflitos usando transforma√ß√µes operacionais</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>