<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©ğŸ¾â€ğŸ’¼ ğŸ™€ ğŸ™‡ Agen Pembelajaran Mesin di Unity â˜®ï¸ ğŸ§–ğŸ¼ ğŸ‘¨ğŸ¾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Artikel ini tentang agen pembelajaran mesin Unity ditulis oleh Michael Lanham, seorang inovator teknis, pengembang aktif untuk Unity, seorang konsulta...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Agen Pembelajaran Mesin di Unity</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454612/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9aa/6fe/055/9aa6fe055c20cde3642bb0f0782f62d3.jpg" alt="gambar"></div><br>  <em>Artikel ini tentang agen pembelajaran mesin Unity ditulis oleh Michael Lanham, seorang inovator teknis, pengembang aktif untuk Unity, seorang konsultan, manajer, dan penulis banyak game Unity, proyek grafis, dan buku.</em> <br><br>  Pengembang Unity telah menerapkan dukungan untuk pembelajaran mesin, dan khususnya pembelajaran penguatan untuk pembuatan SDK pembelajaran penguatan mendalam (DRL) untuk pengembang game dan simulasi.  Untungnya, tim Unity, yang dipimpin oleh Danny Lange, telah berhasil menerapkan mesin DRL yang andal dan modern yang mampu memberikan hasil yang mengesankan.  Unity menggunakan model optimalisasi kebijakan proximal (PPO) sebagai dasar dari mesin DRL;  model ini jauh lebih kompleks dan mungkin berbeda dalam beberapa aspek. <br><br>  Pada artikel ini, saya akan memperkenalkan Anda ke alat dan SDK untuk membuat agen DRL dalam game dan simulasi.  Terlepas dari kebaruan dan kekuatan alat ini, mudah digunakan dan memiliki alat bantu yang memungkinkan Anda untuk mempelajari konsep pembelajaran mesin saat bepergian.  Untuk bekerja dengan tutorial Anda perlu menginstal mesin Unity. <br><a name="habracut"></a><br><h2>  Instal Agen-ML </h2><br>  Pada bagian ini, saya akan secara singkat berbicara tentang langkah-langkah yang harus diambil untuk menginstal ML-Agen SDK.  Materi ini masih dalam versi beta dan dapat bervariasi dari versi ke versi.  Ikuti langkah-langkah ini: <br><br><ol><li>  Instal Git di komputer;  Ini bekerja dari baris perintah.  Git adalah sistem manajemen kode sumber yang sangat populer, dan ada banyak sumber daya di Internet tentang menginstal dan menggunakan Git di seluruh platform.  Setelah menginstal Git, pastikan itu bekerja dengan membuat tiruan dari repositori apa pun. </li><li>  Buka prompt perintah atau shell biasa.  Pengguna Windows dapat membuka jendela Anaconda. </li><li>  Buka folder yang berfungsi di mana Anda ingin menempatkan kode baru Anda dan masukkan perintah berikut (pengguna Windows dapat memilih C: \ ML-Agen): <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents </pre></li><li>  Jadi, Anda mengkloning repositori ml-agen di komputer Anda dan membuat folder baru dengan nama yang sama.  Anda juga dapat menambahkan nomor versi ke nama folder.  Persatuan, seperti hampir seluruh dunia kecerdasan buatan, terus berubah, setidaknya untuk saat ini.  Ini berarti bahwa perubahan baru terus muncul.  Pada saat penulisan, kami mengkloning repositori ke folder ml-agent.6: <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents ml-agents.6 </pre></li><li>  Buat lingkungan virtual baru untuk ml-agen dan tentukan versi 3.6, seperti ini: <br><br><pre>  # Jendela 
 conda create -n ml-agents python = 3.6
 
 #Mac
 Gunakan dokumentasi untuk lingkungan pilihan Anda </pre></li><li>  Aktifkan kembali lingkungan Anda dengan Anaconda: <br><br><pre>  aktifkan ml-agen </pre></li><li>  Instal TensorFlow.  Di Anaconda, ini dapat dilakukan dengan perintah berikut: <br><br><pre>  pip instal tensorflow == 1.7.1 </pre></li><li>  Instal paket Python.  Di Anaconda, masukkan yang berikut ini: <br><br><pre><code class="plaintext hljs">cd ML-Agents #from root folder cd ml-agents or cd ml-agents.6 #for example cd ml-agents pip install -e . or pip3 install -e .</code> </pre> </li><li>  Jadi, Anda menginstal semua paket SDK Agen yang diperlukan;  ini mungkin memakan waktu beberapa menit.  Jangan tutup jendela, ini akan segera berguna. </li></ol><br>  Jadi kami menginstal dan mengkonfigurasi Unity Python SDK untuk ML-Agents.  Di bagian selanjutnya, kita akan belajar cara mengatur dan melatih salah satu dari banyak lingkungan yang disediakan oleh Unity. <br><br><h2>  Pelatihan Agen </h2><br>  Sekarang kita bisa langsung ke bisnis dan mengeksplorasi contoh-contoh yang menggunakan deep reinforcement learning (DRL).  Untungnya, ada beberapa contoh dalam toolkit agen baru untuk menunjukkan kekuatan mesin.  Buka Unity atau Unity Hub, dan ikuti langkah-langkah ini: <br><br><ol><li>  Klik pada tombol Buka proyek di bagian atas kotak dialog Proyek. </li><li>  Temukan dan buka folder proyek UnitySDK, seperti yang ditunjukkan pada tangkapan layar: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/371/706/016/3717060168f78fccd271c064f0e055ce.png"></div><br>  <i>Buka Proyek Unity SDK</i> </li><li>  Tunggu proyek dimuat, dan kemudian buka jendela Proyek di bagian bawah editor.  Jika sebuah jendela terbuka meminta Anda untuk memperbarui proyek, maka pilih ya atau lanjutkan.  Saat ini, semua kode agen kompatibel ke belakang. </li><li>  Temukan dan buka adegan GridWorld seperti yang ditunjukkan pada tangkapan layar: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0aa/6bd/ab5/0aa6bdab5b8d07cdf414e59255862c05.png"></div><br>  <em>Membuka contoh adegan GridWorld</em> </li><li>  Pilih objek GridAcademy di jendela Hierarchy. </li><li>  Pergi ke jendela Inspektur dan di sebelah bidang Otak klik pada ikon untuk membuka dialog pemilihan Otak: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/08c/cda/126/08ccda1263b74d131c70cff42edae92c.png"></div></li><li>  Pilih otak GridWorldPlayer.  Otak ini milik pemain, yaitu, pemain (Anda) dapat mengendalikan permainan. </li><li>  Klik tombol Play di bagian atas editor dan perhatikan lingkungannya.  Karena gim sekarang diatur untuk mengontrol pemain, Anda dapat menggunakan tombol WASD untuk memindahkan kubus.  Tugasnya adalah memindahkan kubus biru ke simbol + hijau, sambil menghindari X merah. </li></ol><br>  Dapatkan nyaman dalam permainan.  Perhatikan bahwa permainan hanya berfungsi untuk periode waktu tertentu dan tidak berbasis giliran.  Di bagian selanjutnya, kita akan belajar bagaimana menjalankan contoh ini dengan agen DRL. <br><br><h2>  Apa yang ada di otak? </h2><br>  Salah satu aspek luar biasa dari platform ML-Agen adalah kemampuan untuk dengan cepat dan mudah beralih dari manajemen pemain ke manajemen AI / agen.  Untuk ini, Unity menggunakan konsep "otak".  Otak dapat dikontrol baik oleh pemain atau oleh agen (learning brain).  Hal yang paling menakjubkan adalah Anda dapat mengumpulkan game dan mengujinya sebagai pemain, dan kemudian memberikannya di bawah kendali agen RL.  Berkat ini, game apa pun yang ditulis dengan sedikit usaha dapat dibuat untuk dikendalikan menggunakan AI. <br><br>  Proses pengaturan dan memulai pelatihan agen RL di Unity cukup sederhana.  Unity menggunakan Python eksternal untuk membangun model otak pembelajaran.  Menggunakan Python sangat masuk akal karena sudah ada beberapa pustaka pembelajaran mendalam (DL) yang dibangun di sekitarnya.  Untuk melatih agen di GridWorld, selesaikan langkah-langkah berikut: <br><br><ol><li>  Pilih GridAcademy lagi dan pilih otak GridWorldLearning di bidang Otak alih-alih GridWorldPlayer: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/869/fc8/b42/869fc8b42b64d8b3ccc785eb9a6e765e.png"></div><br>  <em>Beralih ke Menggunakan Otak GridWorldLearning</em> </li><li>  Centang kotak Kontrol di sebelah kanan.  Parameter sederhana ini melaporkan bahwa otak dapat dikontrol secara eksternal.  Opsi ini harus diaktifkan. </li><li>  Pilih objek trueAgent di jendela Hierarchy, dan kemudian di jendela Inspector mengubah properti Brain di komponen Grid Agent ke otak GridWorldLearning: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/ff3/ed2/832ff3ed2bc17c16cc5ed823d10a7156.png"></div><br>  <em>Pekerjaan otak GridWorldLearning untuk agen</em> </li><li>  Dalam contoh ini, kita membutuhkan Akademi dan Agen untuk menggunakan otak GridWorldLearning yang sama.  Beralih ke jendela Anaconda atau Python dan pilih folder ML-Agents / ml-agen. </li><li>  Jalankan perintah berikut di jendela Anaconda atau Python menggunakan lingkungan virtual ml-agent: <br><br><pre>  mlagents-pelajari config / trainer_config.yaml --run-id = firstRun --train </pre></li><li>  Ini akan meluncurkan model pelatihan PPO Persatuan dan agen contoh dengan konfigurasi yang ditentukan.  Pada titik tertentu, jendela prompt perintah akan meminta Anda untuk memulai editor Unity dengan lingkungan yang dimuat. </li><li>  Klik Mainkan di editor Persatuan untuk meluncurkan lingkungan GridWorld.  Segera setelah itu, Anda akan melihat pelatihan agen dan output ke jendela skrip Python: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/583/cc2/fd9/583cc2fd94c49e4a1039d9af64312f68.png"></div><br>  <em>Menjalankan GridWorld dalam Mode Belajar</em> </li><li>  Perhatikan bahwa skrip mlagents-learn adalah kode Python yang membangun model RL untuk menjalankan agen.  Seperti yang Anda lihat dari output skrip, ada beberapa parameter (hiperparameter) yang perlu dikonfigurasi. </li><li>  Biarkan agen mempelajari beberapa ribu iterasi dan perhatikan seberapa cepat ia belajar.  Model internal yang digunakan di sini disebut PPO telah terbukti menjadi model pembelajaran yang sangat efektif untuk banyak tugas yang berbeda, dan sangat cocok untuk pengembangan game.  Dengan peralatan yang cukup kuat, seorang agen idealnya dapat belajar dalam waktu kurang dari satu jam. </li></ol><br>  Biarkan agen belajar lebih lanjut dan mencari cara lain untuk melacak proses pembelajaran agen, seperti yang disajikan di bagian selanjutnya. <br><br><h2>  Memantau Pembelajaran dengan TensorBoard </h2><br>  Melatih agen menggunakan model RL atau model DL apa pun sering kali merupakan tugas yang menakutkan dan membutuhkan perhatian terhadap detail.  Untungnya, TensorFlow memiliki seperangkat alat charting yang disebut TensorBoard yang dapat Anda gunakan untuk memantau proses belajar Anda.  Ikuti langkah-langkah ini untuk memulai TensorBoard: <br><br><ol><li>  Buka jendela Anaconda atau Python.  Aktifkan lingkungan virtual ml-agen.  Jangan tutup jendela di mana model pelatihan berjalan;  kami membutuhkannya untuk melanjutkan. </li><li>  Buka folder ML-Agen / ml-agen dan jalankan perintah berikut: <br><br><pre>  tensorboard --logdir = ringkasan </pre></li><li>  Jadi kami meluncurkan TensorBoard di server web internal kami sendiri.  Anda dapat memuat halaman menggunakan URL yang ditampilkan setelah perintah sebelumnya. </li><li>  Masukkan URL untuk TensorBoard seperti yang ditunjukkan di jendela, atau ketik localhost: 6006 atau nama mesin: 6006 di browser.  Setelah sekitar satu jam, Anda akan melihat sesuatu seperti ini: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/619/b62/964/619b62964a88d9ee4b7635073799caf8.png"></div><br>  <em>Jendela Grafik TensorBoard</em> </li><li>  Tangkapan layar sebelumnya menunjukkan grafik, yang masing-masing menampilkan aspek pelatihan yang terpisah.  Untuk memahami bagaimana agen kami dilatih, Anda perlu menangani masing-masing grafik ini, jadi kami akan menganalisis output dari setiap bagian: </li></ol><br><ul><li>  Lingkungan: bagian ini menunjukkan bagaimana agen memanifestasikan dirinya dalam lingkungan secara keseluruhan.  Di bawah ini adalah tampilan grafik yang lebih rinci dengan tren yang disukai: </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bc1/596/956/bc15969569f3d959bc550c2ee629ac3d.png"></div><br>  <em>Gambar terperinci dari grafik bagian Lingkungan</em> <br><br><ul><li>  Hadiah Kumulatif: Ini adalah hadiah total yang memaksimalkan agen.  Biasanya perlu meningkat, tetapi untuk beberapa alasan itu dapat menurun.  Itu selalu yang terbaik untuk memaksimalkan hadiah antara 1 dan -1.  Jika jadwal hadiah melampaui kisaran ini, maka ini juga perlu diperbaiki. </li><li>  Panjang Episode: jika nilai ini menurun, maka itu biasanya pertanda baik.  Pada akhirnya, semakin pendek episode, semakin banyak pelatihan.  Namun, perlu diingat bahwa jika perlu, panjang episode dapat meningkat, sehingga gambarnya mungkin berbeda. </li><li>  Pelajaran: bagan ini memperjelas pelajaran yang diberikan agen;  Ini dimaksudkan untuk pembelajaran kurikulum. </li><li>  Kerugian: Bagian ini menunjukkan grafik yang menunjukkan kerugian atau biaya yang dihitung untuk kebijakan dan nilai.  Di bawah ini adalah tangkapan layar bagian ini dengan panah yang menunjuk ke pengaturan optimal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/aaa/8c5/3f6/aaa8c53f67533a8d349b62d216c15a1b.png"></div><br>  <em>Kehilangan dan pelatihan yang disukai</em> </li></ul><br><ul><li>  Kerugian Kebijakan: Grafik ini menentukan jumlah perubahan kebijakan dari waktu ke waktu.  Politik adalah elemen yang mendefinisikan tindakan, dan dalam kasus umum, jadwal ini harus cenderung ke bawah, menunjukkan bahwa politik membuat keputusan yang lebih baik. </li><li>  Kehilangan Nilai: Ini adalah kerugian rata-rata dari fungsi nilai.  Intinya, ia memodelkan seberapa baik agen memprediksi nilai kondisi selanjutnya.  Pada awalnya, nilai ini harus meningkat, dan setelah stabilisasi remunerasi, seharusnya menurun. </li><li>  Kebijakan: untuk menilai kualitas tindakan dalam PPO, konsep kebijakan digunakan, bukan model.  Tangkapan layar di bawah ini menunjukkan grafik kebijakan dan tren yang disukai: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/0ce/2c0/8450ce2c0e0a95bb39e92fe698dbee7c.png"></div><br>  <i>Bagan Kebijakan dan Tren Pilihan</i> </li><li>  Entropi: Grafik ini menunjukkan besarnya agen penelitian.  Nilai ini perlu dikurangi, karena agen belajar lebih banyak tentang lingkungan dan membutuhkan lebih sedikit riset. </li><li>  Learning Rate: dalam hal ini, nilai ini harus secara bertahap menurun secara linear. </li><li>  Perkiraan Nilai: Ini adalah nilai rata-rata yang dikunjungi oleh semua negara agen.  Untuk mencerminkan peningkatan pengetahuan agen, nilai ini harus tumbuh dan kemudian distabilkan. </li></ul><br>  6. Biarkan agen berjalan sampai selesai dan jangan menutup TensorBoard. <br>  7. Kembali ke jendela Anaconda / Python yang melatih otak dan menjalankan perintah ini: <br><br><pre>  mlagents-pelajari config / trainer_config.yaml --run-id = secondRun --train </pre><br>  8. Anda akan kembali diminta untuk mengklik Mainkan di editor;  jadi lakukanlah.  Biarkan agen memulai pelatihan dan melakukan beberapa sesi.  Dalam prosesnya, tonton jendela TensorBoard dan perhatikan bagaimana secondRun ditampilkan pada grafik.  Anda dapat membiarkan agen ini berjalan hingga selesai, tetapi Anda dapat menghentikannya jika diinginkan. <br><br>  Dalam versi ML-Agen sebelumnya, Anda harus terlebih dahulu membangun Unity yang dapat dieksekusi sebagai lingkungan belajar untuk gim, dan kemudian menjalankannya.  Otak luar Python seharusnya bekerja dengan cara yang sama.  Metode ini membuatnya sangat sulit untuk men-debug masalah dalam kode atau dalam game.  Dalam teknik baru, semua kesulitan ini dihilangkan. <br><br>  Sekarang kita telah melihat betapa mudahnya mengatur dan melatih agen, kita akan beralih ke bagian berikutnya, di mana kita belajar bagaimana menjalankan agen tanpa otak eksternal Python dan menjalankannya langsung di Unity. <br><br><h2>  Peluncuran Agen </h2><br>  Pelatihan python sangat bagus, tetapi Anda tidak bisa menggunakannya dalam gim nyata.  Idealnya, kami ingin membuat grafik TensorFlow dan menggunakannya di Unity.  Untungnya, perpustakaan TensorFlowSharp telah dibuat yang memungkinkan .NET untuk menggunakan grafis TensorFlow.  Ini memungkinkan kami membuat model TFModels offline, dan kemudian menyuntikkannya ke dalam gim.  Sayangnya, kita hanya bisa menggunakan model yang terlatih, tetapi tidak melatihnya seperti itu, setidaknya belum. <br><br>  Mari kita lihat bagaimana ini bekerja, menggunakan contoh grafik yang baru saja kita latih untuk lingkungan GridWorld;  gunakan itu sebagai otak bagian dalam Persatuan.  Ikuti langkah-langkah di bagian berikut untuk mengatur dan menggunakan otak bagian dalam Anda: <br><br><ol><li>  Unduh plugin TFSharp <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=" rel="external nofollow">dari sini</a> </li><li>  Dari menu editor, pilih Aset |  Paket Impor |  Paket Kustom ... </li><li>  Temukan paket aset yang baru saja Anda unduh dan gunakan dialog impor untuk memuat plugin ke proyek. </li><li>  Dari menu, pilih Edit |  Pengaturan Proyek.  Jendela Pengaturan terbuka (muncul dalam versi 2018.3) </li><li>  Temukan Scripting Define Symbols karakter dalam opsi Player dan ubah teks menjadi ENABLE_TENSORFLOW, dan juga aktifkan Izinkan Kode Tidak Aman, seperti yang ditunjukkan pada tangkapan layar: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79d/ecf/c31/79decfc310a7f6cf38b33d1998c14c1f.png"></div><br>  <em>Mengatur bendera ENABLE_TENSORFLOW</em> </li><li>  Temukan objek GridWorldAcademy di jendela Hierarchy dan pastikan bahwa ia menggunakan Brains |  GridWorldLearning.  Nonaktifkan opsi Kontrol di bagian Otak skrip Grid Academy. </li><li>  Temukan otak GridWorldLearning di folder Aset / Contoh / GridWorld / Otak dan pastikan bahwa parameter Model di jendela Inspektur diatur, seperti yang ditunjukkan pada tangkapan layar: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb4/7c8/fef/fb47c8fefd97d4e855e8a2c029e4da5d.png"></div><br>  <em>Model tugas untuk otak</em> </li><li>  GridWorldLearning harus sudah ditetapkan sebagai model.  Dalam contoh ini, kami menggunakan model TF yang datang dengan contoh GridWorld. </li><li>  Klik Mainkan untuk memulai editor dan lihat bagaimana agen mengelola kubus. </li></ol><br>  Kami sekarang meluncurkan lingkungan pra-terlatih Unity.  Di bagian selanjutnya, kita akan belajar bagaimana menggunakan otak yang kita latih di bagian sebelumnya. <br><br><h2>  Memuat otak yang terlatih </h2><br>  Semua contoh Unity memiliki otak pra-terlatih yang dapat digunakan untuk mempelajari contoh.  Tentu saja, kami ingin dapat memuat grafik TF kami sendiri ke Unity dan menjalankannya.  Untuk memuat grafik yang terlatih, ikuti langkah-langkah ini: <br><br><ol><li>  Buka folder ML-Agen / ml-agen / model / firstRun-0.  Di dalam folder ini adalah file GridWorldLearning.bytes.  Seret file ini ke folder Project / Asset / ML-Agents / Examples / GridWorld / TFModels di dalam editor Unity: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f51/955/ef5/f51955ef56aae53e7946378048b9e13e.png"></div><br>  <em>Menyeret grafik byte ke Unity</em> </li><li>  Jadi kami mengimpor grafik ke proyek Unity sebagai sumber daya dan menamainya menjadi GridWorldLearning 1. Mesin melakukan ini karena model default sudah memiliki nama yang sama. </li><li>  Temukan GridWorldLearning di folder otak, pilih di jendela Inspektur dan seret model GridWorldLearning 1 yang baru ke dalam bidang Model Parameter Otak: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/784/8a3/9b7/7848a39b79b02ffb11d968f835054295.png"></div><br>  <em>Memuat otak ke dalam bidang Model Grafik</em> </li><li>  Pada tahap ini, kita tidak perlu mengubah parameter lain, tetapi memberi perhatian khusus pada bagaimana otak dikonfigurasi.  Untuk saat ini, pengaturan standar akan dilakukan. </li><li>  Klik Mainkan di editor Persatuan dan lihat bagaimana agen berhasil bergerak di sekitar permainan. </li><li>  Keberhasilan agen dalam permainan tergantung pada waktu pelatihannya.  Jika Anda mengizinkannya menyelesaikan pelatihan, agen tersebut akan serupa dengan agen Persatuan yang sepenuhnya terlatih. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id454612/">https://habr.com/ru/post/id454612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id454600/index.html">Bagaimana kami membuat kesepakatan yang aman pada Freelansim: memberikan pilihan, memotong fitur, membandingkan komisi</a></li>
<li><a href="../id454604/index.html">Membuat Aplikasi Bereaksi dengan Backend GraphQL dalam Menit</a></li>
<li><a href="../id454606/index.html">Fitur dari atribut inputmode untuk OS dan browser seluler</a></li>
<li><a href="../id454608/index.html">Perjanjian Tingkat Layanan: kami menulis SLA untuk ... orang lain, atau kesimpulan dari SLA dengan Operator Telekomunikasi</a></li>
<li><a href="../id454610/index.html">Pemasaran konten, SEO, tes dan polling: 9 alat untuk mempromosikan startup di luar negeri</a></li>
<li><a href="../id454614/index.html">XXE: XML entitas eksternal</a></li>
<li><a href="../id454616/index.html">Penggunaan AI untuk meningkatkan efisiensi pekerja mental</a></li>
<li><a href="../id454618/index.html">Lubang Produktivitas: Bagaimana Slack Menyakiti Alur Kerja Kami</a></li>
<li><a href="../id454620/index.html">#NoDeployFriday: membantu atau membahayakan?</a></li>
<li><a href="../id454622/index.html">Kreisel EVEX 910e: model historis - kehidupan baru</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>