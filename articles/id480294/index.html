<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⛹🏻 🍺 ⛵️ Visi alat berat berkecepatan tinggi di perangkat penyortiran suku cadang LEGO serbaguna 👧🏾 ⏮️ 🚰</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Selama beberapa tahun terakhir, saya telah merancang dan membuat mesin yang dapat mengenali dan mengurutkan bagian-bagian LEGO. Bagian terpenting dari...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Visi alat berat berkecepatan tinggi di perangkat penyortiran suku cadang LEGO serbaguna</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/480294/"> Selama beberapa tahun terakhir, saya telah merancang dan membuat mesin yang dapat mengenali dan mengurutkan bagian-bagian LEGO.  Bagian terpenting dari alat ini adalah <strong>Unit</strong> Pengambilan, kompartemen kecil, yang hampir sepenuhnya tertutup di mana terdapat sabuk konveyor, pencahayaan, dan kamera. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/641/6c7/5d8/6416c75d85738aceecf1162f1d48718d.jpg"></div><br>  <i>Pencahayaan Anda akan melihat sedikit lebih rendah.</i> <br><br>  Kamera mengambil foto bagian-bagian LEGO yang datang melalui conveyor, dan kemudian mentransfer gambar secara nirkabel ke server yang menjalankan algoritma kecerdasan buatan untuk mengenali bagian di antara ribuan elemen LEGO yang mungkin.  Saya akan memberi tahu Anda lebih banyak tentang algoritma AI dalam artikel mendatang, dan artikel ini akan fokus pada pemrosesan yang dilakukan antara output mentah dari kamera video dan input ke jaringan saraf. <br><br>  Masalah utama yang perlu saya pecahkan adalah mengubah aliran video dari conveyor menjadi gambar terpisah dari bagian-bagian yang dapat digunakan oleh jaringan saraf. <br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca8/feb/2ea/ca8feb2eafab64d7d835d50e090e7bc5.gif"></div><br>  <i>Tujuan akhir: untuk beralih dari video mentah (di sebelah kiri) ke satu set gambar dengan ukuran yang sama (di sebelah kanan) untuk mentransfernya ke jaringan saraf.</i>  <i>(Dibandingkan dengan pekerjaan nyata, gif adalah sekitar setengah lambat)</i> <br><br>  Ini adalah contoh yang bagus dari tugas yang di permukaan tampak sederhana, tetapi sebenarnya memiliki banyak kendala unik dan menarik, banyak di antaranya unik untuk platform visi mesin. <br><br>  Mengambil bagian kanan gambar dengan cara ini sering disebut deteksi objek.  Itulah tepatnya yang perlu saya lakukan: untuk mengenali keberadaan objek, lokasi dan ukurannya, sehingga Anda dapat membuat <strong>persegi panjang pembatas</strong> untuk setiap bagian pada setiap bingkai. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfc/265/c31/cfc265c31250940a6170ad404f89de08.gif"></div><br>  <i>Yang paling penting adalah menemukan kotak pembatas yang bagus (ditunjukkan di atas dengan warna hijau)</i> <br><br>  Saya akan mempertimbangkan tiga aspek untuk menyelesaikan masalah: <br><br><ul><li>  Bersiap untuk menghilangkan variabel yang tidak perlu </li><li>  Membuat proses dari operasi penglihatan mesin yang sederhana </li><li>  Mempertahankan kinerja yang memadai pada platform Raspberry Pi dengan sumber daya terbatas </li></ul><br><h2>  Eliminasi variabel yang tidak perlu </h2><br>  Dalam kasus tugas-tugas seperti itu, yang terbaik adalah menghilangkan variabel sebanyak mungkin sebelum menggunakan teknik visi mesin.  Sebagai contoh, saya tidak perlu khawatir tentang kondisi lingkungan, posisi kamera yang berbeda, kehilangan informasi karena tumpang tindih beberapa bagian oleh yang lain.  Tentu saja, adalah mungkin (walaupun sangat sulit) untuk menyelesaikan semua variabel ini secara terprogram, tetapi untungnya bagi saya, mesin ini dibuat dari awal.  Saya sendiri dapat mempersiapkan solusi yang sukses, menghilangkan semua gangguan bahkan sebelum saya mulai menulis kode. <br><br>  Langkah pertama adalah dengan tegas memperbaiki posisi, sudut dan fokus kamera.  Dengan ini, semuanya sederhana - dalam sistem, kamera dipasang di atas conveyor.  Saya tidak perlu khawatir tentang gangguan dari bagian lain;  benda yang tidak diinginkan hampir tidak memiliki peluang untuk masuk ke unit penangkapan.  Sedikit lebih rumit, tetapi sangat penting untuk memastikan <strong>kondisi pencahayaan yang konstan</strong> .  Saya tidak perlu pengenal objek untuk secara keliru menafsirkan bayangan bagian yang bergerak di sepanjang pita sebagai objek fisik.  Untungnya, unit tangkap sangat kecil (seluruh bidang pandang kamera lebih kecil dari sepotong roti), jadi saya memiliki lebih dari cukup kontrol atas kondisi di sekitarnya. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8ec/9da/652/8ec9da6525ba0a6f906d3e1ef6309647.jpg"></div><br>  <i>Unit pengambilan, tampilan bagian dalam.</i>  <i>Kamera berada di sepertiga atas bingkai.</i> <br><br>  Salah satu solusinya adalah membuat kompartemen tertutup sepenuhnya sehingga tidak ada pencahayaan luar yang masuk.  Saya mencoba pendekatan ini menggunakan strip LED sebagai sumber pencahayaan.  Sayangnya, sistem ini ternyata sangat murung - hanya satu lubang kecil di kasing sudah cukup dan cahaya menembus ke dalam kompartemen, sehingga tidak mungkin untuk mengenali objek. <br><br>  Pada akhirnya, solusi terbaik adalah "menyumbat" semua sumber cahaya lainnya dengan mengisi kompartemen kecil dengan cahaya yang kuat.  Ternyata sumber cahaya yang bisa digunakan untuk menerangi tempat tinggal sangat murah dan mudah digunakan. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/16a/031/025/16a031025c08531ff3ef2f0ef20a2394.jpg"></div><br>  <i>Dapatkan bayangannya!</i> <br><br>  Ketika sumber diarahkan ke kompartemen kecil, itu benar-benar menyumbat semua gangguan cahaya eksternal potensial.  Sistem seperti itu juga memiliki efek samping yang nyaman: karena banyaknya cahaya di kamera, Anda dapat menggunakan kecepatan rana yang sangat tinggi, mendapatkan gambar bagian yang sangat jelas bahkan ketika bergerak cepat di sepanjang conveyor. <br><br><h2>  Pengakuan Objek </h2><br>  Bagaimana saya bisa mengubah video yang indah ini dengan penerangan seragam ke dalam kotak pembatas yang saya butuhkan?  Jika Anda bekerja dengan AI, Anda bisa menyarankan agar saya menerapkan jaringan saraf untuk pengenalan objek seperti <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="nofollow">YOLO</a> atau <a href="https://arxiv.org/abs/1506.01497" rel="nofollow">Faster R-CNN</a> .  Jaringan saraf ini dapat dengan mudah mengatasi tugas tersebut.  Sayangnya, saya mengeksekusi kode pengenalan objek pada <a href="https://www.raspberrypi.org/" rel="nofollow">Raspberry pi</a> .  Bahkan komputer yang kuat akan memiliki masalah menjalankan jaringan saraf convolutional ini pada frekuensi yang saya butuhkan sekitar 90FPS.  Dan Raspberry pi, yang tidak memiliki GPU yang kompatibel dengan AI, tidak dapat mengatasi versi salah satu dari algoritma AI tersebut.  Saya dapat melakukan streaming video dari Pi ke komputer lain, tetapi transmisi video waktu-nyata adalah proses yang sangat murung, dan penundaan dan keterbatasan bandwidth menyebabkan masalah serius, terutama ketika Anda membutuhkan kecepatan transfer data yang tinggi. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/K9a6mGNmhbc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>YOLO sangat keren!</i>  <i>Tapi saya tidak butuh semua fungsinya.</i> <br><br>  Untungnya, saya bisa menghindari solusi berbasis AI yang sulit menggunakan teknik visi mesin "old-school".  Teknik pertama adalah <strong>pengurangan latar belakang</strong> , yang mencoba untuk mengisolasi semua bagian gambar yang diubah.  Dalam kasus saya, satu-satunya hal yang bergerak di bidang tampilan kamera adalah detail LEGO.  (Tentu saja, rekaman itu juga bergerak, tetapi karena memiliki warna yang seragam, sepertinya itu stasioner ke kamera).  Pisahkan detail LEGO ini dari latar belakang, dan setengah dari masalah terpecahkan. <br><br>  Agar pengurangan latar belakang berfungsi, objek latar depan harus sangat berbeda dari latar belakang.  Detail LEGO memiliki berbagai warna, jadi saya harus memilih warna latar belakang dengan sangat hati-hati sehingga sejauh mungkin dari warna LEGO.  Itulah mengapa kaset di bawah kamera terbuat dari kertas - tidak hanya harus sangat seragam, tetapi juga tidak dapat terdiri dari LEGO, jika tidak maka akan menjadi warna salah satu bagian yang perlu saya kenali!  Saya memilih merah muda pucat, tetapi warna pastel lainnya, tidak seperti warna LEGO biasa, akan cocok. <br><br>  Pustaka OpenCV yang luar biasa telah memiliki beberapa algoritma untuk pengurangan latar belakang.  MOG2 Background Subtractor adalah yang paling kompleks di antara mereka, dan pada saat yang sama bekerja sangat cepat bahkan pada raspberry pi.  Namun, memberi bingkai video langsung ke MOG2 tidak berfungsi dengan baik.  Sosok abu-abu terang dan putih terlalu dekat dengan kecerahan latar belakang pucat dan hilang di atasnya.  Saya perlu menemukan cara untuk lebih jelas memisahkan pita dari bagian-bagian di atasnya, memesan subtrakter latar belakang untuk melihat lebih dekat pada <i>warna</i> , dan bukan pada <i>kecerahan</i> .  Untuk melakukan ini, cukup bagi saya untuk meningkatkan saturasi gambar sebelum mentransfernya ke subtrakter latar belakang.  Hasilnya telah meningkat secara signifikan. <br><br>  Setelah mengurangi latar belakang, saya perlu menggunakan operasi morfologis untuk menghilangkan kebisingan sebanyak mungkin.  Untuk menemukan kontur area putih, Anda dapat menggunakan fungsi findContours () dari pustaka OpenCV.  Dengan menerapkan berbagai heuristik untuk membelokkan loop yang mengandung kebisingan, Anda dapat dengan mudah mengkonversi loop ini ke kotak pembatas yang telah ditentukan. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/214/785/f54/214785f5468b2b84ae29f212a66d5abd.gif"></div><br><h2>  Performa </h2><br>  Jaringan saraf adalah makhluk yang rakus.  Untuk hasil terbaik dalam klasifikasi, ia membutuhkan gambar dengan resolusi maksimum dan dalam jumlah sebanyak mungkin.  Ini berarti bahwa saya perlu memotret mereka pada frame rate yang sangat tinggi, dengan tetap menjaga kualitas dan resolusi gambar.  Saya harus memeras maksimum yang dimungkinkan dari kamera dan GPU Raspberry PI. <br><br>  <a href="https://picamera.readthedocs.io/en/release-1.13/fov.html" rel="nofollow">Dokumentasi yang</a> sangat terperinci <a href="https://picamera.readthedocs.io/en/release-1.13/fov.html" rel="nofollow">untuk picamera</a> mengatakan bahwa chip kamera V2 dapat menghasilkan gambar berukuran 1280x720 piksel dengan frekuensi maksimum 90 bingkai per detik.  Ini adalah jumlah data yang luar biasa, dan meskipun kamera dapat menghasilkannya, ini tidak berarti bahwa komputer dapat mengatasinya.  Jika saya memproses gambar RGB 24-bit mentah, saya harus mentransfer data pada kecepatan sekitar 237 MB / s, yang terlalu banyak untuk GPU buruk dari komputer Pi dan SDRAM.  Bahkan ketika menggunakan kompresi akselerasi GPU dalam format JPEG, 90fps tidak dapat dicapai. <br><br>  Raspberry Pi mampu menampilkan gambar YUV mentah dan tanpa filter.  Meskipun lebih sulit untuk bekerja dibandingkan dengan dengan RGB, YUV sebenarnya memiliki banyak properti yang nyaman.  Yang paling penting dari mereka adalah bahwa ia hanya menyimpan 12 bit per piksel (untuk RGB itu 24 bit). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d50/28a/686/d5028a6862f79b8caf4d6f895dd46d84.png"></div><br>  <i>Setiap empat byte Y memiliki satu byte U dan satu byte V, yaitu 1,5 byte per piksel.</i> <br><br>  Ini berarti bahwa dibandingkan dengan frame RGB, saya dapat memproses frame YUV <strong>dua kali lebih banyak</strong> , dan ini tidak termasuk waktu tambahan yang dihemat GPU saat mengkonversi ke gambar RGB. <br><br>  Namun, pendekatan ini memberlakukan batasan unik pada proses pemrosesan.  Sebagian besar operasi dengan bingkai video berukuran penuh akan mengkonsumsi sumber daya memori dan CPU yang sangat besar.  Dalam batas waktu saya yang ketat, bahkan tidak mungkin untuk memecahkan kode bingkai YUV layar penuh. <br><br>  Untungnya, saya tidak perlu memproses seluruh bingkai!  Untuk pengenalan objek, persegi panjang pembatas tidak harus akurat, perkiraan akurasi sudah cukup, sehingga seluruh proses mengenali objek dapat dilakukan dengan bingkai yang jauh lebih kecil.  Operasi zoom out tidak diperlukan untuk memperhitungkan semua piksel bingkai berukuran penuh, sehingga bingkai dapat dikurangi dengan sangat cepat dan tanpa biaya.  Kemudian skala persegi pembatas yang dihasilkan meningkat lagi dan digunakan untuk memotong objek dari bingkai YUV berukuran penuh.  Berkat ini, saya tidak perlu men-decode atau memproses seluruh frame beresolusi tinggi. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fcd/d6a/9e4/fcdd6a9e466f0b87be4dbb349f19e402.png"></div><br>  Untungnya, berkat metode penyimpanan format YUV ini (lihat di atas), sangat mudah untuk menerapkan operasi pemangkasan dan pembesaran yang bekerja langsung dengan format YUV.  Selain itu, seluruh proses dapat diparalelkan dengan empat inti Pi tanpa masalah.  Namun, saya menemukan bahwa tidak semua core digunakan untuk potensi penuh mereka, dan ini memberitahu kita bahwa bandwidth memori masih menjadi hambatan.  Namun demikian, saya berhasil mencapai 70-80FPS dalam praktek.  Analisis yang lebih dalam tentang penggunaan memori dapat membantu mempercepat segalanya. <br><br><hr><br>  Jika Anda ingin tahu lebih banyak tentang proyek ini, maka baca artikel saya sebelumnya, <a rel="nofollow" href="https://towardsdatascience.com/how-i-created-over-100-000-labeled-lego-training-images-ec74191bb4ef">"Bagaimana Saya Membuat Lebih Dari 100 Ribu Gambar LEGO Berlabel untuk Belajar</a> . <a rel="nofollow" href="https://towardsdatascience.com/how-i-created-over-100-000-labeled-lego-training-images-ec74191bb4ef">"</a> <br><br>  Video pengoperasian seluruh mesin sortir: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/04JkdHEX3Yk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id480294/">https://habr.com/ru/post/id480294/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id480280/index.html">Studi Keamanan TurboConf</a></li>
<li><a href="../id480282/index.html">Sejarah startup kebugaran Peloton: dari penilaian $ 8 miliar hingga iklan yang tidak berhasil dan perkiraan untuk penurunan 85% pada saham</a></li>
<li><a href="../id480284/index.html">Pengalaman (nano) saya dengan Yandex.Maps API atau mengapa saya perlu instruksi</a></li>
<li><a href="../id480288/index.html">Apakah mungkin untuk mengirim dan menerima informasi tanpa batasan jarak dan kecepatan cahaya?</a></li>
<li><a href="../id480290/index.html">Laptop buatan rumah ZedRipper pada enam belas Z80</a></li>
<li><a href="../id480296/index.html">Pengembangan reaktif bot Telegram</a></li>
<li><a href="../id480300/index.html">Pada 2011, masalah apakah Nginx milik Igor Sysoev atau Rambler</a></li>
<li><a href="../id480304/index.html">Ketik inferensi dalam jscodeshift dan TypeScript</a></li>
<li><a href="../id480306/index.html">Mengapa mengalahkan pintu yang tertutup?</a></li>
<li><a href="../id480310/index.html">Detektif Habra: rahasia editor berita</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>