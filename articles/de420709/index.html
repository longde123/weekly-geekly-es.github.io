<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äç‚ù§Ô∏è‚Äçüë® üëØ ü•ß T2F: Ein Projekt zur Umwandlung von Text in Gesichtszeichnung mit tiefem Lernen üõÄüèø üê± ‚õëÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Projektcode ist im Repository verf√ºgbar . 

 Einf√ºhrung 
 Wenn ich die Beschreibungen des Aussehens von Charakteren in B√ºchern las, war ich immer dara...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>T2F: Ein Projekt zur Umwandlung von Text in Gesichtszeichnung mit tiefem Lernen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/420709/"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/703/0df/5ae7030df8270466b01b81aad0ace49f.jpg"><br><br>  <i>Projektcode ist im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Repository</a> verf√ºgbar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">.</a></i> <br><br><h2>  Einf√ºhrung </h2><br>  Wenn ich die Beschreibungen des Aussehens von Charakteren in B√ºchern las, war ich immer daran interessiert, wie sie im Leben aussahen.  Es ist durchaus m√∂glich, sich eine Person als Ganzes vorzustellen, aber die Beschreibung der auff√§lligsten Details ist eine schwierige Aufgabe, und die Ergebnisse variieren von Person zu Person.  Oft konnte ich mir bis zum Ende der Arbeit nur ein sehr verschwommenes Gesicht des Charakters vorstellen.  Erst wenn das Buch in einen Film verwandelt wird, f√ºllt sich das verschwommene Gesicht mit Details.  Zum Beispiel konnte ich mir nie vorstellen, wie Rachels Gesicht aus dem Buch " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">M√§dchen im Zug</a> " aussieht.  Aber als der Film herauskam, konnte ich Emily Blunts Gesicht mit Rachels Charakter in Einklang bringen.  Sicherlich nehmen sich die an der Auswahl der Schauspieler beteiligten Personen viel Zeit, um die Charaktere im Drehbuch korrekt darzustellen. <br><a name="habracut"></a><br>  Dieses Problem hat mich inspiriert und motiviert, eine L√∂sung zu finden.  Danach begann ich, die Literatur √ºber tiefes Lernen auf der Suche nach etwas √Ñhnlichem zu studieren.  Gl√ºcklicherweise gibt es einige Studien zur Synthese von Bildern aus Text.  Hier sind einige von denen, auf denen ich aufgebaut habe: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv.org/abs/1605.05396</a> "Generativer kontroverser Text zur Bildsynthese" </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv.org/abs/1612.03242</a> ‚ÄûStackGAN: Text zur fotorealistischen Bildsynthese mit gestapelten generativen kontradiktorischen Netzwerken‚Äú </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv.org/abs/1710.10916</a> ‚ÄûStackGAN ++: Realistische Bildsynthese mit gestapelten generativen kontradiktorischen Netzwerken‚Äú </li></ul><br>  [ <i>Projekte verwenden generative gegnerische Netze, GSS (Generative gegnerische Netze, GAN) / ca.</i>  <i>perev.</i>  ]] <br><br>  Nachdem ich die Literatur studiert hatte, entschied ich mich f√ºr eine Architektur, die im Vergleich zu StackGAN ++ vereinfacht war und mein Problem ziemlich gut bew√§ltigt.  In den folgenden Abschnitten werde ich erkl√§ren, wie ich dieses Problem gel√∂st habe, und vorl√§ufige Ergebnisse teilen.  Ich werde auch einige der Programmier- und Trainingsdetails beschreiben, f√ºr die ich viel Zeit aufgewendet habe. <br><br><h2>  Datenanalyse </h2><br>  Der wichtigste Aspekt der Arbeit sind zweifellos die Daten, die zum Trainieren des Modells verwendet werden.  Wie Professor Andrew Eun in seinen Kursen zu deeplearning.ai sagte: ‚ÄûAuf dem Gebiet des maschinellen Lernens hat nicht derjenige den besten Algorithmus, sondern derjenige, der die besten Daten hat.‚Äú  So begann meine Suche nach einem Datensatz √ºber Gesichter mit guten, reichhaltigen und verschiedenen Textbeschreibungen.  Ich bin auf verschiedene Datens√§tze gesto√üen - entweder waren es nur Gesichter oder Gesichter mit Namen oder Gesichter mit einer Beschreibung der Augenfarbe und der Gesichtsform.  Aber es gab keine, die ich brauchte.  Meine letzte Option bestand darin, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein fr√ºhes Projekt zu verwenden</a> - eine Beschreibung der Strukturdaten in einer nat√ºrlichen Sprache zu erstellen.  Eine solche Option w√ºrde jedoch einem bereits recht verrauschten Datensatz zus√§tzliches Rauschen hinzuf√ºgen. <br><br>  Die Zeit verging und irgendwann erschien ein neues <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Face2Text-</a> Projekt.  Es war eine Sammlung einer Datenbank mit detaillierten Textbeschreibungen von Personen.  Ich danke den Autoren des Projekts f√ºr den bereitgestellten Datensatz. <br><br>  Der Datensatz enthielt Textbeschreibungen von 400 zuf√§llig ausgew√§hlten Bildern aus der LFW-Datenbank (markierte Gesichter).  Beschreibungen wurden bereinigt, um mehrdeutige und geringf√ºgige Merkmale zu beseitigen.  Einige Beschreibungen enthielten nicht nur Informationen √ºber die Gesichter, sondern auch einige Schlussfolgerungen, die auf der Grundlage der Bilder gezogen wurden - zum Beispiel ‚Äûdie Person auf dem Foto ist wahrscheinlich ein Verbrecher‚Äú.  All diese Faktoren sowie die geringe Gr√∂√üe des Datensatzes haben dazu gef√ºhrt, dass mein Projekt bisher nur Beweise f√ºr die Funktionsf√§higkeit der Architektur liefert.  Anschlie√üend kann dieses Modell auf einen gr√∂√üeren und vielf√§ltigeren Datensatz skaliert werden. <br><br><h2>  Architektur </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/c70/ea1/6de/c70ea16de2bbfb674e618fa556cfc9ff.jpg"><br><br>  Die Architektur des T2F-Projekts kombiniert zwei StackGAN-Architekturen zum Codieren von bedingt inkrementiertem Text und ProGAN ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">progressives GSS-Wachstum</a> ) zum Synthetisieren von Gesichtsbildern.  Die urspr√ºngliche Stackgan ++ - Architektur verwendete mehrere GSSs mit unterschiedlichen r√§umlichen Aufl√∂sungen, und ich entschied, dass dies ein zu schwerwiegender Ansatz f√ºr jede Korrespondenzverteilungsaufgabe war.  ProGAN verwendet jedoch nur ein GSS, das schrittweise in immer detaillierteren Aufl√∂sungen geschult wird.  Ich habe beschlossen, diese beiden Ans√§tze zu kombinieren. <br><br>  Es gibt eine Erkl√§rung f√ºr den Datenfluss durch: Textbeschreibungen werden durch Einbetten in das Netzwerk LSTM (Embedding) (psy_t) in den endg√ºltigen Vektor codiert (siehe Abbildung).  Dann wird die Einbettung durch den Conditioning Augmentation-Block (eine lineare Schicht) √ºbertragen, um den Textteil des Eigenvektors (unter Verwendung der VAE-Reparametrisierungstechnik) f√ºr das GSS als Eingabe zu erhalten.  Der zweite Teil des Eigenvektors ist zuf√§lliges Gau√üsches Rauschen.  Der resultierende Eigenvektor wird dem GSS-Generator zugef√ºhrt, und die Einbettung wird der letzten Diskriminatorschicht zur bedingten Verteilung der Korrespondenz zugef√ºhrt.  Das Training von GSS-Prozessen verl√§uft genauso wie im Artikel √ºber ProGAN - in Schichten mit zunehmender r√§umlicher Aufl√∂sung.  Mit der Einblendtechnik wird eine neue Ebene eingef√ºhrt, um zu vermeiden, dass fr√ºhere Lernergebnisse gel√∂scht werden. <br><br><h2>  Implementierung und andere Details </h2><br>  Die Anwendung wurde mit dem PyTorch-Framework in Python geschrieben.  Fr√ºher habe ich mit Tensorflow- und Keras-Paketen gearbeitet, aber jetzt wollte ich PyTorch ausprobieren.  Ich mochte es, den integrierten Python-Debugger zu verwenden, um mit der Netzwerkarchitektur zu arbeiten - alles dank der fr√ºhen Ausf√ºhrungsstrategie.  Tensorflow hat k√ºrzlich auch den eifrigen Ausf√ºhrungsmodus aktiviert.  Ich m√∂chte jedoch nicht beurteilen, welches Framework besser ist, sondern nur betonen, dass der Code f√ºr dieses Projekt mit PyTorch geschrieben wurde. <br><br>  Nicht wenige Teile des Projekts scheinen mir wiederverwendbar zu sein, insbesondere ProGAN.  Daher habe ich als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erweiterung des</a> PyTorch-Moduls separaten Code f√ºr sie geschrieben, der auch f√ºr andere Datens√§tze verwendet werden kann.  Es ist nur erforderlich, die Tiefe und Gr√∂√üe der Merkmale des GSS anzugeben.  GSS kann f√ºr jeden Datensatz schrittweise trainiert werden. <br><br><h2>  Trainingsdetails </h2><br>  Ich habe einige Versionen des Netzwerks mit verschiedenen Hyperparametern trainiert.  Arbeitsdetails sind wie folgt: <br><br><ol><li>  Der Diskriminator verf√ºgt nicht √ºber Batch-Norm- oder Layer-Norm-Operationen, sodass der Verlust von WGAN-GP explosionsartig zunehmen kann.  Ich habe eine Driftstrafe mit einem Lambda von 0,001 verwendet. </li><li>  Um Ihre eigene Vielfalt zu steuern, die aus dem codierten Text erhalten wird, ist es erforderlich, den Kullback-Leibler-Abstand f√ºr die Verluste des Generators zu verwenden. </li><li>  Um die resultierenden Bilder besser an die Verteilung des eingehenden Textes anzupassen, ist es besser, die WGAN-Version des entsprechenden (Matching-Aware) Diskriminators zu verwenden. </li><li>  Die Einblendzeit f√ºr die oberen Ebenen sollte die Einblendzeit f√ºr die unteren Ebenen √ºberschreiten.  Ich habe 85% als Einblendwert beim Training verwendet. </li><li>  Ich fand heraus, dass Beispiele mit h√∂herer Aufl√∂sung (32 x 32 und 64 x 64) mehr Hintergrundrauschen erzeugen als Beispiele mit niedrigerer Aufl√∂sung.  Ich denke, das liegt an fehlenden Daten. </li><li>  W√§hrend eines progressiven Trainings ist es besser, mehr Zeit mit niedrigeren Aufl√∂sungen zu verbringen und die Zeit mit h√∂heren Aufl√∂sungen zu reduzieren. </li></ol><br>  Das Video zeigt den Zeitraffer des Generators.  Das Video wird aus Bildern mit unterschiedlichen r√§umlichen Aufl√∂sungen zusammengestellt, die w√§hrend des Trainings des GSS erhalten wurden. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NO_l87rPDb8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Fazit </h2><br>  Nach vorl√§ufigen Ergebnissen kann beurteilt werden, dass das T2F-Projekt praktikabel ist und interessante Anwendungen hat.  Angenommen, es kann zum Erstellen von Fotobots verwendet werden.  Oder f√ºr F√§lle, in denen es notwendig ist, die Vorstellungskraft zu steigern.  Ich werde weiterhin daran arbeiten, dieses Projekt auf Datens√§tze wie Flicker8K, Coco-Untertitel usw. zu skalieren. <br><br>  Progressives GSS-Wachstum ist eine ph√§nomenale Technologie f√ºr ein schnelleres und stabileres GSS-Training.  Es kann mit verschiedenen modernen Technologien kombiniert werden, die in anderen Artikeln erw√§hnt werden.  GSS kann in verschiedenen Bereichen von MO eingesetzt werden. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de420709/">https://habr.com/ru/post/de420709/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de420697/index.html">Das ewige Thema mit PHP und MySQL</a></li>
<li><a href="../de420701/index.html">HSBI l√§dt am 29. August zu einem Vortragsabend zum Thema Spieledesign ein</a></li>
<li><a href="../de420703/index.html">Inhaltsangabe des Buches ‚ÄûVerhandlungen ohne Niederlage. Harvard-Methode</a></li>
<li><a href="../de420705/index.html">8 tiefe Ideen von Tim Ferris 'Stamm der Mentoren</a></li>
<li><a href="../de420707/index.html">Der JITX-Start verwendet AI, um die Entwicklung komplexer Leiterplatten zu automatisieren</a></li>
<li><a href="../de420711/index.html">Moscow Data Science Major: Ank√ºndigung und Registrierung</a></li>
<li><a href="../de420713/index.html">Wie Chuck Hull den 3D-Druck erfand</a></li>
<li><a href="../de420715/index.html">Die harte Wahrheit √ºber die Schwere des Lernens</a></li>
<li><a href="../de420725/index.html">Wie ich AI beigebracht habe, Tetris f√ºr NES zu spielen. Teil 1: Analyse des Spielcodes</a></li>
<li><a href="../de420729/index.html">Offenes Webinar "Naive Bayes Classifier"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>