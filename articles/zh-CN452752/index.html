<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👎🏿 ☪️ 🧥 自制BigData。 第1部分。AWS集群上的Spark流实践 🖐🏿 🗄️ 😅</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="你好 

 互联网上有许多提供云服务的服务。 在他们的帮助下，您可以学习BigData技术。 

 在本文中，我们将在家中的EC2 AWS（Amazon Web Services）平台上安装Apache Kafka，Apache Spark，Zookeeper，Spark-shell，并学习如何使用...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>自制BigData。 第1部分。AWS集群上的Spark流实践</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/homecredit/blog/452752/"> 你好 <br><br> 互联网上有许多提供云服务的服务。 在他们的帮助下，您可以学习BigData技术。 <br><br> 在本文中，我们将在家中的EC2 AWS（Amazon Web Services）平台上安装Apache Kafka，Apache Spark，Zookeeper，Spark-shell，并学习如何使用它们。 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/51c/95c/810/51c95c8104a988a0288067e9eaaf700f.jpg" alt="图片"></div><br><a name="habracut"></a><br><h3> 介绍Amazon Web Services </h3><br> 您必须在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">aws.amazon.com/console</a>进行注册。 输入名称并记住密码。 <br><br> 为Zookeeper和Kafka服务配置节点实例。 <br><br><ul><li> 从菜单中选择“服务-&gt; EC2”。 接下来，选择虚拟机映像的操作系统版本，选择Ubuntu Server 16.04 LTS（HVM），SSD卷类型，单击“选择”。我们继续配置服务器实例：键入“ t3.medium”，其参数为2vCPU，4 GB内存，通用单击“下一步：配置实例详细信息”。 </li><li> 添加实例数1，单击“下一步：添加存储” </li><li> 我们接受8 GB磁盘大小的默认值，并将类型更改为Magnetic（在基于数据量和High Performance SSD的生产设置中） </li><li> 在“名称”的“标签实例”部分中，输入节点“ Home1”的实例的名称（其中1只是序列号），然后单击“下一步：...”。 </li><li> 在“配置安全组”部分中，通过选择安全组的名称（“ Spark_Kafka_Zoo_Project”）来选择“使用现有安全组”选项，并设置传入流量的规则。 点击“下一步：...” </li><li> 滚动查看页面，以验证您的输入并启动启动。 </li><li> 要连接到群集节点，必须创建（在我们的示例中，使用现有的）一对公共密钥以进行标识和授权。 为此，请在列表中选择操作类型“使用现有对”。 </li></ul><br>
<h3> 密钥创建 </h3><br><ul><li> 为客户端<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">下载Putty</a> （https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html）或从终端使用SSH连接。 </li><li> 为了方便起见，.pem密钥文件使用旧格式，我们将其转换为Putty使用的ppk格式。 为此，请运行PuTTYgen实用程序，将旧的.pem格式的密钥加载到该实用程序中。 我们转换密钥并将其保存（保存私钥），以备以后在扩展名为.ppk的主文件夹中使用。 </li></ul><br><h3> 集群启动 </h3><br> 为方便起见，请使用Node01-04表示法重命名群集节点。 要通过SSH从本地计算机连接到群集节点，您需要确定节点的IP地址及其公用/专用DNS名称，一个接一个地选择每个群集节点，并为所选实例写下其公用/专用DNS名称，以通过SSH连接并进行安装将软件添加到文本文件HadoopAdm01.txt。 <br><br> 示例：ec2-35-162-169-76.us-west-2.compute.amazonaws.com <br><br><h3> 在AWS集群节点上以单节点模式安装Apache Kafka </h3><br> 要安装软件，请选择我们的节点（复制其公共DNS）以通过SSH连接。 我们通过SSH配置连接。 我们使用第一个节点的保存名称，使用条款1.3中创建的私钥/公钥对“ HadoopUser01.ppk”，通过SSH配置连接。 我们通过“浏览”按钮转到“连接/身份验证”部分，并找到我们先前保存文件“ HadoopUserXX.ppk”的文件夹。 <br><br> 我们将连接配置保存在设置中。 <br><br> 我们已连接到该节点并使用登录名：ubuntu。 <br><br><ul><li> 使用root特权，我们更新软件包并安装进一步安装和配置集群所需的其他软件包。 <br><br><pre><code class="plaintext hljs">sudo apt-get update sudo apt-get -y install wget net-tools netcat tar</code> </pre> </li><li> 安装Java 8 jdk并检查Java版本。 <br><br><pre> <code class="plaintext hljs">sudo apt-get -y install openjdk-8-jdk</code> </pre> </li><li> 为了使群集节点具有正常的性能，您需要调整内存交换设置。  VM swappines默认设置为60％，这意味着当以60％的比例使用内存时，系统会主动将数据从RAM交换到磁盘。 根据Linux的版本，VM swappines参数可以设置为0或1： <br><br><pre> <code class="plaintext hljs">sudo sysctl vm.swappiness=1</code> </pre> <br></li><li> 要在重新引导期间保存设置，请在配置文件中添加一行。 <br><br><pre> <code class="plaintext hljs">echo 'vm.swappiness=1' | sudo tee --append /etc/sysctl.conf</code> </pre> <br></li><li> 编辑/ etc / hosts文件中的条目，以方便地解析kafka集群的节点名称和 <br>  Zookeeper将私有IP地址分配给分配的群集节点。 <br><br><pre> <code class="plaintext hljs">echo "172.31.26.162 host01" | sudo tee --append /etc/hosts</code> </pre> <br> 我们使用ping任何条目检查名称的正确识别。 <br><br></li><li> 下载kafka和scala发行版的最新当前版本（http://kafka.apache.org/downloads），并准备包含安装文件的目录。 <br><br><pre> <code class="plaintext hljs">wget http://mirror.linux-ia64.org/apache/kafka/2.1.0/kafka_2.12-2.1.0.tgz tar -xvzf kafka_2.12-2.1.0.tgz ln -s kafka_2.12-2.1.0 kafka</code> </pre> <br></li><li> 删除tgz存档文件，我们将不再需要它 <br><br></li><li> 让我们尝试启动Zookeeper服务，为此： <br><br><pre> <code class="plaintext hljs">~/kafka/bin/zookeeper-server-start.sh -daemon ~/kafka/config/zookeeper.properties</code> </pre> <br>  Zookeeper从默认启动选项开始。 您可以检查日志： <br><br><pre> <code class="plaintext hljs">tail -n 5 ~/kafka/logs/zookeeper.out</code> </pre> <br> 为了确保Zookeeper守护程序在重新启动后启动，我们需要将Zookeper作为后台服务启动： <br><br><pre> <code class="plaintext hljs">bin/zookeeper-server-start.sh -daemon config/zookeeper.properties</code> </pre> <br> 要检查Zookepper的发射，请检查 <br><br><pre> <code class="plaintext hljs">netcat -vz localhost 2181</code> </pre> <br> 我们将Zookeeper和Kafka服务配置为工作。 最初，编辑/创建文件/etc/systemd/system/zookeeper.service（下面的文件内容）。 <br><br><pre> <code class="plaintext hljs">[Unit] Description=Apache Zookeeper server Documentation=http://zookeeper.apache.org Requires=network.target remote-fs.target After=network.target remote-fs.target [Service] Type=simple ExecStart=/home/ubuntu/kafka/bin/zookeeper-server-start.sh /home/ubuntu/kafka/config/zookeeper.properties ExecStop=/home/ubuntu/kafka/bin/zookeeper-server-stop.sh [Install] WantedBy=multi-user.target</code> </pre> <br> 接下来，对于Kafka，编辑/创建文件/etc/systemd/system/kafka.service（下面的文件内容）。 <br><br><pre> <code class="plaintext hljs">[Unit] Description=Apache Kafka server (broker) Documentation=http://kafka.apache.org/documentation.html Requires=zookeeper.service [Service] Type=simple ExecStart=/home/ubuntu/kafka/bin/kafka-server-start.sh /home/ubuntu/kafka/config/server.properties ExecStop=/home/ubuntu/kafka/bin/kafka-server-stop.sh [Install] WantedBy=multi-user.target</code> </pre> <br></li><li> 我们为Kafka和Zookeeper服务激活系统脚本。 <br><br><pre> <code class="plaintext hljs">sudo systemctl enable zookeeper sudo systemctl enable kafka</code> </pre> <br></li><li> 检查systemd脚本的操作。 <br><br><pre> <code class="plaintext hljs">sudo systemctl start zookeeper sudo systemctl start kafka sudo systemctl status zookeeper sudo systemctl status kafka sudo systemctl stop zookeeper sudo systemctl stop kafka</code> </pre> <br></li><li> 我们将检查Kafka和Zookeeper服务的功能。 <br><br><pre> <code class="plaintext hljs">netcat -vz localhost 2181 netcat -vz localhost 9092</code> </pre> <br></li><li> 检查Zookeeper日志文件。 <br><br><pre> <code class="plaintext hljs">cat logs/zookeeper.out</code> </pre> </li></ul><br><h3> 第一喜 </h3><br> 我们在组装的kafka服务器上创建第一个主题。 <br><br><ul><li> 如在server.properties配置文件中指出的，使用到“ host01：2181”的连接很重要。 <br></li><li> 我们在主题中编写了一些数据。 <br><br><pre> <code class="plaintext hljs">kafka-console-producer.sh --broker-list host01:9092 --topic first_topic    </code> </pre> <br>  Ctrl-C-退出主题控制台。 <br><br></li><li> 现在尝试从该主题读取数据。 <br><br><pre> <code class="plaintext hljs">kafka-console-consumer.sh --bootstrap-server host01:9092 --topic last_topic --from-beginning</code> </pre> <br></li><li> 让我们看一下kafka主题列表。 <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper spark01:2181 --list</code> </pre> <br></li><li> 编辑kafka服务器设置以进行单集群设置调整 <br>  ＃您需要将ISR参数更改为1。 <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper spark01:2181 --config min.insync.replicas=1 --topic __consumer_offsets --alter</code> </pre> <br></li><li> 我们重新启动Kafka服务器，然后尝试再次连接使用者欧姆 <br><br></li><li> 让我们看一下主题列表。 <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper host01:2181 --list</code> </pre> </li></ul><br><h3> 在单节点集群上配置Apache Spark </h3><br> 我们准备了在AWS上安装了Zookeeper和Kafka服务的节点实例，现在您需要为此安装Apache Spark： <br><br> 下载最新的Apache Spark发行版。 <br><br><pre> <code class="plaintext hljs">wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.6.tgz</code> </pre> <br><br><ul><li> 解压缩发行版并为spark创建一个符号链接，并删除不必要的存档文件。 <br><br><pre> <code class="plaintext hljs">tar -xvf spark-2.4.0-bin-hadoop2.6.tgz ln -s spark-2.4.0-bin-hadoop2.6 spark rm spark*.tgz</code> </pre> <br></li><li> 转到sbin目录并运行spark向导。 <br><br><pre> <code class="plaintext hljs">./start-master.sh</code> </pre> <br></li><li> 我们使用网络浏览器连接到端口8080上的Spark服务器。 <br><br></li><li> 在同一节点上运行spark-slaves <br><br><pre> <code class="plaintext hljs">./start-slave.sh spark://host01:7077</code> </pre> <br></li><li> 在host01上使用主模块运行spark shell。 <br><br><pre> <code class="plaintext hljs">./spark-shell --master spark://host01:7077</code> </pre> <br></li><li> 如果启动不起作用，请在bash中将路径添加到Spark。 <br><br><pre> <code class="plaintext hljs">vi ~/.bashrc #      SPARK_HOME=/home/ubuntu/spark export PATH=$SPARK_HOME/bin:$PATH</code> </pre> <br><pre> <code class="plaintext hljs">source ~/.bashrc</code> </pre> <br></li><li> 在host01上再次与master一起运行spark shell。 <br><br><pre> <code class="plaintext hljs">./spark-shell --master spark://host01:7077</code> </pre> <br> 具有Kafka，Zookeeper和Spark的单节点群集可以工作。 万岁！ </li></ul><br><h3> 一点创意 </h3><br> 下载Scala-IDE编辑器（位于<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">scala-ide.org</a> ）。 我们开始并开始编写代码。 在此不再赘述，因为<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在哈布雷（Habré）上</a>有一篇<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">不错的文章</a> 。 <br><br> 有用的文献和课程可帮助： <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">courses.hadoopinrealworld.com/courses/enrolled/319237</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">data-flair.training/博客/ kafka消费者</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">www.udemy.com/apache-spark-with-scala-hands-on-with-big-data</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN452752/">https://habr.com/ru/post/zh-CN452752/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN452742/index.html">设置混合应用程序的自动测试</a></li>
<li><a href="../zh-CN452744/index.html">没有自由职业者的交流，是否有充裕的生活？</a></li>
<li><a href="../zh-CN452746/index.html">《 R.沉浸在大数据中的编程艺术》一书</a></li>
<li><a href="../zh-CN452748/index.html">NGINX开发现代应用程序的原理。 第一部分</a></li>
<li><a href="../zh-CN452750/index.html">OpenLiteSpeed内部和外部的Nextcloud：配置反向代理</a></li>
<li><a href="../zh-CN452754/index.html">19％的最受欢迎的Docker映像没有root密码</a></li>
<li><a href="../zh-CN452756/index.html">在Unity中创建塔防：敌人</a></li>
<li><a href="../zh-CN452760/index.html">维生素D。喝还是不喝，这就是问题所在。 （或关于我如何通过了未按规定进行分析的故事）</a></li>
<li><a href="../zh-CN452762/index.html">MVCC-7。 自动清洁</a></li>
<li><a href="../zh-CN452764/index.html">[Peter]与Sergei Melnikov会面JUG.ru-超光速分析：理论与实践</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>