<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👊 🔇 🧑🏿‍🤝‍🧑🏽 Application de l'apprentissage automatique aux réseaux de neurones avec une architecture de transformateur 😒 💝 👨‍👨‍👧‍👧</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Depuis le blog Google AI 

 Depuis la publication d'informations à leur sujet en 2017, les réseaux de neurones de l'architecture des transformateurs o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Application de l'apprentissage automatique aux réseaux de neurones avec une architecture de transformateur</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/460099/"> <i>Depuis le blog Google AI</i> <br><br>  Depuis la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">publication d'informations</a> à leur sujet en 2017, les réseaux de neurones de l'architecture des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">transformateurs</a> ont été appliqués à des tâches de toutes sortes, de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">génération de textes de style fantastique</a> à l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">écriture d'harmonies musicales</a> .  Ce qui est important, la haute qualité du travail des «transformateurs» a montré que lorsqu'ils sont appliqués à des tâches séquentielles, telles que la modélisation et la traduction de langage, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les réseaux de neurones à distribution directe</a> peuvent être aussi efficaces que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les réseaux</a> récurrents.  Bien que la popularité des transformateurs et autres modèles de distribution directe utilisés dans les tâches séquentielles augmente, leurs architectures sont presque toujours créées manuellement, contrairement au domaine de la vision par ordinateur, où <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les</a> approches <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">avancées d'</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">apprentissage automatique</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AOM</a> ) ont déjà trouvé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des modèles avancés</a> qui sont en avance sur ceux exposés. réglage manuel.  Naturellement, nous voulions savoir si l'application de l'AOM à des tâches séquentielles pouvait obtenir le même succès. <br><a name="habracut"></a><br>  Après avoir effectué une recherche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">évolutive de</a> recherche d'architecture neuronale (NAS) et utilisé la traduction comme exemple de tâches séquentielles, nous avons découvert un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">transformateur évolutif</a> (ET) - une nouvelle architecture de transformateur qui démontre des améliorations dans diverses tâches de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">traitement du langage naturel</a> (OYA).  ET obtient non seulement des résultats de pointe en traduction, mais démontre également une efficacité améliorée dans la modélisation du langage par rapport au transformateur d'origine.  Nous <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">publions un</a> nouveau modèle dans la bibliothèque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tensor2Tensor</a> , où il peut être utilisé pour n'importe quelle tâche séquentielle. <br><br><h2>  Développement Technicien </h2><br>  Pour commencer la recherche évolutive de la neuroarchitecture, nous devions développer de nouvelles techniques, car la tâche utilisée pour évaluer la «forme physique» de chacune des architectures, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">traduction de l'anglais vers l'allemand WMT'14</a> , exigeait des ressources informatiques.  En conséquence, ces recherches sont plus exigeantes que des recherches similaires dans le domaine de la vision par ordinateur, qui peuvent fonctionner avec des bases de données plus petites, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CIFAR-10</a> .  La première de ces techniques est un démarrage à chaud, semant la population évolutive d'origine avec des architectures de type transformateur au lieu de modèles aléatoires.  Cela permet de concentrer les recherches dans la zone manifestement forte de l'espace de recherche, ce qui nous permet de trouver rapidement les meilleurs modèles. <br><br>  La deuxième technique est une nouvelle méthode développée par nous appelée Progressive Dynamic Hurdles (PDH).  Cet algorithme complète la recherche évolutive, vous permettant d'allouer plus de ressources aux candidats les plus forts, contrairement aux travaux précédents, où chaque modèle candidat dans le NAS se voyait allouer la même quantité de ressources.  PDH nous permet de terminer l'évaluation d'un modèle plus tôt s'il est terriblement mauvais, tout en récompensant les architectures prometteuses avec de nombreuses ressources. <br><br><h2>  Transformateur évolué </h2><br>  En utilisant ces méthodes, nous avons effectué une recherche NAS à grande échelle sur notre tâche de traduction et découvert des ET.  Comme la plupart des architectures de réseaux de neurones du type "séquence à séquence" (séquence à séquence, seq2seq), il dispose d'un encodeur qui code la séquence d'entrée dans les insertions, et d'un décodeur qui utilise ces inserts pour créer la séquence de sortie.  Dans le cas d'une traduction, la séquence d'entrée est une offre de traduction et la séquence de sortie est une traduction. <br><br>  La caractéristique la plus intéressante des ET est les couches convolutives au bas des modules du codeur et du décodeur, ajoutées de manière similaire aux deux endroits (c'est-à-dire que les entrées passent par deux couches convolutives différentes avant de se plier). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/62d/1d1/b8a62d1d155203861756f0960becaaf0.png"><br>  <i>Comparaison de l'architecture du codeur conventionnel et des codeurs ET.</i>  <i>Faites attention à la structure convolutionnelle de branchement au bas du module, formée indépendamment à la fois dans le codeur et dans le décodeur.</i>  <i>Le décodeur est décrit en détail dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">notre travail</a> .</i> <br><br>  Ceci est particulièrement intéressant car l'encodeur et le décodeur pendant le NAS ne partagent pas les architectures les uns avec les autres, et l'utilité de cette architecture a été découverte indépendamment dans l'encodeur et le décodeur, ce qui plaide en faveur d'un tel schéma.  Si le transformateur d'origine reposait entièrement sur l'application de l'attention aux mêmes données qu'il avait lui-même générées [auto-attention], ET est un hybride qui profite à la fois de l'auto-attention et d'une large convolution. <br><br><h2>  Score ET </h2><br>  Pour tester l'efficacité de cette nouvelle architecture, nous l'avons d'abord comparée avec le transformateur d'origine, qui a travaillé avec la tâche de traduire de l'anglais vers l'allemand, que nous avons utilisé lors de la recherche.  Nous avons constaté que ET a les meilleurs indicateurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">BLEU</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">connectivité</a> sur toutes les tailles de paramètres, et le plus grand gain de taille est comparable aux appareils mobiles (~ 7 millions de paramètres), ce qui indique l'utilisation efficace des paramètres.  Sur les plus grandes tailles, ET obtient des résultats de pointe sur WMT '14 En-De avec un BLEU de 29,8 et un SacreBLEU de 29,2. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1a2/4c4/c60/1a24c4c6085c167a5398fdcea218f75c.png"></div><br>  <i>Comparaison de ET et du transformateur d'origine sur WMT'14 En-De avec différents volumes.</i>  <i>Le plus grand avantage est obtenu avec de petites tailles, tandis que ET affiche de bonnes performances sur de plus grandes tailles, devant le plus grand transformateur avec 37,6% de paramètres en moins (des modèles comparables sont en cercle).</i> <br><br>  Pour vérifier la généralisation, nous avons comparé ET avec un transformateur sur des problèmes supplémentaires de traitement du langage naturel.  Tout d'abord, nous avons vérifié les traductions pour différentes paires de langues et constaté que l'efficacité de l'ET est plus élevée et que sa séparation est approximativement la même que celle démontrée dans la traduction anglais-allemand;  et là encore, grâce à l'utilisation efficace des paramètres, l'écart le plus important est observé sur les modèles de taille moyenne.  Nous avons également comparé les décodeurs des deux modèles sur la modélisation du langage dans <a href="">LM1B</a> et constaté une amélioration significative de la connectivité. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c5/c96/1b6/6c5c961b6a09ba83905d9f886c9063ea.png"><br><br><h2>  Plans futurs </h2><br>  Ces résultats constituent la première étape dans l'exploration de l'application de recherche d'architecture pour les modèles de distribution directe séquentielle.  ET est distribué en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">open source</a> dans le cadre du projet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://www.google.com/url%3Fq%3D">Tensor2Tensor</a> , où il peut être utilisé sur tout problème consécutif.  Pour améliorer la reproductibilité, nous ouvrons également <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le code d'espace de recherche</a> que nous avons utilisé dans notre recherche, et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Colab</a> avec l'implémentation PDH.  Nous attendons avec impatience les résultats de la communauté des chercheurs, armés de nouveaux modèles, et nous espérons que d'autres pourront prendre ces nouvelles techniques de recherche comme base! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr460099/">https://habr.com/ru/post/fr460099/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr460087/index.html">Tri pyramidal (HeapSort)</a></li>
<li><a href="../fr460089/index.html">Mise à jour sécurisée de Zimbra Collaboration Suite</a></li>
<li><a href="../fr460091/index.html">Impression directe sur des T-shirts avec Epson SureColor SC - F et sa différence avec la sérigraphie, l'autocollant et la sublimation</a></li>
<li><a href="../fr460095/index.html">Pris une interdiction pour fork deepNude sur gitlab.com</a></li>
<li><a href="../fr460097/index.html">The Matrix vous a: un aperçu des projets utilisant MITRE ATT & CK</a></li>
<li><a href="../fr460101/index.html">Fonctionnement XSS basé sur les cookies | Histoire de Bug Bounty à 2300 $</a></li>
<li><a href="../fr460107/index.html">ISPsystem, pardonne et au revoir! Pourquoi et comment nous avons écrit notre panneau de contrôle du serveur</a></li>
<li><a href="../fr460109/index.html">Angulaire: lorsque vous avez besoin de voir l'application, mais que le backend n'est pas encore prêt</a></li>
<li><a href="../fr460111/index.html">Version mise à jour de SAP Business One 9.3: ce qui a changé</a></li>
<li><a href="../fr460113/index.html">Quelques histoires de la vie de JSOC CERT, ou Unbanal forensics</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>