<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìµ üë≤üèæ üèûÔ∏è Como as redes neurais funcionam e por que come√ßaram a trazer muito dinheiro üôåüèΩ üòÜ üëÜüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As redes neurais cresceram de um estado de curiosidade acad√™mica para uma ind√∫stria massiva 


 Na √∫ltima d√©cada, os computadores melhoraram significa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como as redes neurais funcionam e por que come√ßaram a trazer muito dinheiro</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482258/"><h3>  As redes neurais cresceram de um estado de curiosidade acad√™mica para uma ind√∫stria massiva </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/36a/21a/fd3/36a21afd35a805d95a2a67b2ec52080a.jpg"><br><br>  Na √∫ltima d√©cada, os computadores melhoraram significativamente sua capacidade de entender o mundo ao seu redor.  O software para equipamento fotogr√°fico reconhece automaticamente o rosto das pessoas.  Os smartphones convertem a fala em texto.  Os robomobiles reconhecem objetos na estrada e evitam colis√µes com eles. <br><br>  No centro de todas essas descobertas est√° a tecnologia da intelig√™ncia artificial (IA) chamada aprendizado profundo (GO).  GO √© baseado em redes neurais (NS), estruturas de dados inspiradas em redes compostas por neur√¥nios biol√≥gicos.  Os NS s√£o organizados em camadas e as entradas de uma camada s√£o conectadas √†s sa√≠das da vizinha. <br><br>  Os cientistas da computa√ß√£o v√™m experimentando o NS desde os anos 50.  No entanto, a base da vasta ind√∫stria de GO de hoje foi lan√ßada por dois grandes avan√ßos - um ocorrido em 1986 e o ‚Äã‚Äãsegundo em 2012. O avan√ßo de 2012 - a revolu√ß√£o do GO - foi associado √† descoberta de que o uso de NS com um grande n√∫mero de camadas nos permitir√° melhorar significativamente sua efici√™ncia.  A descoberta foi facilitada pelos volumes crescentes de dados e poder de computa√ß√£o. <br><a name="habracut"></a><br>  Neste artigo, apresentaremos voc√™ ao mundo da Assembl√©ia Nacional.  Explicaremos o que √© o NS, como eles funcionam e de onde vieram.  E estudaremos por que - apesar de muitas d√©cadas de pesquisas anteriores - os SNs se tornaram algo realmente √∫til apenas em 2012. <br><br><h2>  As redes neurais apareceram nos anos 50 </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/770/c2e/327/770c2e3276b0e875a99025f4887dda36.jpg"><br>  <i>Frank Rosenblatt est√° trabalhando em seu perceptron - um modelo inicial de NS</i> <br><br>  A id√©ia da Assembl√©ia Nacional √© bastante antiga - pelo menos para os padr√µes da ci√™ncia da computa√ß√£o.  Em 1957, <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25BE%25D0%25B7%25D0%25B5%25D0%25BD%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2582%25D1%2582,_%25D0%25A4%25D1%2580%25D1%258D%25D0%25BD%25D0%25BA">Frank Rosenblatt,</a> da Cornell University, publicou um <a href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf">relat√≥rio</a> descrevendo um conceito inicial de NS chamado perceptron.  Em 1958, com o apoio da Marinha dos EUA, ele criou um sistema primitivo capaz de analisar 20x20 pixels e reconhecer formas geom√©tricas simples. <br><br>  O principal objetivo de Rosenblatt n√£o era criar um sistema pr√°tico de classifica√ß√£o de imagens.  Ele tentou entender como o c√©rebro humano funciona, criando sistemas de computador organizados √† sua imagem.  No entanto, esse conceito gerou entusiasmo excessivo de terceiros. <br><br>  "Hoje, a Marinha dos EUA revelou ao mundo o germe de um computador eletr√¥nico, que deve andar, conversar, ver, escrever, se reproduzir e estar ciente de sua exist√™ncia", escreveu o New York Times. <br><br>  De fato, cada neur√¥nio no NS √© apenas uma fun√ß√£o matem√°tica.  Cada neur√¥nio calcula a soma ponderada dos dados de entrada - quanto maior o peso de entrada, mais fortemente esses dados de entrada afetam a sa√≠da do neur√¥nio.  Em seguida, a soma ponderada √© alimentada com a fun√ß√£o de "ativa√ß√£o" n√£o linear - nesta etapa, os NSs podem simular fen√¥menos n√£o lineares complexos. <br><br>  As habilidades dos perceptrons iniciais com os quais Rosenblatt experimentou - e NS em geral - decorrem de sua capacidade de "aprender" com exemplos.  Os NS s√£o treinados ajustando os pesos de entrada dos neur√¥nios com base nos resultados da rede com os dados de entrada selecionados, por exemplo.  Se a rede classificar corretamente a imagem, os pesos que contribuem para a resposta correta aumentam, enquanto outros diminuem.  Se a rede estiver errada, os pesos ser√£o ajustados na outra dire√ß√£o. <br><br>  Tal procedimento permitiu que os primeiros SNs ‚Äúaprendessem‚Äù de uma maneira que lembra o comportamento do sistema nervoso humano.  O hype em torno dessa abordagem n√£o parou na d√©cada de 1960.  Contudo, o <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD%25D1%258B_(%25D0%25BA%25D0%25BD%25D0%25B8%25D0%25B3%25D0%25B0)">influente livro de</a> 1969 dos autores dos cientistas da computa√ß√£o Marvin Minsky e Seymour Papert mostrou que esses NA iniciais tinham limita√ß√µes significativas. <br><br>  Os primeiros Rosenblatt NSs tinham apenas uma ou duas camadas treinadas.  Minsky e Papert mostraram que esses NSs s√£o matematicamente incapazes de modelar fen√¥menos complexos do mundo real. <br><br>  Em princ√≠pio, NSs mais profundos eram mais capazes.  No entanto, esse NS sobrecarregaria os miser√°veis ‚Äã‚Äãrecursos de computa√ß√£o que os computadores tinham na √©poca.  Os algoritmos de <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D0%25B8%25D1%2581%25D0%25BA_%25D0%25B2%25D0%25BE%25D1%2581%25D1%2585%25D0%25BE%25D0%25B6%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC_%25D0%25BA_%25D0%25B2%25D0%25B5%25D1%2580%25D1%2588%25D0%25B8%25D0%25BD%25D0%25B5">pesquisa ascendente</a> mais simples usados ‚Äã‚Äãnos primeiros NSs n√£o foram dimensionados para NSs mais profundos. <br><br>  Como resultado, a Assembl√©ia Nacional perdeu todo o apoio na d√©cada de 1970 e no in√≠cio da d√©cada de 1980 - fazia parte da era do "inverno da IA". <br><br><h2>  Algoritmo inovador </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/970/c5b/807/970c5b8074a5b4516be251bd4b9a31b0.jpg"><br>  <i>Minha pr√≥pria rede neural baseada em "equipamentos leves" acredita que a probabilidade de ter um cachorro-quente nesta foto √© 1. Vamos ficar ricos!</i> <br><br>  A sorte voltou-se novamente para o NS gra√ßas ao famoso <a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">trabalho de</a> 1986, que introduziu o conceito de propaga√ß√£o traseira - um m√©todo pr√°tico de ensino do NS. <br><br>  Suponha que voc√™ trabalhe como programador em uma empresa de software imagin√°ria e tenha sido instru√≠do a criar um aplicativo que determine se h√° um cachorro-quente na imagem.  Voc√™ come√ßa a trabalhar com um NS inicializado aleatoriamente, que obt√©m uma imagem de entrada e gera um valor de 0 a 1 - onde 1 significa "cachorro-quente" e 0 significa "cachorro-quente". <br><br>  Para treinar a rede, voc√™ coleta milhares de imagens, em cada uma das quais h√° um r√≥tulo indicando se existe um cachorro-quente nessa imagem.  Voc√™ alimenta a primeira imagem - e h√° um cachorro-quente - na rede neural.  Ele fornece um valor de sa√≠da de 0,07, que significa "sem cachorro-quente".  Esta √© a resposta errada;  a rede deve ter retornado uma resposta pr√≥xima a 1. <br><br>  O objetivo do algoritmo de retropropaga√ß√£o √© ajustar os pesos de entrada para que a rede produza um valor mais alto se receber novamente essa imagem - e, preferencialmente, outras imagens onde houver cachorro-quente.  Para isso, o algoritmo de retropropaga√ß√£o come√ßa examinando os neur√¥nios de entrada da camada de sa√≠da.  Cada valor possui uma vari√°vel de peso.  O algoritmo de retropropaga√ß√£o ajusta cada peso em uma dire√ß√£o que o NS fornece um valor mais alto.  Quanto maior o valor de entrada, mais seu peso aumenta. <br><br>  At√© agora, estou descrevendo a subida mais simples ao topo, familiar aos pesquisadores nos anos 1960.  O avan√ßo da retropropaga√ß√£o foi o pr√≥ximo passo: o algoritmo usa derivadas parciais para distribuir a ‚Äúfalha‚Äù para a sa√≠da incorreta entre as entradas dos neur√¥nios.  O algoritmo calcula como uma pequena altera√ß√£o em cada valor de entrada afetar√° a sa√≠da final de um neur√¥nio e se essa altera√ß√£o aproximar√° o resultado da resposta correta ou vice-versa. <br><br>  O resultado √© um conjunto de valores de erro para cada neur√¥nio na camada anterior - na verdade, um sinal que avalia se o valor de cada neur√¥nio √© muito grande ou muito pequeno.  Em seguida, o algoritmo repete o processo de sintonia de novos neur√¥nios da segunda camada [do final].  Altera ligeiramente os pesos de entrada de cada neur√¥nio para aproximar a rede da resposta correta. <br><br>  Em seguida, o algoritmo novamente usa derivadas parciais para calcular como o valor de cada entrada da camada anterior afetou os erros de sa√≠da dessa camada - e propaga esses erros de volta √† camada pr√©-anterior, onde o processo se repete novamente. <br><br>  Este √© apenas um modelo simplificado de retropropaga√ß√£o.  Se voc√™ precisar de detalhes matem√°ticos detalhados, recomendo o livro de Michael Nielsen sobre esse assunto [ <a href="https://habr.com/ru/post/456738/">e temos a tradu√ß√£o dela</a> / aprox.  transl.].  Para nossos prop√≥sitos, basta que a distribui√ß√£o reversa altere radicalmente a faixa de NS treinada.  As pessoas n√£o estavam mais limitadas a redes simples com uma ou duas camadas.  Eles poderiam criar redes com cinco, dez ou cinquenta camadas, e essas redes poderiam ter uma estrutura interna arbitrariamente complexa. <br><br>  A inven√ß√£o da retropropaga√ß√£o lan√ßou o segundo boom da Assembl√©ia Nacional, que come√ßou a produzir resultados pr√°ticos.  Em 1998, um grupo de pesquisadores da AT&amp;T mostrou como as redes neurais podem ser usadas para reconhecer n√∫meros manuscritos, o que tornou poss√≠vel automatizar o processamento de cheques. <br><br>  "A principal mensagem deste trabalho √© que podemos criar sistemas aprimorados para reconhecer padr√µes, confiando mais no aprendizado autom√°tico e menos nas heur√≠sticas desenvolvidas manualmente", escreveram os autores. <br><br>  E, no entanto, nessa fase, os NSs eram apenas uma das muitas tecnologias √† disposi√ß√£o dos pesquisadores de aprendizado de m√°quina.  Quando estudei em um curso de IA no instituto em 2008, as redes neurais eram apenas um dos nove algoritmos MO, dos quais poder√≠amos escolher a op√ß√£o adequada para a tarefa.  No entanto, a GO j√° estava se preparando para ofuscar o restante da tecnologia. <br><br>  Big data demonstra o poder da aprendizagem profunda <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0ff/b2d/663/0ffb2d6630240722208088bd6be34644.jpg"><br>  <i>Relaxamento detectado.</i>  <i>Possibilidade de praia 1.0.</i>  <i>Iniciamos o procedimento de uso do Mai Tai.</i> <br><br>  A retropropaga√ß√£o facilitou o processo de c√°lculo do NS, mas as redes mais profundas ainda precisavam de mais recursos de computa√ß√£o do que as pequenas.  Os resultados de estudos realizados nas d√©cadas de 1990 e 2000 mostraram frequentemente que era poss√≠vel obter cada vez menos benef√≠cios de complica√ß√µes adicionais da SN. <br><br>  Ent√£o o pensamento das pessoas foi mudado pelo famoso trabalho de 2012, que descreveu o NS sob o nome AlexNet, em homenagem ao pesquisador Alex Krizhevsky.  Muito parecido com redes mais profundas, poderia proporcionar efici√™ncia revolucion√°ria, mas apenas em combina√ß√£o com uma abund√¢ncia de energia do computador e uma enorme quantidade de dados. <br><br>  A AlexNet desenvolveu um trio de cientistas da computa√ß√£o da Universidade de Toronto para participar do concurso de ci√™ncias da ImageNet.  Os organizadores do concurso coletaram um milh√£o de imagens na Internet, cada uma delas rotulada e atribu√≠da a uma das milhares de categorias de objetos, por exemplo, ‚Äúcereja‚Äù, ‚Äúnavio porta-cont√™ineres‚Äù ou ‚Äúleopardo‚Äù.  Foi pedido aos pesquisadores de IA que treinassem seus programas de MO em partes dessas imagens e tentassem colocar os r√≥tulos corretos para outras imagens que o software n√£o havia encontrado antes.  O software teve que selecionar cinco r√≥tulos poss√≠veis para cada imagem, e a tentativa foi considerada bem-sucedida se um deles coincidisse com o real. <br><br>  Essa foi uma tarefa dif√≠cil e at√© 2012 os resultados n√£o foram muito bons.  Para o vencedor de 2011, a taxa de erro foi de 25%. <br><br>  Em 2012, a equipe AlexNet superou todos os concorrentes, dando respostas com 15% de erros.  Para o concorrente mais pr√≥ximo, esse n√∫mero foi de 26%. <br><br>  Pesquisadores de Toronto combinaram v√°rias t√©cnicas para alcan√ßar resultados inovadores.  Um deles foi o uso de <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">neuroses convolucionais</a> (SNS).  De fato, o SNA, por assim dizer, treina pequenas redes neurais - cujos dados de entrada s√£o quadrados com um lado de 7 a 11 pixels - e depois os "sobrep√µe" a uma imagem maior. <br><br>  "√â como se voc√™ pegasse um pequeno modelo ou est√™ncil e tentasse compar√°-lo com cada ponto da imagem", nos disse no ano passado a pesquisadora de IA Jie Tan.  - Voc√™ tem um est√™ncil de cachorro e o anexa √† imagem e v√™ se h√° um cachorro ali?  Caso contr√°rio, mova o est√™ncil.  E assim para toda a imagem.  E n√£o importa onde o cachorro apare√ßa na foto.  O est√™ncil ir√° coincidir com ele.  Cada subse√ß√£o de rede n√£o deve se tornar um classificador de c√£es separado. ‚Äù <br><br>  Outro fator importante para o sucesso da AlexNet foi o uso de placas gr√°ficas para acelerar o processo de aprendizado.  As placas gr√°ficas t√™m poder de processamento paralelo, adequado para a computa√ß√£o repetitiva necess√°ria para treinar uma rede neural.  Transferindo a carga da computa√ß√£o para um par de GPUs - a Nvidia GTX 580, com 3 GB de mem√≥ria cada -, os pesquisadores foram capazes de desenvolver e treinar uma rede extremamente grande e complexa.  AlexNet tinha oito camadas trein√°veis, 650.000 neur√¥nios e 60 milh√µes de par√¢metros. <br><br>  Por fim, o sucesso do AlexNet tamb√©m foi garantido pelo grande tamanho do banco de dados de imagens de treinamento da ImageNet: um milh√£o de pe√ßas.  Muitas imagens s√£o necess√°rias para ajustar 60 milh√µes de par√¢metros.  Para alcan√ßar uma vit√≥ria decisiva, a AlexNet foi ajudada por uma combina√ß√£o de uma rede complexa e um grande conjunto de dados. <br><br>  Eu me pergunto por que esse avan√ßo n√£o ocorreu anteriormente: <br><br><ul><li>  O par de GPU de n√≠vel de consumidor usado pelos pesquisadores da AlexNet estava longe de ser o dispositivo de computa√ß√£o mais poderoso para 2012.  Cinco e at√© dez anos antes disso, havia computadores mais poderosos.  Al√©m disso, a tecnologia de acelerar o aprendizado do NS usando placas gr√°ficas √© conhecida desde pelo menos 2004. </li><li>  A base de um milh√£o de imagens era extraordinariamente grande para o ensino de algoritmos MO em 2012, no entanto, a coleta desses dados n√£o era uma tecnologia nova para aquele ano.  Uma equipe de pesquisa bem financiada poderia facilmente montar um banco de dados desse tamanho cinco ou dez anos antes. </li><li>  Os principais algoritmos usados ‚Äã‚Äãno AlexNet n√£o eram novos.  O algoritmo de retropropaga√ß√£o at√© 2012 j√° existia h√° cerca de um quarto de s√©culo.  As principais id√©ias relacionadas √†s redes neurais convolucionais foram desenvolvidas nas d√©cadas de 1980 e 1990. </li></ul><br>  Portanto, cada um dos elementos de sucesso do AlexNet existia separadamente muito antes do avan√ßo.  Obviamente, nunca ocorreu a ningu√©m combin√°-los - na maior parte porque ningu√©m sabia o qu√£o poderosa essa combina√ß√£o seria. <br><br>  Aumentar a profundidade do NS praticamente n√£o melhorou a efici√™ncia de seu trabalho se eles n√£o usassem conjuntos de dados de treinamento grandes o suficiente.  E expandir o conjunto de dados n√£o melhorou o desempenho de pequenas redes.  Para ver o aumento da efici√™ncia, precis√°vamos de redes mais profundas e de conjuntos de dados maiores - al√©m de um poder computacional significativo que nos permitiu conduzir o processo de treinamento em um per√≠odo de tempo razo√°vel.  A equipe AlexNet foi a primeira a reunir os tr√™s elementos em um programa. <br><br><h2>  O boom da aprendizagem profunda </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/ef0/fb4/922/ef0fb4922c3164251c722134b84460e3.jpg"><br><br>  A demonstra√ß√£o de todo o poder da NS profunda, fornecida por uma quantidade suficiente de dados de treinamento, foi observada por muitas pessoas - tanto entre cientistas, pesquisadores quanto entre representantes da ind√∫stria. <br><br>  O primeiro concurso ImageNet a mudar.  At√© 2012, a maioria dos participantes usava outras tecnologias al√©m do aprendizado profundo.  Na competi√ß√£o de 2013, como escreveram os patrocinadores, "a maioria" dos participantes usou o GO. <br><br>  A porcentagem de erros entre os vencedores diminuiu gradualmente - de impressionantes 16% na AlexNet em 2012 para 2,3% em 2017: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ba/95e/b4b/9ba95eb4baee6580ea97ac76347a02e4.png"><br><br>  A revolu√ß√£o GO se espalhou rapidamente por todo o setor.  Em 2013, o Google adquiriu uma startup formada pelos autores da AlexNet e usou sua tecnologia como base para a fun√ß√£o de pesquisa de imagens no Google Fotos.  Em 2014, o Facebook divulgava seu pr√≥prio software que reconhece imagens usando o GO.  A Apple usa o GO para reconhecimento de rosto no iOS desde pelo menos 2016. <br><br>  O GO tamb√©m est√° subjacente √† recente melhoria na tecnologia de reconhecimento de voz.  Siri da Apple, Alexa da Amazon, Cortana da Microsoft e o assistente do Google usam o GO - para entender as palavras de uma pessoa ou para gerar uma voz mais natural, ou ambas. <br><br>  Nos √∫ltimos anos, surgiu uma tend√™ncia auto-sustent√°vel no setor, na qual o aumento no poder da computa√ß√£o, no volume de dados e na profundidade da rede se sustentam.  A equipe da AlexNet usou a GPU porque ofereceu computa√ß√£o paralela por um pre√ßo razo√°vel.  Mas, nos √∫ltimos anos, mais e mais empresas come√ßaram a desenvolver seus pr√≥prios chips, projetados especificamente para uso no campo da MO. <br><br>  O Google anunciou o lan√ßamento do chip Tensor Processing Unit projetado especificamente para o NS em 2016. No mesmo ano, a Nvidia anunciou o lan√ßamento de uma nova GPU chamada Tesla P100, otimizada para o NS.  A Intel respondeu √† chamada com seu chip de IA em 2017. Em 2018, a Amazon anunciou o lan√ßamento de seu pr√≥prio chip de AI, que pode ser usado como parte dos servi√ßos em nuvem da empresa.  Dizem que at√© a Microsoft est√° trabalhando em seu chip de IA. <br><br>  Os fabricantes de smartphones tamb√©m est√£o trabalhando em chips que permitir√£o que os dispositivos m√≥veis fa√ßam mais computa√ß√£o usando o NS localmente, sem precisar enviar dados para os servidores.  Essa computa√ß√£o em dispositivos reduz a lat√™ncia e melhora a privacidade. <br><br>  At√© Tesla entrou neste jogo com fichas especiais.  Este ano, a Tesla mostrou um novo computador poderoso, otimizado para o c√°lculo do NS.  A Tesla nomeou-o como Computador Aut√¥nomo Completo e o apresentou como um momento-chave na estrat√©gia da empresa de transformar a frota da Tesla em ve√≠culos rob√≥ticos. <br><br>  A disponibilidade das capacidades do computador otimizadas para IA gerou uma solicita√ß√£o dos dados necess√°rios para treinar NSs cada vez mais complexos.  Essa din√¢mica √© mais evidente no setor de autom√≥veis, onde as empresas coletam dados sobre milh√µes de quil√¥metros de estradas reais.  A Tesla pode coletar esses dados automaticamente dos carros dos usu√°rios e seus concorrentes Waymo e Cruise pagaram aos motoristas que dirigiam seus carros nas vias p√∫blicas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2eb/80a/3be/2eb80a3be8fd038777829faff752e8bf.jpg"><br><br>  A solicita√ß√£o de dados oferece uma vantagem para grandes empresas online que j√° t√™m acesso a grandes volumes de dados do usu√°rio. <br><br>  O aprendizado profundo conquistou muitas √°reas diferentes devido √† sua extrema flexibilidade.  D√©cadas de tentativa e erro permitiram que os pesquisadores desenvolvessem os elementos b√°sicos para as tarefas mais comuns no campo da MO - como redes de convolu√ß√£o para reconhecimento eficiente de imagens.  No entanto, se voc√™ tiver uma rede de alto n√≠vel adequada ao esquema e dados suficientes, o processo de treinamento ser√° simples.  Os NSs profundos s√£o capazes de reconhecer uma variedade excepcionalmente ampla de padr√µes complexos sem a orienta√ß√£o especial de desenvolvedores humanos. <br><br>  Existem limita√ß√µes, √© claro.  Por exemplo, algumas pessoas se entregaram √† id√©ia de treinar robomobiles com a ajuda de apenas GO - ou seja, alimentar imagens recebidas de uma c√¢mera, uma rede neural e receber instru√ß√µes dela para girar o volante e o pedal.  Eu sou c√©tico em rela√ß√£o a essa abordagem.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A Assembl√©ia Nacional ainda n√£o demonstrou a capacidade de conduzir um racioc√≠nio l√≥gico complexo, necess√°rio para entender certas condi√ß√µes que surgem na estrada. </font><font style="vertical-align: inherit;">Al√©m disso, os NSs s√£o "caixas pretas", cujo fluxo de trabalho √© praticamente invis√≠vel. </font><font style="vertical-align: inherit;">Seria dif√≠cil avaliar e confirmar a seguran√ßa desse sistema. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, o GO permitiu dar saltos muito amplos em uma variedade inesperadamente grande de aplicativos. </font><font style="vertical-align: inherit;">Nos pr√≥ximos anos, pode-se esperar o pr√≥ximo progresso nessa √°rea.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt482258/">https://habr.com/ru/post/pt482258/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt482248/index.html">Compartilhamos nossa experi√™ncia de como os SSDs aparecem sob RAID e qual n√≠vel de matriz √© mais lucrativo</a></li>
<li><a href="../pt482250/index.html">Uma m√°quina de estado simples para VueJS</a></li>
<li><a href="../pt482252/index.html">Toalete autom√°tico para gatos - continua√ß√£o</a></li>
<li><a href="../pt482254/index.html">Experi√™ncia VonmoTrade. Parte 3: Livro de warrants. Processamento e armazenamento de informa√ß√µes comerciais</a></li>
<li><a href="../pt482256/index.html">IA e o futuro do trabalho: perspectivas de emprego no futuro pr√≥ximo</a></li>
<li><a href="../pt482262/index.html">Como fazer login no Talend Open Studio</a></li>
<li><a href="../pt482264/index.html">Brasil, magia negra, Mortal Kombat, Marte e 15.000 pessoas. Resultados do ano Ontiko</a></li>
<li><a href="../pt482268/index.html">Megaestruturas do futuro: a esfera de Dyson, o motor estelar e a "bomba do buraco negro"</a></li>
<li><a href="../pt482270/index.html">Transmiss√£o WebRTC dentro e ao redor da realidade virtual</a></li>
<li><a href="../pt482272/index.html">Escolhendo um data warehouse para Prometheus: Thanos vs VictoriaMetrics</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>