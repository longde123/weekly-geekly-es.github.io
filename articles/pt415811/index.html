<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë∂üèΩ üÜî ü§úüèº AI, curso pr√°tico. Vis√£o geral de redes neurais para classifica√ß√£o de imagens ü§πüèæ üëê üéâ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este artigo fornece uma vis√£o geral te√≥rica acess√≠vel de redes neurais convolucionais (CNN) e explica sua aplica√ß√£o ao problema de classifica√ß√£o de im...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AI, curso pr√°tico. Vis√£o geral de redes neurais para classifica√ß√£o de imagens</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/intel/blog/415811/">  Este artigo fornece uma vis√£o geral te√≥rica acess√≠vel de redes neurais convolucionais (CNN) e explica sua aplica√ß√£o ao problema de classifica√ß√£o de imagens. <br><br><img src="https://habrastorage.org/webt/_d/ve/hi/_dvehi4kbgauxndfn56s7-tmtku.jpeg"><br><a name="habracut"></a><br><h2>  <font color="#0071c5">Abordagem comum: sem aprendizado profundo</font> </h2><br>  O termo "processamento de imagem" refere-se a uma ampla classe de tarefas para as quais os dados de entrada s√£o imagens e a sa√≠da pode ser imagens ou conjuntos de recursos caracter√≠sticos associados.  Existem muitas op√ß√µes: classifica√ß√£o, segmenta√ß√£o, anota√ß√£o, detec√ß√£o de objeto etc. Neste artigo, examinamos a classifica√ß√£o de imagens, n√£o apenas por ser a tarefa mais simples, mas tamb√©m por estar subjacente a muitas outras tarefas. <br><br>  A abordagem geral para a classifica√ß√£o de imagens consiste nas duas etapas a seguir: <br><br><ol><li>  Gera√ß√£o de caracter√≠sticas significativas da imagem. </li><li>  Classifica√ß√£o de uma imagem com base em seus atributos. </li></ol><br>  A sequ√™ncia comum de opera√ß√µes usa modelos simples, como MultiLayer Perceptron (MLP), SVM (Support Vector Machine), m√©todo de vizinhos mais pr√≥ximos e regress√£o log√≠stica sobre os recursos criados manualmente.  Os atributos s√£o gerados usando v√°rias transforma√ß√µes (por exemplo, detec√ß√£o de escala de cinza e limite) e descritores, por exemplo, histograma de gradientes orientados ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HOG</a> ) ou transforma√ß√µes de transformadores de recurso invari√°veis ‚Äã‚Äãem escala ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SIFT</a> ) e etc. <br><br>  A principal limita√ß√£o dos m√©todos geralmente aceitos √© a participa√ß√£o de um especialista que escolhe um conjunto e uma sequ√™ncia de etapas para gerar recursos. <br><br>  Com o tempo, percebeu-se que a maioria das t√©cnicas para gerar recursos pode ser generalizada usando n√∫cleos (filtros) - matrizes pequenas (geralmente com tamanho 5 √ó 5), que s√£o convolu√ß√µes das imagens originais.  A convolu√ß√£o pode ser considerada como um processo seq√ºencial em duas etapas: <br><br><ol><li> Passe o mesmo n√∫cleo fixo por toda a imagem de origem. </li><li>  Em cada etapa, calcule o produto escalar do kernel e a imagem original no local atual do kernel. </li></ol><br>  O resultado da convolu√ß√£o da imagem e do kernel √© chamado de mapa de recursos. <br>  Uma explica√ß√£o matematicamente mais rigorosa √© dada no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cap√≠tulo relevante do</a> livro recentemente publicado, Deep Learning, de I. Goodfellow, I. Benjio e A. Courville. <br><br><img src="https://habrastorage.org/webt/kw/lm/rd/kwlmrdg1y8wsniz94riol_8fiie.png"><br>  <i>O processo de convolu√ß√£o do n√∫cleo (verde escuro) com a imagem original (verde), que resulta em um mapa de sinais (amarelo).</i> <br><br>  Um exemplo simples de uma transforma√ß√£o que pode ser feita com filtros √© desfocar uma imagem.  Pegue um filtro composto por todas as unidades.  Calcula a m√©dia da vizinhan√ßa determinada pelo filtro.  Nesse caso, o bairro √© uma se√ß√£o quadrada, mas pode ser cruciforme ou qualquer outra coisa.  A m√©dia leva √† perda de informa√ß√µes sobre a posi√ß√£o exata dos objetos, desfocando a imagem inteira.  Uma explica√ß√£o intuitiva semelhante pode ser fornecida para qualquer filtro criado manualmente. <br><br><img src="https://habrastorage.org/webt/5t/ud/g7/5tudg7ebng4ocb6jdc1-1alsqpo.png"><br>  <i>O resultado da convolu√ß√£o da imagem do pr√©dio da Universidade de Harvard com tr√™s n√∫cleos diferentes.</i> <br><br><h2>  <font color="#0071c5">Redes neurais convolucionais</font> </h2><br>  A abordagem convolucional da classifica√ß√£o de imagens tem v√°rias desvantagens significativas: <br><br><ul><li>  Um processo de v√°rias etapas em vez de uma sequ√™ncia de ponta a ponta. </li><li>  Os filtros s√£o uma √≥tima ferramenta de generaliza√ß√£o, mas s√£o matrizes fixas.  Como escolher pesos nos filtros? </li></ul><br>  Felizmente, foram inventados filtros aprend√≠veis, que s√£o o princ√≠pio b√°sico subjacente √† CNN.  O princ√≠pio √© simples: treinaremos os filtros aplicados √† descri√ß√£o das imagens para melhor cumprir sua tarefa. <br><br>  A CNN n√£o possui um inventor, mas um dos primeiros casos de sua aplica√ß√£o √© o LeNet-5 * no trabalho <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">‚Äú</a> Aprendizagem baseada em gradiente aplicada ao reconhecimento de documentos‚Äù de I. LeCun e outros autores. <br><br>  A CNN mata dois coelhos com uma cajadada: n√£o h√° necessidade de uma defini√ß√£o preliminar de filtros, e o processo de aprendizado se torna de ponta a ponta.  Uma arquitetura t√≠pica da CNN consiste nas seguintes partes: <br><br><ul><li>  Camadas convolucionais </li><li>  Camadas de subamostragem </li><li>  Camadas densas (totalmente conectadas) </li></ul><br>  Vamos considerar cada parte em mais detalhes. <br><br><h3>  <font color="#0071c5">Camadas convolucionais</font> </h3><br>  A camada convolucional √© o principal elemento estrutural da CNN.  A camada convolucional possui um conjunto de caracter√≠sticas: <br>  <i>Conectividade local (esparsa)</i> .  Nas camadas densas, cada neur√¥nio √© conectado a cada neur√¥nio da camada anterior (portanto, eles eram chamados densos).  Na camada convolucional, cada neur√¥nio est√° conectado a apenas uma pequena parte dos neur√¥nios da camada anterior. <br><br><img src="https://habrastorage.org/webt/nl/1a/6t/nl1a6t8bvbzna0rv_y3qjv_gsua.png"><br>  <i>Um exemplo de uma rede neural unidimensional.</i>  <i>(esquerda) Conex√£o de neur√¥nios em uma rede densa t√≠pica, (direita) Caracteriza√ß√£o da conectividade local inerente √† camada convolucional.</i>  <i>Imagens tiradas de I. Goodfellow e outros por Deep Learning</i> <br><br>  <i>O tamanho da √°rea √† qual o neur√¥nio est√° conectado</i> √© chamado de tamanho do filtro (o comprimento do filtro no caso de dados unidimensionais, por exemplo, s√©ries temporais ou a largura / altura no caso de dados bidimensionais, por exemplo, imagens).  Na figura √† direita, o tamanho do filtro √© 3. Os <i>pesos com os quais a conex√£o</i> √© feita s√£o chamados de filtro (um vetor no caso de dados unidimensionais e uma matriz para dados bidimensionais).  <i>A etapa √© a dist√¢ncia que o filtro se move sobre os dados</i> (na figura √† direita, a etapa √© 1).  A id√©ia de conectividade local nada mais √© do que um kernel que avan√ßa um passo.  Cada neur√¥nio de n√≠vel convolucional representa e implementa uma posi√ß√£o espec√≠fica do n√∫cleo deslizando ao longo da imagem original. <br><br><img src="https://habrastorage.org/webt/nu/we/gf/nuwegftvmoigtcdz2mlt0xwmm5g.png"><br>  <i>Duas camadas convolucionais unidimensionais adjacentes</i> <br><br>  Outra propriedade importante √© a chamada <i>zona de suscetibilidade</i> .  Ele reflete o n√∫mero de posi√ß√µes do sinal original que o neur√¥nio atual pode "ver".  Por exemplo, a zona de suscetibilidade da primeira camada de rede, mostrada na figura, √© igual ao tamanho do filtro 3, pois cada neur√¥nio est√° conectado a apenas tr√™s neur√¥nios do sinal original.  No entanto, na segunda camada, a zona de suscetibilidade j√° √© 5, pois o neur√¥nio da segunda camada agrega tr√™s neur√¥nios da primeira camada, cada um dos quais com uma zona de suscetibilidade 3. Com o aumento da profundidade, a zona de suscetibilidade cresce linearmente. <br><br>  <i>Par√¢metros compartilhados</i> .  Lembre-se de que no processamento cl√°ssico de imagens, o mesmo n√∫cleo deslizou por toda a imagem.  A mesma id√©ia se aplica aqui.  Fixamos apenas o tamanho do filtro de pesos para uma camada e aplicaremos esses pesos a todos os neur√¥nios da camada.  Isso √© equivalente a deslizar o mesmo n√∫cleo por toda a imagem.  Mas a quest√£o pode surgir: como podemos aprender algo com um n√∫mero t√£o pequeno de par√¢metros? <br><br><img src="https://habrastorage.org/webt/ue/qo/gi/ueqogixaqstaotlmnlfnjfn3dtc.png"><br>  <i>As setas escuras representam os mesmos pesos.</i>  <i>(esquerda) MLP regular, em que cada fator de pondera√ß√£o √© um par√¢metro separado, (direita) Um exemplo de separa√ß√£o de par√¢metros, em que v√°rios fatores de pondera√ß√£o indicam o mesmo par√¢metro de treinamento</i> <br><br>  <i>Estrutura espacial</i> .  A resposta a esta pergunta √© simples: treinaremos v√°rios filtros em uma camada!  Eles s√£o colocados paralelos um ao outro, formando assim uma nova dimens√£o. <br><br>  Paramos um pouco e explicamos a ideia apresentada pelo exemplo de uma imagem RGB bidimensional de 227 √ó 227. Observe que aqui estamos lidando com uma imagem de entrada de tr√™s canais, o que, em ess√™ncia, significa que temos tr√™s imagens de entrada ou dados de entrada tridimensionais. <br><br><img src="https://habrastorage.org/webt/ol/9r/ty/ol9rtyiz5s9btzfnldhf6bo_0wi.png"><br>  <i>A estrutura espacial da imagem de entrada</i> <br><br>  Consideraremos as dimens√µes dos canais como a profundidade da imagem (observe que essa n√£o √© a mesma que a profundidade das redes neurais, que √© igual ao n√∫mero de camadas da rede).  A quest√£o √© como determinar o kernel para este caso. <br><br><img src="https://habrastorage.org/webt/l6/pt/be/l6ptberff4a-rbz81bq-uxa-10w.png"><br>  <i>Um exemplo de um n√∫cleo bidimensional, que √© essencialmente uma matriz tridimensional com uma medi√ß√£o de profundidade adicional.</i>  <i>Este filtro fornece uma convolu√ß√£o com a imagem;</i>  <i>ou seja, desliza sobre a imagem no espa√ßo, calculando produtos escalares</i> <br><br>  A resposta √© simples, embora ainda n√£o seja √≥bvia: tornaremos o n√∫cleo tamb√©m tridimensional.  As duas primeiras dimens√µes permanecem as mesmas (largura e altura do n√∫cleo) e a terceira dimens√£o √© sempre igual √† profundidade dos dados de entrada. <br><br><img src="https://habrastorage.org/webt/yu/e3/xz/yue3xziqupgpwtmqvtit8ik5hos.png"><br>  <i>Um exemplo de uma etapa de convolu√ß√£o espacial.</i>  <i>O resultado do produto escalar do filtro e uma pequena parte da imagem 5 √ó 5 √ó 3 (ou seja, 5 √ó 5 √ó 5 + 1 = 76, a dimens√£o do produto escalar + deslocamento) √© um n√∫mero</i> <br><br>  Nesse caso, toda a se√ß√£o 5 √ó 5 √ó 3 da imagem original √© transformada em um n√∫mero e a pr√≥pria imagem tridimensional ser√° transformada em <i>um mapa de recursos</i> ( <i>mapa de ativa√ß√£o</i> ).  Um mapa de recursos √© um conjunto de neur√¥nios, cada um dos quais calcula sua pr√≥pria fun√ß√£o, levando em considera√ß√£o dois princ√≠pios b√°sicos discutidos acima: <i>conectividade local</i> (cada neur√¥nio est√° associado a apenas uma pequena parte dos dados de entrada) e <i>separa√ß√£o de par√¢metros</i> (todos os neur√¥nios usam o mesmo filtro).  Idealmente, esse mapa de recursos ser√° o mesmo que o j√° encontrado no exemplo de uma rede geralmente aceita - ele armazena os resultados da convolu√ß√£o da imagem e filtro de entrada. <br><br><img src="https://habrastorage.org/webt/iw/4w/qr/iw4wqr77vslpfjqblrgtjlxyxkw.png"><br>  <i>Mapa de recursos como resultado da convolu√ß√£o do n√∫cleo com todas as posi√ß√µes espaciais</i> <br><br>  Observe que a profundidade do mapa de recursos √© 1, pois usamos apenas um filtro.  Mas nada nos impede de usar mais filtros;  por exemplo, 6. Todos eles interagir√£o com os mesmos dados de entrada e funcionar√£o independentemente um do outro.  Vamos dar um passo adiante e combinar esses cart√µes de recursos.  Suas dimens√µes espaciais s√£o as mesmas, pois as dimens√µes dos filtros s√£o as mesmas.  Assim, os mapas de caracter√≠sticas coletados juntos podem ser considerados como uma nova matriz tridimensional, cuja dimens√£o de profundidade √© representada por mapas de caracter√≠sticas de diferentes n√∫cleos.  Nesse sentido, os canais RGB da imagem de entrada n√£o s√£o outro sen√£o os tr√™s mapas de recursos originais. <br><br><img src="https://habrastorage.org/webt/c8/f1/23/c8f123tp1nnbklt5oimm4gkmwj4.png"><br>  <i>A aplica√ß√£o paralela de v√°rios filtros √† imagem de entrada e o conjunto resultante de cart√µes de ativa√ß√£o</i> <br><br>  Essa compreens√£o dos mapas de recursos e sua combina√ß√£o √© muito importante, pois, tendo percebido isso, podemos expandir a arquitetura da rede e instalar camadas convolucionais uma em cima da outra, aumentando a zona de suscetibilidade e enriquecendo nosso classificador. <br><br><img src="https://habrastorage.org/webt/7_/3g/hp/7_3ghputtiaz-2l-hbbs7dagmvi.png"><br>  <i>Camadas convolucionais instaladas umas sobre as outras.</i>  <i>Em cada camada, o tamanho dos filtros e seu n√∫mero podem variar</i> <br><br>  Agora entendemos o que √© uma rede convolucional.  O objetivo principal dessas camadas √© o mesmo da abordagem geralmente aceita - detectar sinais significativos da imagem.  E, se na primeira camada esses sinais puderem ser muito simples (a presen√ßa de linhas verticais / horizontais), a profundidade da rede aumentar√° o grau de sua abstra√ß√£o (a presen√ßa de um cachorro / gato / pessoa). <br><br><h3>  <font color="#0071c5">Camadas de subamostragem</font> </h3><br>  As camadas convolucionais s√£o o principal componente da CNN.  Mas h√° outra parte importante e frequentemente usada - s√£o camadas de subamostras.  No processamento de imagem convencional, n√£o h√° anal√≥gico direto, mas uma subamostra pode ser considerada como outro tipo de kernel.  O que √© isso? <br><br><img src="https://habrastorage.org/webt/n4/fm/df/n4fmdf4o-i1pa7qs4w4oj7wmzgy.png"><br>  <i>Exemplos de subamostragem.</i>  <i>(esquerda) Como uma subamostra altera os tamanhos espaciais (mas n√£o os canais!) das matrizes de dados, (direita) Um esquema b√°sico de como uma subamostra funciona</i> <br><br>  Uma subamostra filtra uma parte da vizinhan√ßa de cada pixel dos dados de entrada com uma fun√ß√£o de agrega√ß√£o espec√≠fica, por exemplo, m√°xima, m√©dia etc. A subamostra √© essencialmente a mesma que convolu√ß√£o, mas a fun√ß√£o de combina√ß√£o de pixels n√£o se limita ao produto escalar.  Outra diferen√ßa importante √© que a subamostragem funciona apenas na dimens√£o espacial.  Uma caracter√≠stica da camada de subamostragem √© que o <i>tom geralmente √© igual ao tamanho do filtro</i> (o valor t√≠pico √© 2). <br><br>  Uma subamostra tem tr√™s objetivos principais: <br><br><ul><li>  Diminui√ß√£o da dimens√£o espacial ou subamostragem.  Isso √© feito para reduzir o n√∫mero de par√¢metros. </li><li>  O crescimento da zona de suscetibilidade.  Devido aos neur√¥nios da subamostra nas camadas subseq√ºentes, mais etapas do sinal de entrada s√£o acumuladas </li><li>  Invari√¢ncia translacional a pequenas heterogeneidades na posi√ß√£o dos padr√µes no sinal de entrada.  Ao calcular estat√≠sticas de agrega√ß√£o de pequenas vizinhan√ßas do sinal de entrada, uma subamostra pode ignorar pequenos deslocamentos espaciais nele. </li></ul><br><h3>  <font color="#0071c5">Camadas espessas</font> </h3><br>  Camadas convolucionais e subamostras servem ao mesmo objetivo - gerando atributos de imagem.  A etapa final √© classificar a imagem de entrada com base nos recursos detectados.  Na CNN, camadas densas na parte superior da rede fazem isso.  Essa parte da rede √© chamada de <i>classifica√ß√£o</i> .  Ele pode conter v√°rias camadas umas sobre as outras com conectividade completa, mas geralmente termina com uma camada de classe <i>softmax</i> ativada por uma fun√ß√£o de ativa√ß√£o log√≠stica <i>multivari√°vel</i> , na qual o n√∫mero de blocos √© igual ao n√∫mero de classes.  Na sa√≠da dessa camada est√° a distribui√ß√£o de probabilidade por classe para o objeto de entrada.  Agora a imagem pode ser classificada escolhendo a classe mais prov√°vel. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt415811/">https://habr.com/ru/post/pt415811/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt415795/index.html">Escrevendo uma interface do Snapchat no Swift</a></li>
<li><a href="../pt415797/index.html">Express√µes regulares + programa√ß√£o l√≥gica. Qual √© o resultado?</a></li>
<li><a href="../pt415801/index.html">Google: nossa IA de "telefone" n√£o √© boa o suficiente para ser perigosa</a></li>
<li><a href="../pt415805/index.html">Modifica√ß√£o do m√≥dulo de barreira GSM Doorhan para controle da Internet</a></li>
<li><a href="../pt415809/index.html">Como usar soy, requirejs, backbone js em plugins para o Atlassian Jira</a></li>
<li><a href="../pt415813/index.html">Algumas notas sobre o estado atual do Cloud Gaming</a></li>
<li><a href="../pt415815/index.html">Na vanguarda da ci√™ncia: uma an√°lise dos artigos do arxiv.org</a></li>
<li><a href="../pt415817/index.html">Fazemos um overclock do backup. Palestra Yandex</a></li>
<li><a href="../pt415819/index.html">Relat√≥rio do Clube de Roma de 2018, Cap√≠tulo 3.16: Governo Global</a></li>
<li><a href="../pt415821/index.html">A maneira de organizar uma casa "inteligente" com o controle el√©trico mais amplo poss√≠vel</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>