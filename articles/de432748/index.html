<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõ©Ô∏è üöΩ üë®‚Äçüëß‚Äçüë¶ Tipps und Tricks von Kubernetes: Knotenzuweisung und Laden von Webanwendungen ‚ú°Ô∏è ü¶ñ ü•ß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In Fortsetzung unserer praktischen Artikel dar√ºber, wie Sie das Leben in der t√§glichen Arbeit mit Kubernetes erleichtern k√∂nnen, sprechen wir √ºber zwe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tipps und Tricks von Kubernetes: Knotenzuweisung und Laden von Webanwendungen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/432748/"><img src="https://habrastorage.org/webt/7r/gv/ix/7rgvixbeoe0vyfkw__obak6ow_y.jpeg"><br><br>  In Fortsetzung unserer praktischen Artikel dar√ºber, wie Sie das Leben in der t√§glichen Arbeit mit Kubernetes erleichtern k√∂nnen, sprechen wir √ºber zwei Geschichten aus der Arbeitswelt: die Zuweisung einzelner Knoten f√ºr bestimmte Aufgaben und die Konfiguration von php-fpm (oder einem anderen Anwendungsserver) f√ºr schwere Lasten.  Die hier beschriebenen L√∂sungen erheben nach wie vor keinen Anspruch auf Idealheit, sondern dienen als Ausgangspunkt f√ºr Ihre spezifischen F√§lle und als Grundlage f√ºr √úberlegungen.  Fragen und Verbesserungen in den Kommentaren sind willkommen! <a name="habracut"></a><br><br><h2>  1. Die Zuordnung einzelner Knoten f√ºr bestimmte Aufgaben </h2><br>  Wir erstellen einen Kubernetes-Cluster auf virtuellen Servern, Clouds oder Bare-Metal-Servern.  Wenn Sie die gesamte Systemsoftware und die Clientanwendungen auf denselben Knoten installieren, treten wahrscheinlich folgende Probleme auf: <br><br><ul><li>  Die Client-Anwendung beginnt pl√∂tzlich, aus dem Speicher zu "lecken", obwohl ihre Grenzen sehr hoch sind. </li><li>  Komplexe einmalige Anfragen an Loghouse, Prometheus oder Ingress * f√ºhren zu OOM, wodurch die Clientanwendung darunter leidet. </li><li>  Ein Speicherverlust aufgrund eines Fehlers in der Systemsoftware beendet die Clientanwendung, obwohl die Komponenten m√∂glicherweise nicht logisch miteinander verbunden sind. </li></ul><br>  <i>* Unter anderem war es f√ºr √§ltere Versionen von Ingress relevant, als aufgrund der gro√üen Anzahl von Websocket-Verbindungen und des st√§ndigen Nachladens von Nginx ‚Äûh√§ngende Nginx-Prozesse‚Äú auftraten, die sich auf Tausende beliefen und eine enorme Menge an Ressourcen verbrauchten.</i> <br><br>  Der eigentliche Fall ist die Installation von Prometheus mit einer gro√üen Anzahl von Metriken, bei denen beim Anzeigen des "schweren" Dashboards, in dem eine gro√üe Anzahl von Anwendungscontainern dargestellt wird, aus denen jeweils Diagramme erstellt werden, der Speicherverbrauch schnell auf ~ 15 GB anstieg.  Infolgedessen k√∂nnte der OOM-Killer auf das Host-System "kommen" und andere Dienste beenden, was wiederum zu einem "unverst√§ndlichen Verhalten der Anwendungen im Cluster" f√ºhrte.  Und aufgrund der hohen CPU-Auslastung der Clientanwendung ist es einfach, eine instabile Verarbeitungszeit f√ºr Ingress-Abfragen zu erhalten ... <br><br>  Die L√∂sung setzte sich schnell durch: Es war notwendig, einzelne Maschinen f√ºr verschiedene Aufgaben zuzuweisen.  Wir haben 3 Haupttypen von Aufgabengruppen identifiziert: <br><br><ol><li>  <b>Fronten</b> , an denen wir nur Ingresss platzieren, um sicherzustellen, dass keine anderen Dienste die Verarbeitungszeit von Anforderungen beeinflussen k√∂nnen; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Systemknoten</a> , auf denen wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VPNs</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Loghouse</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Prometheus</a> , Dashboard, CoreDNS usw. Bereitstellen; </li><li>  <b>Knoten f√ºr Anwendungen</b> - in der Tat, wo Clientanwendungen eingef√ºhrt werden.  Sie k√∂nnen auch f√ºr Umgebungen oder Funktionen zugewiesen werden: dev, prod, perf, ... </li></ol><br><h3>  L√∂sung </h3><br>  Wie setzen wir das um?  Sehr einfach: zwei native Kubernetes-Mechanismen.  Der erste ist <b>nodeSelector</b> , um den gew√ºnschten Knoten auszuw√§hlen, auf den die Anwendung gehen soll, basierend auf den auf jedem Knoten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">installierten</a> Beschriftungen. <br><br>  <code>kube-system-1</code> wir haben einen <code>kube-system-1</code> Knoten.  Wir f√ºgen ein zus√§tzliches Etikett hinzu: <br><br><pre> <code class="bash hljs">$ kubectl label node kube-system-1 node-role/monitoring=</code> </pre> <br>  ... und in <code>Deployment</code> , das auf diesen Knoten ausgerollt werden soll, schreiben wir: <br><br><pre> <code class="plaintext hljs">nodeSelector: node-role/monitoring: ""</code> </pre> <br>  Der zweite Mechanismus sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>Verschmutzungen und Toleranzen</b></a> .  Mit seiner Hilfe weisen wir ausdr√ºcklich darauf hin, dass auf diesen Maschinen nur Container gestartet werden k√∂nnen, die diesen Makel tolerieren. <br><br>  Zum Beispiel gibt es eine <code>kube-frontend-1</code> Maschine, auf der wir nur Ingress rollen werden.  F√ºgen Sie diesem Knoten Taint hinzu: <br><br><pre> <code class="bash hljs">$ kubectl taint node kube-frontend-1 node-role/frontend=<span class="hljs-string"><span class="hljs-string">""</span></span>:NoExecute</code> </pre> <br>  ... und bei <code>Deployment</code> schaffen wir Toleranz: <br><br><pre> <code class="plaintext hljs">tolerations: - effect: NoExecute key: node-role/frontend</code> </pre> <br>  Bei Kops k√∂nnen einzelne Instanzgruppen f√ºr die gleichen Anforderungen erstellt werden: <br><br><pre> <code class="bash hljs">$ kops create ig --name cluster_name IG_NAME</code> </pre> <br>  ... und Sie erhalten so etwas wie diese Instanzgruppenkonfiguration in kops: <br><br><pre> <code class="plaintext hljs">apiVersion: kops/v1alpha2 kind: InstanceGroup metadata: creationTimestamp: 2017-12-07T09:24:49Z labels: dedicated: monitoring kops.k8s.io/cluster: k-dev.k8s name: monitoring spec: image: kope.io/k8s-1.8-debian-jessie-amd64-hvm-ebs-2018-01-14 machineType: m4.4xlarge maxSize: 2 minSize: 2 nodeLabels: dedicated: monitoring role: Node subnets: - eu-central-1c taints: - dedicated=monitoring:NoSchedule</code> </pre> <br>  Daher f√ºgen Knoten aus dieser Instanzgruppe automatisch eine zus√§tzliche Bezeichnung und einen zus√§tzlichen Makel hinzu. <br><br><h2>  2. Konfigurieren von php-fpm f√ºr schwere Lasten </h2><br>  Es gibt eine Vielzahl von Servern, auf denen Webanwendungen ausgef√ºhrt werden: php-fpm, gunicorn und dergleichen.  Ihre Verwendung in Kubernetes bedeutet, dass Sie immer √ºber verschiedene Dinge nachdenken sollten: <br><br><ul><li>  Es ist ungef√§hr zu verstehen, <b>wie viele Arbeiter</b> wir bereit sind, in php-fpm in jedem Container zuzuweisen.  Zum Beispiel k√∂nnen wir 10 Mitarbeiter f√ºr die Verarbeitung eingehender Anforderungen zuweisen, weniger Ressourcen f√ºr Pods zuweisen und anhand der Anzahl der Pods skalieren - dies ist eine gute Vorgehensweise.  Ein anderes Beispiel ist, 500 Arbeiter f√ºr jeden Pod zuzuweisen und 2-3 solcher Pods in Produktion zu haben ... aber das ist eine ziemlich schlechte Idee. </li><li>  <b>Lebendigkeits- / Bereitschaftstests sind</b> erforderlich, um den korrekten Betrieb jedes Pods zu √ºberpr√ºfen, und falls der Pod aufgrund von Netzwerkproblemen oder aufgrund des Datenbankzugriffs ‚Äû <b>h√§ngen</b> bleibt‚Äú (m√∂glicherweise gibt es eine Ihrer Optionen und Gr√ºnde).  In solchen Situationen m√ºssen Sie den problematischen Pod neu erstellen. </li><li>  Es ist wichtig, <b>Anforderungen</b> explizit zu registrieren <b>und Ressourcen</b> f√ºr jeden Container zu <b>begrenzen,</b> damit die Anwendung nicht "flie√üt" und nicht beginnt, alle Dienste auf diesem Server zu besch√§digen. </li></ul><br><h3>  L√∂sungen </h3><br>  Leider <b>gibt es keine Silberkugel</b> , mit der Sie sofort verstehen k√∂nnen, wie viele Ressourcen (CPU, RAM) eine Anwendung m√∂glicherweise ben√∂tigt.  Eine m√∂gliche Option besteht darin, den Ressourcenverbrauch zu beobachten und jedes Mal die optimalen Werte auszuw√§hlen.  Um ungerechtfertigte OOM-Kill'ov- und Drosselungs-CPUs zu vermeiden, die den Service stark beeintr√§chtigen, k√∂nnen Sie Folgendes anbieten: <br><br><ul><li>  F√ºgen Sie die richtigen Lebendigkeits- / Bereitschaftstests hinzu, damit wir sicher sein k√∂nnen, dass dieser Container ordnungsgem√§√ü funktioniert.  H√∂chstwahrscheinlich handelt es sich um eine Serviceseite, die die Verf√ºgbarkeit aller Infrastrukturelemente √ºberpr√ºft (damit die Anwendung im Pod funktioniert) und einen 200-OK-Antwortcode zur√ºckgibt. </li><li>  W√§hlen Sie die Anzahl der Mitarbeiter, die Anfragen bearbeiten, richtig aus und verteilen Sie sie richtig. </li></ul><br>  Zum Beispiel haben wir 10 Pods, die aus zwei Containern bestehen: nginx (zum Senden von Statik- und Proxy-Anforderungen an das Backend) und php-fpm (eigentlich das Backend, das dynamische Seiten verarbeitet).  Der Php-fpm-Pool ist f√ºr eine statische Anzahl von Workern konfiguriert (10).  Somit k√∂nnen wir in einer Zeiteinheit 100 aktive Anfragen an Backends verarbeiten.  Lassen Sie jede Anfrage von PHP in 1 Sekunde verarbeitet werden. <br><br>  Was passiert, wenn 1 weitere Anfrage in einem bestimmten Pod eintrifft, in dem derzeit 10 Anfragen aktiv verarbeitet werden?  PHP kann es nicht verarbeiten und Ingress sendet es, um es erneut mit dem n√§chsten Pod zu versuchen, wenn es sich um eine GET-Anforderung handelt.  Wenn eine POST-Anforderung aufgetreten ist, wird ein Fehler zur√ºckgegeben. <br><br>  Und wenn wir ber√ºcksichtigen, dass wir w√§hrend der Verarbeitung aller 10 Anfragen einen Scheck von kubelet (Liveness Probe) erhalten, endet dieser mit einem Fehler und Kubernetes beginnt zu glauben, dass etwas mit diesem Container nicht stimmt, und beendet ihn.  In diesem Fall enden alle Anforderungen, die im Moment verarbeitet wurden, mit einem Fehler (!). Zum Zeitpunkt des Neustarts des Containers ger√§t er aus dem Gleichgewicht, was zu einer Erh√∂hung der Anforderungen f√ºr alle anderen Backends f√ºhrt. <br><br><h4>  Klar </h4><br>  Angenommen, wir haben 2 Pods, f√ºr die jeweils 10 PHP-Fpm-Worker konfiguriert sind.  Hier ist ein Diagramm, das Informationen w√§hrend der "Ausfallzeit" anzeigt, d. H.  Wenn der einzige, der php-fpm anfordert, der php-fpm-Exporteur ist (wir haben jeweils einen aktiven Mitarbeiter): <br><br><img src="https://habrastorage.org/webt/zo/hw/ea/zohwea7nsfbofpgvhixp7edc-qk.png"><br><br>  Starten Sie nun den Start mit Parallelit√§t 19: <br><br><img src="https://habrastorage.org/webt/my/ba/hp/mybahpcgbfoqzzazbwtzu9dc46a.png"><br><br>  Versuchen wir nun, die Parallelit√§t h√∂her zu machen, als wir es k√∂nnen (20) ... sagen wir 23. Dann sind alle PHP-Fpm-Mitarbeiter damit besch√§ftigt, Client-Anfragen zu verarbeiten: <br><br><img src="https://habrastorage.org/webt/tq/v6/f1/tqv6f1fopruyz8_zittdan1pkho.png"><br><br>  Vorkers reichen nicht mehr aus, um eine Lebendigkeitsprobe zu verarbeiten. Daher sehen wir dieses Bild im Kubernetes-Dashboard (oder <code>describe pod</code> ): <br><br><img src="https://habrastorage.org/webt/7z/qo/fw/7zqofwlbjcmxl0qkz2g2rkerece.png"><br><br>  Wenn nun einer der Pods neu gestartet wird, tritt ein <b>Lawineneffekt auf</b> : Anforderungen fallen auf den zweiten Pod, der sie ebenfalls nicht verarbeiten kann, weshalb wir eine gro√üe Anzahl von Fehlern von Clients erhalten.  Nachdem die Pools aller Container voll sind, ist es problematisch, den Service zu erh√∂hen - dies ist nur durch einen starken Anstieg der Anzahl der Pods oder Arbeiter m√∂glich. <br><br><h4>  Erste Option </h4><br>  In einem Container mit PHP k√∂nnen Sie 2 fpm-Pools konfigurieren: einen f√ºr die Verarbeitung von Clientanforderungen und einen f√ºr die √úberpr√ºfung der ‚Äû√úberlebensf√§higkeit‚Äú des Containers.  Dann m√ºssen Sie auf dem Nginx-Container eine √§hnliche Konfiguration vornehmen: <br><br><pre> <code class="plaintext hljs"> upstream backend { server 127.0.0.1:9000 max_fails=0; } upstream backend-status { server 127.0.0.1:9001 max_fails=0; }</code> </pre> <br>  Alles, was bleibt, ist das Senden des Liveness-Samples zur Verarbeitung an den als <code>backend-status</code> Upstream. <br><br>  Nachdem der Liveness-Test separat verarbeitet wurde, treten bei einigen Clients immer noch Fehler auf, aber es gibt zumindest keine Probleme beim Neustart des Pods und beim Trennen des Restes der Clients.  Dadurch werden wir die Anzahl der Fehler erheblich reduzieren, auch wenn unsere Backends die aktuelle Last nicht bew√§ltigen k√∂nnen. <br><br>  Diese Option ist nat√ºrlich besser als nichts, aber sie ist auch schlecht, weil mit dem Hauptpool etwas passieren kann, was wir √ºber die Verwendung des Lebendigkeitstests nicht wissen werden. <br><br><h4>  Zweite Option </h4><br>  Sie k√∂nnen auch das nicht sehr beliebte Nginx-Modul namens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nginx-Limit-Upstream verwenden</a> .  Dann werden wir in PHP 11 Worker angeben und im Container mit nginx eine √§hnliche Konfiguration vornehmen: <br><br><pre> <code class="plaintext hljs"> limit_upstream_zone limit 32m; upstream backend { server 127.0.0.1:9000 max_fails=0; limit_upstream_conn limit=10 zone=limit backlog=10 timeout=5s; } upstream backend-status { server 127.0.0.1:9000 max_fails=0; }</code> </pre> <br>  Auf der Frontend-Ebene begrenzt nginx die Anzahl der Anforderungen, die an das Backend gesendet werden (10).  Ein interessanter Punkt ist, dass ein spezielles Backlog erstellt wird: Wenn die 11. Anforderung f√ºr nginx vom Client stammt und nginx feststellt, dass der PHP-Fpm-Pool ausgelastet ist, wird diese Anforderung f√ºr 5 Sekunden in das Backlog gestellt.  Wenn w√§hrend dieser Zeit php-fpm nicht freigegeben wurde, wird Ingress erst dann aktiv, wodurch die Anforderung an einen anderen Pod wiederholt wird.  Dies gl√§ttet das Bild, da wir immer 1 freien PHP-Mitarbeiter f√ºr die Verarbeitung eines Liveness-Samples haben - wir k√∂nnen den Lawineneffekt vermeiden. <br><br><h4>  Andere Gedanken </h4><br>  F√ºr vielseitigere und sch√∂nere M√∂glichkeiten zur L√∂sung dieses Problems lohnt es sich, in Richtung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Envoy</a> und seiner Analoga zu schauen. <br><br>  Im Allgemeinen empfehle ich dringend, fertige <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Exporteure</a> mit der Konvertierung von Daten aus der Software in das Prometheus-Format zu beauftragen, damit Prometheus eine klare Besch√§ftigung von Arbeitnehmern hat, was wiederum dazu beitr√§gt, das Problem schnell zu finden (und dar√ºber zu informieren). <br><br><h2>  PS </h2><br>  Andere aus dem K8s Tipps &amp; Tricks-Zyklus: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Personalisierte Fehlerseiten in NGINX Ingress</a> "; </li><li>  ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úbertragung von Ressourcen, die in einem Cluster arbeiten, an das Helm 2-Management</a> ‚Äú; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang zu Dev-Sites</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beschleunigen Sie den Bootstrap gro√üer Datenbanken.</a> " </li></ul><br>  Lesen Sie auch in unserem Blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wie hohe Verf√ºgbarkeit bei Kubernetes bereitgestellt wird</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Monitoring and Kubernetes</a> " <i>(R√ºckblick und Videobericht)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unsere Erfahrung mit Kubernetes in kleinen Projekten</a> " <i>(Videobericht, der eine Einf√ºhrung in das technische Ger√§t von Kubernetes enth√§lt)</i> . </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de432748/">https://habr.com/ru/post/de432748/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de432736/index.html">Devops, JUnit5 und Microservice-Tests: Ein subjektiver Blick auf den Moskauer Heisenbag</a></li>
<li><a href="../de432740/index.html">"CMS" basierend auf Google Spreadsheets f√ºr statische Websites</a></li>
<li><a href="../de432742/index.html">Zeitdruck der Unternehmen</a></li>
<li><a href="../de432744/index.html">DWDM: Die L√∂sung ist um 30-50% billiger als der Carrier (Enterprise-Klasse)</a></li>
<li><a href="../de432746/index.html">F√ºr drei Tage auf der Intensivstation oder was ist los mit dem Abschnitt Work-Life-Balance auf Mobius'18?</a></li>
<li><a href="../de432750/index.html">Die Freude von Haxe. Ein Roman mit einer vernachl√§ssigten Programmiersprache</a></li>
<li><a href="../de432752/index.html">Ameisenh√ºgel oder Festung? Ich baue ein Haus zum Preis einer Wohnung. 3 Teil. Stromversorgung</a></li>
<li><a href="../de432754/index.html">Datenspeicherung In-Memory und On-Disk werden der √ñffentlichkeit zug√§nglich gemacht</a></li>
<li><a href="../de432756/index.html">Wir implementieren Barrierefreiheitsunterst√ºtzung, ohne die visuelle Komponente der mobilen Anwendung zu √§ndern</a></li>
<li><a href="../de432760/index.html">Vektorproduktansichten oder eine andere Verwendung des Word2Vec-Modells</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>