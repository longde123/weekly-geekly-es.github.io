<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëºüèΩ üíÜüèª üåï O cache √© o rei do desempenho: os processadores precisam de um quarto n√≠vel de cache üîØ üë©üèø‚Äçü§ù‚Äçüë®üèª üì™</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A diferen√ßa entre a velocidade dos processadores no sentido geral e a velocidade da DRAM principal, tamb√©m no sentido geral, tem sido um problema nos ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O cache √© o rei do desempenho: os processadores precisam de um quarto n√≠vel de cache</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/485374/"><img src="https://habrastorage.org/getpro/habr/post_images/2ac/73c/219/2ac73c219095e9b2a9f772e767ba9cb7.jpg"><br><br>  A diferen√ßa entre a velocidade dos processadores no sentido geral e a velocidade da DRAM principal, tamb√©m no sentido geral, tem sido um problema nos √∫ltimos 30 anos - durante esse per√≠odo, a diferen√ßa come√ßou a realmente crescer.  E vale a pena dizer honestamente que os engenheiros que desenvolveram o equipamento e os programas que criaram a hierarquia de cache e o software que poderiam tirar proveito disso foram brilhantes.  Essa √© uma das arquiteturas mais dif√≠ceis j√° concebidas pelo homem. <br><br>  No entanto, agora que estamos √† beira do surgimento de uma hierarquia cada vez maior de mem√≥ria, quando a mem√≥ria n√£o vol√°til, como o Optane 3D XPoint (uma variante de mem√≥ria com altera√ß√£o de fase) nos formatos DIMM e SSD, come√ßa a aparecer, bem como novos protocolos (CXL, OpenCAPI, CCIX, NVLink e Gen-Z), surge a pergunta: √© hora de adicionar um cache de quarto n√≠vel aos servidores?  Como o trabalho de um n√∫mero t√£o grande de dispositivos depende do complexo da CPU - alguns est√£o mais pr√≥ximos e outros mais distantes - √© l√≥gico pensar se precisamos de outro n√≠vel de cache que oculte os atrasos desses outros tipos de mem√≥ria e aumente a taxa de transfer√™ncia de todo o sistema. <br><a name="habracut"></a><br>  Para apresentar as oportunidades, vasculhamos nossa pr√≥pria mem√≥ria e, ao mesmo tempo, conversamos com desenvolvedores de arquitetura de chips da IBM, Intel, AMD e Marvell para entender o que eles pensam sobre o uso do cache L4 nos servidores.  O cache L4, √© claro, n√£o √© uma palavra nova em velocidade, mas n√£o √© t√£o comum em arquiteturas de sistema. <br><br>  No entanto, antes de analisarmos a hist√≥ria do problema. <br><br>  A adi√ß√£o de um cache de primeiro n√≠vel aos processadores que tinham apenas um n√∫cleo no momento foi um comprometimento na d√©cada de 1980 que adicionou lat√™ncia aos subsistemas de mem√≥ria e reduziu a lat√™ncia m√©dia das solicita√ß√µes e instru√ß√µes de dados dos processadores.  Os caches L1 estavam originalmente localizados na SRAM externa localizada nas placas-m√£e e conectadas ao complexo de mem√≥ria da CPU.  Esse cache L1 estava muito pr√≥ximo do processador, tanto em termos de frequ√™ncia de clock quanto em termos de espa√ßo f√≠sico na placa, e tornou poss√≠vel aumentar a carga da CPU.  Em seguida, esses caches foram divididos para que os dados usados ‚Äã‚Äãcom freq√º√™ncia pudessem ser armazenados em um bloco, e as instru√ß√µes populares no segundo, e esse desempenho aumentou ligeiramente.  Em algum momento do aumento da velocidade do clock do processador e da diferen√ßa correspondente na velocidade da CPU e DRAM, foram adicionados caches L2 mais gordos, mas tamb√©m mais lentos (mas mais baratos em termos de largura de banda), novamente no in√≠cio eles estavam fora do gabinete da CPU, mas ent√£o integrado a ele.  E quando mais e mais n√∫cleos come√ßaram a ser adicionados √† CPU, assim como mais e mais controladores DRAM para carreg√°-los, blocos de cache L3 ainda maiores foram adicionados √† hierarquia. <br><br>  Na maior parte, esse sistema funcionou muito bem.  Em alguns circuitos da CPU, vemos at√© algumas regras pr√°ticas que refletem os n√≠veis da hierarquia de cache, o que nos permitir√° estimar as possibilidades associadas ao quarto n√≠vel. <br><br>  Chris Gianos, engenheiro de chips e arquiteto da Intel que liderou o desenvolvimento de muitas gera√ß√µes passadas de processadores Xeon, explica o seguinte: ‚ÄúCom cada n√≠vel de cache, geralmente precisamos crescer o suficiente em rela√ß√£o ao n√≠vel anterior para fazer tudo isso valer a pena, porque para obter um aumento not√°vel no desempenho do sistema, √© necess√°rio obter uma frequ√™ncia bastante interessante de chamadas bem-sucedidas.  Se voc√™ "cair" em dados armazenados em cache em apenas alguns por cento dos casos, ser√° dif√≠cil perceber.  Tudo o mais diminui sua velocidade, e esse aumento ser√° impercept√≠vel.  Portanto, s√£o necess√°rios caches relativamente grandes e, quando se trata de n√≠veis mais altos, s√£o necess√°rios caches realmente grandes.  Hoje, L2 √© medido em megabytes, L3 √© medido em dezenas ou centenas de megabytes.  Portanto, fica claro que, se voc√™ come√ßar a pensar no cache L4, falaremos sobre centenas de megabytes, se n√£o gigabytes.  E esse tamanho definitivamente levar√° ao seu alto custo.  √â necess√°rio que existam certas condi√ß√µes, para que essa op√ß√£o se torne interessante e certamente n√£o ser√° barata. ‚Äù <br><br>  Os engenheiros da AMD que entrevistamos desejavam permanecer an√¥nimos porque n√£o queriam dar a impress√£o de que a empresa iria adicionar o cache L4 √† linha de processadores Epyc - e, para ser mais preciso, a AMD n√£o prometeu nada disso.  Entretanto, a empresa reconhece que este √© o pr√≥ximo passo √≥bvio a considerar e, assim como a Intel, acredita que todos os engenheiros est√£o pensando em implementar o cache L4.  Em ess√™ncia, a AMD diz que as compensa√ß√µes associadas aos n√≠veis e lat√™ncias de cache foram extensivamente estudadas tanto na ind√∫stria quanto na academia, e que a cada novo n√≠vel maior e mais lento que o anterior, existe uma compensa√ß√£o no aumento do caminho geral para a DRAM.  Isso tamb√©m √© indicado pela Intel Gianos, falando sobre a necessidade de encontrar um equil√≠brio entre solicita√ß√µes de cache bem-sucedidas e seu volume. <br><br>  A IBM, √© claro, adicionou cache L4 a alguns de seus chipsets X86 nos anos 2000 e, nos anos 2010, adicionou L4 aos chipsets NUMA ( <a href="https://ru.wikipedia.org/wiki/Non-Uniform_Memory_Access" rel="nofollow">acesso desigual √† mem√≥ria</a> ) nos mainframes do System z11.  O processador z11 possui quatro n√∫cleos, cache L1 de 64 KB para obter instru√ß√µes e cache L1 de 128 KB para dados, al√©m de cache L2 de 1,5 MB para cada n√∫cleo e cache compartilhado L3 de 24 MB para todos os n√∫cleos.  O chipset NUMA para z10 tinha dois bancos de 96 MB de cache L4, ou seja, 192 MB no total.  Com o lan√ßamento do z12, a IBM reduziu o tamanho do cache L1 para 98 KB por n√∫cleo, mas aumentou o cache L2 para 2 MB por n√∫cleo, dividindo-o em duas partes, para instru√ß√µes e dados, como no caso de L1.  Ela tamb√©m dobrou o tamanho do cache L3 para 48 MB em seis n√∫cleos, e o tamanho do cache L4 foi aumentado para 384 MB para um par de chips no chipset.  √Ä medida que as gera√ß√µes de processadores System z mudam, os tamanhos de cache aumentam e, para os processadores z15 anunciados em setembro, um par de caches L1 pesar√° 128 KB cada, um par de caches L2 pesar√° 4 MB cada e um cache L3 compartilhado para 256 n√∫cleos ter√° capacidade de 256 MB.  O cache L4 em cada compartimento de mainframe √© de 960 MB e seu volume total para todo o sistema, que consiste em cinco compartimentos, √© de 4,68 GB. <br><br>  Como <a href="https://www.nextplatform.com/2018/08/28/ibm-power-chips-blur-the-lines-to-memory-and-accelerators/" rel="nofollow">mencionamos anteriormente</a> , os processadores Power8 e Power9 t√™m buffer de mem√≥ria e a IBM adicionou cache de 16 MB L4 a cada buffer do Centaur, que √© cache de 128 MB L4 por soquete para 32 slots de mem√≥ria.  As m√°quinas Power9 mais baratas n√£o possuem um buffer de mem√≥ria e, portanto, nenhum cache L4.  Os arquitetos que desenvolveram o circuito Power10 estavam ocupados desenvolvendo o circuito para o Power11 e, portanto, n√£o puderam responder nossas perguntas, mas William Stark, que gerenciou o desenvolvimento do Power10, encontrou um pouco de tempo para n√≥s e observou o seguinte: <br><br>  "Em geral, chegamos √† conclus√£o de que caches de alto n√≠vel do √∫ltimo n√≠vel s√£o √∫teis para aumentar a velocidade dos sistemas industriais", explicou Stark, por email.  "A alta lat√™ncia associada √† mem√≥ria n√£o vol√°til, em particular √† mem√≥ria de estado de fase, gera uma solicita√ß√£o de cache - possivelmente para um cache do tipo L4 - na hierarquia da mem√≥ria de armazenamento". <br><br>  Isso √© exatamente o que pensamos.  E, a prop√≥sito, n√£o afirmamos que o cache L4 esteja necessariamente pr√≥ximo da mem√≥ria buffer do futuro DDR5 DIMM.  Talvez seja melhor coloc√°-lo entre o cache do processador PCI-Express e L3 e, melhor ainda, nos buffers de mem√≥ria e entre o cache do processador PCI-Express e L3.  Talvez tenha que ser colocado em cima do controlador de E / S e da mem√≥ria na futura arquitetura do servidor, que √© um pouco como <a href="https://www.nextplatform.com/2018/12/13/intel-bets-heavily-on-chip-stacking-for-the-future-of-compute/" rel="nofollow">a tecnologia Foveros da Intel</a> . <br><br>  √â poss√≠vel analisar isso de um ponto de vista diferente - por exemplo, a IBM teve a oportunidade de alterar o tamanho do cristal e os engenheiros decidiram adicionar o cache L4 ao barramento System z NUMA ou ao chip de buffer de mem√≥ria Power8 e Power9, n√£o por si s√≥, mas simplesmente porque eles ainda tiveram a oportunidade de adicionar transistores depois que todas as fun√ß√µes necess√°rias foram implementadas.  √Äs vezes, parece-nos que o n√∫mero de n√∫cleos nos processadores Intel X86 depende do tamanho do cache L3 que eles podem pagar.  √Äs vezes, parece que a Intel atribui o tamanho m√°ximo do cache L3 a um cristal e, depois disso, cristais Xeon de tr√™s tamanhos diferentes s√£o simplesmente feitos de acordo com essas especifica√ß√µes - nas √∫ltimas gera√ß√µes eles t√™m 10, 18 ou 28 n√∫cleos em um processo de fabrica√ß√£o de 14 nm. <br><br>  Tudo isso, √© claro, s√£o quest√µes puramente acad√™micas, mas elas nos d√£o a motiva√ß√£o potencial para a IBM e outros fabricantes de chipsets adicionarem o cache L4.  Isso n√£o apenas pode ajudar em alguns casos, mas √© uma coisa bastante √≥bvia.  Acreditamos que em um monstro de E / S como o mainframe System z, o cache L4 est√° em seu lugar sem perguntas e beneficia todos os clientes, aumentando a taxa de transfer√™ncia dessas m√°quinas e permitindo que trabalhem com 98-99% da carga do processador, desde quantos n√∫cleos , e a escala do NUMA nos mainframes aumentou muito recentemente. <br><br>  N√£o h√° raz√£o para fazer o cache L4 exclusivamente na DRAM interna (como a IBM faz com seus chips) ou com base em uma SRAM muito mais cara - √© disso que Rabin Sugumar, arquiteto de chips da Cray Research, Sun Microsystems, Oracle, Broadcom nos lembra. , Cavium e Marvell: <br><br>  "Nossos caches L3 j√° s√£o grandes o suficiente", diz Sugumar.  - Ent√£o o L4, no caso de seu interesse, precisa ser feito usando uma tecnologia diferente.  Talvez eDRAM ou mesmo HBM ou DRAM.  Nesse contexto, uma implementa√ß√£o do cache L4 baseado no HBM parece uma op√ß√£o interessante, e esse cache n√£o resolve tanto o problema de lat√™ncia quanto a largura de banda.  Como a capacidade da HBM √© limitada e a largura de banda √© grande, podemos obter um certo aumento de velocidade - e em alguns casos especiais, realmente vemos um aumento significativo na largura de banda ".  Sugumar acrescenta que, para um n√∫mero bastante grande de aplicativos, √© observado um n√∫mero relativamente grande de falhas de cache.  No entanto, voc√™ precisa calcular se a adi√ß√£o do pr√≥ximo n√≠vel de cache valer√° a pena. <br><br>  Outro caso de uso poss√≠vel para algo como o cache L4, diz Sugumar, √© usar a DRAM local como cache.  ‚ÄúN√≥s n√£o realizamos nenhum desses estudos em laborat√≥rio, mas suponhamos que tenhamos uma interface de alta largura de banda no chip, conectada a uma mem√≥ria compartilhada em algum lugar do outro lado do loop, a uma dist√¢ncia de 500 ns a 1 Œºs.  Um dos casos de uso ser√° criar um cache que mova esses dados da mem√≥ria compartilhada para a DRAM local.  Voc√™ pode imaginar o trabalho da m√°quina de estado gerenciando essa mem√≥ria; portanto, na maioria das vezes, as chamadas ser√£o direcionadas para a DRAM local e voc√™ pode minimizar o n√∫mero de chamadas para a DRAM distribu√≠da geral ". <br><br>  Esta op√ß√£o nos parece um tipo muito interessante de NUMA.  A prop√≥sito, Sugumar estava trabalhando na mem√≥ria distribu√≠da para sistemas paralelos de alta velocidade na Sun Microsystems, mesmo antes do surgimento da mem√≥ria n√£o vol√°til.  E um dos problemas com essas diferentes variantes da hierarquia de mem√≥ria era que, se uma delas se perder devido a uma falha na rede ou no barramento, a m√°quina inteira falhar√°.  "Nos sistemas de mem√≥ria distribu√≠da, as falhas de rede precisam ser tratadas com mais eleg√¢ncia, e isso causa muitos desafios de design". <br><br>  Outro ponto √© que queremos que qualquer cache de alto n√≠vel, nem mesmo o L4, seja realizado ao m√°ximo com a ajuda do ferro e com o m√≠nimo com a ajuda do software.  Os kernels do sistema operacional e outros softwares sempre precisam de algum tempo para acompanhar o hardware, seja adicionando novos kernels ou caches L3 ou L4 ou mem√≥ria n√£o vol√°til endere√ß√°vel. <br><br>  "Em algum momento, um n√≠vel extra de cache se tornar√° inevit√°vel", diz Gianos.  - Chegamos ao primeiro n√≠vel de cache e, em algum momento, o segundo apareceu.  E ent√£o finalmente adicionamos um terceiro.  E um dia teremos um quarto.  A √∫nica quest√£o √© quando e por qu√™.  E parece-me que suas observa√ß√µes sobre os recursos desse cache s√£o bastante interessantes.  Mas a Intel ainda n√£o decidiu quando ou por que essas coisas ser√£o tornadas p√∫blicas.  Outras empresas tamb√©m est√£o estudando esse problema;  seria tolice n√£o examin√°-lo.  Cedo ou tarde isso acontecer√°, mas logo ser√°, ou n√£o ser√° muito - ainda n√£o est√° claro. ‚Äù </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt485374/">https://habr.com/ru/post/pt485374/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt485358/index.html">O desenvolvimento m√≥vel √© f√°cil e chato? Relat√≥rio Yandex</a></li>
<li><a href="../pt485362/index.html">Inova√ß√µes do JavaScript ES2020 com exemplos simples</a></li>
<li><a href="../pt485364/index.html">Como parar de desperdi√ßar tempo dos desenvolvedores com d√≠vidas t√©cnicas</a></li>
<li><a href="../pt485370/index.html">Como um desenvolvedor pode ajudar um gerente a fazer um acordo</a></li>
<li><a href="../pt485372/index.html">Sobre o imut√°vel: hist√≥ria do 9¬∫ lugar da Copa da AI russa 2019</a></li>
<li><a href="../pt485376/index.html">Como tornar o frontend tr√™s vezes mais r√°pido e quando aplicar comandos em vez de reposit√≥rios? V√≠deo</a></li>
<li><a href="../pt485378/index.html">Estudo de caso: como se destacar no Google Play e adaptar o ASO a diferentes pa√≠ses</a></li>
<li><a href="../pt485380/index.html">Artesanato e sucesso em TI</a></li>
<li><a href="../pt485384/index.html">NeurIPS 2019: tend√™ncias de ML que estar√£o conosco na pr√≥xima d√©cada</a></li>
<li><a href="../pt485386/index.html">Micronavegadores est√£o por toda parte. Mas o que sabemos sobre eles?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>