<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔂 👩🏻‍🚒 🏇 Rahasia toleransi kesalahan kantor depan kami 🤞 🧟 🥁</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bagaimana bank modern diatur? Ada kantor di mana berbagai operasi dilakukan, akun dan laporan disimpan. Ada kantor pusat di mana keputusan dibuat dan ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Rahasia toleransi kesalahan kantor depan kami</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/419815/">  Bagaimana bank modern diatur?  Ada kantor di mana berbagai operasi dilakukan, akun dan laporan disimpan.  Ada kantor pusat di mana keputusan dibuat dan risiko dinilai, di mana risiko kredit dinilai dan penipu dilawan.  Dan ada kantor depan di mana mereka melayani pelanggan dan bertanggung jawab atas interaksi mereka dengan bank melalui berbagai saluran. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aad/a71/55c/aada7155cf378d6d91c3b52a528b4384.png"><br><br>  Sberbank memiliki ratusan sistem dengan ketersediaan dan keandalan yang beragam.  Ini memiliki pengembangan sendiri, dan solusi kotak dengan berbagai tingkat penyesuaian, SLA berbeda.  Semua sistem terintegrasi satu sama lain dalam banyak cara.  Dalam posting ini, kami akan memberi tahu Anda bagaimana seluruh bukit semut front-end ini dibangun sedemikian rupa untuk menyediakan layanan pelanggan yang berkelanjutan. <br><a name="habracut"></a><br>  Mari kita mulai dengan teorinya.  Prinsip-prinsip utama dimana sistem gagal-aman dibangun dapat dipinjam dari kapal selam: <br><br><ol><li>  Kapal selam ini dibagi menjadi kompartemen independen.  Jika satu kompartemen terendam, sisanya masih bertahan. <br></li><li>  Semua komponen penting dicadangkan.  Mesin, tangki oksigen.  Dan The Beatles juga memesan periskop dengan lubang intip. <br></li><li>  Kapal selam dilindungi dari kondisi kritis di permukaan - jika perlu, itu bisa masuk lebih dalam dan bekerja di sana seolah-olah tidak ada yang terjadi. <br></li></ol><br>  Kami menggambarkan prinsip pertama dengan contoh dari praktik kami.  Kami memiliki sistem cache terdistribusi.  Dan begitu dimuat, salah satu node data cache gagal.  Tidak apa-apa: untuk mempertahankan replikasi yang tepat, controller mendistribusikan kembali data ke node yang tersisa.  Tetapi sebagai akibat dari redistribusi, lalu lintas jaringan melonjak dan paket-paket mulai hilang - termasuk overhead cache.  Pada satu titik, controller memutuskan bahwa node data lain gagal, mendistribusikan kembali data lagi, lalu lintas meningkat ... Akibatnya, dalam waktu kurang dari satu menit sistem turun sepenuhnya.  Untungnya, itu adalah sirkuit beban dan tidak ada yang terluka.  Tapi kami menghabiskan banyak waktu mencari penyebabnya. <br><br>  Dapat diperdebatkan bahwa ini tidak terjadi dengan database berkerumun dan server high-end - ada redundansi dibangun di tingkat perangkat keras.  Mengutip Werner Vogels, CTO Amazon: "Semuanya gagal sepanjang waktu."  Kami jatuh dan cluster basis data, dan server kelas atas.  Jatuh karena kesalahan konfigurasi, karena masalah dalam perangkat lunak manajemen.  Dengan solusi dari setiap masalah, kepercayaan kami pada solusi tersebut menurun.  Sebagai hasilnya, kami sampai pada kesimpulan: hanya sistem yang dibagi menjadi beberapa bagian yang independen satu sama lain - terutama independen dalam manajemen - tidak menolak. <br><br><h2>  Arsitektur multi-blok </h2><br>  Solusi untuk masalah kami adalah arsitektur multi-blok.  Di dalamnya, semua komponen perangkat keras, termasuk basis data, dibagi menjadi beberapa blok yang bebas dan hampir bebas.  Setiap blok melayani sebagian dari klien, seperti ketika sharding di database.  Node dalam setiap blok dicadangkan di semua level, termasuk geo-redundansi.  Masalah apa pun dalam satu blok tidak memengaruhi yang lain.  Dengan peningkatan jumlah pelanggan, kami dapat dengan mudah menambahkan blok baru dan terus bekerja secara normal. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/066/cb5/6c5/066cb56c5cf947823870e31d00c89403.png"><br>  <i>Arsitektur blok umum.</i>  <i>Semua blok dicadangkan sesuai dengan skema 2N.</i>  <i>Setiap pusat data memiliki penyeimbang beban perangkat keras yang kuat.</i>  <i>Pusat data dihubungkan oleh 2-3 saluran komunikasi independen.</i> <br><br>  Server didistribusikan dalam blok lima jenis: <br><br><ul><li>  Router - unit kontrol yang mendistribusikan klien ke seluruh unit <br></li><li>  Blok klien - blok utama yang melayani hingga 10 juta klien <br></li><li>  Blok percontohan - di sini kami menguji versi aplikasi baru pada pelanggan setia (sekitar 300 ribu orang, terutama karyawan Sberbank) <br></li><li>  Unit tamu - pengguna yang tidak diauthentikasi dilayani melaluinya;  mereka, misalnya, yang datang melalui situs <br></li><li>  Blok cadangan - blok pengaman yang cukup kuat untuk menggantikan dua blok klien sekaligus <br></li></ul><br>  Di dalam setiap blok, server aplikasi dan server web dipisahkan oleh saluran layanan, tetapi basis datanya umum.  Jadi kami dapat mengisolasi skenario kegagalan yang paling umum sehingga tidak melampaui batas saluran kami. <br><br><h2>  Bagaimana cara kerjanya? </h2><br>  Pertama, pengguna memasuki unit router.  Blok ini memeriksa di mana klien memblokir orang tersebut dan mengirimkannya ke sana (atau ke blok tamu).  Selanjutnya, seseorang dengan tenang bekerja di dalam bloknya.  Jika terjadi kegagalan pada unit asli, orang tersebut kembali ke router dan secara otomatis menerima arahan ke unit cadangan untuk pekerjaan lebih lanjut. <br><br>  Apa yang terjadi pada data selama operasi?  Informasi tentang interaksi klien dengan bank terus direplikasi dari blok klien ke database arsip.  Setelah bertemu dengan pengguna, unit cadangan mengambil informasi yang diperlukan tentang dia dari database arsip dan, jika perlu, menyediakan data - sehingga pengguna tidak membeku ketika masalah muncul di pihak kami. <br><br>  Operasi yang dilakukan di unit cadangan disimpan di dalamnya.  Ketika blok klien asli pengguna dipulihkan, itu kembali.  Operasi yang terakumulasi dalam blok cadangan ditransfer secara tidak sinkron ke blok klien yang diperlukan.  Sementara data direduksi menjadi konsistensi, klien melihat pesan yang menyatakan bahwa semua operasi telah diterima dan disimpan, tetapi karena pekerjaan teknis, operasi terbaru mungkin tidak ditampilkan. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a98/da5/362/a98da53620d3a2d57119975d2872e156.png"><br>  <i>Skema umum sistem</i> <br><br>  Dalam beberapa kasus, beralih ke blok cadangan direncanakan di muka - misalnya, ketika memperbarui di blok klien.  Kemudian unit cadangan tidak mengambil sesi klien, tetapi pada saat tertentu hanya memulai semua operasi baru.  Jika Anda perlu segera beralih ke unit cadangan, administrator dapat menonaktifkan semua sesi.  Dalam hal ini, sesi pengguna akan terganggu, dan ia akan memulai yang baru pada unit cadangan.  Unit router, omong-omong, memiliki unit cadangan khusus sendiri.  Jadi tidak ada yang tersisa tanpa roda cadangan. <br><br><h2>  Pembaruan sistem </h2><br>  Versi perangkat lunak baru digunakan terlebih dahulu di blok uji coba dan ditampilkan kepada khalayak terbatas.  Kemudian secara bertahap pada blok klien, dan sudah pada akhirnya - pada yang cadangan.  Jadi jika ada masalah di blok klien dengan versi baru dari perangkat lunak, kami dapat mentransfer klien ke blok cadangan dari yang lama. <br><br>  Ketika fungsi baru bergulir ke blok, itu tidak menyala secara otomatis.  Administrator melakukan ini menggunakan kotak centang - fitur toggle.  Anda dapat mengalihkan klien ke versi baru dengan grup - beginilah cara kami memeriksa reaksi pembaruan terhadap pertumbuhan pemirsa. <br><br><h2>  Otonomi </h2><br>  Sistem kami sendiri dapat diandalkan, tetapi masih tergantung pada backend yang digunakan untuk operasi.  Bagaimana cara melindungi diri dari masalah?  Kami menggunakan tiga alat. <br><br><ol><li>  <i>Permintaan yang tertunda</i> .  Klien meminta operasi untuk diselesaikan.  Kami menyimpannya di database kami dan mencoba menjalankannya di backend.  Jika backend tidak merespons, kami menunjukkan kepada klien pesan bahwa operasi telah diterima dan sedang diproses.  Ketika backend naik, "buruh pelabuhan" yang terpisah membaca operasi yang tidak lengkap dari database, dan "mendorong" mereka ke dalam batch dalam sistem backend.  Agar tidak membebani tabel utama dengan operasi dengan sejumlah besar kueri efisien rendah, kami juga menggunakan tabel token yang disebut - daftar pengidentifikasi operasi tidak lengkap.  Agar tidak menjatuhkan backend yang baru saja naik ratusan ribu operasi, kami menggunakan batching - kami menjatuhkan dua ratus operasi dan menunggu, misalnya, selama beberapa detik. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cd3/af8/261/cd3af826195c4c177e55a7bdbbd2d786.png"><br><br>  Tetapi bagaimana jika perubahan penting terjadi antara permintaan pengguna dan pemulihan backend?  Misalnya, sudahkah nilai tukar bergerak?  Dalam hal ini, verifikasi ganda dipicu.  Operasi-operasi ini disimpan saat masuk, dan kemudian diverifikasi pada saat dieksekusi.  Jika sesuatu tidak konvergen, operasi akan disesuaikan atau ditolak. <br></li><li>  <i>Caching data</i> .  Ketika seorang pengguna masuk, misalnya, Sberbank Online, semua informasi yang diperlukan tentang dirinya sudah terlihat - tagihan, kartu, pinjaman, dll.  Data ini diminta melalui bus layanan dari selusin sistem.  Jika respons dikumpulkan dengan cepat, dalam beberapa detik, kami menampilkan data kepada klien dan menyimpannya dalam cache sistem dari basis data kami.  Jika tidak, maka kami mencari di dalam database untuk data yang sebelumnya di-cache dan menunjukkannya kepada klien.  Tentu saja, untuk ini, cache harus tidak lebih dari usia tertentu.  Namun, ketika bus layanan mengumpulkan data yang diperlukan berdasarkan permintaan, bus layanan diperbarui dalam cache database dan dikirim ke klien alih-alih yang lebih lama. <br><br>  Saat menggunakan aplikasi, ini berarti seseorang akan melihat status akunnya beberapa detik setelah masuk.  Meskipun data mungkin agak ketinggalan jaman.  Jika ini terjadi, maka setelah beberapa detik, data biasanya diganti dengan yang sebenarnya - yang berarti bahwa service bus telah mengumpulkan semua yang Anda butuhkan. <br><br>  Selain itu, kami memiliki pra-caching menggunakan replikasi.  Sebagian besar untuk data referensi yang berbeda.  Kami memasukkan data ini ke backend, klien dengan tenang membuat permintaan untuk operasi, dan kami mengirimkannya.  Bahkan jika sistem yang bertanggung jawab untuk menjaga data tidak berfungsi, pengguna tidak harus menunggu lagi. <br></li><li>  <i>Jeda teknis</i> .  Jika sistem backend macet atau sedang dalam pemeliharaan, kami menandainya.  Dan kemudian operasi yang melewatinya segera dipenuhi oleh penolakan.  Jadi kami menyimpan server aplikasi dari meluap dengan permintaan menunggu respons dengan batas waktu.  Dalam mode ini, caching operasi dan data yang kami jelaskan sebelumnya dapat digunakan.  Istirahat teknis ditetapkan untuk setiap skenario integrasi, secara manual oleh administrator atau secara otomatis, berdasarkan jumlah permintaan. <br></li></ol><br><br><img src="https://habrastorage.org/getpro/habr/post_images/ce4/643/7d5/ce46437d574e8a2353f82f3856c2648f.png"><br><br>  Dalam hal apa pun, kami berupaya meminimalkan ekspektasi pengguna - jika ada masalah, ia segera menerima pesan tentang ketidakmungkinan operasi.  Kami mencoba meminimalkan jumlah pesan seperti itu, oleh karena itu kami meningkatkan masa pakai beberapa data yang di-cache - ini memungkinkan kami untuk memperluas interaksi normal dengan layanan bank. <br><br>  Dalam beberapa skenario, caching tidak boleh dibawa pergi - misalnya, ketika mengeluarkan uang tunai.  Mungkin penipuan pada bagian dari klien.  Operasi seperti itu di ATM dan cabang tidak di-cache.  Di bank Internet, ini lebih mudah - kami menerima aplikasi, lalu memprosesnya atau menolaknya. <br><br>  Akibatnya, dengan mengamati prinsip-prinsip yang dijelaskan dalam artikel, Anda bisa mendapatkan sistem dengan ketersediaan 99,99% dan lebih tinggi. <br><br><h2>  Rencana kami </h2><br>  Sekarang rencananya adalah untuk meminimalkan waktu-ke-pasar sistem tunggal kami, untuk memastikan omnichannelity, dengan mempertimbangkan fitur-fitur teknis dan bisnis saluran.  Dan juga memigrasi sistem lawas sambil mempertahankan operabilitasnya selama beraktivitas. <br><br>  <i>Kami berterima kasih kepada Roman Shekhovtsov untuk bantuan aktif dalam persiapan posting</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id419815/">https://habr.com/ru/post/id419815/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id419803/index.html">WANHAO DUPLICATOR 8 Mini Review</a></li>
<li><a href="../id419805/index.html">The Super Tiny Compiler - sekarang dalam bahasa Rusia</a></li>
<li><a href="../id419807/index.html">Glaukoma - bagaimana tidak menjadi buta: mari kita bicara tentang perawatan ...</a></li>
<li><a href="../id419811/index.html">Evolusi tampilan yang fleksibel</a></li>
<li><a href="../id419813/index.html">Webinar Skillbox: pilihan hari Jumat</a></li>
<li><a href="../id419817/index.html">Meluncurkan Cluster RabbitMQ di Kubernetes</a></li>
<li><a href="../id419819/index.html">Biomarker penuaan. Panel Frailty. Bagian 2</a></li>
<li><a href="../id419823/index.html">Duet yang tidak biasa - frasa sandi dan gambar mnemonik</a></li>
<li><a href="../id419825/index.html">Menguji kinerja beberapa jenis drive di lingkungan virtual</a></li>
<li><a href="../id419829/index.html">Enkripsi kunci default OpenSSH lebih buruk daripada tidak sama sekali</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>