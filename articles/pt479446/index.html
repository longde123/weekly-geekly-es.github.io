<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äç‚öñÔ∏è üí∏ üëÜüèΩ Os carros j√° est√£o √† frente das pessoas nos testes de leitura; mas eles entendem o que l√™em? üßîüèΩ üç° üëÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Uma ferramenta chamada BERT √© capaz de ultrapassar pessoas em testes de leitura e compreens√£o. No entanto, tamb√©m demonstra para que lado a IA ainda p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Os carros j√° est√£o √† frente das pessoas nos testes de leitura; mas eles entendem o que l√™em?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/479446/"><h3>  Uma ferramenta chamada BERT √© capaz de ultrapassar pessoas em testes de leitura e compreens√£o.  No entanto, tamb√©m demonstra para que lado a IA ainda precisa seguir. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/1e6/359/58b/1e635958bf5af44bcc9b6256e1a0101f.jpg"><br><br>  No outono de 2017, <a href="http://www.nyu.edu/projects/bowman/">Sam Bowman</a> , linguista computacional da Universidade de Nova York, decidiu que os computadores ainda n√£o entendem muito bem o texto.  √â claro que eles aprenderam bastante bem para simular esse entendimento em certas √°reas estreitas, como tradu√ß√µes autom√°ticas ou an√°lise de sentimentos (por exemplo, para determinar se uma frase √© "rude ou doce", como ele disse).  No entanto, Bowman queria um testemunho mensur√°vel: uma verdadeira compreens√£o do que foi escrito, delineado na linguagem humana.  E ele veio com um teste. <br><a name="habracut"></a><br>  Em um artigo de abril de 2018 escrito em colabora√ß√£o com colegas da Universidade de Washington e da DeepMind, uma empresa de propriedade do Google envolvida em intelig√™ncia artificial, Bowman apresentou um conjunto de nove tarefas de compreens√£o de leitura para computadores sob o nome geral GLUE (General Language Understanding Evaluation) [avalia√ß√£o de compreens√£o linguagem generalizada].  O teste foi projetado como "um exemplo bastante indicativo do que a comunidade de pesquisa considera tarefas interessantes", disse Bowman, mas de uma maneira "f√°cil para as pessoas".  Por exemplo, em uma tarefa, a pergunta √© feita sobre a verdade de uma frase, que deve ser estimada com base nas informa√ß√µes de uma frase anterior.  Se voc√™ pode dizer que a mensagem ‚ÄúO Presidente Trump chegou ao Iraque, tendo iniciado sua visita de sete dias‚Äù implica que ‚ÄúO Presidente Trump est√° visitando o exterior‚Äù, voc√™ passa no teste. <br><br>  Os carros falharam com ele.  At√© as redes neurais avan√ßadas obtiveram no m√°ximo 69 dos 100 pontos no total em todos os testes - os tr√™s primeiros com menos.  Bowman e seus colegas n√£o ficaram surpresos.  As redes neurais - constru√ß√µes multicamadas com conex√µes computacionais que se assemelham ao trabalho dos neur√¥nios no c√©rebro de mam√≠feros - mostram bons resultados na √°rea de "Processamento de linguagem natural", mas os pesquisadores n√£o tinham certeza de que esses sistemas aprendiam algo s√©rio sobre idioma.  E o GLUE prova isso.  "Os primeiros resultados mostram que a aprova√ß√£o nos testes do GLUE vai al√©m das capacidades dos modelos e m√©todos existentes", Bowman et al. <br><br>  Mas a avalia√ß√£o deles n√£o durou muito.  Em outubro de 2018, o Google introduziu um novo m√©todo, o BERT (Representa√ß√µes de codificadores bidirecionais de transformadores) [apresenta√ß√µes de codificadores bidirecionais para transformadores].  Ele recebeu uma pontua√ß√£o de 80,5 no GLUE.  Em apenas seis meses, os carros saltaram de tr√™s com menos para quatro com menos neste novo teste, que mede a verdadeira compreens√£o da linguagem natural pelas m√°quinas. <br><br>  ‚ÄúFoi como 'caramba'‚Äù, lembra Bowman, usando uma palavra mais colorida.  - Esta mensagem foi recebida com desconfian√ßa pela comunidade.  O BERT recebeu em muitos testes notas pr√≥ximas ao que consideramos o m√°ximo poss√≠vel. ‚Äù  De fato, antes do surgimento do BERT no teste GLUE, n√£o havia sequer avalia√ß√µes das realiza√ß√µes humanas com as quais comparar.  Quando Bowman e um de seus alunos de p√≥s-gradua√ß√£o os adicionaram ao GLUE em fevereiro de 2019, eles duraram apenas alguns meses, e o modelo baseado no BERT da Microsoft tamb√©m os <a href="https://blogs.msdn.microsoft.com/stevengu/2019/06/20/microsoft-achieves-human-performance-estimate-on-glue-benchmark/">venceu</a> . <br><br>  No momento da reda√ß√£o deste artigo, quase todos os <a href="https://gluebenchmark.com/leaderboard/">primeiros lugares</a> nos testes GLUE s√£o ocupados por sistemas que incluem, estendem ou otimizam o modelo BERT.  Cinco deles s√£o superiores em habilidades humanas. <br><br>  Mas isso significa que a IA est√° come√ßando a entender nossa linguagem ou est√° apenas aprendendo a vencer nossos sistemas?  Depois que as redes neurais baseadas no BERT fizeram os testes do tipo GLUE, surgiram novos m√©todos de avalia√ß√£o que consideravam esses sistemas de PNL vers√µes de computador do ‚Äú <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D0%25BC%25D0%25BD%25D1%258B%25D0%25B9_%25D0%2593%25D0%25B0%25D0%25BD%25D1%2581">smart Hans</a> ‚Äù <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D0%25BC%25D0%25BD%25D1%258B%25D0%25B9_%25D0%2593%25D0%25B0%25D0%25BD%25D1%2581">,</a> um cavalo que viveu no in√≠cio do s√©culo XX e era supostamente inteligente o suficiente para fazer c√°lculos aritm√©ticos na mente, mas, na verdade, ler os sinais inconscientes dados a ele por seu propriet√°rio. <br><br>  "Sabemos que estamos em algum lugar na zona cinzenta entre entender o idioma em um sentido muito chato e estreito e criar a IA", disse Bowman.  - Em geral, a rea√ß√£o dos especialistas pode ser descrita da seguinte forma: Como isso aconteceu?  O que isso significa?  O que devemos fazer agora? <br><br><h2>  Escrevendo suas pr√≥prias regras </h2><br>  No famoso experimento de pensamento ‚Äú <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B8%25D1%2582%25D0%25B0%25D0%25B9%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D0%25BA%25D0%25BE%25D0%25BC%25D0%25BD%25D0%25B0%25D1%2582%25D0%25B0">Sala Chinesa</a> ‚Äù, uma pessoa que n√£o conhece o idioma chin√™s fica em uma sala cheia de muitos livros com regras.  Nos livros, voc√™ pode encontrar as instru√ß√µes exatas sobre como aceitar a sequ√™ncia de caracteres chineses que entram na sala e dar uma resposta adequada.  Uma pessoa do lado de fora palma perguntas escritas em chin√™s embaixo da porta da sala.  A pessoa de dentro se volta para os livros com as regras e formula respostas perfeitamente razo√°veis ‚Äã‚Äãem chin√™s. <br><br>  Esse experimento foi usado para provar que, apesar da impress√£o externa, n√£o se pode dizer que a pessoa na sala entende alguma coisa de chin√™s.  No entanto, mesmo uma simula√ß√£o de entendimento era uma meta aceit√°vel da PNL. <br><br>  O √∫nico problema √© a falta de livros perfeitos com regras, porque a linguagem natural √© muito complexa e sistem√°tica para ser reduzida a um conjunto s√≥lido de especifica√ß√µes.  Tomemos, por exemplo, a sintaxe: regras (incluindo emp√≠ricas) que determinam o agrupamento de palavras em senten√ßas significativas.  A frase " <a href="https://books.google.com/books%3Fid%3D55YaAAAAIAAJ%26dq%3Dcolorless%2Bgreen%2Bideas%2Bsleep%2Bfuriously">adormecendo violentamente id√©ias verdes incolores</a> " tem a sintaxe, mas qualquer pessoa que conhece o idioma entende sua falta de sentido.  Que livro de regras especialmente projetado poderia incluir esse fato n√£o escrito relacionado √† linguagem natural - para n√£o mencionar outros fatos? <br><br>  Os pesquisadores da PNL tentaram encontrar essa <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B2%25D0%25B0%25D0%25B4%25D1%2580%25D0%25B0%25D1%2582%25D1%2583%25D1%2580%25D0%25B0_%25D0%25BA%25D1%2580%25D1%2583%25D0%25B3%25D0%25B0">quadratura do c√≠rculo</a> , for√ßando as redes neurais a escrever seus pr√≥prios livros de regras artesanais no processo dos chamados  "Pr√©-treinamento" ou pr√©-treinamento. <br><br>  At√© 2018, uma das principais ferramentas de treinamento era algo como um dicion√°rio.  Este dicion√°rio usou uma <a href="https://ru.wikipedia.org/wiki/%25D0%2592%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D1%2581%25D1%2582%25D0%25B0%25D0%25B2%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581%25D0%25BB%25D0%25BE%25D0%25B2">representa√ß√£o vetorial das palavras</a> [incorpora√ß√£o de palavras], descrevendo as conex√µes entre palavras na forma de n√∫meros, para que as redes neurais pudessem perceber essas informa√ß√µes como entrada - algo como um gloss√°rio aproximado para uma pessoa em uma sala chinesa.  No entanto, os pr√©-treinados na rede neural do dicion√°rio de vetores ainda permaneciam cegos para o significado das palavras no n√≠vel da frase.  "Do ponto de vista dela, as frases 'homem mordeu o cachorro' e 'cachorro mordeu o homem' s√£o id√™nticas", disse <a href="http://tallinzen.net/">Tel Linsen</a> , linguista computacional da Universidade Johns Hopkins. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/55a/245/f48/55a245f4866cf5ffcf2689d2161f8536.jpg" width="50%"><br>  <i>Tel Linsen, Linguista de Computa√ß√£o na Universidade Johns Hopkins.</i> <br><br>  O m√©todo aprimorado usa pr√©-treinamento para fornecer √† rede neural livros de regras mais ricos - n√£o apenas um dicion√°rio, mas tamb√©m sintaxe com um contexto - antes de ensin√°-lo a executar uma tarefa espec√≠fica da PNL.  No in√≠cio de 2018, pesquisadores da OpenAI, da Universidade de S√£o Francisco, do Instituto Allen de Intelig√™ncia Artificial e da Universidade de Washington, ao mesmo tempo, encontraram uma maneira complicada de se aproximar disso.  Em vez de treinar apenas uma, a primeira camada da rede usando a representa√ß√£o vetorial de palavras, os pesquisadores come√ßaram a treinar toda a rede para uma tarefa mais geral chamada modelagem de linguagem. <br><br>  "A maneira mais simples de modelar uma linguagem √© a seguinte: vou ler v√°rias palavras e tentar prever o seguinte", explicou <a href="https://research.fb.com/people/ott-myle/">Mile Ott</a> , pesquisadora do Facebook.  "Se eu disser 'George W. Bush nasceu', os modelos precisam prever a pr√≥xima palavra nesta frase". <br><br>  Tais modelos de linguagem com treinamento profundo podem ser criados com bastante efici√™ncia.  Os pesquisadores simplesmente alimentam grandes quantidades de texto escrito de recursos gratuitos, como a Wikipedia, para suas redes neurais - bilh√µes de palavras dispostas em frases gramaticalmente corretas - e permitem que a rede preveja a pr√≥xima palavra por conta pr√≥pria.  De fato, isso √© equivalente ao fato de convidarmos uma pessoa em uma sala chinesa para criar seu pr√≥prio conjunto de regras, usando as mensagens chinesas recebidas como refer√™ncia. <br><br>  "A beleza dessa abordagem √© que o modelo ganha uma tonelada de conhecimento de sintaxe", disse Ott. <br><br>  Al√©m disso, essas redes neurais pr√©-treinadas podem aplicar suas representa√ß√µes de linguagem para ensinar uma tarefa mais restrita, n√£o relacionada √† previs√£o de palavras, ao processo de ajuste fino. <br><br>  ‚ÄúVoc√™ pode pegar o modelo da fase de pr√©-treinamento e adapt√°-lo a qualquer tarefa real que voc√™ precisar‚Äù, explicou Ott.  "E depois disso voc√™ obt√©m resultados muito melhores do que se tentasse resolver seu problema diretamente desde o in√≠cio". <br><br>  Em junho de 2018, quando a OpenAI lan√ßou sua <a href="https://openai.com/blog/language-unsupervised/">rede neural GPT</a> , com um modelo de linguagem inclu√≠do, que passou um m√™s treinando um bilh√£o de palavras (retiradas de 11.038 livros digitais), seu resultado no teste GLUE, 72,8 pontos, tornou-se imediatamente o mais o melhor  No entanto, Sam Bowman sugeriu que essa √°rea se desenvolveria por muito tempo antes que qualquer sistema pudesse pelo menos se aproximar do n√≠vel do homem. <br><br>  E ent√£o BERT apareceu. <br><br><h2>  Receita promissora </h2><br>  Ent√£o, o que √© o BERT? <br><br>  Em primeiro lugar, n√£o √© uma rede neural totalmente treinada, capaz de fornecer imediatamente resultados em n√≠vel humano.  Bowman diz que esta √© uma "receita muito precisa para treinar a rede neural".  Como um padeiro, seguindo a receita, pode garantir deliciosos bolos - que podem ser usados ‚Äã‚Äãpara diferentes bolos, de mirtilo a quiche de espinafre - e os pesquisadores do Google criaram uma receita BERT que pode servir como base ideal para a cria√ß√£o de redes neurais (ou seja, , seu ajuste fino), para que eles lidem bem com v√°rias tarefas no processamento da linguagem natural.  O Google abriu o c√≥digo BERT, o que significa que outros pesquisadores n√£o precisam mais repetir esta receita do zero - eles podem simplesmente fazer o download;  √© como comprar bolo pr√©-cozido para bolo na loja. <br><br>  Se o BERT √© uma receita, qual √© a sua lista de ingredientes?  "Este √© o resultado de tr√™s coisas diferentes conectadas entre si para que o sistema comece a funcionar", disse <a href="https://levyomer.wordpress.com/">Omer Levy</a> , pesquisador do Facebook que <a href="https://arxiv.org/abs/1906.04341">analisou</a> o dispositivo BERT. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1ee/f23/473/1eef234733ede2389825e047170009e0.jpg" width="50%"><br>  <i>Omer Levy, Pesquisador do Facebook</i> <br><br>  O primeiro √© o modelo de idioma pr√©-treinado, ou seja, os mesmos diret√≥rios da sala chinesa.  A segunda √© a oportunidade de decidir quais dos recursos da proposta s√£o os mais importantes. <br><br>  Em 2017, <a href="http://jakob.uszkoreit.net/">Jacob Uzkoreit</a> , engenheiro do Google Brain, trabalhou em maneiras de acelerar as tentativas da empresa de entender o idioma.  Ele observou que todas as redes neurais avan√ßadas sofrem com suas limita√ß√µes inerentes: elas estudam a frase por palavras.  Essa "sequ√™ncia" parecia coincidir com a ideia de como as pessoas leem o texto.  No entanto, Uzkoreit ficou interessado ", n√£o seria poss√≠vel que o entendimento da linguagem em um modo linear e seq√ºencial n√£o seja o mais ideal"? <br><br>  A taxa restrita com colegas desenvolveu uma nova arquitetura de redes neurais, com foco na "aten√ß√£o", um mecanismo que permite que cada camada da rede neural atribua grandes pesos a certos recursos dos dados de entrada em compara√ß√£o com outros.  Essa nova arquitetura com aten√ß√£o, um transformador, pode usar uma frase como "um cachorro morde o homem" como entrada e codificar cada palavra em paralelo de maneiras diferentes.  Por exemplo, um transformador pode vincular ‚Äúmordidas‚Äù e ‚Äúpessoa‚Äù como verbo e sujeito-objeto, ignorando o artigo ‚Äúa‚Äù;  ao mesmo tempo, ela pode relacionar ‚Äúmordida‚Äù e ‚Äúcachorro‚Äù como verbo e sujeito-sujeito, ignorando o artigo ‚Äúo‚Äù. <br><br>  A natureza inconsistente do transformador apresenta senten√ßas de forma mais expressiva, ou, como Uzkoreit diz, em forma de √°rvore.  Cada camada da rede neural estabelece muitas conex√µes paralelas entre certas palavras, ignorando o restante - aproximadamente como um aluno da escola prim√°ria desmonta uma frase em partes.  Essas conex√µes geralmente s√£o feitas entre palavras que podem n√£o estar pr√≥ximas.  "Tais estruturas parecem uma sobreposi√ß√£o de v√°rias √°rvores", explicou Uzkoreit. <br><br>  Tais representa√ß√µes de senten√ßas semelhantes a √°rvores d√£o aos transformadores a oportunidade de modelar significados contextuais, bem como estudar efetivamente as conex√µes entre palavras que est√£o distantes em senten√ßas complexas.  "Isso √© um tanto contra-intuitivo", disse Uzkoreit, "mas vem da lingu√≠stica, que h√° muito tempo se envolve em modelos de linguagem semelhantes a √°rvores". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/0a1/fe2/21e0a1fe2331c110f5cb2a966b2b173d.jpg" width="50%"><br>  <i>Jacob Uzkoreit, chefe da equipe de Berlim, Google AI Brain</i> <br><br>  Finalmente, o terceiro ingrediente da receita BERT expande ainda mais a leitura n√£o linear. <br><br>  Ao contr√°rio de outros modelos de linguagem pr√©-treinados criados pelo processamento de terabytes de texto da esquerda para a direita por redes neurais, o modelo BERT l√™ da direita para a esquerda e simultaneamente da esquerda para a direita e aprende a prever quais palavras foram exclu√≠das aleatoriamente das frases.  Por exemplo, o BERT pode aceitar uma frase no formato "George W. Bush [...] em Connecticut em 1946" e prever qual palavra est√° oculta no meio da frase (neste caso, "nascido"), depois de processar o texto nas duas dire√ß√µes.  "Essa bidirecionalidade for√ßa a rede neural a extrair o m√°ximo de informa√ß√£o poss√≠vel de qualquer subconjunto de palavras", disse Uzkoreit. <br><br>  O fingimento baseado no BERT usado como um jogo de palavras - modelagem de linguagem com mascaramento - n√£o √© algo novo.  Ele √© usado h√° d√©cadas para medir a compreens√£o das pessoas sobre o idioma.  Para o Google, ele forneceu uma maneira pr√°tica de usar a bidirecionalidade em redes neurais, em vez dos m√©todos unidirecionais de pr√©-treinamento que j√° haviam dominado essa √°rea antes.  "Antes do BERT, a modelagem unidirecional da linguagem era o padr√£o, embora essa seja uma limita√ß√£o opcional", disse <a href="https://kentonl.com/">Kenton Lee</a> , pesquisador do Google. <br><br>  Cada um desses tr√™s ingredientes - um modelo de linguagem profunda com pr√©-treinamento, aten√ß√£o e bidirecionalidade - existia antes do BERT separadamente.  Mas at√© o Google divulgar sua receita no final de 2018, ningu√©m as combinou de maneira t√£o bem-sucedida. <br><br><h2>  Receita de refino </h2><br>  Como qualquer boa receita, o BRET logo foi adaptado por v√°rios chefs ao seu gosto.  Na primavera de 2019, houve um per√≠odo "em que a Microsoft e a Alibaba pisaram umas nas outras, trocando de lugar no ranking semanalmente, ajustando o modelo", lembra Bowman.  Quando a vers√£o aprimorada do BERT foi lan√ßada em agosto, sob o nome de RoBERTa, o pesquisador <a href="http://ruder.io/">Sebastian Ruder,</a> do DeepMind, observou secamente em seu popular <a href="http://newsletter.ruder.io/issues/nlp-in-industry-leaderboard-madness-fast-ai-nlp-transfer-learning-tools-186245">boletim de not√≠cias da PNL</a> : "Novo m√™s e um novo modelo avan√ßado de linguagem com pr√©-treinamento". <br><br>  Como o bolo, o BERT tem v√°rias decis√µes de design que afetam a qualidade de seu trabalho.  Isso inclui o tamanho da rede neural convencional, a quantidade de dados usados ‚Äã‚Äãno pr√©-treinamento, o m√©todo de mascarar palavras e por quanto tempo a rede neural trabalha com esses dados.  E nas receitas subsequentes, como RoBERTa, os pesquisadores ajustam essas decis√µes - como um chef especificando uma receita. <br><br>  No caso do RoBERTa, pesquisadores do Facebook e da Universidade de Washington aumentaram o n√∫mero de alguns ingredientes (dados de pr√©-treinamento, dura√ß√£o das seq√º√™ncias recebidas, tempo de treinamento), um ingrediente foi exclu√≠do (a tarefa de "prever a pr√≥xima frase", originalmente no BERT, afetando negativamente os resultados ) e o outro foi alterado (complicou a tarefa de mascarar palavras individuais).  Como resultado, eles ficaram em primeiro lugar no ranking GLUE.  Seis semanas depois, pesquisadores da Microsoft e da Universidade de Maryland <a href="https://arxiv.org/abs/1909.11764">adicionaram</a> seus refinamentos ao RoBERTa e conseguiram a pr√≥xima vit√≥ria.  No momento, outro modelo ficou em primeiro lugar na GLUE, ALBERT (a abrevia√ß√£o de ‚Äúlite BERT‚Äù, ou seja, ‚Äúlite BERT‚Äù), o que mudou ligeiramente a estrutura b√°sica do BERT. <br><br>  "Ainda estamos escolhendo quais receitas funcionam, quais n√£o", disse Ott, do Facebook, que trabalhou no RoBERTa. <br><br>  Por√©m, como o aprimoramento da t√©cnica de pr√©-assar bolos n√£o ensina o b√°sico da qu√≠mica, o aprimoramento gradual do BERT n√£o fornecer√° muito conhecimento te√≥rico sobre o desenvolvimento da PNL.  "Serei extremamente honesto com voc√™ - n√£o sigo esses trabalhos, pois para mim eles s√£o extremamente chatos", disse Linsen, linguista computacional da Universidade Johns Hopkins.  "H√° um certo mist√©rio cient√≠fico aqui", ele admite, mas n√£o como tornar o BERT e todos os seus descendentes mais inteligentes, e nem mesmo para descobrir por que eles s√£o t√£o inteligentes.  Em vez disso, "estamos tentando entender o quanto esses modelos realmente entendem a linguagem", disse ele, "em vez de aprender truques estranhos que de alguma forma funcionam nos conjuntos de dados nos quais geralmente avaliamos esses modelos". <br><br>  Em outras palavras, o BERT est√° fazendo algo certo.  Mas e se ele fizer isso pelo motivo errado? <br><br><h2>  Complicado, mas n√£o inteligente </h2><br>  Em julho de 2019, dois pesquisadores da Universidade Estadual de Taiwan, Cheng Kun, usaram o BERT com resultados impressionantes em um teste de desempenho relativamente pouco conhecido chamado de "tarefa de compreens√£o de argumentos".  Para concluir a tarefa, √© necess√°rio escolher uma condi√ß√£o inicial impl√≠cita ("fundamento") que ap√≥ie o argumento a favor de qualquer declara√ß√£o.  Por exemplo, para provar que ‚Äúfumar causa c√¢ncer‚Äù (declara√ß√£o), uma vez que ‚Äúestudos cient√≠ficos mostraram uma liga√ß√£o entre fumar e c√¢ncer‚Äù (argumenta√ß√£o), √© preciso escolher o argumento ‚Äúa pesquisa cient√≠fica pode ser confi√°vel‚Äù (‚Äúbase‚Äù) e n√£o outra op√ß√£o: ‚ÄúA pesquisa cient√≠fica √© cara‚Äù (no entanto, isso n√£o √© relevante neste contexto).  Est√° tudo claro? <br><br>  Se n√£o for tudo, n√£o se preocupe.  Mesmo as pessoas n√£o s√£o muito boas nessa tarefa sem pr√°tica.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A linha de base m√©dia para uma pessoa que n√£o faz exerc√≠cios √© de 80 em 100. O BERT alcan√ßou 77 - o que os autores disseram ser "inesperado".</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas, em vez de decidir que o BERT √© capaz de dar √†s redes neurais a capacidade de raciocinar nada pior que Arist√≥teles, eles suspeitam que tudo seja realmente mais simples: o BERT encontrou padr√µes superficiais na formula√ß√£o dos fundamentos. De fato, depois de analisar seus dados de treinamento, os autores encontraram muitas evid√™ncias desse chamado "Pistas falsas." Por exemplo, se voc√™ apenas selecionar todas as bases que cont√™m a part√≠cula ‚Äúnot‚Äù, poder√° responder corretamente √†s perguntas em 61% dos casos. Tendo eliminado todas essas regularidades dos dados, os cientistas descobriram que o resultado do BERT caiu de 77 para 53 - o que √© quase equivalente a uma escolha aleat√≥ria. Um artigo da revista de aprendizado de m√°quina The Gradient, do Stanford Artificial Intelligence Lab, </font></font><a href="https://thegradient.pub/nlps-clever-hans-moment-has-arrived/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">comparou</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BERT com Smart Hans, um cavalo supostamente forte em aritm√©tica. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em outro artigo, ‚Äú </font></font><a href="https://www.aclweb.org/anthology/P19-1334"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Rights for</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wrong Reasons‚Äù, Linsen et al publicaram evid√™ncias de que os altos resultados do BERT em certos testes GLUE tamb√©m podem ser atribu√≠dos √† exist√™ncia de pistas falsas nos dados de treinamento. Um conjunto alternativo de dados foi desenvolvido que foi projetado para privar o BERT da capacidade de trabalhar dessa maneira. O conjunto de dados foi chamado Hans (An√°lise Heur√≠stica para Sistemas de Infer√™ncia de Linguagem Natural, HANS) [an√°lise heur√≠stica de sistemas que tira conclus√µes com base na linguagem natural].</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ent√£o, BERT e todos os seus parentes invadindo as tabelas de recordes s√£o apenas uma farsa? Bowman concorda com Lensen que alguns dos dados do GLUE s√£o desleixados. Eles s√£o preenchidos com as distor√ß√µes cognitivas inerentes √†s pessoas que o criaram, e isso pode ser potencialmente explorado por uma poderosa rede baseada em BERT. "N√£o existe um truque universal que resolva todos os problemas do GLUE, mas h√° muitas possibilidades de" cortar custos "que ajudam nisso", disse Bowman, "e o modelo pode encontr√°-los." Mas ele n√£o acha que o BERT seja baseado em algo de valor. "Aparentemente, temos um modelo que aprendeu algo realmente interessante sobre o idioma", disse ele. "No entanto, ela certamente n√£o entende a linguagem humana em um sentido geral."</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De acordo com Yojin Choi, cientista da computa√ß√£o da Universidade de Washington e do Instituto Allen, uma das maneiras de estimular o progresso em dire√ß√£o a um entendimento comum da linguagem √© concentrar-se n√£o apenas na melhoria das vers√µes do BERT, mas tamb√©m no desenvolvimento de testes e dados de treinamento de melhor qualidade que reduzem a probabilidade de ocorr√™ncia tecnologia falsa no estilo de "Hans inteligente". Seu trabalho explora uma abordagem de filtragem contradit√≥ria que usa algoritmos para validar dados de treinamento para PNLs e remover exemplos que s√£o repetidos demais ou que deixam pistas impl√≠citas para as redes neurais. Ap√≥s essa filtragem competitiva, "a efic√°cia do BERT pode cair significativamente", disse ela, e "a efic√°cia do ser humano n√£o est√° caindo tanto".</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, alguns pesquisadores da PNL acreditam que, mesmo com a melhoria dos procedimentos de ensino para os modelos de linguagem, ainda haver√° obst√°culos reais √† verdadeira compreens√£o da l√≠ngua. Mesmo com treinamento poderoso, o BERT n√£o √© capaz de modelar perfeitamente o idioma no caso geral. Ap√≥s os ajustes, ele modela "uma tarefa espec√≠fica da PNL, ou mesmo um conjunto de dados espec√≠fico para essa tarefa", disse </font></font><a href="https://www.cs.uml.edu/~arogers/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Anna Rogers</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , linguista computacional do Machine Text Laboratory da Universidade de Massachusetts. √â prov√°vel que nenhum conjunto de dados de treinamento, por mais cuidadosamente preparado ou cuidadosamente filtrado, seja capaz de incluir todos os casos extremos e dados de entrada imprevis√≠veis com os quais as pessoas que usam linguagem natural podem lidar facilmente.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bowman ressalta que √© dif√≠cil at√© entender o que pode nos convencer de que a rede neural alcan√ßou um entendimento real da linguagem. Os testes padr√£o devem revelar algo socializado em rela√ß√£o ao conhecimento dos testados. No entanto, qualquer aluno sabe que os testes s√£o f√°ceis de enganar. "√â muito dif√≠cil fazer testes pesados ‚Äã‚Äão suficiente e suficientemente protegidos contra enganos, para que a solu√ß√£o deles nos conven√ßa de que realmente resolvemos o problema em algum aspecto das tecnologias de linguagem da IA", disse ele. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bowman e colegas apresentaram recentemente um teste chamado </font></font><a href="https://super.gluebenchmark.com/leaderboard"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SuperGLUE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">projetado especificamente para ser complexo para sistemas baseados em BERT. At√© agora, nenhuma rede conseguiu ultrapassar uma pessoa nela. Mas mesmo que (ou quando) isso aconte√ßa, isso significa que as m√°quinas podem aprender a entender o idioma melhor do que antes? Ou ser√° que a ci√™ncia se tornar√° melhor ao ensinar √†s m√°quinas como passar neste teste?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Boa analogia", disse Bowman. </font><font style="vertical-align: inherit;">"Descobrimos como passar nos testes LSAT e MCAT, mas podemos n√£o ter as qualifica√ß√µes para nos tornarmos m√©dicos ou advogados". </font><font style="vertical-align: inherit;">E, no entanto, a julgar por tudo, √© exatamente assim que a pesquisa no campo da IA ‚Äã‚Äãse move. </font><font style="vertical-align: inherit;">"O xadrez parecia um teste s√©rio de intelig√™ncia at√© que descobrimos como escrever um programa para o jogo", disse ele. </font><font style="vertical-align: inherit;">"Definitivamente, entramos em uma era em que o objetivo era inventar tarefas cada vez mais complexas, que representassem uma compreens√£o da linguagem, e criar maneiras de resolv√™-las".</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt479446/">https://habr.com/ru/post/pt479446/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt479426/index.html">Security Week 50: Ataques intermedi√°rios no Confluence e Linux</a></li>
<li><a href="../pt479428/index.html">Eventos digitais em Moscou, de 9 a 15 de dezembro</a></li>
<li><a href="../pt479430/index.html">Eventos digitais em S√£o Petersburgo, de 9 a 15 de dezembro</a></li>
<li><a href="../pt479432/index.html">Yandex.Maps: Fui ao controlador da placa - imediatamente obtive a posi√ß√£o do usu√°rio (ok, agora s√©rio)</a></li>
<li><a href="../pt479442/index.html">Alexey Savvateev: Modelo te√≥rico dos jogos de cisma social (+ pesquisa nginx)</a></li>
<li><a href="../pt479450/index.html">AppCode 2019.3: funciona mais r√°pido, entende melhor o Swift, conhece o Mac Catalyst, exibe convenientemente mensagens de montagem</a></li>
<li><a href="../pt479452/index.html">Como o sistema de nomes de dom√≠nio se desenvolveu: a era da ARPANET</a></li>
<li><a href="../pt479458/index.html">Beleza ou praticidade na sala do servidor</a></li>
<li><a href="../pt479460/index.html">Um guia para carros voadores</a></li>
<li><a href="../pt479462/index.html">Serializa√ß√£o em C ++</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>