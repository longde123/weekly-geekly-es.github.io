<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíê ‚ôÄÔ∏è üëé Algunos algoritmos bajo el cap√≥ del cerebro üåâ ‚õπüèø üê±</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hace alg√∫n tiempo quer√≠a estudiar materiales modernos sobre neurobiolog√≠a desde el punto de vista de un programador. Es decir, extraer los algoritmos ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Algunos algoritmos bajo el cap√≥ del cerebro</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/390385/"><img src="https://habrastorage.org/files/fd6/2f9/d18/fd62f9d188ce4709a8df65a5b4e77a62.jpg" align="left"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hace alg√∫n tiempo quer√≠a estudiar materiales modernos sobre neurobiolog√≠a desde el punto de vista de un programador. </font><font style="vertical-align: inherit;">Es decir, extraer los algoritmos b√°sicos de ellos, habi√©ndolos limpiado de detalles qu√≠micos / biol√≥gicos innecesarios. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, si alguien ama las redes neuronales artificiales y quiere buscar inspiraci√≥n en lo natural, este art√≠culo puede aparecer. </font><font style="vertical-align: inherit;">Por supuesto, no fue posible cubrir todo en un art√≠culo, hay muchos datos.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primero, una breve descripci√≥n del trabajo de la bio-neurona, para que quede m√°s claro. </font><font style="vertical-align: inherit;">Quien ya conoce lo b√°sico: si√©ntase libre de omitir.</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neurona biol√≥gica: bagre, dendritas, sinapsis, espiga, EPSP, potencial de membrana</font></font></b><div class="spoiler_text">     :  (  ),    ,   .   ,       ,   .        .  ‚Äî   .        .    .    .<br>
<br>
 ,    ,   .     ,        .      ‚Äú ‚Äù ,‚Äú‚Äù   ‚ÄúU‚Äù.          ,       .     <i></i>.       U. <br>
<br>
         ,   .   ,  U     .        ‚Äî     .   .       ,       ,        .      ,       ,          .       ‚Äú+‚Äù   EPSP ().    ‚Äú-‚Äù   IPSP (). <br>
<br>
         .  EPSP-  IPSP-    , ,   ,   .   ,    . <br>
<br>
<img src="https://habrastorage.org/files/646/5b9/f06/6465b9f06205482ba1b2be407f036404.jpg"><br>
<br>
 ,  ,   ,     .      ,    .      ,    .  , ,         .      .       ,     EPSP     ,   .<br>
<br>
     .                    ,  .    ‚Äú ‚Äù   .     ,   .       ,          ,     (. ).<br>
<br>
<img src="https://habrastorage.org/files/c33/322/6e7/c333226e79664e0b87791955852fc838.png"><br>
<br>
  ‚Äî   .   ‚Äî       .     EPSP ‚Äî   ‚Äú‚Äù. ,  ,  ,       .      EPSP (  )      ,     , ..        ,      .<br>
</div></div><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">STDP</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se supone que la base de la memoria a largo plazo es c√≥mo se distribuye la eficiencia sobre las sinapsis de las neuronas. </font><font style="vertical-align: inherit;">Algunas sinapsis se debilitan, otras se fortalecen. </font><font style="vertical-align: inherit;">Esto se llama ductilidad. </font><font style="vertical-align: inherit;">Pero, ¬øcon qu√© algoritmo se decide qu√© sinapsis y c√≥mo cambiar√°n los pesos? </font><font style="vertical-align: inherit;">El principio m√°s famoso en las redes neuronales vivas es la </font></font><abbr title="ductilidad d√∫ctil dependiente del tiempo"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">plasticidad dependiente del tiempo de Spike</font></font></abbr><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se observa desde los insectos hasta los humanos y se formula de la siguiente manera:</font></font><br>
<br>
<blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si el pico de entrada a esta sinapsis tiende a aparecer justo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">antes de que</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> la neurona misma genere el pico, entonces la sinapsis se amplifica. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Si el pico de entrada a esta sinapsis tiende a ocurrir inmediatamente </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">despu√©s de la</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> generaci√≥n del pico por la neurona misma, entonces la sinapsis se debilita.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Esos insumos que han contribuido al aumento tienen mayor prioridad en el futuro y no se aportan menos. </font><font style="vertical-align: inherit;">El proceso contin√∫a hasta que quede un cierto subconjunto de los pesos originales, y el resto se reducir√° a cero. </font><font style="vertical-align: inherit;">Tendremos en cuenta el hecho de que una neurona genera un pico cuando </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">muchas de</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sus entradas se </font><font style="vertical-align: inherit;">activan </font><i><font style="vertical-align: inherit;">inmediatamente</font></i><font style="vertical-align: inherit;"> en un corto per√≠odo de tiempo </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">De esto se puede suponer que las entradas restantes que no son cero ten√≠an correlaciones en el tiempo. </font></font><br>
<br>
<img src="https://habrastorage.org/files/190/931/0cb/1909310cb8f3456ca4540a680c255863.JPEG"><br>
<sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dependencia del aumento / disminuci√≥n del peso de la sinapsis en el pico de entrada (pre) y salida (post) de miel de Œît</font></font></sub><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Este fen√≥meno es muy conocido y confirmado en muchos experimentos, pero debe tenerse en cuenta que al pasar por diferentes protocolos experimentales, a√∫n se puede lograr la distorsi√≥n / violaci√≥n de esta ley, no rompiendo los requisitos formales para los picos pre-post </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[0]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Propagaci√≥n de espiga trasera</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La neurona piramidal (quiz√°s el tipo m√°s famoso de neuronas corticales) tiene miles de sinapsis esparcidas por su √°rbol dendr√≠tico (de hecho, tiene </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dos</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √°rboles dendr√≠ticos). </font><font style="vertical-align: inherit;">Si seleccionamos varias sinapsis, cerca del bagre, lejos y bastante lejos de √©l, y vemos qu√© tipo de gr√°ficos STDP obtienen, entonces los gr√°ficos ser√°n diferentes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para las sinapsis cercanas al bagre, el horario se ver√° cl√°sico, como una variaci√≥n sobre el tema del entrenamiento de Hebb. Es decir, como en la imagen de arriba. Cuanto m√°s lejos est√© el bagre, menor ser√° la amplitud de este entrenamiento. Y si tomas sinapsis distantes, entonces puedes ver cosas muy extra√±as. Por ejemplo, si el pico de entrada (que lleg√≥ a la sinapsis) precedi√≥ a la salida, entonces el experimento no observ√≥ amplificaci√≥n, sino atenuaci√≥n de esta sinapsis. Entrenamiento antihebbovskoe, Karl! Sin embargo, era posible devolverlo al canal de Hebb si se generaban adherencias dendr√≠ticas en dendritas distantes. En general, con el entrenamiento de las sinapsis m√°s cercanas al bagre, todo est√° bastante claro, pero en los distantes parece que hay un </font><font style="vertical-align: inherit;">algoritmo de aprendizaje diferente </font><font style="vertical-align: inherit;">en el </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">desorden </font></font></s><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[1]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Veamos de d√≥nde vienen las patas de todo esto. Entonces, para la iteraci√≥n de STDP, es necesario que se genere un pico en el bagre. Despu√©s de esto, las sinapsis deber√≠an </font><font style="vertical-align: inherit;">descubrirlo </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r√°pidamente</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Para hacer esto, es deseable que la espiga se extienda desde la loma del ax√≥n. La buena noticia es que esto realmente est√° sucediendo. Una neurona env√≠a un pico no solo a otras neuronas, sino tambi√©n a sus dendritas. La "mala" noticia es que con la propagaci√≥n hacia atr√°s este pico se desvanece. Demasiado r√°pido para alcanzar dendritas distantes. Ahora est√° claro por qu√© la formaci√≥n de sinapsis cercanas y lejanas es diferente. Queda por descubrir qu√© est√° sucediendo en las sinapsis distantes.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Es importante recordar aqu√≠ que las dendritas no son conductores pasivos de perturbaciones. </font><font style="vertical-align: inherit;">Ellos mismos saben c√≥mo generar picos cuando "lo consideran necesario". </font><font style="vertical-align: inherit;">Si por alguna raz√≥n la dendrita gener√≥ una espiga, y al mismo tiempo una espiga deca√≠da del bagre entr√≥ en esta zona, entonces, habi√©ndose formado con la espiga dendr√≠tica, (tal vez) recibir√° una </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">patada</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> como </font><s><font style="vertical-align: inherit;">un</font></s><font style="vertical-align: inherit;"> segundo viento y puede extenderse m√°s en este sub√°rbol. </font><font style="vertical-align: inherit;">Al menos existe tal hip√≥tesis. </font><font style="vertical-align: inherit;">Otra hip√≥tesis es que quiz√°s las sinapsis en dendritas distantes se entrenan generalmente sin el uso de una comisura som√°tica de propagaci√≥n hacia atr√°s. </font><font style="vertical-align: inherit;">M√°s sobre esto a continuaci√≥n.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Efectos interesantes en las dendritas: los picos dendr√≠ticos son suficientes para la plasticidad</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, ¬øqu√© necesitas para que la sinapsis comience a aumentar de peso? Es necesario que en este lugar de la neurona se encienda la maquinaria qu√≠mica correspondiente. Para esto, a su vez, es necesario que el potencial en la membrana en este lugar sea desplazado por alguna </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">perturbaci√≥n suficientemente grande</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . ¬øC√≥mo se puede crear? El EPSP promedio habitual es demasiado peque√±o para esto. Pero la cantidad de EPSP y la comisura som√°tica de propagaci√≥n inversa ya pueden ser adecuadas. Durante alg√∫n tiempo se crey√≥ que esta es la principal forma de causar plasticidad.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Luego, en los experimentos, result√≥ que era posible suprimir artificialmente las adherencias som√°ticas en una neurona y a√∫n as√≠ registrar un aumento en el peso de sus sinapsis en respuesta a su estimulaci√≥n (no ninguna). </font><font style="vertical-align: inherit;">Result√≥ que la plasticidad ocurre cuando la estimulaci√≥n de las sinapsis es lo suficientemente fuerte como para que aparezcan comisuras dendr√≠ticas en este lugar. </font><font style="vertical-align: inherit;">Son </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">una indignaci√≥n lo suficientemente fuerte como</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para desencadenar la plasticidad en este lugar. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[2]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> S√≠, s√≠, sin la participaci√≥n del resto de la neurona. </font><font style="vertical-align: inherit;">Es decir, el "elemento m√≠nimo" del procesamiento de la informaci√≥n puede considerarse ni siquiera una neurona, sino una rama dendr√≠tica separada.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Efectos interesantes en las dendritas - agrupamiento</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En las neuronas del hipocampo, se descubri√≥ que el LTP (aumento de peso a largo plazo) de una sinapsis reduce el umbral para que se produzca LTP en las sinapsis vecinas. Luego, se realiz√≥ un estudio para las neuronas de la corteza sensorial del rat√≥n, en el que las neuronas procesaron los datos de los bigotes. Y se descubri√≥ que la plasticidad sin√°ptica tiende a agruparse. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se tom√≥ una neurona espec√≠fica y se seleccion√≥ el 15% de las sinapsis, que fueron sometidas m√°s intensamente a la plasticidad. Su distribuci√≥n por neurona no fue accidental: una parte significativa result√≥ ser vecina: 50 de 161. Luego se tom√≥ una neurona de la corteza sensorial del rat√≥n, de la cual se cort√≥ el bigote (es decir, la neurona sufr√≠a de falta de informaci√≥n). El efecto de agrupamiento estaba ausente en dicha neurona. Pero fue globalmente m√°s sensible a las se√±ales de entrada </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[3]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Efectos interesantes en las dendritas: todo cambia desde la reorganizaci√≥n de los t√©rminos</font></font></h3><br>
<img src="https://habrastorage.org/files/c42/24c/38e/c4224c38ebec4afa8aced89c08e5a4df.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Entonces, deje que dos EPSP lleguen a nuestra dendrita, como en la imagen de arriba. </font><font style="vertical-align: inherit;">La perturbaci√≥n que causan en el soma depende no solo de la magnitud de estos EPSP, sino tambi√©n de: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
1) su distancia del soma </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
2) su distancia el uno del otro </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
. </font><font style="vertical-align: inherit;">Si EPSP vino de la sinapsis a la dendrita, entonces se propagar√° al bagre y se descompondr√° en el camino. </font><font style="vertical-align: inherit;">Es decir, cuanto m√°s lejos est√©, m√°s se desvanecer√°. </font><font style="vertical-align: inherit;">Y si una sinapsis inhibitoria activada se interpone en su camino, entonces EPSP se extinguir√° inmediatamente. </font><font style="vertical-align: inherit;">Por lo tanto, si tuvi√©ramos dos EPSP bastante distantes entre s√≠, entonces traer√°n dos peque√±as perturbaciones al soma, y ‚Äã‚Äãluego, si tiene suerte.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pero si estuvieran uno al lado del otro, entonces la dendrita puede generar un pico en este lugar. </font><font style="vertical-align: inherit;">Esta es una perturbaci√≥n en la que la amplitud es mayor que solo la suma de los EPSP. </font><font style="vertical-align: inherit;">El pico dendr√≠tico ya es mucho m√°s probable que corra hacia el bagre, y la contribuci√≥n ser√° mayor. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[4]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enlace caracter√≠sticas de sincronizaci√≥n</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(la sincronizaci√≥n codifica la relaci√≥n, el enlace de caracter√≠sticas) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
De los p√°rrafos anteriores se deduce que la sincronizaci√≥n relativa de los picos es importante para los procesos de plasticidad en las sinapsis. </font><font style="vertical-align: inherit;">Y a la luz de esto, no se puede evitar el fen√≥meno de la sincronizaci√≥n temporal de las neuronas. </font><font style="vertical-align: inherit;">Este fen√≥meno es omnipresente y, aparentemente, muy fundamental, porque </font><font style="vertical-align: inherit;">comer en todos los niveles del cerebro. </font><font style="vertical-align: inherit;">Considere un ejemplo espec√≠fico.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se implantan electrodos en la corteza visual, y luego se le muestran varios est√≠mulos visuales. En los registros de los electrodos se ve que algunos grupos de neuronas est√°n involucrados en la actividad oscilatoria sincr√≥nica en fase. Estas neuronas pueden estar en diferentes lugares. Las neuronas tienden a sincronizarse si se activaron por contornos en la imagen, que son continuos o se mueven a la misma velocidad y en una direcci√≥n (el principio del destino com√∫n). Para la corteza visual, la probabilidad de sincronizaci√≥n se correlaciona con cu√°nto satisfacen los est√≠mulos de entrada el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">criterio Gestalt</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/files/95f/87f/42e/95f87f42e6264f8581324fc43c5be467.jpg"><br>
<sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Todos los puntos verdes aqu√≠ se perciben no por separado, sino como un todo</font></font></sub><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La hip√≥tesis de trabajo sugiere que la corteza utiliza la sincronizaci√≥n de descarga en las neuronas para codificar la relaci√≥n "completa" acerca de esas porciones de se√±al a las que respondieron estas neuronas. </font><font style="vertical-align: inherit;">Es decir </font><font style="vertical-align: inherit;">Adem√°s, sus respuestas ser√°n procesadas en su conjunto por redes neuronales superiores, porque son precisamente estos picos los que llegar√°n all√≠ al mismo tiempo, lo que significa que resumir√© antes de que se desvanezcan. </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La sincronizaci√≥n de la actividad en la corteza a grandes distancias es un requisito previo para que la se√±al que provoc√≥ que esta actividad (por ejemplo, una palabra se vea) tenga acceso a la zona de percepci√≥n consciente. </font><font style="vertical-align: inherit;">Una se√±al similar, que se procesa inconscientemente, solo causar√° sincronizaci√≥n local </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[5]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
C√≥mo se sincronizan las poblaciones de neuronas es el tema de investigaci√≥n </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[6]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Plasticidad no sin√°ptica</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s de los pesos de sinapsis, otras caracter√≠sticas de la neurona tambi√©n pueden sufrir cambios durante el entrenamiento. </font><font style="vertical-align: inherit;">Su excitabilidad general puede cambiar (leer, el umbral para la generaci√≥n de picos). </font><font style="vertical-align: inherit;">Si se han fortalecido muchos pesos, entonces tiene sentido que una neurona reduzca su excitabilidad. </font><font style="vertical-align: inherit;">Y si, por el contrario, un bajo nivel de cambio en las escalas, entonces tiene sentido aumentar. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Otro ejemplo: un ax√≥n puede cambiar el tiempo durante el cual se disparar√° a los destinatarios. </font><font style="vertical-align: inherit;">A√∫n as√≠, si estimula una neurona a baja frecuencia durante mucho tiempo, reduce su excitabilidad, y este es un efecto a largo plazo.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Repetici√≥n</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En el hipocampo, hay neuronas que responden a un lugar espec√≠fico en el espacio. Se </font><font style="vertical-align: inherit;">llaman las </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">celdas del lugar</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Es decir, si la rata est√° en un lugar del laberinto, entonces una celda del lugar reacciona al m√°ximo, si en otra es otra. Cuando la rata est√° durmiendo o descansando, en el hipocampo esas secuencias de c√©lulas del lugar que corresponden a las distancias realmente recorridas antes de que estas rutas comiencen a jugar a un ritmo acelerado. Para las c√©lulas restantes esto no se observa, es decir, estas secuencias no son aleatorias. </font></font><br>
<br>
<img src="https://habrastorage.org/files/322/18a/af8/32218aaf8cbf45b78fc27ea51d0ee687.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En la imagen: la rata sigue una pista recta, y la jaula "azul" del lugar se activa primero, luego "rojo" y luego "verde". Luego recibe refuerzo y comienza la reproducci√≥n acelerada de esta secuencia en el orden inverso "azul rojo verde".</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s, el hipocampo puede jugar en lugares futuros antes de que la rata los visite (preplay). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No menos interesante: el proceso de reproducci√≥n en un sue√±o tambi√©n afecta la corteza cerebral. Es decir, tanto en la corteza como en el hipocampo, se replica y est√° conectado con la misma experiencia experimentada durante la vigilia anterior </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[7]</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Existe la hip√≥tesis de que el hipocampo es un m√≥dulo para la memorizaci√≥n r√°pida, y la corteza es para el aprendizaje lento y profundo. Quiz√°s el hipocampo recuerda los episodios de eventos en el orden en que se siguen, y luego los "pierde" en la corteza una y otra vez a un ritmo acelerado para que extraiga patrones ocultos de estas secuencias. Entonces est√° claro por qu√© un paciente con un hipocampo remoto pierde la capacidad de crear nuevos recuerdos a largo plazo, pero no pierde los que se hicieron antes de la operaci√≥n.</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LTP-L (finales de fase </font></font><abbr title="mejora de la sinapsis a largo plazo"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LTP</font></font></abbr><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La longevidad de los cambios en los pesos de las sinapsis depende no solo del est√≠mulo inicial que los caus√≥, sino tambi√©n de los eventos que ocurren antes y </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">despu√©s</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Los experimentos muestran que la memoria a corto plazo de un est√≠mulo puede consolidarse en una m√°s larga si el animal experimenta un evento fuerte dentro de una ventana de tiempo determinada alrededor de este est√≠mulo. </font><font style="vertical-align: inherit;">Esto se registra a nivel de neuronas individuales. </font><font style="vertical-align: inherit;">Deje que alg√∫n protocolo de estimulaci√≥n logre causar una mejora de la sinapsis a corto plazo. </font><font style="vertical-align: inherit;">Se puede hacer a largo plazo si un est√≠mulo fuerte (tet√°nicamente) estimula alg√∫n otro camino que converja en la misma neurona (dentro de la ventana de tiempo correcta, por supuesto).</font></font><br>
<br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inhibici√≥n lateral</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 (inhibici√≥n lateral, inhibici√≥n envolvente, supresi√≥n envolvente) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Este principio simple es conocido por los cient√≠ficos desde tiempos de cueva. Tal vez viste ilusiones como esta: </font></font><br>
 <br>
<img src="https://habrastorage.org/files/97d/2c6/36c/97d2c636c4734fc6810bff8150370018.jpg" width="350"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
en los albores de la neurociencia, se cre√≠a que una neurona estaba buscando un est√≠mulo en su campo receptivo, y si el est√≠mulo deseado se encontraba all√≠, entonces la neurona generaba picos intensivos. Si no se encontr√≥ del todo, pero parece, tambi√©n genera picos, pero no tan intensamente. Y luego result√≥ que la actividad de una neurona puede ser suprimida por actividades </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">externas a su campo receptivo</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s de la red excitadora (tambi√©n es la principal), hay una m√°s en el cerebro: una red de interneuronas inhibidoras. Las neuronas "principales" proporcionan excitaci√≥n en la red, y las interneuronas, como regla, lo inhiben. Las interneuronas tienen un √°rea de acci√≥n local. Hay m√°s neuronas principales que interneuronas, pero las interneuronas son m√°s diversas. Espec√≠ficamente, la ilusi√≥n visual mencionada supuestamente se toma debido a la interacci√≥n de las neuronas excitadoras con las inhibidoras. La l√≥gica del proceso es la siguiente: cuanto m√°s se activa la neurona, m√°s inhibe (con la ayuda de interneuronas) la actividad en el vecindario. Las neuronas excitadoras compiten entre s√≠ por el derecho de hacer la mayor contribuci√≥n a la se√±al para la siguiente capa de neuronas. Si estuvieras muy activado, entonces reducir√°s considerablemente la velocidad de los vecinos. Si los vecinos se activaron d√©bilmente, entonces lo retrasar√°n un poco.Como resultado, todas las no linealidades en los datos de entrada se "mantendr√°n" a√∫n m√°s fuertes, y la siguiente capa ya funcionar√° con esto.</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Una ilustraci√≥n muy clara del principio.</font></font></b><div class="spoiler_text"><img src="https://habrastorage.org/files/a3b/05e/3b7/a3b05e3b71d7481eb659f6cf0f4fb2a4.gif"><br>
<br>
-    .     , ,       .<br>
</div></div><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Detectores de errores de predicci√≥n </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[8]</font></font></a></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Las neuronas de dopamina aprenden a unir alg√∫n tipo de "claves" en la se√±al de entrada para recibir una recompensa. </font><font style="vertical-align: inherit;">Utilizan el siguiente algoritmo: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(1) Si se produce una recompensa impredecible, las neuronas responden aumentando la frecuencia de los picos ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tenemos un error positivo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(2) Despu√©s del entrenamiento, ya reaccionan al ‚Äúevento clave‚Äù y no al premio en s√≠. </font><font style="vertical-align: inherit;">Es decir </font><font style="vertical-align: inherit;">sobre el predecesor de la recompensa emiten un aumento en la frecuencia. </font><font style="vertical-align: inherit;">Y para la recompensa, si llega a tiempo, no hay reacci√≥n ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">no tenemos ning√∫n error</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
(3) Si una neurona entrenada predijo una recompensa, pero no sucedi√≥, responde disminuyendo la frecuencia de los picos ( </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tenemos un error negativo</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font><br>
<br>
<img src="https://habrastorage.org/files/742/aba/8be/742aba8be3c24db4a52d865ddac8db7c.jpg"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se cree que no solo hay formaci√≥n asociativa, sino el establecimiento de una relaci√≥n "causa-efecto". </font><font style="vertical-align: inherit;">Es decir, por ejemplo, las nubes de tormenta se correlacionan con la lluvia, porque a menudo sucede: ves nubes y luego ves lluvia. </font><font style="vertical-align: inherit;">Lo mismo sucede con un paraguas: ves paraguas en las personas y luego ves lluvia. </font><font style="vertical-align: inherit;">Pero si al menos una vez que comenz√≥ la lluvia sin paraguas, ya est√° claro que los paraguas no son la causa de la lluvia.</font></font><br>
<br>
<div class="spoiler"><b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bonificaci√≥n para aquellos que leen: la aparici√≥n de un reflejo condicionado en una neurona espec√≠fica</font></font></b><div class="spoiler_text">,          -  .   :      ,   (   )     ,   . ,  .         .<br>
<br>
  ,   ‚Äî . ,    ,      .         ‚Äú ‚Äù.       .  ,      ,    -.            .    ( )     :<br>
<br>
1)  100 000      (   )<br>
2)        (   -   inferior olive)<br>
<br>
            ( ,  ),      ‚Äî   .  ,    ,    ,  :<br>
<br>
<img src="https://habrastorage.org/files/398/e6c/8cf/398e6c8cfe7a483899b79a4770936c33.jpg"><br>
<br>
  ,           (),   ().   ‚Äî    ,    ‚Äî    (ISI)   (CS)   (US) ,    .<br>
<br>
 ,           <i> </i>         ,  .  , .   .    , ,   -   .<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">[9]</a>. <br>
 ,                (supervised learning), ,     (unsupervised learning),    ‚Äî   (reinforcement). <br>
</div></div><br>
<h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conclusi√≥n</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A primera vista, el mismo STDP puede parecer un algoritmo de aprendizaje completo para redes de neuronas espigadas. Pero en realidad, actualmente no hay modelos de espiga artificial de aprendizaje efectivo. Es decir, pueden hacer algo, por ejemplo, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dar el 95% en el punto de referencia MNIST</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pero las tareas m√°s no triviales en ellos no son muy buenas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Se ha avanzado mucho m√°s en los √∫ltimos a√±os en redes donde los picos est√°n ausentes como clase. Los algoritmos de entrenamiento se basan en el descenso del gradiente sobre la superficie del error, donde el error es una funci√≥n de los pesos de sinapsis. El trabajo con el aspecto del tiempo se logra mediante retroalimentaciones en la topolog√≠a. La atenci√≥n y el refuerzo se est√°n introduciendo con √©xito en estas redes. En este contexto, los modelos de espiga todav√≠a se ven "pobres".</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øQu√© conclusi√≥n es esa? </font><font style="vertical-align: inherit;">Es dif√≠cil decirlo con certeza. </font><font style="vertical-align: inherit;">Tal vez los avances en las redes de picos todav√≠a nos esperan: no es por nada que el cerebro consiste en neuronas de pico. </font><font style="vertical-align: inherit;">Y quiz√°s los modelos actuales de picos simplemente no tienen el poder de procesamiento de nuestro hardware para "mostrarse". </font><font style="vertical-align: inherit;">Finalmente, quiz√°s los picos son una caracter√≠stica de bajo nivel de bio-hierro, adem√°s de lo cual el cerebro implementa el mismo descenso de gradiente por error en alguna forma. </font><font style="vertical-align: inherit;">Sin embargo, esto no est√° incluido en la hip√≥tesis de trabajo actual de la neurobiolog√≠a debido a la falta de fundamento.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es390385/">https://habr.com/ru/post/es390385/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es390371/index.html">15 a√±os del primer contacto del aparato de la Tierra con un asteroide</a></li>
<li><a href="../es390373/index.html">El n√∫mero de usuarios de sitios y aplicaciones para citas en l√≠nea est√° creciendo r√°pidamente</a></li>
<li><a href="../es390375/index.html">Ubuntu Tablet: "todo lo que necesitas de una PC est√° en una tableta"</a></li>
<li><a href="../es390379/index.html">¬øTrabajas demasiado? Las vacaciones no ayudar√°n</a></li>
<li><a href="../es390381/index.html">14 de febrero - D√≠a de Lawrence: Regalos para tu ser querido</a></li>
<li><a href="../es390387/index.html">Los titulares de derechos inventan nuevas formas de convertir a los piratas en usuarios que pagan</a></li>
<li><a href="../es390389/index.html">Tecnolog√≠a FRAM</a></li>
<li><a href="../es390391/index.html">Calidad de comunicaci√≥n: aplicaci√≥n de Android del Ministerio de Comunicaciones</a></li>
<li><a href="../es390393/index.html">C√≥mo despertar juegos de inter√©s en la historia o la mesa redonda de Wargaming</a></li>
<li><a href="../es390395/index.html">El troyano bancario inteligente le permite retirar una cantidad casi ilimitada de dinero en cajeros autom√°ticos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>