<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úçüèΩ üõÄ üñáÔ∏è Presentaci√≥n de Airflow para administrar Spark Jobs en ivi: esperanzas y muletas üéá üçí ‚ôÇÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La tarea de desplegar modelos de aprendizaje autom√°tico en la producci√≥n siempre es dolorosa, porque es muy inc√≥modo salir de una acogedora computador...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Presentaci√≥n de Airflow para administrar Spark Jobs en ivi: esperanzas y muletas</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ivi/blog/456630/">  La tarea de desplegar modelos de aprendizaje autom√°tico en la producci√≥n siempre es dolorosa, porque es muy inc√≥modo salir de una acogedora computadora port√°til Jupyter al mundo de la monitorizaci√≥n y la tolerancia a fallas. <br><br>  Ya escribimos sobre la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primera iteraci√≥n de refactorizar el</a> sistema de recomendaci√≥n del cine en l√≠nea ivi.  Durante el a√±o pasado, casi no finalizamos la arquitectura de la aplicaci√≥n (de global, solo pasamos de python 2.7 y python 3.4 obsoletos a python 3.6 "nuevo"), pero agregamos algunos modelos nuevos de ML e inmediatamente nos encontramos con el problema de implementar nuevos algoritmos en producci√≥n.  En el art√≠culo, contar√© sobre nuestra experiencia en la implementaci√≥n de una herramienta de gesti√≥n de flujo de tareas como Apache Airflow: por qu√© el equipo ten√≠a esta necesidad, qu√© no se adaptaba a la soluci√≥n existente, qu√© muletas ten√≠an que cortarse en el camino y qu√© surgi√≥ de ella. <br><br>  ‚Üí La versi√≥n de video del informe se puede ver en YouTube (a partir de las 03:00:00) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/bw/tt/qcbwttjzivyzuedihk88u6nwn5o.png"></div><br><a name="habracut"></a><br><br><h2>  <font color="#fd004c">Equipo Hydra</font> </h2><br>  Te contar√© un poco sobre el proyecto: ivi es varias decenas de miles de unidades de contenido, tenemos uno de los directorios legales m√°s grandes de RuNet.  La p√°gina principal de la versi√≥n web de ivi es un corte personalizado del cat√°logo, que est√° dise√±ado para proporcionar al usuario el contenido m√°s rico y relevante en funci√≥n de sus comentarios (vistas, calificaciones, etc.). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vj/-g/nh/vj-gnheaviq-z7tibuzawo-qdqs.png"></div><br>  La parte en l√≠nea del sistema de recomendaci√≥n es una aplicaci√≥n de fondo de Flask con una carga de hasta 600 RPS.  Fuera de l√≠nea, el modelo est√° entrenado en m√°s de 250 millones de vistas de contenido por mes.  Los canales de preparaci√≥n de datos para la capacitaci√≥n se implementan en Spark, que se ejecuta en la parte superior del repositorio de Hive. <br><br>  El equipo ahora tiene 7 desarrolladores que se dedican tanto a crear modelos como a implementarlos en producci√≥n; este es un equipo bastante grande que requiere herramientas convenientes para administrar los flujos de tareas. <br><br><h2>  <font color="#fd004c">Arquitectura sin conexi√≥n</font> </h2><br>  A continuaci√≥n puede ver el diagrama de infraestructura de flujo de datos para el sistema de recomendaci√≥n. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xs/10/nv/xs10nvc8r3mz9bd8osjrn6sqn3u.png"></div><br>  Aqu√≠ se muestran dos almacenamientos de datos: Hive para comentarios de los usuarios (vistas, calificaciones) y Postgres para diversa informaci√≥n comercial (tipos de monetizaci√≥n de contenido, etc.), mientras se ajusta la transferencia de Postgres a Hive.  Un paquete de aplicaciones Spark absorbe datos de Hive: y entrena a nuestros modelos con estos datos (ALS para recomendaciones personales, varios modelos colaborativos de similitud de contenido). <br><br>  Las aplicaciones de Spark se han gestionado tradicionalmente desde una m√°quina virtual dedicada, a la que llamamos Hydra-Updater utilizando un mont√≥n de scripts cron + shell.  Este paquete fue creado en el departamento de operaciones de ivi en tiempos inmemoriales y funcion√≥ muy bien.  Shell-script fue un punto de entrada √∫nico para lanzar aplicaciones de chispa, es decir, cada nuevo modelo comenz√≥ a girar en el producto solo despu√©s de que los administradores terminaron este script. <br><br>  Algunos de los artefactos de la capacitaci√≥n de modelos se almacenan en HDFS para el almacenamiento eterno (y esperando que alguien los descargue de all√≠ y los transfiera al servidor donde gira la parte en l√≠nea), y algunos se escriben directamente desde el controlador Spark al almacenamiento r√°pido de Redis, que usamos como general memoria para varias docenas de procesos python de la parte en l√≠nea. <br><br>  Tal arquitectura ha acumulado una serie de desventajas con el tiempo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ri/x2/b0/rix2b0qjxi1qa0igp-bpl23bn04.png"></div><br>  El diagrama muestra que los flujos de datos tienen una estructura bastante complicada y complicada: sin una herramienta simple y clara para administrar este bien, el desarrollo y la operaci√≥n se convertir√°n en horror, decadencia y sufrimiento. <br><br>  Adem√°s de administrar aplicaciones de chispa, el script de administraci√≥n hace muchas cosas √∫tiles: reiniciar servicios en batalla, un volcado de Redis y otras cosas del sistema.  Obviamente, durante un largo per√≠odo de operaci√≥n, el script ha crecido con muchas funciones, ya que cada nuevo modelo nuestro gener√≥ un par de docenas de l√≠neas.  El script comenz√≥ a verse demasiado sobrecargado en t√©rminos de funcionalidad, por lo tanto, como equipo del sistema de recomendaci√≥n, quer√≠amos eliminar en alg√∫n lugar una parte de la funcionalidad que se refiere al lanzamiento y administraci√≥n de aplicaciones Spark.  Para estos fines, decidimos usar Airflow. <br><br><h2>  <font color="#fd004c">Muletas para flujo de aire</font> </h2><br>  Adem√°s de resolver todos estos problemas, por supuesto, en la forma en que creamos otros nuevos para nosotros: implementar Airflow para iniciar y monitorear aplicaciones Spark result√≥ ser dif√≠cil. <br><br>  La principal dificultad fue que nadie nos remodelar√≠a toda la infraestructura, porque  El recurso devops es algo escaso.  Por esta raz√≥n, tuvimos que no solo implementar Airflow, sino integrarlo en el sistema existente, que es mucho m√°s dif√≠cil de ver desde cero. <br><br>  Quiero hablar sobre los dolores que encontramos durante el proceso de implementaci√≥n, y las muletas que tuvimos que cortar para obtener Airflow. <br><br>  <b>El primer y principal dolor</b> : c√≥mo integrar Airflow en un gran script de shell del departamento de operaciones. <br><br>  Aqu√≠ la soluci√≥n es la m√°s obvia: comenzamos a activar gr√°ficos directamente desde el script de shell utilizando el binario de flujo de aire con la tecla trigger_dag.  Con este enfoque, no utilizamos el programador Airflow y, de hecho, la aplicaci√≥n Spark se inicia con la misma corona; esto es religiosamente poco correcto.  Pero obtuvimos una integraci√≥n perfecta con una soluci√≥n existente.  As√≠ es como se ve el comienzo del script de shell de nuestra aplicaci√≥n principal de Spark, que hist√≥ricamente se llama hidramatrices. <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$FUNCNAME</span></span></span><span class="hljs-string"> started"</span></span> <span class="hljs-built_in"><span class="hljs-built_in">local</span></span> RETVAL=0 <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_CONFIG=/opt/airflow/airflow.cfg AIRFLOW_API=api/dag_last_run/hydramatrices/all <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"run /var/www/airflow/bin/airflow trigger_dag hydramatrices"</span></span> /var/www/airflow/bin/airflow trigger_dag hydramatrices 2&gt;&amp;1 | tee -a <span class="hljs-variable"><span class="hljs-variable">$LOGFILE</span></span></code> </pre> <br>  <b>Dolor: el</b> script de shell del departamento de operaciones debe determinar de alguna manera el estado del gr√°fico Airflow para controlar su propio flujo de ejecuci√≥n. <br><br>  Crutch: ampliamos la API REST de Airflow con un punto final para la supervisi√≥n de DAG dentro de los scripts de shell.  Ahora cada gr√°fico tiene tres estados: CORRER, EXITAR, FALLAR. <br><br>  De hecho, despu√©s de comenzar los c√°lculos en Airflow, simplemente sondeamos regularmente el gr√°fico en ejecuci√≥n: enviamos la solicitud GET para determinar si el DAG se ha completado o no.  Cuando el punto final de supervisi√≥n responde sobre la ejecuci√≥n exitosa del gr√°fico, el script de shell contin√∫a ejecutando su flujo. <br>  Quiero decir que la API REST de Airflow es solo una cosa ardiente que le permite configurar de manera flexible sus tuber√≠as; por ejemplo, puede reenviar par√°metros POST a gr√°ficos. <br><br>  La extensi√≥n API de Airflow es solo una clase de Python que se parece a esto: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> settings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DagBag, DagRun <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> flask <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Blueprint, request, Response airflow_api_blueprint = Blueprint(<span class="hljs-string"><span class="hljs-string">'airflow_api'</span></span>, __name__, url_prefix=<span class="hljs-string"><span class="hljs-string">'/api'</span></span>) AIRFLOW_DAGS = <span class="hljs-string"><span class="hljs-string">'{}/dags'</span></span>.format( os.path.dirname(os.path.dirname(os.path.abspath(__file__))) ) <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ApiResponse</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    GET """</span></span> STATUS_OK = <span class="hljs-number"><span class="hljs-number">200</span></span> STATUS_NOT_FOUND = <span class="hljs-number"><span class="hljs-number">404</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">standard_response</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(status: int, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> json_data = json.dumps(payload) resp = Response(json_data, status=status, mimetype=<span class="hljs-string"><span class="hljs-string">'application/json'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> resp <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">success</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(self.STATUS_OK, payload) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">error</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, status: int, message: str)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(status, {<span class="hljs-string"><span class="hljs-string">'error'</span></span>: message}) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">not_found</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, message: str = </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'Resource not found'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.error(self.STATUS_NOT_FOUND, message)</code> </pre><br>  Usamos la API en el script de shell: sondeamos el punto final cada 10 minutos: <br><br><pre> <code class="bash hljs"> TRIGGER=$? [ <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TRIGGER</span></span></span><span class="hljs-string">"</span></span> -eq <span class="hljs-string"><span class="hljs-string">"0"</span></span> ] &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG succeeded"</span></span> || { <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG failed"</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> 1; } CMD=<span class="hljs-string"><span class="hljs-string">"curl -s http://</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HYDRA_SERVER</span></span></span><span class="hljs-string">/</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$AIRFLOW_API</span></span></span><span class="hljs-string"> | jq .dag_last_run.state"</span></span> STATE=$(<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span> <span class="hljs-variable"><span class="hljs-variable">$CMD</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> [ <span class="hljs-variable"><span class="hljs-variable">$STATE</span></span> == \<span class="hljs-string"><span class="hljs-string">"running\" ]; do log "</span></span>Generating matrices <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> progress...<span class="hljs-string"><span class="hljs-string">" sleep 600 STATE=</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(eval $CMD)</span></span></span><span class="hljs-string"> done [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$STATE</span></span></span><span class="hljs-string"> == \"success\" ] &amp;&amp; RETVAL=0 || RETVAL=1 [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span><span class="hljs-string"> -eq 0 ] &amp;&amp; log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> succeeded<span class="hljs-string"><span class="hljs-string">" || log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> failed<span class="hljs-string"><span class="hljs-string">" return </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span></span></code> </pre><br>  <b>Dolor</b> : si alguna vez ejecuta un trabajo de Spark usando el env√≠o de chispa en modo de cl√∫ster, entonces sabe que los registros en STDOUT son una hoja no informativa con las l√≠neas "SPARK APPLICATION_ID IS RUNNING".  Los registros de la aplicaci√≥n Spark en s√≠ podr√≠an verse, por ejemplo, utilizando el comando yarn logs.  En un script de shell, este problema se resolvi√≥ simplemente: se abri√≥ un t√∫nel SSH en una de las m√°quinas de cl√∫ster y se ejecut√≥ el env√≠o de chispas en modo cliente para esta m√°quina.  En este caso, STDOUT tendr√° registros legibles y comprensibles.  En Airflow, decidimos usar siempre cluster-decide, y ese n√∫mero no funcionar√°. <br><br>  Muleta: despu√©s de que spark-submit ha funcionado, extraemos los registros del controlador de HDFS mediante application_id y lo mostramos en la interfaz Airflow simplemente a trav√©s del operador Python print ().  Lo √∫nico negativo: en la interfaz Airflow, los registros aparecen solo despu√©s de que el env√≠o de chispas ha funcionado, debe controlar el tiempo real en otros lugares, por ejemplo, el hocico web YARN. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_logs</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(config: BaseConfig, app_id: str)</span></span></span><span class="hljs-function"> -&gt; </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">None</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   :param config: :param app_id: """</span></span> hdfs = HDFSInteractor(config) logs_path = <span class="hljs-string"><span class="hljs-string">'/tmp/logs/{username}/logs/{app_id}'</span></span>.format(username=config.CURRENT_USERNAME, app_id=app_id) logs_files = hdfs.files_in_folder(logs_path) logs_files = [file <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> file[<span class="hljs-number"><span class="hljs-number">-4</span></span>:] != <span class="hljs-string"><span class="hljs-string">'.tmp'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> hdfs.hdfs_client.read(os.path.join(logs_path, file), encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>, delimiter=<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> reader: print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> reader: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stdout'</span></span>, line) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> len(line) &gt; <span class="hljs-number"><span class="hljs-number">30</span></span>: print_line = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stderr'</span></span>, line): print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> print_line: print(line)</code> </pre><br>  <b>Dolor</b> : para los probadores y desarrolladores, ser√≠a bueno tener un banco de pruebas de Airflow, pero estamos ahorrando recursos de DevOps, por lo que pensamos en c√≥mo implementar el entorno de prueba durante mucho tiempo. <br><br>  Muleta: empacamos Airflow en un contenedor acoplable, y Dockerfile lo coloc√≥ en el repositorio con trabajos de chispa.  Por lo tanto, cada desarrollador o probador puede aumentar su propio flujo de aire en una m√°quina local.  Debido al hecho de que las aplicaciones se ejecutan en modo de cl√∫ster, los recursos locales para Docker casi no son necesarios. <br><br>  Una instalaci√≥n local de la chispa estaba oculta dentro del contenedor acoplable y su configuraci√≥n completa a trav√©s de variables de entorno: ya no necesita pasar varias horas configurando el entorno.  A continuaci√≥n, proporcion√© un ejemplo con un fragmento de archivo acoplable para un contenedor con Airflow, donde puede ver c√≥mo se configura Airflow utilizando variables de entorno: <br><br><pre> <code class="bash hljs">FROM ubuntu:16.04 ARG AIRFLOW_VERSION=1.9.0 ARG AIRFLOW_HOME ARG USERNAME=airflow ARG USER_ID ARG GROUP_ID ARG LOCALHOST ARG AIRFLOW_PORT ARG PIPENV_PATH ARG PROJECT_HYDRAMATRICES_DOCKER_PATH RUN apt-get update \ &amp;&amp; apt-get install -y \ python3.6 \ python3.6-dev \ &amp;&amp; update-alternatives --install /usr/bin/python3 python3.6 /usr/bin/python3.6 0 \ &amp;&amp; apt-get -y install python3-pip RUN mv /root/.pydistutils.cf /root/.pydistutils.cfg RUN pip3 install pandas==0.20.3 \ apache-airflow==<span class="hljs-variable"><span class="hljs-variable">$AIRFLOW_VERSION</span></span> \ psycopg2==2.7.5 \ ldap3==2.5.1 \ cryptography <span class="hljs-comment"><span class="hljs-comment">#   ,       ENV PROJECT_HYDRAMATRICES_DOCKER_PATH=${PROJECT_HYDRAMATRICES_DOCKER_PATH} ENV PIPENV_PATH=${PIPENV_PATH} ENV SPARK_HOME=/usr/lib/spark2 ENV HADOOP_CONF_DIR=$PROJECT_HYDRAMATRICES_DOCKER_PATH/etc/hadoop-conf-preprod ENV PYTHONPATH=${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${SPARK_HOME}/python/lib/pyspark.zip:${SPARK_HOME}/python/lib ENV PIP_NO_BINARY=numpy ENV AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW_DAGS=${AIRFLOW_HOME}/dags ENV AIRFLOW_LOGS=${AIRFLOW_HOME}/logs ENV AIRFLOW_PLUGINS=${AIRFLOW_HOME}/plugins #      Airflow (log url) BASE_URL="http://${AIRFLOW_CURRENT_HOST}:${AIRFLOW_PORT}" ; #   Airflow ENV AIRFLOW__WEBSERVER__BASE_URL=${BASE_URL} ENV AIRFLOW__WEBSERVER__ENDPOINT_URL=${BASE_URL} ENV AIRFLOW__CORE__AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_DAGS} ENV AIRFLOW__CORE__BASE_LOG_FOLDER=${AIRFLOW_LOGS} ENV AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_PLUGINS} ENV AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY=${AIRFLOW_LOGS}/scheduler</span></span></code> </pre><br>  Como resultado de la implementaci√≥n de Airflow, logramos los siguientes resultados: <br><br><ul><li>  Reducci√≥n del ciclo de lanzamiento: la implementaci√≥n de un nuevo modelo (o una tuber√≠a de preparaci√≥n de datos) ahora se reduce a escribir un nuevo gr√°fico de Airflow, los gr√°ficos se almacenan en el repositorio y se implementan con el c√≥digo.  Este proceso est√° completamente en manos del desarrollador.  Los administradores est√°n contentos, ya no los tiramos de bagatelas. </li><li>  Los registros de aplicaciones de Spark que sol√≠an ir directamente al infierno ahora se almacenan en Aiflow con una interfaz de acceso conveniente.  Puede ver los registros de cualquier d√≠a sin seleccionar en los directorios HDFS. </li><li>  El c√°lculo fallido puede reiniciarse con un bot√≥n en la interfaz, es muy conveniente, incluso junio puede manejarlo. </li><li>  Puede hacer vi√±etas de trabajos de chispa desde la interfaz sin tener que ejecutar la configuraci√≥n de Spark en la m√°quina local.  Los probadores est√°n contentos: todas las configuraciones para que el env√≠o de chispas funcione correctamente ya est√°n hechas en Dockerfile </li><li>  Bollos est√°ndar de Aiflow: horarios, reinicio de trabajos ca√≠dos, gr√°ficos hermosos (por ejemplo, tiempo de ejecuci√≥n de la aplicaci√≥n, estad√≠sticas de lanzamientos exitosos y no exitosos). </li></ul><br>  ¬øA d√≥nde ir despu√©s?  Ahora tenemos una gran cantidad de fuentes de datos y sumideros, cuyo n√∫mero crecer√°.  Los cambios en cualquier clase de repositorio de hidramatrices pueden bloquearse en otra tuber√≠a (o incluso en la parte en l√≠nea): <br><br><ul><li>  Clickhouse se desborda ‚Üí Colmena </li><li>  preprocesamiento de datos: Hive ‚Üí Hive </li><li>  implementar modelos c2c: Colmena ‚Üí Redis </li><li>  preparaci√≥n de directorios (como el tipo de monetizaci√≥n de contenido): Postgres ‚Üí Redis </li><li>  preparaci√≥n del modelo: FS local ‚Üí HDFS </li></ul><br>  En tal situaci√≥n, realmente necesitamos un soporte para las pruebas autom√°ticas de tuber√≠as en la preparaci√≥n de datos.  Esto reducir√° en gran medida el costo de probar los cambios en el repositorio, acelerar√° la implementaci√≥n de nuevos modelos en producci√≥n y aumentar√° dr√°sticamente el nivel de endorfinas en los probadores.  ¬°Pero sin Airflow, ser√≠a imposible implementar un soporte para este tipo de prueba autom√°tica! <br><br>  Escrib√≠ este art√≠culo para hablar sobre nuestra experiencia en la implementaci√≥n de Airflow, que puede ser √∫til para otros equipos en una situaci√≥n similar: ya tiene un gran sistema de trabajo y desea probar algo nuevo, moderno y juvenil.  No es necesario tener miedo a las actualizaciones del sistema de trabajo, debe probar y experimentar, tales experimentos generalmente abren nuevos horizontes para un mayor desarrollo. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/456630/">https://habr.com/ru/post/456630/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456614/index.html">C√≥mo se desarrolla un cuento de peste: el marco de la inocencia</a></li>
<li><a href="../456616/index.html">3 millones de rublos para aquellos que pueden codificar</a></li>
<li><a href="../456618/index.html">Larabeer Mosc√∫ - 21 de junio</a></li>
<li><a href="../456622/index.html">C√≥mo crear un sistema operativo certificado seg√∫n la protecci√≥n de clase I</a></li>
<li><a href="../456624/index.html">Herramientas √∫tiles de Python</a></li>
<li><a href="../456632/index.html">Estamos construyendo el cuarto piso de plantillas C ++ en RESTinio. ¬øPor qu√© y c√≥mo?</a></li>
<li><a href="../456634/index.html">Recetas Nginx: CAS (Servicio de autorizaci√≥n central)</a></li>
<li><a href="../456638/index.html">Comparando el mismo proyecto en Rust, Haskell, C ++, Python, Scala y OCaml</a></li>
<li><a href="../456640/index.html">An√°lisis del concurso de inteligencia competitiva en PHDays 9</a></li>
<li><a href="../456642/index.html">La primera graduaci√≥n del programa de Maestr√≠a Corporativa de JetBrains y la Universidad ITMO</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>