<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨‍👨‍👧‍👦 🥓 🎭 Comment nous avons implémenté le cache sur la base de données Tarantool 🍊 🐧 👊🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour! 

 Je veux partager avec vous une histoire sur l'implémentation du cache sur la base de données Tarantool et mes fonctionnalités de travail. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment nous avons implémenté le cache sur la base de données Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441686/"> Bonjour! <br><br>  Je veux partager avec vous une histoire sur l'implémentation du cache sur la base de données Tarantool et mes fonctionnalités de travail. <br>  Je travaille en tant que développeur Java dans une entreprise de télécommunications.  La tâche principale: la mise en œuvre de la logique métier pour la plateforme que l'entreprise a achetée au vendeur.  Parmi les premières fonctionnalités, il s'agit du travail de savon et de l'absence presque totale de mise en cache, sauf dans la mémoire JVM.  Tout cela est bien sûr bon jusqu'à ce que le nombre d'instances d'application dépasse deux douzaines ... <br><br>  Au fil des travaux et de l'émergence d'une compréhension des fonctionnalités de la plateforme, une tentative de mise en cache a été tentée.  A cette époque, MongoDB était déjà lancé, et en conséquence, nous n'avons pas obtenu de résultats positifs spéciaux comme dans le test. <br><br>  Dans une nouvelle recherche d'alternatives et de conseils de mon bon ami <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">mr_elzor</a> , il a été décidé d'essayer la base de données Tarantool. <br><a name="habracut"></a><br>  Dans une étude superficielle, seul le doute est apparu en lua, car je n'y avais pas écrit du mot «complètement».  Mais repoussant tous les doutes, il se mit à installer.  Concernant les réseaux fermés et les pare-feu, je pense que peu de gens sont intéressés, mais je vous conseille d'essayer de les contourner et de tout mettre à partir de sources publiques. <br><br>  Serveurs de test avec configuration: 8 CPU, 16 Go de RAM, 100 Go de disque dur, Debian 9.4. <br><br>  L'installation était conforme aux instructions du site.  Et donc j'ai eu un exemple d'option.  L'idée est immédiatement apparue d'une interface visuelle avec laquelle le support fonctionnerait commodément.  Lors d'une recherche rapide, j'ai trouvé et configuré <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tarantool-admin</a> .  Fonctionne chez Docker et couvre à 100% les tâches de support, du moins pour l'instant. <br><br>  Mais parlons de plus intéressant. <br><br>  L'idée suivante a été de configurer ma version dans la configuration maître-esclave au sein du même serveur, car la documentation ne contient que des exemples avec deux serveurs différents. <br><br>  Après avoir passé un peu de temps à comprendre lua et à décrire la configuration, je lance l'assistant. <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@master Job for tarantool@master.service failed because the control process exited with error code. See "systemctl status tarantool@master.service" and "journalctl -xe" for details.</span></span></code> </pre> <br>  Je tombe immédiatement dans une stupeur et je ne comprends pas pourquoi l’erreur est, mais je vois qu’elle est en état de «chargement». <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@master ● tarantool@master.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: activating (start) since Tue 2019-02-19 17:03:24 MSK; 17s ago Docs: man:tarantool(1) Process: 20111 ExecStop=/usr/bin/tarantoolctl stop master (code=exited, status=0/SUCCESS) Main PID: 20120 (tarantool) Status: "loading" Tasks: 5 (limit: 4915) CGroup: /system.slice/system-tarantool.slice/tarantool@master.service └─20120 tarantool master.lua &lt;loading&gt; Feb 19 17:03:24 tarantuldb-tst4 systemd[1]: Starting Tarantool Database Server... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Starting instance master... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Run console at unix/:/var/run/tarantool/master.control Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: started</span></span></code> </pre><br>  Je lance l'esclave: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@slave2 Job for tarantool@slave2.service failed because the control process exited with error code. See "systemctl status tarantool@slave2.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Et je vois la même erreur.  Ici, je commence généralement à tendre et à ne pas comprendre ce qui se passe, car il n'y a absolument rien dans la documentation à ce sujet ... Mais en vérifiant le statut, je vois qu'il n'a pas du tout démarré, bien qu'il indique que le statut est "en cours d'exécution": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@slave2 ● tarantool@slave2.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2019-02-19 17:04:52 MSK; 27s ago Docs: man:tarantool(1) Process: 20258 ExecStop=/usr/bin/tarantoolctl stop slave2 (code=exited, status=0/SUCCESS) Process: 20247 ExecStart=/usr/bin/tarantoolctl start slave2 (code=exited, status=1/FAILURE) Main PID: 20247 (code=exited, status=1/FAILURE) Status: "running" Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Service hold-off time over, scheduling restart. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Stopped Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Start request repeated too quickly. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Failed to start Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'.</span></span></code> </pre><br>  Mais en même temps, le maître a commencé à travailler: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20158 1 0 17:04 ? 00:00:00 tarantool master.lua &lt;running&gt; root 20268 2921 0 17:06 pts/1 00:00:00 grep taran</span></span></code> </pre><br>  Le redémarrage de l'esclave n'aide pas.  Je me demande pourquoi? <br><br>  J'arrête le maître.  Et effectuez les actions dans l'ordre inverse. <br><br>  Je vois que l'esclave essaie de démarrer. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20399 1 0 17:09 ? 00:00:00 tarantool slave2.lua &lt;loading&gt;</span></span></code> </pre><br>  Je lance l'assistant et constate qu'il n'a pas augmenté et est généralement passé au statut d'orphelin, tandis que l'esclave est généralement tombé. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20428 1 0 17:09 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Cela devient encore plus intéressant. <br><br>  Je vois dans les journaux de l'esclave qu'il a même vu le maître et essayé de se synchroniser. <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: binding to 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto/101/main I&gt; binary: bound to 0.0.0.0:3302 2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: listening on 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main/101/slave2 I&gt; connecting to 1 replicas 2019-02-19 17:13:45.113 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECT 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t I&gt; remote master 825af7c3-f8df-4db0-8559-a866b8310077 at 10.78.221.74:3301 running Tarantool 1.10.2 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECTED 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; connected to 1 replicas 2019-02-19 17:13:45.114 [20751] coio V&gt; loading vylog 14 2019-02-19 17:13:45.114 [20751] coio V&gt; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span> loading vylog 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovery start 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovering from `/var/lib/tarantool/cache_slave2/00000000000000000014.snap<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(47) = 0x7f99a4000080 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; cluster uuid 4035b563-67f8-4e85-95cc-e03429f1fa4d 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(11) = 0x7f99a4004080 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(17) = 0x7f99a4008068</span></span></code> </pre><br>  Et la tentative a réussi: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a40004c0 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 1 to replica 825af7c3-f8df-4db0-8559-a866b8310077 2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a4000500 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 2 to replica 403c0323-5a9b-480d-9e71-5ba22d4ccf1b 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; recover from `/var/lib/tarantool/slave2/00000000000000000014.xlog<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; done `/var/lib/tarantool/slave2/00000000000000000014.xlog'</span></span></code> </pre><br>  Cela a même commencé: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.119 [20751] main/101/slave2 D&gt; systemd: sending message <span class="hljs-string"><span class="hljs-string">'STATUS=running'</span></span></code> </pre><br>  Mais pour des raisons inconnues, il a perdu la connexion et est tombé: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.129 [20751] main/101/slave2 D&gt; SystemError at /build/tarantool-1.10.2.146/src/coio_task.c:416 2019-02-19 17:13:45.129 [20751] main/101/slave2 tarantoolctl:532 E&gt; Start failed: /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/share/lua/5.1/http/server.lua:1146: Can<span class="hljs-string"><span class="hljs-string">'t create tcp_server: Input/output error</span></span></code> </pre><br>  Essayer de redémarrer l'esclave n'aide pas. <br><br>  Supprimez maintenant les fichiers créés par les instances.  Dans mon cas, je supprime tout du répertoire / var / lib / tarantool. <br><br>  Je commence l'esclave d'abord, et ensuite seulement le maître.  Et voilà ... <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 17:20 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 20933 1 1 17:21 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Je n'ai trouvé aucune explication à ce comportement, sauf en tant que «fonctionnalité de ce logiciel». <br>  Cette situation apparaîtra à chaque fois si votre serveur a complètement redémarré. <br><br>  Après une analyse plus approfondie de l'architecture de ce logiciel, il s'avère qu'il est prévu d'utiliser un seul vCPU pour une instance et de nombreuses autres ressources restent gratuites. <br><br>  Dans l'idéologie de n vCPU, nous pouvons élever le maître et n-2 esclaves pour la lecture. <br><br>  Étant donné que sur le serveur de test 8 vCPU, nous pouvons augmenter le maître et 6 instances pour la lecture. <br>  Je copie le fichier pour esclave, corrige les ports et exécute, c'est-à-dire  quelques esclaves supplémentaires sont ajoutés. <br><br>  Important!  Lors de l'ajout d'une autre instance, vous devez l'enregistrer sur l'assistant. <br>  Mais vous devez d'abord démarrer un nouvel esclave, puis seulement redémarrer le maître. <br><br><h4>  Exemple </h4><br>  J'avais déjà une configuration en cours d'exécution avec un assistant et deux esclaves. <br><br>  J'ai décidé d'ajouter un troisième esclave. <br><br>  Je l'ai enregistré sur le maître et l'ai redémarré en premier, et voici ce que j'ai vu: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:29 tarantool slave3.lua &lt;running&gt; taranto+ 21519 1 0 09:16 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  C'est-à-dire  notre maître est devenu un solitaire, et la réplication s'est effondrée. <br><br>  Le démarrage d'un nouvel esclave n'aidera plus et entraînera une erreur: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl restart tarantool@slave4 Job for tarantool@slave4.service failed because the control process exited with error code. See "systemctl status tarantool@slave4.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Et dans les journaux, j'ai vu une petite entrée informative: <br><br><pre> <code class="bash hljs">2019-02-20 09:20:10.616 [21601] main/101/slave4 I&gt; bootstrapping replica from 3c77eb9d-2fa1-4a27-885f-e72defa5cd96 at 10.78.221.74:3301 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t I&gt; can<span class="hljs-string"><span class="hljs-string">'t join/subscribe 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t xrow.c:896 E&gt; ER_READONLY: Can'</span></span>t modify data because this instance is <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-built_in"><span class="hljs-built_in">read</span></span>-only mode. 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; STOPPED 2019-02-20 09:20:10.617 [21601] main/101/slave4 xrow.c:896 E&gt; ER_READONLY: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode. 2019-02-20 09:20:10.617 [21601] main/101/slave4 F&gt; can'</span></span>t initialize storage: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode.</span></span></code> </pre><br>  Nous arrêtons l'assistant et démarrons un nouvel esclave.  Il y aura également une erreur, comme au premier démarrage, mais nous verrons qu'il est en cours de chargement. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21659 1 0 09:23 ? 00:00:00 tarantool slave4.lua &lt;loading&gt;</span></span></code> </pre><br>  Mais lorsque vous démarrez master, le nouvel esclave se bloque et master ne suit pas l'état en cours d'exécution. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21670 1 0 09:23 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Dans cette situation, il n'y a qu'une seule issue.  Comme je l'ai écrit plus tôt, je supprime les fichiers créés par les instances et exécute d'abord les esclaves, puis le maître. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tarantool taranto+ 21892 1 0 09:30 ? 00:00:00 tarantool slave4.lua &lt;running&gt; taranto+ 21907 1 0 09:30 ? 00:00:00 tarantool slave3.lua &lt;running&gt; taranto+ 21922 1 0 09:30 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 21931 1 0 09:30 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Tout a commencé avec succès. <br><br>  C'est ainsi que, par essais et erreurs, j'ai compris comment configurer et démarrer correctement la réplication. <br><br>  En conséquence, la configuration suivante a été assemblée: <br><br>  <i>2 serveurs.</i> <i><br></i>  <i>2 maître.</i>  <i>Réserve chaude.</i> <i><br></i>  <i>12 esclaves.</i>  <i>Tous sont actifs.</i> <br><br>  Dans la logique de tarantool, http.server a été utilisé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://github.com/tarantool/">pour</a> ne pas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://github.com/tarantool/">bloquer l'</a> adaptateur supplémentaire (rappelez-vous le fournisseur, la plate-forme et le savon) ou attacher la bibliothèque à chaque processus métier. <br><br>  Afin d'éviter une divergence entre les masters, sur l'équilibreur (NetScaler, HAProxy ou tout autre votre favori), nous fixons la règle de réserve, c'est-à-dire  les opérations d'insertion, de mise à jour et de suppression ne vont qu'au premier maître actif. <br><br>  À ce moment, le second réplique simplement les enregistrements du premier.  Les esclaves eux-mêmes sont connectés au premier maître spécifié à partir de la configuration, ce dont nous avons besoin dans cette situation. <br><br>  Sur lua, implémentation des opérations CRUD pour la valeur-clé.  Pour le moment, cela suffit pour résoudre le problème. <br><br>  Compte tenu des caractéristiques de l'utilisation de soap, un processus métier proxy a été mis en œuvre, dans lequel la logique de travailler avec une tarentule via http a été posée. <br><br>  Si les données clés sont présentes, elles sont retournées immédiatement.  Sinon, une demande est envoyée au système maître et stockée dans la base de données Tarantool. <br><br>  En conséquence, un processus métier dans les tests traite jusqu'à 4 000 requêtes.  Dans ce cas, le temps de réponse de la tarentule est d'environ 1 ms.  Le temps de réponse moyen peut atteindre 3 ms. <br><br>  Voici quelques informations issues des tests: <br><br><img src="https://habrastorage.org/webt/n6/ae/lg/n6aelg4tipin2jgomzrgsw_8nie.png"><br><br>  Il y avait 50 processus métier qui vont à 4 systèmes maîtres et mettent en cache les données dans leur mémoire.  Duplication des informations en pleine croissance à chaque instance.  Étant donné que java aime déjà la mémoire ... la perspective n'est pas la meilleure. <br><br><h4>  Maintenant </h4><br>  50 processus métier demandent des informations via le cache.  Désormais, les informations de 4 instances de l'assistant sont stockées au même endroit et ne sont pas mises en cache dans chaque instance.  Il a été possible de réduire considérablement la charge sur le système maître, il n'y a pas de doublons d'informations et la consommation de mémoire sur les instances avec logique métier a diminué. <br><br>  Un exemple de la taille du stockage d'informations dans la mémoire de la tarentule: <br><br><img src="https://habrastorage.org/webt/es/93/ex/es93exozhrhnihbnq6-zpzjobma.png"><br><br>  À la fin de la journée, ces chiffres peuvent doubler, mais il n'y a pas de «rabattement» des performances. <br><br>  Au combat, la version actuelle crée 2k - 2,5k requêtes par seconde de charge réelle.  Le temps de réponse moyen est similaire aux tests jusqu'à 3 ms. <br><br>  Si vous regardez htop sur l'un des serveurs avec tarantool, nous verrons qu'ils "refroidissent": <br><br><img src="https://habrastorage.org/webt/jt/mt/vf/jtmtvfat0ohxhugounnve47l7ju.png"><br><br><h4>  Résumé </h4><br>  Malgré toutes les subtilités et nuances de la base de données Tarantool, vous pouvez obtenir d'excellentes performances. <br><br>  J'espère que ce projet se développera et que ces moments inconfortables seront résolus. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr441686/">https://habr.com/ru/post/fr441686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr441676/index.html">Comment se faire des amis PLUTO et HDSDR</a></li>
<li><a href="../fr441678/index.html">Jeu physique des tornades: comment l'aérodynamique est implémentée dans Just Cause 4 (trafic)</a></li>
<li><a href="../fr441680/index.html">Programme de la conférence Lua à Moscou 2019</a></li>
<li><a href="../fr441682/index.html">HyperX Fury 3D - SSD avec un pedigree clair</a></li>
<li><a href="../fr441684/index.html">Prédictions: les nuages ​​vont changer 2019</a></li>
<li><a href="../fr441688/index.html">Les jeux changent le monde: comment Hellblade attire l'attention sur les problèmes des personnes atteintes de maladie mentale</a></li>
<li><a href="../fr441690/index.html">Vous n'avez pas besoin de blockchain: huit cas d'utilisation bien connus et pourquoi ils ne fonctionnent pas</a></li>
<li><a href="../fr441692/index.html">Comment couvrir les pistes de la blockchain? Notre concept de mélangeur de transactions</a></li>
<li><a href="../fr441694/index.html">Pourquoi les graphiques de trafic "mentent"</a></li>
<li><a href="../fr441696/index.html">L'histoire de Cyrillic LiveJournal: comment la direction russe a écrasé l'essor des blogs en russe</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>