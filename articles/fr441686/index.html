<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüë®‚Äçüëß‚Äçüë¶ ü•ì üé≠ Comment nous avons impl√©ment√© le cache sur la base de donn√©es Tarantool üçä üêß üëäüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour! 

 Je veux partager avec vous une histoire sur l'impl√©mentation du cache sur la base de donn√©es Tarantool et mes fonctionnalit√©s de travail. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment nous avons impl√©ment√© le cache sur la base de donn√©es Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441686/"> Bonjour! <br><br>  Je veux partager avec vous une histoire sur l'impl√©mentation du cache sur la base de donn√©es Tarantool et mes fonctionnalit√©s de travail. <br>  Je travaille en tant que d√©veloppeur Java dans une entreprise de t√©l√©communications.  La t√¢che principale: la mise en ≈ìuvre de la logique m√©tier pour la plateforme que l'entreprise a achet√©e au vendeur.  Parmi les premi√®res fonctionnalit√©s, il s'agit du travail de savon et de l'absence presque totale de mise en cache, sauf dans la m√©moire JVM.  Tout cela est bien s√ªr bon jusqu'√† ce que le nombre d'instances d'application d√©passe deux douzaines ... <br><br>  Au fil des travaux et de l'√©mergence d'une compr√©hension des fonctionnalit√©s de la plateforme, une tentative de mise en cache a √©t√© tent√©e.  A cette √©poque, MongoDB √©tait d√©j√† lanc√©, et en cons√©quence, nous n'avons pas obtenu de r√©sultats positifs sp√©ciaux comme dans le test. <br><br>  Dans une nouvelle recherche d'alternatives et de conseils de mon bon ami <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">mr_elzor</a> , il a √©t√© d√©cid√© d'essayer la base de donn√©es Tarantool. <br><a name="habracut"></a><br>  Dans une √©tude superficielle, seul le doute est apparu en lua, car je n'y avais pas √©crit du mot ¬´compl√®tement¬ª.  Mais repoussant tous les doutes, il se mit √† installer.  Concernant les r√©seaux ferm√©s et les pare-feu, je pense que peu de gens sont int√©ress√©s, mais je vous conseille d'essayer de les contourner et de tout mettre √† partir de sources publiques. <br><br>  Serveurs de test avec configuration: 8 CPU, 16 Go de RAM, 100 Go de disque dur, Debian 9.4. <br><br>  L'installation √©tait conforme aux instructions du site.  Et donc j'ai eu un exemple d'option.  L'id√©e est imm√©diatement apparue d'une interface visuelle avec laquelle le support fonctionnerait commod√©ment.  Lors d'une recherche rapide, j'ai trouv√© et configur√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tarantool-admin</a> .  Fonctionne chez Docker et couvre √† 100% les t√¢ches de support, du moins pour l'instant. <br><br>  Mais parlons de plus int√©ressant. <br><br>  L'id√©e suivante a √©t√© de configurer ma version dans la configuration ma√Ætre-esclave au sein du m√™me serveur, car la documentation ne contient que des exemples avec deux serveurs diff√©rents. <br><br>  Apr√®s avoir pass√© un peu de temps √† comprendre lua et √† d√©crire la configuration, je lance l'assistant. <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@master Job for tarantool@master.service failed because the control process exited with error code. See "systemctl status tarantool@master.service" and "journalctl -xe" for details.</span></span></code> </pre> <br>  Je tombe imm√©diatement dans une stupeur et je ne comprends pas pourquoi l‚Äôerreur est, mais je vois qu‚Äôelle est en √©tat de ¬´chargement¬ª. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@master ‚óè tarantool@master.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: activating (start) since Tue 2019-02-19 17:03:24 MSK; 17s ago Docs: man:tarantool(1) Process: 20111 ExecStop=/usr/bin/tarantoolctl stop master (code=exited, status=0/SUCCESS) Main PID: 20120 (tarantool) Status: "loading" Tasks: 5 (limit: 4915) CGroup: /system.slice/system-tarantool.slice/tarantool@master.service ‚îî‚îÄ20120 tarantool master.lua &lt;loading&gt; Feb 19 17:03:24 tarantuldb-tst4 systemd[1]: Starting Tarantool Database Server... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Starting instance master... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Run console at unix/:/var/run/tarantool/master.control Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: started</span></span></code> </pre><br>  Je lance l'esclave: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@slave2 Job for tarantool@slave2.service failed because the control process exited with error code. See "systemctl status tarantool@slave2.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Et je vois la m√™me erreur.  Ici, je commence g√©n√©ralement √† tendre et √† ne pas comprendre ce qui se passe, car il n'y a absolument rien dans la documentation √† ce sujet ... Mais en v√©rifiant le statut, je vois qu'il n'a pas du tout d√©marr√©, bien qu'il indique que le statut est "en cours d'ex√©cution": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@slave2 ‚óè tarantool@slave2.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2019-02-19 17:04:52 MSK; 27s ago Docs: man:tarantool(1) Process: 20258 ExecStop=/usr/bin/tarantoolctl stop slave2 (code=exited, status=0/SUCCESS) Process: 20247 ExecStart=/usr/bin/tarantoolctl start slave2 (code=exited, status=1/FAILURE) Main PID: 20247 (code=exited, status=1/FAILURE) Status: "running" Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Service hold-off time over, scheduling restart. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Stopped Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Start request repeated too quickly. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Failed to start Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'.</span></span></code> </pre><br>  Mais en m√™me temps, le ma√Ætre a commenc√© √† travailler: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20158 1 0 17:04 ? 00:00:00 tarantool master.lua &lt;running&gt; root 20268 2921 0 17:06 pts/1 00:00:00 grep taran</span></span></code> </pre><br>  Le red√©marrage de l'esclave n'aide pas.  Je me demande pourquoi? <br><br>  J'arr√™te le ma√Ætre.  Et effectuez les actions dans l'ordre inverse. <br><br>  Je vois que l'esclave essaie de d√©marrer. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20399 1 0 17:09 ? 00:00:00 tarantool slave2.lua &lt;loading&gt;</span></span></code> </pre><br>  Je lance l'assistant et constate qu'il n'a pas augment√© et est g√©n√©ralement pass√© au statut d'orphelin, tandis que l'esclave est g√©n√©ralement tomb√©. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20428 1 0 17:09 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Cela devient encore plus int√©ressant. <br><br>  Je vois dans les journaux de l'esclave qu'il a m√™me vu le ma√Ætre et essay√© de se synchroniser. <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: binding to 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto/101/main I&gt; binary: bound to 0.0.0.0:3302 2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: listening on 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main/101/slave2 I&gt; connecting to 1 replicas 2019-02-19 17:13:45.113 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECT 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t I&gt; remote master 825af7c3-f8df-4db0-8559-a866b8310077 at 10.78.221.74:3301 running Tarantool 1.10.2 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECTED 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; connected to 1 replicas 2019-02-19 17:13:45.114 [20751] coio V&gt; loading vylog 14 2019-02-19 17:13:45.114 [20751] coio V&gt; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span> loading vylog 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovery start 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovering from `/var/lib/tarantool/cache_slave2/00000000000000000014.snap<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(47) = 0x7f99a4000080 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; cluster uuid 4035b563-67f8-4e85-95cc-e03429f1fa4d 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(11) = 0x7f99a4004080 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(17) = 0x7f99a4008068</span></span></code> </pre><br>  Et la tentative a r√©ussi: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a40004c0 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 1 to replica 825af7c3-f8df-4db0-8559-a866b8310077 2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a4000500 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 2 to replica 403c0323-5a9b-480d-9e71-5ba22d4ccf1b 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; recover from `/var/lib/tarantool/slave2/00000000000000000014.xlog<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; done `/var/lib/tarantool/slave2/00000000000000000014.xlog'</span></span></code> </pre><br>  Cela a m√™me commenc√©: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.119 [20751] main/101/slave2 D&gt; systemd: sending message <span class="hljs-string"><span class="hljs-string">'STATUS=running'</span></span></code> </pre><br>  Mais pour des raisons inconnues, il a perdu la connexion et est tomb√©: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.129 [20751] main/101/slave2 D&gt; SystemError at /build/tarantool-1.10.2.146/src/coio_task.c:416 2019-02-19 17:13:45.129 [20751] main/101/slave2 tarantoolctl:532 E&gt; Start failed: /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/share/lua/5.1/http/server.lua:1146: Can<span class="hljs-string"><span class="hljs-string">'t create tcp_server: Input/output error</span></span></code> </pre><br>  Essayer de red√©marrer l'esclave n'aide pas. <br><br>  Supprimez maintenant les fichiers cr√©√©s par les instances.  Dans mon cas, je supprime tout du r√©pertoire / var / lib / tarantool. <br><br>  Je commence l'esclave d'abord, et ensuite seulement le ma√Ætre.  Et voil√† ... <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 17:20 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 20933 1 1 17:21 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Je n'ai trouv√© aucune explication √† ce comportement, sauf en tant que ¬´fonctionnalit√© de ce logiciel¬ª. <br>  Cette situation appara√Ætra √† chaque fois si votre serveur a compl√®tement red√©marr√©. <br><br>  Apr√®s une analyse plus approfondie de l'architecture de ce logiciel, il s'av√®re qu'il est pr√©vu d'utiliser un seul vCPU pour une instance et de nombreuses autres ressources restent gratuites. <br><br>  Dans l'id√©ologie de n vCPU, nous pouvons √©lever le ma√Ætre et n-2 esclaves pour la lecture. <br><br>  √âtant donn√© que sur le serveur de test 8 vCPU, nous pouvons augmenter le ma√Ætre et 6 instances pour la lecture. <br>  Je copie le fichier pour esclave, corrige les ports et ex√©cute, c'est-√†-dire  quelques esclaves suppl√©mentaires sont ajout√©s. <br><br>  Important!  Lors de l'ajout d'une autre instance, vous devez l'enregistrer sur l'assistant. <br>  Mais vous devez d'abord d√©marrer un nouvel esclave, puis seulement red√©marrer le ma√Ætre. <br><br><h4>  Exemple </h4><br>  J'avais d√©j√† une configuration en cours d'ex√©cution avec un assistant et deux esclaves. <br><br>  J'ai d√©cid√© d'ajouter un troisi√®me esclave. <br><br>  Je l'ai enregistr√© sur le ma√Ætre et l'ai red√©marr√© en premier, et voici ce que j'ai vu: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:29 tarantool slave3.lua &lt;running&gt; taranto+ 21519 1 0 09:16 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  C'est-√†-dire  notre ma√Ætre est devenu un solitaire, et la r√©plication s'est effondr√©e. <br><br>  Le d√©marrage d'un nouvel esclave n'aidera plus et entra√Ænera une erreur: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl restart tarantool@slave4 Job for tarantool@slave4.service failed because the control process exited with error code. See "systemctl status tarantool@slave4.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Et dans les journaux, j'ai vu une petite entr√©e informative: <br><br><pre> <code class="bash hljs">2019-02-20 09:20:10.616 [21601] main/101/slave4 I&gt; bootstrapping replica from 3c77eb9d-2fa1-4a27-885f-e72defa5cd96 at 10.78.221.74:3301 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t I&gt; can<span class="hljs-string"><span class="hljs-string">'t join/subscribe 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t xrow.c:896 E&gt; ER_READONLY: Can'</span></span>t modify data because this instance is <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-built_in"><span class="hljs-built_in">read</span></span>-only mode. 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; STOPPED 2019-02-20 09:20:10.617 [21601] main/101/slave4 xrow.c:896 E&gt; ER_READONLY: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode. 2019-02-20 09:20:10.617 [21601] main/101/slave4 F&gt; can'</span></span>t initialize storage: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode.</span></span></code> </pre><br>  Nous arr√™tons l'assistant et d√©marrons un nouvel esclave.  Il y aura √©galement une erreur, comme au premier d√©marrage, mais nous verrons qu'il est en cours de chargement. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21659 1 0 09:23 ? 00:00:00 tarantool slave4.lua &lt;loading&gt;</span></span></code> </pre><br>  Mais lorsque vous d√©marrez master, le nouvel esclave se bloque et master ne suit pas l'√©tat en cours d'ex√©cution. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21670 1 0 09:23 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Dans cette situation, il n'y a qu'une seule issue.  Comme je l'ai √©crit plus t√¥t, je supprime les fichiers cr√©√©s par les instances et ex√©cute d'abord les esclaves, puis le ma√Ætre. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tarantool taranto+ 21892 1 0 09:30 ? 00:00:00 tarantool slave4.lua &lt;running&gt; taranto+ 21907 1 0 09:30 ? 00:00:00 tarantool slave3.lua &lt;running&gt; taranto+ 21922 1 0 09:30 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 21931 1 0 09:30 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Tout a commenc√© avec succ√®s. <br><br>  C'est ainsi que, par essais et erreurs, j'ai compris comment configurer et d√©marrer correctement la r√©plication. <br><br>  En cons√©quence, la configuration suivante a √©t√© assembl√©e: <br><br>  <i>2 serveurs.</i> <i><br></i>  <i>2 ma√Ætre.</i>  <i>R√©serve chaude.</i> <i><br></i>  <i>12 esclaves.</i>  <i>Tous sont actifs.</i> <br><br>  Dans la logique de tarantool, http.server a √©t√© utilis√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://github.com/tarantool/">pour</a> ne pas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://github.com/tarantool/">bloquer l'</a> adaptateur suppl√©mentaire (rappelez-vous le fournisseur, la plate-forme et le savon) ou attacher la biblioth√®que √† chaque processus m√©tier. <br><br>  Afin d'√©viter une divergence entre les masters, sur l'√©quilibreur (NetScaler, HAProxy ou tout autre votre favori), nous fixons la r√®gle de r√©serve, c'est-√†-dire  les op√©rations d'insertion, de mise √† jour et de suppression ne vont qu'au premier ma√Ætre actif. <br><br>  √Ä ce moment, le second r√©plique simplement les enregistrements du premier.  Les esclaves eux-m√™mes sont connect√©s au premier ma√Ætre sp√©cifi√© √† partir de la configuration, ce dont nous avons besoin dans cette situation. <br><br>  Sur lua, impl√©mentation des op√©rations CRUD pour la valeur-cl√©.  Pour le moment, cela suffit pour r√©soudre le probl√®me. <br><br>  Compte tenu des caract√©ristiques de l'utilisation de soap, un processus m√©tier proxy a √©t√© mis en ≈ìuvre, dans lequel la logique de travailler avec une tarentule via http a √©t√© pos√©e. <br><br>  Si les donn√©es cl√©s sont pr√©sentes, elles sont retourn√©es imm√©diatement.  Sinon, une demande est envoy√©e au syst√®me ma√Ætre et stock√©e dans la base de donn√©es Tarantool. <br><br>  En cons√©quence, un processus m√©tier dans les tests traite jusqu'√† 4 000 requ√™tes.  Dans ce cas, le temps de r√©ponse de la tarentule est d'environ 1 ms.  Le temps de r√©ponse moyen peut atteindre 3 ms. <br><br>  Voici quelques informations issues des tests: <br><br><img src="https://habrastorage.org/webt/n6/ae/lg/n6aelg4tipin2jgomzrgsw_8nie.png"><br><br>  Il y avait 50 processus m√©tier qui vont √† 4 syst√®mes ma√Ætres et mettent en cache les donn√©es dans leur m√©moire.  Duplication des informations en pleine croissance √† chaque instance.  √âtant donn√© que java aime d√©j√† la m√©moire ... la perspective n'est pas la meilleure. <br><br><h4>  Maintenant </h4><br>  50 processus m√©tier demandent des informations via le cache.  D√©sormais, les informations de 4 instances de l'assistant sont stock√©es au m√™me endroit et ne sont pas mises en cache dans chaque instance.  Il a √©t√© possible de r√©duire consid√©rablement la charge sur le syst√®me ma√Ætre, il n'y a pas de doublons d'informations et la consommation de m√©moire sur les instances avec logique m√©tier a diminu√©. <br><br>  Un exemple de la taille du stockage d'informations dans la m√©moire de la tarentule: <br><br><img src="https://habrastorage.org/webt/es/93/ex/es93exozhrhnihbnq6-zpzjobma.png"><br><br>  √Ä la fin de la journ√©e, ces chiffres peuvent doubler, mais il n'y a pas de ¬´rabattement¬ª des performances. <br><br>  Au combat, la version actuelle cr√©e 2k - 2,5k requ√™tes par seconde de charge r√©elle.  Le temps de r√©ponse moyen est similaire aux tests jusqu'√† 3 ms. <br><br>  Si vous regardez htop sur l'un des serveurs avec tarantool, nous verrons qu'ils "refroidissent": <br><br><img src="https://habrastorage.org/webt/jt/mt/vf/jtmtvfat0ohxhugounnve47l7ju.png"><br><br><h4>  R√©sum√© </h4><br>  Malgr√© toutes les subtilit√©s et nuances de la base de donn√©es Tarantool, vous pouvez obtenir d'excellentes performances. <br><br>  J'esp√®re que ce projet se d√©veloppera et que ces moments inconfortables seront r√©solus. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr441686/">https://habr.com/ru/post/fr441686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr441676/index.html">Comment se faire des amis PLUTO et HDSDR</a></li>
<li><a href="../fr441678/index.html">Jeu physique des tornades: comment l'a√©rodynamique est impl√©ment√©e dans Just Cause 4 (trafic)</a></li>
<li><a href="../fr441680/index.html">Programme de la conf√©rence Lua √† Moscou 2019</a></li>
<li><a href="../fr441682/index.html">HyperX Fury 3D - SSD avec un pedigree clair</a></li>
<li><a href="../fr441684/index.html">Pr√©dictions: les nuages ‚Äã‚Äãvont changer 2019</a></li>
<li><a href="../fr441688/index.html">Les jeux changent le monde: comment Hellblade attire l'attention sur les probl√®mes des personnes atteintes de maladie mentale</a></li>
<li><a href="../fr441690/index.html">Vous n'avez pas besoin de blockchain: huit cas d'utilisation bien connus et pourquoi ils ne fonctionnent pas</a></li>
<li><a href="../fr441692/index.html">Comment couvrir les pistes de la blockchain? Notre concept de m√©langeur de transactions</a></li>
<li><a href="../fr441694/index.html">Pourquoi les graphiques de trafic "mentent"</a></li>
<li><a href="../fr441696/index.html">L'histoire de Cyrillic LiveJournal: comment la direction russe a √©cras√© l'essor des blogs en russe</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>