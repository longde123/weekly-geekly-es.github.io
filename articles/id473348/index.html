<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ’…ğŸ» ğŸˆ ğŸ Ide inersia (SGDm), gagasan scaling (Adagrad) dan regularisasi dalam pembelajaran mesin menggunakan masalah klasifikasi sebagai contoh ğŸ… ğŸ‘‡ğŸ¾ âœŠğŸ¿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dataset yang digunakan selanjutnya diambil dari kompetisi kaggle yang telah berlalu dari sini . 
 Pada tab Data, Anda dapat membaca deskripsi semua bi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ide inersia (SGDm), gagasan scaling (Adagrad) dan regularisasi dalam pembelajaran mesin menggunakan masalah klasifikasi sebagai contoh</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/473348/">  Dataset yang digunakan selanjutnya diambil dari kompetisi kaggle yang telah berlalu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dari sini</a> . <br>  Pada tab Data, Anda dapat membaca deskripsi semua bidang. <br><br>  Semua kode sumber dalam format laptop di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><a name="habracut"></a><br>  Kami memuat data, periksa bahwa kami umumnya memiliki: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd dataset = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'../input/ghouls-goblins-and-ghosts-boo/train.csv'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   X_test = pd.read_csv('../input/ghouls-goblins-and-ghosts-boo/test.csv') #   print(dataset.shape) print(dataset[:10])</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/ga/3u/p7/ga3up7ht-h4udt6hk9ptpnsw_cs.jpeg"><br><br>  Nilai-nilai bidang tipe (Ghoul, Ghost, Goblin) hanya diganti dengan 0, 1 dan 2. <br><br>  Warna - juga perlu diproses terlebih dahulu (kita hanya perlu nilai numerik untuk membangun model).  Kami akan menggunakan LabelEncoder dan OneHotEncoder untuk ini.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Lebih detail</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabelEncoder, OneHotEncoder labelencoder_X_1 = LabelEncoder() X_train[:, <span class="hljs-number"><span class="hljs-number">4</span></span>] = labelencoder_X_1.fit_transform(X_train[:, <span class="hljs-number"><span class="hljs-number">4</span></span>]) labelencoder_X_2 = LabelEncoder() X_test[:, <span class="hljs-number"><span class="hljs-number">4</span></span>] = labelencoder_X_2.fit_transform(X_test[:, <span class="hljs-number"><span class="hljs-number">4</span></span>]) labelencoder_Y_2 = LabelEncoder() Y_train = labelencoder_Y_2.fit_transform(Y_train) one_hot_encoder = OneHotEncoder(categorical_features = [<span class="hljs-number"><span class="hljs-number">4</span></span>]) X_train = one_hot_encoder.fit_transform(X_train).toarray() X_test = one_hot_encoder.fit_transform(X_test).toarray()</code> </pre><br>  Nah, pada titik ini data kami sudah siap.  Masih melatih model kami. <br><br>  Pertama, terapkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Adagrad</a> : <br><br>  Intinya, ini adalah modifikasi dari penurunan gradien stokastik, tentang yang saya tulis terakhir kali: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">habr.com/en/post/472300</a> <br><br>  Metode ini memperhitungkan sejarah semua gradien masa lalu untuk setiap parameter individu (ide penskalaan).  Ini memungkinkan Anda mengurangi ukuran langkah pembelajaran untuk parameter yang memiliki gradien besar: <br><br><img src="https://habrastorage.org/webt/_x/tl/an/_xtlannyj0jnvhq-smoh6yop4ly.jpeg"><br><br>  g adalah parameter penskalaan (g0 = 0) <br>  Î¸ - parameter (berat) <br>  epsilon adalah konstanta kecil yang diperkenalkan untuk mencegah pembagian dengan nol <br><br>  Bagilah dataset menjadi 2 bagian: <br>  Sampel pelatihan (kereta) dan validasi (val): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = <span class="hljs-number"><span class="hljs-number">0.2</span></span>)</code> </pre><br>  Sedikit persiapan untuk pelatihan model: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np device = <span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, loss_fn, optimizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y)</span></span></span><span class="hljs-function">:</span></span> model.train() yhat = model(x) loss = loss_fn(yhat, y) loss.backward() optimizer.step() optimizer.zero_grad() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_step</code> </pre> <br>  Model pelatihan mandiri: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optim, nn model = torch.nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">270</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">270</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>)) lr = <span class="hljs-number"><span class="hljs-number">0.01</span></span> n_epochs = <span class="hljs-number"><span class="hljs-number">500</span></span> loss_fn = torch.nn.CrossEntropyLoss() optimizer = optim.Adagrad(model.parameters(), lr=lr) train_step = make_train_step(model, loss_fn, optimizer) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_epochs): x_train, y_train = shuffle(x_train, y_train) <span class="hljs-comment"><span class="hljs-comment">#    X = torch.FloatTensor(x_train) y = torch.LongTensor(y_train) loss = train_step(X, y) print(loss)</span></span></code> </pre> <br>  Peringkat Model: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  : test_var = torch.FloatTensor(x_val) with torch.no_grad(): result = model(test_var) values, labels = torch.max(result, 1) num_right = np.sum(labels.data.numpy() == y_val) print(' {:.2f}'.format(num_right / len(y_val)))</span></span></code> </pre> <br>  Di sini, selain lapisan, kami hanya memiliki 2 parameter yang dapat dikonfigurasi (untuk saat ini): <br>  tingkat belajar dan n_epochs (jumlah era). <br><br>  Bergantung pada bagaimana kami menggabungkan dua parameter ini, 3 situasi dapat muncul: <br><br>  1 - semuanya baik-baik saja, mis.  model menunjukkan kehilangan yang rendah pada sampel pelatihan dan akurasi yang tinggi pada yang validasi. <br><br>  2 - kurang fit - kerugian besar pada sampel pelatihan dan akurasi rendah pada yang validasi. <br><br>  3 - overfitting - kerugian rendah pada sampel pelatihan, tetapi akurasi rendah pada yang validasi. <br><br>  Dengan yang pertama, semuanya jelas :) <br><br>  Dengan yang kedua, tampaknya juga - untuk bereksperimen dengan tingkat pembelajaran dan n_epochs. <br><br>  Dan apa yang harus dilakukan dengan yang ketiga?  Jawabannya sederhana - regularisasi! <br><br>  Sebelumnya, kami memiliki fungsi kehilangan bentuk: <br>  L = MSE (Y, y) tanpa persyaratan tambahan <br>  Inti dari regularisasi adalah bahwa, menambahkan istilah pada fungsi objektif, "baik" gradien jika terlalu besar.  Dengan kata lain, kami memberlakukan batasan pada fungsi tujuan kami. <br><br>  Ada banyak metode regularisasi.  Lebih lanjut tentang L1 dan L2 - regularisasi: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">craftappmobile.com/l1-vs-l2-regularisasi/#_L1_L2</a> <br><br>  Metode Adagrad mengimplementasikan regularisasi L2, mari kita terapkan! <br><br>  Pertama, untuk kejelasan, kami melihat indikator model tanpa regularisasi: <br><br>  lr = 0,01, n_epochs = 500: <br>  kerugian = 0,44 ... <br>  Akurasi: 0,71 <br><br>  lr = 0,01, n_epochs = 1000: <br>  kerugian = 0,41 ... <br>  Akurasi: 0,75 <br><br>  lr = 0,01, n_epochs = 2000: <br>  kerugian = 0,39 ... <br>  Akurasi: 0,75 <br><br>  lr = 0,01, n_epochs = 3000: <br>  kerugian = 0,367 ... <br>  Akurasi: 0,76 <br><br>  lr = 0,01, n_epochs = 4000: <br>  kerugian = 0,355 ... <br>  Akurasi: 0,72 <br><br>  lr = 0,01, n_epochs = 10000: <br>  kerugian = 0,285 ... <br>  Akurasi: 0,69 <br><br>  Di sini Anda dapat melihat bahwa pada era 4k + - modelnya sudah overfit.  Sekarang mari kita coba untuk menghindari ini: <br><br>  Untuk melakukan ini, tambahkan parameter weight_decay untuk metode optimasi kami: <br><br><pre> <code class="python hljs">optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay = <span class="hljs-number"><span class="hljs-number">0.001</span></span>)</code> </pre> <br>  Dengan lr = 0,01, m_epochs = 10000: <br>  kerugian = 0,367 ... <br>  Akurasi: 0,73 <br><br>  Pada 4000 era: <br>  kerugian = 0,389 ... <br>  Akurasi: 0,75 <br><br>  Ternyata jauh lebih baik, tetapi kami hanya menambahkan 1 parameter di optimizer :) <br><br>  Sekarang pertimbangkan SGDm (ini adalah penurunan gradien stokastik dengan ekstensi kecil - heuristik, jika Anda suka). <br><br>  Intinya adalah bahwa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SGD</a> memperbarui parameter dengan cukup kuat setelah setiap iterasi.  Adalah logis untuk "menghaluskan" gradien menggunakan gradien dari iterasi sebelumnya (ide inersia): <br><br><img src="https://habrastorage.org/webt/qv/nq/ty/qvnqtydmtauek29nbjajagl-i6c.jpeg"><br><br>  Î¸ - parameter (berat) <br>  Î¼ - hiperparameter inersia <br><br>  SGD tanpa parameter momentum: <br><br><img src="https://habrastorage.org/webt/rj/rv/u8/rjrvu8nfx7_mcna815jgzx_hk3c.jpeg"><br><br>  SGD dengan parameter momentum: <br><br><img src="https://habrastorage.org/webt/xg/jq/-h/xgjq-hid1sb5cffzcxau7j_bclu.jpeg"><br><br>  Ternyata tidak jauh lebih baik, tetapi intinya di sini adalah bahwa ada metode yang segera menggunakan gagasan penskalaan dan inersia.  Misalnya, Adam atau Adadelta, yang sekarang menunjukkan hasil yang baik.  Nah, untuk memahami metode ini, saya pikir perlu untuk memahami beberapa ide dasar yang digunakan dalam metode yang lebih sederhana. <br><br>  Terima kasih atas perhatian Anda! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id473348/">https://habr.com/ru/post/id473348/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id473338/index.html">TDD: cara menulis spesifikasi dengan benar (menjelaskan)</a></li>
<li><a href="../id473340/index.html">Intisari bahan-bahan segar dari dunia front-end untuk minggu terakhir No. 386 (21 - 27 Oktober 2019)</a></li>
<li><a href="../id473342/index.html">"Jalan panjang sedang menunggu Anda ..." atau menyelesaikan masalah perkiraan di C # menggunakan Ml.NET (DataScience)</a></li>
<li><a href="../id473344/index.html">Konser dan acara KudaGo di cermin Anda</a></li>
<li><a href="../id473346/index.html">Membuat API REST dengan Node.js dan Oracle Database. Bagian 2</a></li>
<li><a href="../id473350/index.html">Membuat API REST dengan Node.js dan Oracle Database. Bagian 3</a></li>
<li><a href="../id473352/index.html">Koleksi serentak dalam 10 menit</a></li>
<li><a href="../id473354/index.html">Tentang keanehan habrostatistics</a></li>
<li><a href="../id473358/index.html">Instal dan konfigurasikan Nexus Sonatype menggunakan infrastruktur sebagai pendekatan kode</a></li>
<li><a href="../id473362/index.html">Pengalaman GSoC: Bagaimana Dua (Tiga) Siswa Benar-Benar Meningkatkan Kode CRIU</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>