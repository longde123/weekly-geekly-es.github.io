<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî™ üë®‚Äçüë©‚Äçüë¶ üë± Gesichtserkennung mit siamesischen Netzwerken üò≤ üç¶ üßñ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das siamesische neuronale Netzwerk ist einer der einfachsten und beliebtesten Algorithmen f√ºr das einzelne Lernen. Methoden, bei denen f√ºr jede Klasse...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Gesichtserkennung mit siamesischen Netzwerken</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jetinfosystems/blog/465279/"><img src="https://habrastorage.org/webt/mg/dh/mb/mgdhmb0xf-4onn6dny1renuuhla.jpeg"><br><br>  Das siamesische neuronale Netzwerk ist einer der einfachsten und beliebtesten Algorithmen f√ºr das einzelne Lernen.  Methoden, bei denen f√ºr jede Klasse nur eine Fallstudie durchgef√ºhrt wird.  Daher wird das siamesische Netzwerk normalerweise in Anwendungen verwendet, in denen nicht viele Dateneinheiten in jeder Klasse vorhanden sind. <br><br>  Angenommen, wir m√ºssen ein Gesichtserkennungsmodell f√ºr eine Organisation erstellen, die etwa 500 Mitarbeiter besch√§ftigt.  Wenn Sie ein solches Modell basierend auf dem Convolutional Neural Network (CNN) von Grund auf neu erstellen, ben√∂tigen wir viele Bilder von jeder dieser 500 Personen, um das Modell zu trainieren und eine gute Erkennungsgenauigkeit zu erzielen.  Es ist jedoch offensichtlich, dass wir einen solchen Datensatz nicht erfassen k√∂nnen. Sie sollten daher kein Modell erstellen, das auf CNN oder einem anderen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep-Learning-</a> Algorithmus basiert, wenn wir nicht √ºber gen√ºgend Daten verf√ºgen.  In solchen F√§llen k√∂nnen Sie den komplexen einmaligen Lernalgorithmus wie das siamesische Netzwerk verwenden, der mit weniger Daten trainiert werden kann. <br><a name="habracut"></a><br>  Tats√§chlich bestehen siamesische Netze aus zwei symmetrischen neuronalen Netzen mit den gleichen Gewichten und der gleichen Architektur, die am Ende die Energiefunktion kombinieren und nutzen - E. <br>  Schauen wir uns das siamesische Netzwerk an und erstellen ein darauf basierendes Gesichtserkennungsmodell.  Wir werden sie lehren zu bestimmen, wann zwei Gesichter gleich sind und wann nicht.  F√ºr den Anfang verwenden wir den Datensatz AT &amp; T Database of Faces, der von der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Computerlabor-</a> Website der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Universit√§t Cambridge</a> heruntergeladen werden kann. <br><br>  Laden Sie Ordner von s1 bis s40 herunter, entpacken Sie sie und sehen Sie sie sich an: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/152/0fc/158/1520fc1584e7ba6335ba2ea99460d8f6.png"><br><br>  Jeder Ordner enth√§lt 10 verschiedene Fotos einer einzelnen Person aus verschiedenen Blickwinkeln.  Hier ist der Inhalt des s1-Ordners: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5d4/02b/86f/5d402b86fc12e1b23512b1a54c7fea57.png"><br><br>  Und hier ist, was sich im s13-Ordner befindet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7c/819/716/b7c8197164861f8568e86d32a2294bc2.png"><br><br>  Siamesische Netzwerke m√ºssen gepaarte Werte mit Markierungen eingeben, also erstellen wir solche Mengen.  Nehmen Sie zwei zuf√§llige Fotos aus demselben Ordner auf und markieren Sie sie als ‚Äûechtes‚Äú Paar.  Dann machen wir zwei Fotos aus verschiedenen Ordnern und markieren sie als "falsches" Paar (imposit): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5b3/1f3/8bf/5b31f38bf3363aaa53110772b7643e3b.png"><br><br>  Nachdem wir alle Fotos in markierten Paaren verteilt haben, werden wir das Netzwerk untersuchen.  Von jedem Paar √ºbertragen wir ein Foto an Netzwerk A und das zweite an Netzwerk B. Beide Netzwerke extrahieren nur Eigenschaftsvektoren.  Dazu verwenden wir zwei Faltungsschichten mit Aktivierung der gleichgerichteten Lineareinheit (ReLU).  Nachdem wir die Eigenschaften untersucht haben, √ºbertragen wir die von beiden Netzwerken erzeugten Vektoren in eine Energiefunktion, die die √Ñhnlichkeit absch√§tzt.  Wir verwenden den euklidischen Abstand als Funktion. <br><br>
<h2>  Betrachten Sie nun alle diese Schritte genauer. </h2><br>  Importieren Sie zun√§chst die erforderlichen Bibliotheken: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> PIL <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential, Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RMSprop</code> </pre> <br>  Nun definieren wir eine Funktion zum Lesen von Eingabebildern.  Die Funktion <code>read_image</code> macht ein Bild und gibt ein NumPy-Array zur√ºck: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, byteorder=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'&gt;'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#first we read the image, as a raw file to the buffer with open(filename, 'rb') as f: buffer = f.read() #using regex, we extract the header, width, height and maxval of the image header, width, height, maxval = re.search( b"(^P5\s(?:\s*#.*[\r\n])*" b"(\d+)\s(?:\s*#.*[\r\n])*" b"(\d+)\s(?:\s*#.*[\r\n])*" b"(\d+)\s(?:\s*#.*[\r\n]\s)*)", buffer).groups() #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array return np.frombuffer(buffer, dtype='u1' if int(maxval)</span></span></code> </pre> <br>  √ñffnen Sie zum Beispiel dieses Foto: <br><br><pre> <code class="python hljs">Image.open(<span class="hljs-string"><span class="hljs-string">"data/orl_faces/s1/1.pgm"</span></span>)</code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/ee2/954/697/ee2954697908ce55ffaf4fc8080914c7.png"><br><br>  Wir √ºbergeben es an die Funktion <code>read_image</code> und erhalten ein NumPy-Array: <br><br><pre> <code class="python hljs">img = read_image(<span class="hljs-string"><span class="hljs-string">'data/orl_faces/s1/1.pgm'</span></span>) img.shape (<span class="hljs-number"><span class="hljs-number">112</span></span>, <span class="hljs-number"><span class="hljs-number">92</span></span>)</code> </pre> <br>  Jetzt definieren wir die Funktion <code>get_data</code> , die die Daten generiert.  Ich m√∂chte Sie daran erinnern, dass siamesische Netzwerke Datenpaare (echt und imposant) mit bin√§rer Markierung einreichen m√ºssen. <br><br>  Lesen Sie zuerst die Bilder ( <code>img1</code> , <code>img2</code> ) aus einem Verzeichnis, speichern Sie sie im Array <code>x_genuine_pair,</code> setzen Sie <code>y_genuine</code> auf <code>1</code> .  Dann lesen wir die Bilder ( <code>img1</code> , <code>img2</code> ) aus verschiedenen Verzeichnissen, speichern sie im <code>x_imposite,</code> Paar und setzen <code>y_imposite</code> auf <code>0</code> . <br><br>  <code>x_genuine_pair</code> <code>x_imposite</code> <code>x_genuine_pair</code> und <code>x_imposite</code> in <code>X</code> und <code>y_genuine</code> und <code>y_imposite</code> in <code>Y</code> : <br><br><pre> <code class="python hljs">size = <span class="hljs-number"><span class="hljs-number">2</span></span> total_sample_size = <span class="hljs-number"><span class="hljs-number">10000</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(size, total_sample_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#read the image image = read_image('data/orl_faces/s' + str(1) + '/' + str(1) + '.pgm', 'rw+') #reduce the size image = image[::size, ::size] #get the new size dim1 = image.shape[0] dim2 = image.shape[1] count = 0 #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2] x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2]) # 2 is for pairs y_genuine = np.zeros([total_sample_size, 1]) for i in range(40): for j in range(int(total_sample_size/40)): ind1 = 0 ind2 = 0 #read images from same directory (genuine pair) while ind1 == ind2: ind1 = np.random.randint(10) ind2 = np.random.randint(10) # read the two images img1 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind1 + 1) + '.pgm', 'rw+') img2 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind2 + 1) + '.pgm', 'rw+') #reduce the size img1 = img1[::size, ::size] img2 = img2[::size, ::size] #store the images to the initialized numpy array x_geuine_pair[count, 0, 0, :, :] = img1 x_geuine_pair[count, 1, 0, :, :] = img2 #as we are drawing images from the same directory we assign label as 1. (genuine pair) y_genuine[count] = 1 count += 1 count = 0 x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2]) y_imposite = np.zeros([total_sample_size, 1]) for i in range(int(total_sample_size/10)): for j in range(10): #read images from different directory (imposite pair) while True: ind1 = np.random.randint(40) ind2 = np.random.randint(40) if ind1 != ind2: break img1 = read_image('data/orl_faces/s' + str(ind1+1) + '/' + str(j + 1) + '.pgm', 'rw+') img2 = read_image('data/orl_faces/s' + str(ind2+1) + '/' + str(j + 1) + '.pgm', 'rw+') img1 = img1[::size, ::size] img2 = img2[::size, ::size] x_imposite_pair[count, 0, 0, :, :] = img1 x_imposite_pair[count, 1, 0, :, :] = img2 #as we are drawing images from the different directory we assign label as 0. (imposite pair) y_imposite[count] = 0 count += 1 #now, concatenate, genuine pairs and imposite pair to get the whole data X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255 Y = np.concatenate([y_genuine, y_imposite], axis=0) return X, Y</span></span></code> </pre> <br>  Jetzt werden wir die Daten generieren und ihre Gr√∂√üe √ºberpr√ºfen.  Wir haben 20.000 Fotos, von denen 10.000 echte und 10.000 falsche Paare gesammelt wurden: <br><br><pre> <code class="python hljs">X, Y = get_data(size, total_sample_size) X.shape (<span class="hljs-number"><span class="hljs-number">20000</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">46</span></span>) Y.shape (<span class="hljs-number"><span class="hljs-number">20000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Wir werden die gesamte Reihe von Informationen teilen: 75% der Paare werden zum Training gehen und 25% - zum Testen: <br> <code>x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)</code> <br> <br>  Erstellen Sie jetzt ein siamesisches Netzwerk.  Zuerst definieren wir das Kernnetzwerk - es wird ein neuronales Faltungsnetzwerk sein, um Eigenschaften zu extrahieren.  Erstellen Sie zwei Faltungsebenen mit ReLU-Aktivierungen und eine Ebene mit maximalem Pooling nach einer flachen Ebene: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">build_base_network</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_shape)</span></span></span><span class="hljs-function">:</span></span> seq = Sequential() nb_filter = [<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>] kernel_size = <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-comment"><span class="hljs-comment">#convolutional layer 1 seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape, border_mode='valid', dim_ordering='th')) seq.add(Activation('relu')) seq.add(MaxPooling2D(pool_size=(2, 2))) seq.add(Dropout(.25)) #convolutional layer 2 seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='th')) seq.add(Activation('relu')) seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) seq.add(Dropout(.25)) #flatten seq.add(Flatten()) seq.add(Dense(128, activation='relu')) seq.add(Dropout(0.1)) seq.add(Dense(50, activation='relu')) return seq</span></span></code> </pre> <br><br>  Dann werden wir ein Paar von Bildern des Kernnetzwerks √ºbertragen, die Vektordarstellungen, dh Eigenschaftsvektoren, zur√ºckgeben: <br><br><pre> <code class="python hljs">input_dim = x_train.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>:] img_a = Input(shape=input_dim) img_b = Input(shape=input_dim) base_network = build_base_network(input_dim) feat_vecs_a = base_network(img_a) feat_vecs_b = base_network(img_b)</code> </pre><br>  <code>feat_vecs_a</code> und <code>feat_vecs_b</code> sind Eigenschaftsvektoren eines Bildpaares.  Lassen Sie uns ihre Energiefunktionen √ºbergeben, um den Abstand zwischen ihnen zu berechnen.  Und als Funktion der Energie verwenden wir die euklidische Distanz: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">euclidean_distance</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vects)</span></span></span><span class="hljs-function">:</span></span> x, y = vects <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.sqrt(K.sum(K.square(x - y), axis=<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdims=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">eucl_dist_output_shape</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(shapes)</span></span></span><span class="hljs-function">:</span></span> shape1, shape2 = shapes <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (shape1[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])</code> </pre> <br>  Wir setzen die Anzahl der Epochen auf 13, wenden die RMS-Eigenschaft zur Optimierung an und deklarieren das Modell: <br><br><pre> <code class="python hljs">epochs = <span class="hljs-number"><span class="hljs-number">13</span></span> rms = RMSprop() model = Model(input=[input_a, input_b], output=distance)</code> </pre> <br>  Nun definieren wir die Verlustfunktion <code>contrastive_loss</code> und kompilieren das Modell: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">contrastive_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> margin = <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.mean(y_true * K.square(y_pred) + (<span class="hljs-number"><span class="hljs-number">1</span></span> - y_true) * K.square(K.maximum(margin - y_pred, <span class="hljs-number"><span class="hljs-number">0</span></span>))) model.compile(loss=contrastive_loss, optimizer=rms)</code> </pre> <br>  Lassen Sie uns das Modell studieren: <br><br><pre> <code class="python hljs">img_1 = x_train[:, <span class="hljs-number"><span class="hljs-number">0</span></span>] img_2 = x_train[:, <span class="hljs-number"><span class="hljs-number">1</span></span>] model.fit([img_1, img_2], y_train, validation_split=<span class="hljs-number"><span class="hljs-number">.25</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">2</span></span>, nb_epoch=epochs)</code> </pre> <br>  Sie sehen, wie die Verluste im Laufe der Zeit abnehmen: <br><br><pre> <code class="python hljs">Train on <span class="hljs-number"><span class="hljs-number">11250</span></span> samples, validate on <span class="hljs-number"><span class="hljs-number">3750</span></span> samples Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">60</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.2179</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.2156</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">53</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1520</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.2102</span></span> Epoch <span class="hljs-number"><span class="hljs-number">3</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">53</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1190</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1545</span></span> Epoch <span class="hljs-number"><span class="hljs-number">4</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">55</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0959</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1705</span></span> Epoch <span class="hljs-number"><span class="hljs-number">5</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0801</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1181</span></span> Epoch <span class="hljs-number"><span class="hljs-number">6</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0684</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0821</span></span> Epoch <span class="hljs-number"><span class="hljs-number">7</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0591</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0762</span></span> Epoch <span class="hljs-number"><span class="hljs-number">8</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0526</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0655</span></span> Epoch <span class="hljs-number"><span class="hljs-number">9</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0475</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0662</span></span> Epoch <span class="hljs-number"><span class="hljs-number">10</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0444</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0469</span></span> Epoch <span class="hljs-number"><span class="hljs-number">11</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0408</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0478</span></span> Epoch <span class="hljs-number"><span class="hljs-number">12</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0381</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0498</span></span> Epoch <span class="hljs-number"><span class="hljs-number">13</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">54</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0356</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0363</span></span></code> </pre> <br>  Und jetzt testen wir das Modell anhand von Testdaten: <br><br><pre> <code class="python hljs">pred = model.predict([x_test[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], x_test[:, <span class="hljs-number"><span class="hljs-number">1</span></span>]])</code> </pre> <br>  Definieren Sie eine Funktion zur Berechnung der Genauigkeit: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compute_accuracy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(predictions, labels)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> labels[predictions.ravel()</code> </pre> <br>  Wir berechnen die Genauigkeit: <br><br><pre> <code class="plaintext hljs">compute_accuracy(pred, y_test) 0.9779092702169625</code> </pre><br><h2>  Schlussfolgerungen </h2><br>  In diesem Handbuch haben wir gelernt, wie Gesichtserkennungsmodelle basierend auf siamesischen Netzwerken erstellt werden.  Die Architektur solcher Netze besteht aus zwei identischen neuronalen Netzen mit gleichem Gewicht und gleicher Struktur, und die Ergebnisse ihrer Arbeit werden auf eine Energiefunktion √ºbertragen - dies bestimmt die Identit√§t der Eingabedaten.  Weitere Informationen zum Meta-Lernen mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python finden</a> Sie unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Praktisches Meta-Lernen mit Python.</a> <br><br><h2>  Mein Kommentar </h2><br>  F√ºr die Arbeit mit Bildern sind derzeit Kenntnisse √ºber siamesische Netzwerke erforderlich.  Es gibt viele Ans√§tze, um Netzwerke in kleinen Stichproben, neue Datengenerierung und Erweiterungsmethoden zu trainieren.  Diese Methode erm√∂glicht es relativ "billig", gute Ergebnisse zu erzielen. Hier ist ein klassischeres Beispiel f√ºr das siamesische Netzwerk in "Hallo Welt" f√ºr neuronale Netze - Datensatz MNIST <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">keras.io/examples/mnist_siamese</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de465279/">https://habr.com/ru/post/de465279/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de465269/index.html">Schulung Cisco 200-125 CCNA v3.0. Tag 26. DNS und DHCP</a></li>
<li><a href="../de465271/index.html">Hacker stehlen und waschen Geld durch Lieferservice und Hotelreservierung.</a></li>
<li><a href="../de465273/index.html">Wie Microgaming-Softwareentwickler Benutzer vor Hacks sch√ºtzen</a></li>
<li><a href="../de465275/index.html">Alice bekommt Geschicklichkeit</a></li>
<li><a href="../de465277/index.html">Analysieren und Analysieren der Semantik f√ºr SEO: 5 kostenlose Google Sheets-Vorlagen</a></li>
<li><a href="../de465281/index.html">Kontinuierliche Glukose√ºberwachung (NMH) mit Medtronic 640g Pumpe</a></li>
<li><a href="../de465283/index.html">‚ÄûEs gibt alles, was ben√∂tigt wird, und nichts macht w√ºtend‚Äú - die Wahrheit spricht durch die Lippen des Klienten</a></li>
<li><a href="../de465285/index.html">Wie wir geschrieben haben, das Frontend unseres eigenen Hosting-Control-Panels: Framework und Backdoors</a></li>
<li><a href="../de465289/index.html">Die Zusammenfassung der Ereignisse f√ºr HR-Experten im Bereich IT f√ºr September 2019</a></li>
<li><a href="../de465291/index.html">N√§her am Boden: Wie ich Coworking in ein Dorfhaus verwandelte</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>