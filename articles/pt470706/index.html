<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëáüèª ü§¥üèΩ üì± Python + Keras + LSTM: fa√ßa um tradutor de texto em meia hora üëó üö¥üèº üîõ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Oi Habr. 

 Na parte anterior, observei a cria√ß√£o de um reconhecimento de texto simples baseado em uma rede neural. Hoje vamos usar uma abordagem seme...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python + Keras + LSTM: fa√ßa um tradutor de texto em meia hora</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470706/">  Oi Habr. <br><br>  Na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">parte anterior,</a> observei a cria√ß√£o de um reconhecimento de texto simples baseado em uma rede neural.  Hoje vamos usar uma abordagem semelhante e escrever um tradutor autom√°tico de textos de ingl√™s para alem√£o. <br><br><img src="https://habrastorage.org/webt/gf/ft/jx/gfftjxwflb7yxrwtffish1hkqsc.jpeg"><br><br>  Para aqueles que est√£o interessados ‚Äã‚Äãem como isso funciona, os detalhes est√£o ocultos. <br><a name="habracut"></a><br>  <i>Nota</i> : este projeto de utiliza√ß√£o de uma rede neural para tradu√ß√£o √© exclusivamente educacional, portanto a quest√£o ‚Äúpor que‚Äù n√£o √© considerada.  Apenas por divers√£o.  N√£o pretendo provar que esse ou aquele m√©todo seja melhor ou pior, apenas foi interessante verificar o que acontece.  O m√©todo usado abaixo √©, obviamente, simplificado, mas espero que ningu√©m espere escrever um segundo Lingvo em meia hora. <br><br><h2>  Coleta de dados </h2><br>  Um arquivo encontrado na rede contendo frases em ingl√™s e alem√£o separadas por guias foi usado como o conjunto de dados de origem.  Um conjunto de frases √© mais ou menos assim: <br><br><pre><code class="python hljs">Hi. Hallo! Hi. Gr√º√ü Gott! Run! Lauf! Wow! Potzdonner! Wow! Donnerwetter! Fire! Feuer! Help! Hilfe! Help! Zu H√ºlf! Stop! Stopp! Wait! Warte! Go on. Mach weiter. Hello! Hallo! I ran. Ich rannte. I see. Ich verstehe. ...</code> </pre> <br>  O arquivo cont√©m 192 mil linhas e um tamanho de 13 MB.  Carregamos o texto na mem√≥ria e dividimos os dados em dois blocos, para palavras em ingl√™s e alem√£o. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, mode=<span class="hljs-string"><span class="hljs-string">'rt'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> file: text = file.read() sents = text.strip().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [i.split(<span class="hljs-string"><span class="hljs-string">'\t'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sents] data = read_text(<span class="hljs-string"><span class="hljs-string">"deutch.txt"</span></span>) deu_eng = np.array(data) deu_eng = deu_eng[:<span class="hljs-number"><span class="hljs-number">30000</span></span>,:] print(<span class="hljs-string"><span class="hljs-string">"Dictionary size:"</span></span>, deu_eng.shape) <span class="hljs-comment"><span class="hljs-comment"># Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower()</span></span></code> </pre><br>  Tamb√©m convertemos todas as palavras para min√∫sculas e removemos os sinais de pontua√ß√£o. <br><br>  O pr√≥ximo passo √© preparar os dados para a rede neural.  A rede n√£o sabe o que s√£o palavras e trabalha exclusivamente com n√∫meros.  Felizmente para n√≥s, o keras j√° possui a classe Tokenizer incorporada, que substitui palavras em frases por c√≥digos digitais. <br><br>  Seu uso √© simplesmente ilustrado com um exemplo: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.sequence <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pad_sequences s = <span class="hljs-string"><span class="hljs-string">"To be or not to be"</span></span> eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts([s]) seq = eng_tokenizer.texts_to_sequences([s]) seq = pad_sequences(seq, maxlen=<span class="hljs-number"><span class="hljs-number">8</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'post'</span></span>) print(seq)</code> </pre><br>  A frase ‚Äúser ou n√£o ser‚Äù ser√° substitu√≠da pela matriz [1 2 3 4 1 2 0 0], onde n√£o √© dif√≠cil adivinhar, 1 = para, 2 = ser, 3 = ou, 4 = n√£o.  J√° podemos enviar esses dados para a rede neural. <br><br><h2>  Treinamento em redes neurais </h2><br>  Nossos dados est√£o prontos digitalmente.  Dividimos a matriz em dois blocos para dados de entrada (linhas em ingl√™s) e sa√≠da (linhas em alem√£o).  Tamb√©m prepararemos uma unidade separada para validar o processo de aprendizado. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1])</span></span></code> </pre><br>  Agora podemos criar um modelo de rede neural e iniciar seu treinamento.  Como voc√™ pode ver, a rede neural cont√©m camadas LSTM com c√©lulas de mem√≥ria.  Embora provavelmente funcione em uma rede "regular", quem desejar pode verificar por conta pr√≥pria. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_vocab, out_vocab, in_timesteps, out_timesteps, n)</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(LSTM(n)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(Dense(out_vocab, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(optimizer=optimizers.RMSprop(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model eng_vocab_size = len(eng_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> deu_vocab_size = len(deu_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> eng_length, deu_length = <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span> model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, <span class="hljs-number"><span class="hljs-number">512</span></span>) num_epochs = <span class="hljs-number"><span class="hljs-number">40</span></span> model.fit(trainX, trainY.reshape(trainY.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], trainY.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>), epochs=num_epochs, batch_size=<span class="hljs-number"><span class="hljs-number">512</span></span>, validation_split=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, callbacks=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.save(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>)</code> </pre><br>  O treinamento em si √© mais ou menos assim: <br><br><img src="https://habrastorage.org/webt/fj/xl/b4/fjxlb4yorszz5ojzpcih4ixlria.png"><br><br>  O processo, como voc√™ pode ver, n√£o √© r√°pido e leva cerca de meia hora em uma Core i7 + GeForce 1060 para um conjunto de 30 mil linhas.  No final do treinamento (ele precisa ser feito apenas uma vez), o modelo √© salvo em um arquivo e pode ser reutilizado. <br><br>  Para obter a tradu√ß√£o, usamos a fun√ß√£o predict_classes, cuja entrada enviamos algumas frases simples.  A fun√ß√£o get_word √© usada para inverter palavras em n√∫meros. <br><br><pre> <code class="python hljs">model = load_model(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_word</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n, tokenizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n == <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word, index <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tokenizer.word_index.items(): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> index == n: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> phrs_enc = encode_sequences(eng_tokenizer, eng_length, [<span class="hljs-string"><span class="hljs-string">"the weather is nice today"</span></span>, <span class="hljs-string"><span class="hljs-string">"my name is tom"</span></span>, <span class="hljs-string"><span class="hljs-string">"how old are you"</span></span>, <span class="hljs-string"><span class="hljs-string">"where is the nearest shop"</span></span>]) preds = model.predict_classes(phrs_enc) print(<span class="hljs-string"><span class="hljs-string">"Preds:"</span></span>, preds.shape) print(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer))</code> </pre><br><h2>  Resultados </h2><br>  Agora, na verdade, o mais curioso s√£o os resultados.  √â interessante ver como a rede neural aprende e "lembra" a correspond√™ncia entre frases em ingl√™s e alem√£o.  Eu especificamente tomei 2 frases mais f√°ceis e 2 mais dif√≠ceis de ver a diferen√ßa. <br><br>  <b>5 minutos de treinamento</b> <br><br>  ‚ÄúO tempo est√° bom hoje‚Äù - ‚Äúdas ist ist tom‚Äù <br>  "Meu nome √© tom" - "wie f√ºr tom tom" <br>  "Quantos anos voc√™ tem" - "wie geht ist es" <br>  "Onde fica a loja mais pr√≥xima" - "wo ist der" <br><br>  Como voc√™ pode ver, at√© agora existem poucos "hits".  Um fragmento da frase ‚Äúquantos anos voc√™ tem‚Äù confundiu a rede neural com a frase ‚Äúcomo voc√™ est√°‚Äù e produziu a tradu√ß√£o ‚Äúwie geht ist es‚Äù (como voc√™ est√°?).  Na frase ‚Äúonde est√° ...‚Äù, a rede neural identificou apenas o verbo where e produziu a tradu√ß√£o ‚Äúwo ist der‚Äù (onde est√°?). Que, em princ√≠pio, n√£o deixa de ter significado.  Em geral, aproximadamente o mesmo que traduz para o alem√£o um rec√©m-chegado ao grupo A1;) <br><br>  <b>10 minutos de treinamento</b> <br><br>  ‚ÄúO tempo est√° bom hoje‚Äù - ‚Äúdas haus ist bereit‚Äù <br>  "Meu nome √© tom" - "mein hei√üe hei√üe tom" <br>  "Quantos anos voc√™ tem" - "wie alt sind sie" <br>  "Onde fica a loja mais pr√≥xima" - "wo ist paris" <br><br>  Algum progresso √© vis√≠vel.  A primeira frase est√° completamente fora de lugar.  Na segunda frase, a rede neural ‚Äúaprendeu‚Äù o verbo hei√üen (chamado), mas ‚Äúmein hei√üe hei√üe tom‚Äù ainda est√° incorreta, embora voc√™ j√° possa adivinhar o significado.  A terceira frase j√° est√° correta.  Na quarta, a primeira parte correta √© ‚Äúwo ist‚Äù, mas a loja mais pr√≥xima foi substitu√≠da por algum motivo por paris. <br><br>  <b>30 minutos de treinamento</b> <br><br>  ‚ÄúO tempo est√° bom hoje‚Äù - ‚Äúdas ist ist aus‚Äù <br>  "Meu nome √© tom" - "" tom "√© meu nome" <br>  "Quantos anos voc√™ tem" - "wie alt sind sie" <br>  "Onde fica a loja mais pr√≥xima" - "wo ist der" <br><br>  Como voc√™ pode ver, a segunda frase se tornou correta, embora o design pare√ßa um tanto incomum.  A terceira frase est√° correta, mas a primeira e a quarta frases ainda n√£o foram "aprendidas".  Com isso <s>para economizar eletricidade,</s> terminei o processo. <br><br><h2>  Conclus√£o </h2><br>  Como voc√™ pode ver, em princ√≠pio, isso funciona.  Gostaria de memorizar um novo idioma com tanta velocidade :) Obviamente, o resultado n√£o √© perfeito at√© agora, mas o treinamento em um conjunto completo de 190 mil linhas levaria mais de uma hora. <br><br>  Para aqueles que querem experimentar por conta pr√≥pria, o c√≥digo-fonte est√° sob o spoiler.  Teoricamente, o programa pode usar qualquer par de idiomas, n√£o apenas ingl√™s e alem√£o (o arquivo deve estar na codifica√ß√£o UTF-8).  A quest√£o da qualidade da tradu√ß√£o tamb√©m permanece em aberto, h√° algo a ser testado. <br><br><div class="spoiler">  <b class="spoiler_title">keras_translate.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-comment"><span class="hljs-comment"># os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # Force CPU os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed import string import re import numpy as np import pandas as pd from keras.models import Sequential from keras.layers import Dense, LSTM, Embedding, RepeatVector from keras.preprocessing.text import Tokenizer from keras.callbacks import ModelCheckpoint from keras.preprocessing.sequence import pad_sequences from keras.models import load_model from keras import optimizers from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt pd.set_option('display.max_colwidth', 200) # Read raw text file def read_text(filename): with open(filename, mode='rt', encoding='utf-8') as file: text = file.read() sents = text.strip().split('\n') return [i.split('\t') for i in sents] data = read_text("deutch.txt") deu_eng = np.array(data) deu_eng = deu_eng[:30000,:] print("Dictionary size:", deu_eng.shape) # Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # Convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower() # Prepare English tokenizer eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts(deu_eng[:, 0]) eng_vocab_size = len(eng_tokenizer.word_index) + 1 eng_length = 8 # Prepare Deutch tokenizer deu_tokenizer = Tokenizer() deu_tokenizer.fit_on_texts(deu_eng[:, 1]) deu_vocab_size = len(deu_tokenizer.word_index) + 1 deu_length = 8 # Encode and pad sequences def encode_sequences(tokenizer, length, lines): # integer encode sequences seq = tokenizer.texts_to_sequences(lines) # pad sequences with 0 values seq = pad_sequences(seq, maxlen=length, padding='post') return seq # Split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # Prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # Prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1]) # Build NMT model def make_model(in_vocab, out_vocab, in_timesteps, out_timesteps, n): model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=True)) model.add(LSTM(n)) model.add(Dropout(0.3)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=True)) model.add(Dropout(0.3)) model.add(Dense(out_vocab, activation='softmax')) model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy') return model print("deu_vocab_size:", deu_vocab_size, deu_length) print("eng_vocab_size:", eng_vocab_size, eng_length) # Model compilation (with 512 hidden units) model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, 512) # Train model num_epochs = 250 history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=num_epochs, batch_size=512, validation_split=0.2, callbacks=None, verbose=1) # plt.plot(history.history['loss']) # plt.plot(history.history['val_loss']) # plt.legend(['train','validation']) # plt.show() model.save('en-de-model.h5') # Load model model = load_model('en-de-model.h5') def get_word(n, tokenizer): if n == 0: return "" for word, index in tokenizer.word_index.items(): if index == n: return word return "" phrs_enc = encode_sequences(eng_tokenizer, eng_length, ["the weather is nice today", "my name is tom", "how old are you", "where is the nearest shop"]) print("phrs_enc:", phrs_enc.shape) preds = model.predict_classes(phrs_enc) print("Preds:", preds.shape) print(preds[0]) print(get_word(preds[0][0], deu_tokenizer), get_word(preds[0][1], deu_tokenizer), get_word(preds[0][2], deu_tokenizer), get_word(preds[0][3], deu_tokenizer)) print(preds[1]) print(get_word(preds[1][0], deu_tokenizer), get_word(preds[1][1], deu_tokenizer), get_word(preds[1][2], deu_tokenizer), get_word(preds[1][3], deu_tokenizer)) print(preds[2]) print(get_word(preds[2][0], deu_tokenizer), get_word(preds[2][1], deu_tokenizer), get_word(preds[2][2], deu_tokenizer), get_word(preds[2][3], deu_tokenizer)) print(preds[3]) print(get_word(preds[3][0], deu_tokenizer), get_word(preds[3][1], deu_tokenizer), get_word(preds[3][2], deu_tokenizer), get_word(preds[3][3], deu_tokenizer)) print()</span></span></code> </pre><br></div></div><br>  O dicion√°rio em si √© muito grande para anexar ao artigo, o link est√° nos coment√°rios. <br><br>  Como sempre, todas as experi√™ncias bem-sucedidas. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt470706/">https://habr.com/ru/post/pt470706/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt470688/index.html">O que se sabe sobre o VMworld 2019</a></li>
<li><a href="../pt470692/index.html">Como criamos um novo site do Rosbank e o que veio dele</a></li>
<li><a href="../pt470694/index.html">Escolhendo uma plataforma de email marketing: o que prestar aten√ß√£o √†s empresas russas</a></li>
<li><a href="../pt470696/index.html">Por que Kaldi √© bom para reconhecimento de fala? (atualizado em 25.12.2019)</a></li>
<li><a href="../pt470700/index.html">Mesa. Met√°lico Silencioso O seu</a></li>
<li><a href="../pt470710/index.html">Machine Learning para sua ca√ßa plana. Parte 2</a></li>
<li><a href="../pt470714/index.html">Como fui para a final do avan√ßo digital</a></li>
<li><a href="../pt470718/index.html">"Efeitos alg√©bricos" na linguagem humana</a></li>
<li><a href="../pt470720/index.html">Como escrever um contrato inteligente com Python ontologia? Parte 2: API de armazenamento</a></li>
<li><a href="../pt470722/index.html">Como escrever um contrato inteligente com Python ontologia? Parte 3: API de tempo de execu√ß√£o</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>