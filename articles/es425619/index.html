<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåù üëßüèΩ ü§µüèº El problema de los bandidos con varios brazos: compare la estrategia codiciosa de Epsilon y el muestreo de Thompson üë®üèΩ‚Äçüè´ üë®üèª‚Äçüîß ‚ùóÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Les presento la portada de Solving multiarmed bandits: una comparaci√≥n de epsilon-codicioso y Thompson . 

 El problema del bandido multi-a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>El problema de los bandidos con varios brazos: compare la estrategia codiciosa de Epsilon y el muestreo de Thompson</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425619/">  <i>Hola Habr!</i>  <i>Les presento la portada de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Solving multiarmed bandits: una comparaci√≥n de epsilon-codicioso y Thompson</a> .</i> <br><br><h1>  El problema del bandido multi-armado </h1><br><p>  El problema del bandido multi-armado es una de las tareas m√°s b√°sicas en la ciencia de las soluciones.  A saber, este es el problema de la asignaci√≥n √≥ptima de recursos en condiciones de incertidumbre.  El nombre "bandido multi-armado" en s√≠ proviene de m√°quinas tragamonedas antiguas controladas por manijas.  Estos rifles de asalto fueron apodados "bandidos", porque despu√©s de hablar con ellos, la gente generalmente se sent√≠a robada.  Ahora imagine que hay varias m√°quinas de este tipo y la posibilidad de ganar contra diferentes autos es diferente.  Desde que comenzamos a jugar con estas m√°quinas, queremos determinar qu√© posibilidad es mayor y usar esta m√°quina con m√°s frecuencia que otras. </p><br><p>  El problema es este: ¬øc√≥mo entendemos de manera m√°s eficiente qu√© m√°quina es la m√°s adecuada y, al mismo tiempo, probamos muchas funciones en tiempo real?  Este no es un tipo de problema te√≥rico, es un problema que una empresa enfrenta todo el tiempo.  Por ejemplo, una empresa tiene varias opciones para los mensajes que deben mostrarse a los usuarios (por ejemplo, los mensajes incluyen anuncios, sitios, im√°genes) para que los mensajes seleccionados maximicen una determinada tarea empresarial (conversi√≥n, clics, etc.) </p><br><a name="habracut"></a><p>  Una forma t√≠pica de resolver este problema es ejecutar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pruebas A / B</a> muchas veces.  Es decir, durante varias semanas para mostrar cada una de las opciones con la misma frecuencia, y luego, con base en pruebas estad√≠sticas, decidir qu√© opci√≥n es mejor.  Este m√©todo es adecuado cuando hay pocas opciones, digamos, 2 o 4. Pero cuando hay muchas opciones, este enfoque se vuelve ineficaz, tanto en tiempo perdido como en p√©rdida de ganancias. </p><br><p>  El origen del tiempo perdido deber√≠a ser f√°cil de entender.  M√°s opciones: se necesitan m√°s pruebas A / B, se necesita m√°s tiempo para tomar una decisi√≥n.  La ocurrencia de ganancias perdidas no es tan obvia.  P√©rdida de oportunidad (costo de oportunidad): los costos asociados con el hecho de que en lugar de una acci√≥n realizamos otra, es decir, esto es lo que perdimos al invertir en A en lugar de B. Invertir en B es la ganancia perdida de invertir en A. Lo mismo con la comprobaci√≥n de opciones.  Las pruebas A / B no deben interrumpirse hasta que hayan terminado.  Esto significa que el experimentador no sabe qu√© opci√≥n es mejor hasta que finalice la prueba.  Sin embargo, todav√≠a se cree que una opci√≥n ser√° mejor que la otra.  Esto significa que al prolongar las pruebas A / B, no mostramos las mejores opciones para un n√∫mero suficientemente grande de visitantes (aunque no sabemos qu√© opciones no son las mejores), con lo que perdemos nuestras ganancias.  Este es el beneficio perdido de las pruebas A / B.  Si solo hay una prueba A / B, entonces tal vez la ganancia perdida no sea del todo grande.  Una gran cantidad de pruebas A / B significa que durante mucho tiempo tenemos que mostrar a los clientes muchas de las mejores opciones.  Ser√≠a mejor si pudiera descartar r√°pidamente las malas opciones en tiempo real, y solo entonces, cuando quedan pocas opciones, use las pruebas A / B para ellas. </p><br><p>  Los muestreadores o agentes son formas de probar y optimizar r√°pidamente la distribuci√≥n de opciones.  En este art√≠culo, le presentar√© el <i>muestreo de Thompson</i> y sus propiedades.  Tambi√©n comparar√© el muestreo de Thompson con el algoritmo de √©psilon codicioso, otra opci√≥n popular para el problema de los bandidos con m√∫ltiples brazos.  Todo se implementar√° en Python desde cero: todo el c√≥digo se puede encontrar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . </p><br><h2>  Breve diccionario de conceptos </h2><br><p></p><ul><li>  Agente, muestra, bandido ( <i>agente, muestra, bandido</i> ): un algoritmo que toma decisiones sobre qu√© opci√≥n mostrar. </li><li>  Variante: una variante diferente del mensaje que ve el visitante. </li><li>  Acci√≥n: la acci√≥n que ha elegido el algoritmo (qu√© opci√≥n mostrar). </li><li>  Uso ( <i>explotaci√≥n</i> ): elija maximizar la recompensa total en funci√≥n de los datos disponibles. </li><li>  Explore, <i>explore</i> : tome decisiones para comprender mejor el reembolso de cada opci√≥n. </li><li>  Premio, puntos ( <i>puntaje, recompensa</i> ): una tarea empresarial, por ejemplo, conversi√≥n o clics.  Por simplicidad, creemos que se distribuye binomialmente y es igual a 1 o 0 - clic o no. </li><li>  Entorno, el contexto en el que opera el agente, opciones y su "recuperaci√≥n" oculta para el usuario. </li><li>  Retribuci√≥n, probabilidad de √©xito ( <i>tasa de pago</i> ): una variable oculta igual a la probabilidad de obtener una puntuaci√≥n = 1, para cada opci√≥n es diferente.  Pero el usuario no la ve. </li><li>  Prueba ( <i>prueba</i> ): el usuario visita la p√°gina. </li><li>  Lamentar es la diferencia entre cu√°l ser√≠a el mejor resultado de todas las opciones disponibles y cu√°l fue el resultado de la opci√≥n disponible en el intento actual.  Cuanto menos se arrepientan de las acciones ya tomadas, mejor. </li><li>  Mensaje ( <i>mensaje</i> ): un banner, una opci√≥n de p√°gina y m√°s, diferentes versiones de las cuales queremos probar. </li><li>  Muestreo: la generaci√≥n de una muestra a partir de una distribuci√≥n dada. </li></ul><br><h2>  Explora y explota </h2><br><p>  Los agentes son algoritmos que buscan un enfoque para la toma de decisiones en tiempo real con el fin de lograr un equilibrio entre explorar el espacio de opciones y usar la mejor opci√≥n.  Este equilibrio es muy importante.  El espacio de opciones debe investigarse para tener una idea de qu√© opci√≥n es la mejor.  Si descubrimos por primera vez esta opci√≥n √≥ptima, y ‚Äã‚Äãluego la usamos todo el tiempo, maximizaremos la recompensa total que nos ofrece el medio ambiente.  Por otro lado, tambi√©n queremos explorar otras opciones posibles: ¬øqu√© pasar√≠a si resultaran mejores en el futuro, pero todav√≠a no lo sabemos?  En otras palabras, queremos asegurarnos contra posibles p√©rdidas, tratando de experimentar un poco con opciones sub√≥ptimas para aclarar por s√≠ mismos su recuperaci√≥n.  Si su recuperaci√≥n es realmente mayor, se pueden mostrar con m√°s frecuencia.  Otra ventaja de investigar opciones es que podemos comprender mejor no solo la recuperaci√≥n promedio, sino tambi√©n la distribuci√≥n aproximada de la recuperaci√≥n, es decir, podemos estimar mejor la incertidumbre. <br>  El principal problema, por lo tanto, es resolver: cu√°l es la mejor manera de salir del dilema entre exploraci√≥n y explotaci√≥n (compensaci√≥n de exploraci√≥n-explotaci√≥n). </p><br><h2>  Algoritmo codicioso de Epsilon </h2><br><p>  Una forma t√≠pica de salir de este dilema es el algoritmo √©psilon-codicioso.  "Codicioso" significa exactamente lo que pensabas.  Despu√©s de un per√≠odo inicial, cuando accidentalmente hacemos intentos, digamos 1000 veces, el algoritmo elige ansiosamente la mejor opci√≥n k en <i>el</i> porcentaje de intentos.  Por ejemplo, si <i>e</i> = 0.05, el algoritmo 95% del tiempo selecciona la mejor opci√≥n, y en el 5% restante selecciona intentos aleatorios.  De hecho, este es un algoritmo bastante efectivo, sin embargo, puede que no sea suficiente para explorar el espacio de opciones, y por lo tanto, no ser√° lo suficientemente bueno para evaluar qu√© opci√≥n es la mejor, para quedarse atascado en una opci√≥n sub√≥ptima.  Vamos a mostrar en el c√≥digo c√≥mo funciona este algoritmo. </p><br><p>  Pero primero, algunas dependencias.  Debemos definir el entorno.  Este es el contexto en el que se ejecutar√°n los algoritmos.  En este caso, el contexto es muy simple.  Llama al agente para que el agente decida qu√© acci√≥n elegir, luego el contexto lanza esta acci√≥n y devuelve los puntos recibidos por el agente (que de alguna manera actualiza su estado). </p><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Environment</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, variants, payouts, n_trials, variance=False)</span></span></span><span class="hljs-function">:</span></span> self.variants = variants <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> variance: self.payouts = np.clip(payouts + np.random.normal(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.04</span></span>, size=len(variants)), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">.2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: self.payouts = payouts <span class="hljs-comment"><span class="hljs-comment">#self.payouts[5] = self.payouts[5] if i &lt; n_trials/2 else 0.1 self.n_trials = n_trials self.total_reward = 0 self.n_k = len(variants) self.shape = (self.n_k, n_trials) def run(self, agent): """Run the simulation with the agent. agent must be a class with choose_k and update methods.""" for i in range(self.n_trials): # agent makes a choice x_chosen = agent.choose_k() # Environment returns reward reward = np.random.binomial(1, p=self.payouts[x_chosen]) # agent learns of reward agent.reward = reward # agent updates parameters based on the data agent.update() self.total_reward += reward agent.collect_data() return self.total_reward</span></span></code> </pre> <br>  Los puntos se distribuyen binomialmente con probabilidad p dependiendo del n√∫mero de la acci√≥n (as√≠ como podr√≠an distribuirse continuamente, la esencia no habr√≠a cambiado).  Tambi√©n definir√© la clase BaseSampler: se necesita solo para almacenar registros y varios atributos. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">BaseSampler</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_samples=None, n_learning=None, e=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.05</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.env = env self.shape = (env.n_k, n_samples) self.variants = env.variants self.n_trials = env.n_trials self.payouts = env.payouts self.ad_i = np.zeros(env.n_trials) self.r_i = np.zeros(env.n_trials) self.thetas = np.zeros(self.n_trials) self.regret_i = np.zeros(env.n_trials) self.thetaregret = np.zeros(self.n_trials) self.a = np.ones(env.n_k) self.b = np.ones(env.n_k) self.theta = np.zeros(env.n_k) self.data = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.total_reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.k = <span class="hljs-number"><span class="hljs-number">0</span></span> self.i = <span class="hljs-number"><span class="hljs-number">0</span></span> self.n_samples = n_samples self.n_learning = n_learning self.e = e self.ep = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, size=env.n_trials) self.exploit = (<span class="hljs-number"><span class="hljs-number">1</span></span> - e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">collect_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.data = pd.DataFrame(dict(ad=self.ad_i, reward=self.r_i, regret=self.regret_i))</code> </pre> <br>  A continuaci√≥n definimos 10 opciones y amortizaci√≥n para cada una.  La mejor opci√≥n es la opci√≥n 9 con una recuperaci√≥n de la inversi√≥n del 0,11%. <br><br><pre> <code class="python hljs">variants = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>] payouts = [<span class="hljs-number"><span class="hljs-number">0.023</span></span>, <span class="hljs-number"><span class="hljs-number">0.03</span></span>, <span class="hljs-number"><span class="hljs-number">0.029</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.06</span></span>, <span class="hljs-number"><span class="hljs-number">0.0234</span></span>, <span class="hljs-number"><span class="hljs-number">0.035</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.11</span></span>]</code> </pre> <br>  Para tener algo sobre lo que construir, tambi√©n definimos la clase RandomSampler.  Esta clase es necesaria como modelo de referencia.  Simplemente elige al azar una opci√≥n en cada intento y no actualiza sus par√°metros. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RandomSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.k = np.random.choice(self.variants) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.k <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># nothing to update #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.thetaregret) - self.theta[self.k] #self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.theta) - self.theta[self.k] self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><p>  Otros modelos tienen la siguiente estructura.  Todos tienen choose_k y m√©todos de actualizaci√≥n.  choose_k implementa el m√©todo por el cual el agente selecciona una opci√≥n.  actualizar actualiza los par√°metros del agente: este m√©todo caracteriza c√≥mo cambia la capacidad del agente para elegir la opci√≥n (con RandomSampler esta capacidad no cambia de ninguna manera).  Ejecutamos el agente en el entorno utilizando el siguiente patr√≥n. </p><br><pre> <code class="python hljs">en0 = Environment(machines, payouts, n_trials=<span class="hljs-number"><span class="hljs-number">10000</span></span>) rs = RandomSampler(env=en0) en0.run(agent=rs)</code> </pre> <br><p>  La esencia del algoritmo √©psilon-codicioso es la siguiente. <br><br></p><ol><li>  Seleccione aleatoriamente k para n intentos. </li><li>  En cada intento, para cada opci√≥n, eval√∫e las ganancias. </li><li>  Despu√©s de todos los intentos n: </li><li>  Con probabilidad 1 - <i>e</i> elija k con la ganancia m√°s alta; </li><li>  Con probabilidad <i>e</i> elija K al azar. </li></ol><br>  C√≥digo codicioso de Epsilon: <br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">eGreedy</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_learning, e)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env, n_learning, e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># e% of the time take a random draw from machines # random k for n learning trials, then the machine with highest theta self.k = np.random.choice(self.variants) if self.i &lt; self.n_learning else np.argmax(self.theta) # with 1 - e probability take a random sample (explore) otherwise exploit self.k = np.random.choice(self.variants) if self.ep[self.i] &gt; self.exploit else self.k return self.k # every 100 trials update the successes # update the count of successes for the chosen machine def update(self): # update the probability of payout for each machine self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b #self.total_reward += self.reward #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] #self.thetaregret[self.i] = self.thetaregret[self.i] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><br><p>  A continuaci√≥n, en el gr√°fico, puede ver los resultados de un muestreo puramente aleatorio, es decir, aqu√≠ no hay ning√∫n modelo.  El gr√°fico muestra qu√© elecci√≥n realiz√≥ el algoritmo en cada intento, si hubo 10 mil intentos.  El algoritmo solo lo intenta, pero no aprende.  En total, anot√≥ 418 puntos. <br> <a href=""><img src="https://habrastorage.org/webt/sn/ql/2r/snql2roqbdiruuskxsathithz8i.jpeg"></a> </p><br><p>  Veamos c√≥mo se comporta el algoritmo √©psilon-codicioso en el mismo entorno.  Ejecute el algoritmo para 10 mil intentos con <i>e</i> = 0.1 y n_learning = 500 (el agente simplemente intenta los primeros 500 intentos, luego lo intenta con probabilidad <i>e</i> = 0.1).  Vamos a evaluar el algoritmo de acuerdo con el n√∫mero total de puntos que punt√∫a en el entorno. </p><br><pre> <code class="python hljs">en1 = Environment(machines, payouts, n_trials) eg = eGreedy(env=en1, n_learning=<span class="hljs-number"><span class="hljs-number">500</span></span>, e=<span class="hljs-number"><span class="hljs-number">0.1</span></span>) en1.run(agent=eg)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/-f/zf/14/-fzf14djbqtdl5-0vyrapuhcp5c.jpeg"></a> <br><p>  El algoritmo codicioso de Epsilon obtuvo 788 puntos, casi 2 veces mejor que el algoritmo aleatorio, ¬°s√∫per!  El segundo gr√°fico explica este algoritmo bastante bien.  Vemos que para los primeros 500 pasos las acciones se distribuyen aproximadamente de manera uniforme y K se elige al azar.  Sin embargo, entonces comienza a explotar fuertemente la opci√≥n 5: esta es una opci√≥n bastante fuerte, pero no la mejor.  Tambi√©n vemos que el agente todav√≠a selecciona aleatoriamente el 10% del tiempo. </p><br><p>  Esto es bastante bueno: escribimos solo unas pocas l√≠neas de c√≥digo, y ahora ya tenemos un algoritmo bastante poderoso que puede explorar el espacio de opciones y tomar decisiones cercanas a lo √≥ptimo.  Por otro lado, el algoritmo no encontr√≥ la mejor opci√≥n.  S√≠, podemos aumentar el n√∫mero de pasos para aprender, pero de esta manera pasaremos a√∫n m√°s tiempo en una b√∫squeda aleatoria, empeorando a√∫n m√°s el resultado final.  Adem√°s, la aleatoriedad se cose en este proceso de forma predeterminada; es posible que no se encuentre el mejor algoritmo. </p><br><p>  M√°s tarde, ejecutar√© cada uno de los algoritmos muchas veces para que podamos compararlos entre s√≠.  Pero por ahora, echemos un vistazo al muestreo de Thompson y prob√©moslo en el mismo entorno. </p><br><h2>  Muestreo de Thompson </h2><br><p>  El muestreo de Thompson es fundamentalmente diferente del algoritmo de √©psilon codicioso en tres puntos principales: <br><br></p><ol><li>  No es codicioso. </li><li>  Hace intentos de una manera m√°s sofisticada. </li><li>  Es bayesiano. </li></ol><br>  El punto principal es el p√°rrafo 3, los p√°rrafos 1 y 2 se siguen de √©l. <br><p>  La esencia del algoritmo es esta: <br><br></p><ol><li>  Establezca la distribuci√≥n beta inicial entre 0 y 1 para el reembolso de cada opci√≥n. </li><li>  Pruebe las opciones de esta distribuci√≥n, seleccione el par√°metro Theta m√°ximo. </li><li>  Elija la opci√≥n k que est√° asociada con la mayor theta. </li><li>  Ver cu√°ntos puntos se han puntuado, actualizar los par√°metros de distribuci√≥n. </li></ol><br>  Lea m√°s sobre la distribuci√≥n beta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . <br>  Y sobre su uso en Python, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . <br><p>  C√≥digo de algoritmo: <br><br></p><pre> <code class="python hljs"> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ThompsonSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># sample from posterior (this is the thompson sampling approach) # this leads to more exploration because machines with &gt; uncertainty can then be selected as the machine self.theta = np.random.beta(self.a, self.b) # select machine with highest posterior p of payout self.k = self.variants[np.argmax(self.theta)] #self.k = np.argmax(self.a/(self.a + self.b)) return self.k def update(self): #update dist (a, b) = (a, b) + (r, 1 - r) self.a[self.k] += self.reward self.b[self.k] += 1 - self.reward # ie only increment b when it's a swing and a miss. 1 - 0 = 1, 1 - 1 = 0 #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br>  La notaci√≥n formal del algoritmo se ve as√≠. <br> <a href=""><img src="https://habrastorage.org/webt/5f/n6/xe/5fn6xew2i7v1jjh_10h9jqkjdzu.png"></a> <br><p>  Programemos este algoritmo.  Al igual que otros agentes, ThompsonSampler hereda de BaseSampler y define sus propios m√©todos choose_k y update.  Ahora lanza nuestro nuevo agente. </p><br><pre> <code class="python hljs"> en2 = Environment(machines, payouts, n_trials) tsa = ThompsonSampler(env=en2) en2.run(agent=tsa)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/ml/kj/1p/mlkj1pvs8xnpxtkqmehmm_3xhgo.jpeg"></a> <br><p>  Como puede ver, obtuvo m√°s puntajes que el algoritmo de √©psilon codicioso.  Genial  Veamos el gr√°fico de la selecci√≥n de intentos.  Dos cosas interesantes son visibles en √©l.  En primer lugar, el agente descubri√≥ correctamente la mejor opci√≥n (opci√≥n 9) y la utiliz√≥ al m√°ximo.  En segundo lugar, el agente us√≥ otras opciones, pero de una manera m√°s complicada: despu√©s de aproximadamente 1000 intentos, el agente, adem√°s de la opci√≥n principal, utiliz√≥ principalmente las opciones m√°s poderosas entre las dem√°s.  En otras palabras, no eligi√≥ al azar, sino de manera m√°s competente. </p><br><p>  ¬øPor qu√© funciona esto?  Es simple: la incertidumbre en la distribuci√≥n posterior de los beneficios esperados para cada opci√≥n significa que cada opci√≥n se selecciona con una probabilidad aproximadamente proporcional a su forma, determinada por los par√°metros alfa y beta.  En otras palabras, en cada intento, el muestreo de Thompson activa la opci√≥n de acuerdo con la probabilidad posterior de que tenga el beneficio m√°ximo.  En t√©rminos generales, teniendo en cuenta la informaci√≥n de distribuci√≥n sobre la incertidumbre, el agente decide cu√°ndo examinar el entorno y cu√°ndo usar la informaci√≥n.  Por ejemplo, una opci√≥n d√©bil con alta incertidumbre posterior puede pagar m√°s por este intento.  Pero para la mayor√≠a de los intentos, cuanto m√°s fuerte es su distribuci√≥n posterior, mayor es su promedio y menor es su desviaci√≥n est√°ndar, y por lo tanto, mayor es la posibilidad de elegirlo. </p><br><p>  Otra propiedad notable del algoritmo de Thompson: dado que es bayesiano, podemos estimar la incertidumbre en la estimaci√≥n de recuperaci√≥n de la inversi√≥n para cada opci√≥n utilizando sus par√°metros.  El siguiente gr√°fico muestra distribuciones posteriores en 6 puntos diferentes y en 20,000 intentos.  Ver√° c√≥mo las distribuciones comienzan a converger gradualmente a la opci√≥n con la mejor recuperaci√≥n de la inversi√≥n. </p><br> <a href=""><img src="https://habrastorage.org/webt/bb/ka/fb/bbkafb4nv1pajwkygxy2brtmowy.jpeg"></a> <br><p>  Ahora compare los 3 agentes en 100 simulaciones.  1 simulaci√≥n es un lanzamiento de agente en 10,000 intentos. </p><br> <a href=""><img src="https://habrastorage.org/webt/j6/v1/sm/j6v1smcrwkwyhlo27ffhly13pwk.jpeg"></a> <br><p>  Como puede ver en el gr√°fico, tanto la estrategia de √©psilon codicioso como el muestreo de Thompson funcionan mucho mejor que el muestreo aleatorio.  Puede sorprenderle que la estrategia de √©psilon codicioso y el muestreo de Thompson sean realmente comparables en t√©rminos de su rendimiento.  La estrategia codiciosa de Epsilon puede ser muy efectiva, pero es m√°s arriesgada, ya que puede atascarse en una opci√≥n sub√≥ptima; esto se puede ver en las fallas del gr√°fico.  Pero el muestreo de Thompson no puede, porque hace la elecci√≥n en el espacio de opciones de una manera m√°s compleja. </p><br><h2>  Arrepentimiento </h2><br><p>  Otra forma de evaluar qu√© tan bien funciona el algoritmo es evaluar el arrepentimiento.  En t√©rminos generales, cuanto m√°s peque√±o es, en relaci√≥n con las acciones ya tomadas, mejor.  A continuaci√≥n se muestra un gr√°fico del arrepentimiento total y el arrepentimiento por el error.  Una vez m√°s, cuanto menos arrepentimiento, mejor. </p><br> <a href=""><img src="https://habrastorage.org/webt/8p/kd/o3/8pkdo3bilrde28bwsimdnbesqwg.jpeg"></a> <br><p>  En el gr√°fico superior, vemos el arrepentimiento total, y en el arrepentimiento inferior el intento.  Como se puede ver en los gr√°ficos, el muestreo de Thompson converge a un arrepentimiento m√≠nimo mucho m√°s r√°pido que la estrategia de √©psilon codicioso.  Y converge a un nivel inferior.  Con el muestreo de Thompson, el agente lamenta menos porque puede detectar mejor la mejor opci√≥n y probar mejor las opciones m√°s prometedoras, por lo que el muestreo de Thompson es particularmente adecuado para casos de uso m√°s avanzados, como modelos estad√≠sticos o redes neuronales para elegir k. </p><br><h2>  Conclusiones </h2><br><p>  Esta es una publicaci√≥n t√©cnica bastante larga.  Para resumir, podemos usar m√©todos de muestreo bastante sofisticados si tenemos muchas opciones que queremos probar en tiempo real.  Una de las caracter√≠sticas muy buenas del muestreo de Thompson es que equilibra el uso y la exploraci√≥n de una manera bastante complicada.  Es decir, podemos permitirle optimizar la distribuci√≥n de opciones de soluci√≥n en tiempo real.  Estos son algoritmos geniales, y deber√≠an ser m√°s √∫tiles para un negocio que las pruebas A / B. </p><br><p>  <b>Importante!</b>  <b>El muestreo de Thompson no significa que no necesite hacer pruebas A / B.</b>  <b>Por lo general, primero encuentran las mejores opciones con su ayuda, y luego hacen pruebas A / B en ellas.</b> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es425619/">https://habr.com/ru/post/es425619/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es425605/index.html">Aceptaci√≥n de pagos desde una tarjeta sin jur. caras en Yandex.Money</a></li>
<li><a href="../es425607/index.html">Identifique el fraude utilizando el conjunto de datos de Enron. Parte 2, encontrar el mejor modelo</a></li>
<li><a href="../es425609/index.html">Teor√≠a de juegos: toma de decisiones con ejemplos en Kotlin</a></li>
<li><a href="../es425611/index.html">Arquitectura frontend de nivel superior. Conferencia de Yandex</a></li>
<li><a href="../es425613/index.html">C√≥mo combin√© los datos del complemento Tempo para Jira Server y Jira Cloud y los volv√≠ a migrar a Jira Cloud</a></li>
<li><a href="../es425621/index.html">Una compa√±√≠a que utiliza di√≥xido de carbono atmosf√©rico lanza producci√≥n de metano</a></li>
<li><a href="../es425623/index.html">Recorrido fotogr√°fico de coworking "Key"</a></li>
<li><a href="../es425625/index.html">Gastado, o por qu√© los localizadores traducen mal los juegos</a></li>
<li><a href="../es425627/index.html">IaaS para desarrollar servicios: qui√©n y por qu√© se cambi√≥ a infraestructura virtual</a></li>
<li><a href="../es425629/index.html">C√≥mo hicimos un juego de mesa con control remoto</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>