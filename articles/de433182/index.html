<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☦️ 🔦 👩🏽‍🤝‍👩🏻 Ist es möglich, einen Agenten für den Handel an der Börse mit Verstärkung auszubilden? Implementierung der R-Sprache 🌉 🙆🏿 💩</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Lassen Sie uns einen Prototyp des Verstärkungslernagenten (RL) erstellen, der die Handelsfähigkeiten beherrscht. 

 Da die Implementierung des Prototy...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ist es möglich, einen Agenten für den Handel an der Börse mit Verstärkung auszubilden? Implementierung der R-Sprache</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/433182/"> Lassen Sie uns einen Prototyp des Verstärkungslernagenten (RL) erstellen, der die Handelsfähigkeiten beherrscht. <br><br>  Da die Implementierung des Prototyps in der R-Sprache funktioniert, ermutige ich R-Benutzer und Programmierer, sich den in diesem Artikel vorgestellten Ideen anzunähern. <br><br>  Dies ist eine Übersetzung meines englischen Artikels: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kann Reinforcement Learning Trade Stock?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Implementierung in R.</a> <br><br>  <i>Ich möchte die Codejäger warnen, dass es in dieser Notiz nur einen Code für ein neuronales Netzwerk gibt, das für R angepasst ist.</i> <br><br>  Wenn ich mich nicht in gutem Russisch auszeichnete, weisen Sie auf die Fehler hin (der Text wurde mit Hilfe eines automatischen Übersetzers erstellt). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/97b/b88/e48/97bb88e4896efcd9f281cfb1b72830cf.png" alt="Bild"><br><a name="habracut"></a><br><h3>  Einführung in das Problem </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/986/550/1f7/9865501f78bdeb575a3017a0c2466d54.png" alt="Bild"><br><br>  Ich rate Ihnen, mit diesem Artikel in das Thema <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einzutauchen</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepMind</a> <br><br>  Es führt Sie in die Idee ein, das Deep Q-Network (DQN) zu verwenden, um eine Wertefunktion zu approximieren, die für Markov-Entscheidungsprozesse von entscheidender Bedeutung ist. <br><br>  Ich empfehle auch, mit dem Vorabdruck dieses Buches von Richard S. Sutton und Andrew J. Barto: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Reinforcement Learning</a> tiefer in die Mathematik <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einzusteigen</a> <br><br>  Im Folgenden werde ich eine erweiterte Version des ursprünglichen DQN vorstellen, die weitere Ideen enthält, die dem Algorithmus helfen, schnell und effizient zu konvergieren, nämlich: <br><br>  <b>Deep Double Dueling Noisy NN</b> mit Prioritätsauswahl aus dem Erfahrungswiedergabepuffer. <br><br>  Was macht diesen Ansatz besser als klassisches DQN? <br><br><ul><li>  Double: Es gibt zwei Netzwerke, von denen eines trainiert ist und das andere die folgenden Werte von Q auswertet </li><li>  Duell: Es gibt Neuronen, die eindeutig Wert legen und davon profitieren </li><li>  Rauschen: Auf die Gewichte der Zwischenschichten werden Rauschmatrizen angewendet, bei denen der Mittelwert und die Standardabweichung trainierte Gewichte sind </li><li>  Priorität der Probenahme: Beobachtungsstapel aus dem Wiedergabepuffer enthalten Beispiele, aufgrund derer das vorherige Training der Funktionen zu großen Rückständen führte, die im Hilfsarray gespeichert werden können. </li></ul><br><h4>  Was ist mit dem Handel des DQN-Agenten?  Dies ist ein interessantes Thema als solches. </h4><br>  Es gibt Gründe, warum dies interessant ist: <br><br><ul><li>  Absolute Wahlfreiheit bei der Darstellung von Status, Aktionen, Auszeichnungen und Architektur von NN.  Sie können den Einstiegsbereich mit allem bereichern, was Sie für einen Versuch wert halten, von Nachrichten bis zu anderen Aktien und Indizes. </li><li>  Die Entsprechung der Handelslogik mit der Verstärkungslernlogik besteht darin, dass: der Agent diskrete (oder kontinuierliche) Aktionen ausführt, selten belohnt wird (nachdem die Transaktion abgeschlossen wurde oder der Zeitraum abläuft), die Umgebung teilweise beobachtbar ist und Informationen über die nächsten Schritte enthalten kann. Der Handel ist ein episodisches Spiel. </li><li>  Sie können DQN-Ergebnisse mit verschiedenen Benchmarks vergleichen, z. B. mit Indizes und technischen Handelssystemen. </li><li>  Der Agent kann ständig neue Informationen lernen und sich so an die sich ändernden Spielregeln anpassen. </li></ul><br>  Um das Material nicht zu dehnen, schauen Sie sich den Code dieses NN an, den ich teilen möchte, da dies einer der mysteriösen Teile des gesamten Projekts ist. <br><br><h4>  R-Code für ein neuronales Wertnetzwerk, das Keras zum Erstellen unseres RL-Agenten verwendet </h4><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># configure critic NN ------------ library('keras') library('R6') learning_rate &lt;- 1e-3 state_names_length &lt;- 12 # just for example a_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { x - k_mean(x, axis = 2, keepdims = T) } ) ) a_normalize_layer &lt;- function(object) { create_layer(a_CustomLayer, object, list(name = 'a_normalize_layer')) } v_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { k_concatenate(list(x, x, x), axis = 2) } , compute_output_shape = function(input_shape) { output_shape = input_shape output_shape[[2]] &lt;- input_shape[[2]] * 3L output_shape } ) ) v_normalize_layer &lt;- function(object) { create_layer(v_CustomLayer, object, list(name = 'v_normalize_layer')) } noise_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , lock_objects = FALSE , public = list( initialize = function(output_dim) { self$output_dim &lt;- output_dim } , build = function(input_shape) { self$input_dim &lt;- input_shape[[2]] sqr_inputs &lt;- self$input_dim ** (1/2) self$sigma_initializer &lt;- initializer_constant(.5 / sqr_inputs) self$mu_initializer &lt;- initializer_random_uniform(minval = (-1 / sqr_inputs), maxval = (1 / sqr_inputs)) self$mu_weight &lt;- self$add_weight( name = 'mu_weight', shape = list(self$input_dim, self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_weight &lt;- self$add_weight( name = 'sigma_weight', shape = list(self$input_dim, self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) self$mu_bias &lt;- self$add_weight( name = 'mu_bias', shape = list(self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_bias &lt;- self$add_weight( name = 'sigma_bias', shape = list(self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) } , call = function(x, mask = NULL) { #sample from noise distribution e_i = k_random_normal(shape = list(self$input_dim, self$output_dim)) e_j = k_random_normal(shape = list(self$output_dim)) #We use the factorized Gaussian noise variant from Section 3 of Fortunato et al. eW = k_sign(e_i) * (k_sqrt(k_abs(e_i))) * k_sign(e_j) * (k_sqrt(k_abs(e_j))) eB = k_sign(e_j) * (k_abs(e_j) ** (1/2)) #See section 3 of Fortunato et al. noise_injected_weights = k_dot(x, self$mu_weight + (self$sigma_weight * eW)) noise_injected_bias = self$mu_bias + (self$sigma_bias * eB) output = k_bias_add(noise_injected_weights, noise_injected_bias) output } , compute_output_shape = function(input_shape) { output_shape &lt;- input_shape output_shape[[2]] &lt;- self$output_dim output_shape } ) ) noise_add_layer &lt;- function(object, output_dim) { create_layer( noise_CustomLayer , object , list( name = 'noise_add_layer' , output_dim = as.integer(output_dim) , trainable = T ) ) } critic_input &lt;- layer_input( shape = c(as.integer(state_names_length)) , name = 'critic_input' ) common_layer_dense_1 &lt;- layer_dense( units = 20 , activation = "tanh" ) critic_layer_dense_v_1 &lt;- layer_dense( units = 10 , activation = "tanh" ) critic_layer_dense_v_2 &lt;- layer_dense( units = 5 , activation = "tanh" ) critic_layer_dense_v_3 &lt;- layer_dense( units = 1 , name = 'critic_layer_dense_v_3' ) critic_layer_dense_a_1 &lt;- layer_dense( units = 10 , activation = "tanh" ) # critic_layer_dense_a_2 &lt;- layer_dense( # units = 5 # , activation = "tanh" # ) critic_layer_dense_a_3 &lt;- layer_dense( units = length(acts) , name = 'critic_layer_dense_a_3' ) critic_model_v &lt;- critic_input %&gt;% common_layer_dense_1 %&gt;% critic_layer_dense_v_1 %&gt;% critic_layer_dense_v_2 %&gt;% critic_layer_dense_v_3 %&gt;% v_normalize_layer critic_model_a &lt;- critic_input %&gt;% common_layer_dense_1 %&gt;% critic_layer_dense_a_1 %&gt;% #critic_layer_dense_a_2 %&gt;% noise_add_layer(output_dim = 5) %&gt;% critic_layer_dense_a_3 %&gt;% a_normalize_layer critic_output &lt;- layer_add( list( critic_model_v , critic_model_a ) , name = 'critic_output' ) critic_model_1 &lt;- keras_model( inputs = critic_input , outputs = critic_output ) critic_optimizer = optimizer_adam(lr = learning_rate) keras::compile( critic_model_1 , optimizer = critic_optimizer , loss = 'mse' , metrics = 'mse' ) train.x &lt;- rnorm(state_names_length * 10) train.x &lt;- array(train.x, dim = c(10, state_names_length)) predict(critic_model_1, train.x) layer_name &lt;- 'noise_add_layer' intermediate_layer_model &lt;- keras_model(inputs = critic_model_1$input, outputs = get_layer(critic_model_1, layer_name)$output) predict(intermediate_layer_model, train.x)[1,] critic_model_2 &lt;- critic_model_1</span></span></code> </pre> <br></div></div><br>  Ich habe diese Quelle verwendet, um Python-Code für den Rauschteil des Netzwerks <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">anzupassen</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github Repo</a> <br><br>  Dieses neuronale Netzwerk sieht folgendermaßen aus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dc/20c/e1e/3dc20ce1eedec47b871ef59ab7911ef3.png" alt="Bild"><br><br>  Denken Sie daran, dass wir in der Duellarchitektur Gleichheit verwenden (Gleichung 1): <br><br>  Q = A '+ V, wobei <br><br>  A '= A - Durchschnitt (A); <br><br>  Q = Wert der Zustandsaktion; <br><br>  V = Zustandswert; <br><br>  A = Vorteil. <br><br>  Andere Variablen im Code sprechen für sich.  Darüber hinaus eignet sich diese Architektur nur für eine bestimmte Aufgabe. Nehmen Sie sie daher nicht als selbstverständlich an. <br><br>  Der Rest des Codes wird höchstwahrscheinlich allgemein genug für die Veröffentlichung sein, und es wird für den Programmierer interessant sein, ihn selbst zu schreiben. <br><br>  Und jetzt - die Experimente.  Das Testen der Arbeit des Agenten wurde in einem Sandkasten mit einem echten Makler durchgeführt, weit entfernt von den Realitäten des Handels auf einem Live-Markt. <br><br><h3>  Phase I. </h3><br>  Wir führen unseren Agenten gegen einen synthetischen Datensatz aus.  Unsere Transaktionskosten betragen 0,5: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb2/f15/3b6/fb2f153b6c4e14ff1dc19d61a524f5e8.png" alt="Bild"><br><br>  Das Ergebnis ist ausgezeichnet.  Die maximale durchschnittliche episodische Belohnung in diesem Experiment <br>  sollte 1,5 sein. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/417/0a0/a84/4170a0a842560a2a2fd3112d601dba54.jpg" alt="Bild"><br><br>  Wir sehen: Verlust der Kritik (das sogenannte Wertnetzwerk im Schauspieler-Kritiker-Ansatz), durchschnittliche Belohnung für eine Episode, akkumulierte Belohnung, Stichprobe der jüngsten Belohnungen. <br><br><h3>  Phase II </h3><br>  Wir bringen unserem Agenten ein willkürlich ausgewähltes Aktiensymbol bei, das ein interessantes Verhalten zeigt: einen flachen Start, ein schnelles Wachstum in der Mitte und ein trostloses Ende.  In unserem Trainingskit ca. 4300 Tage.  Die Transaktionskosten betragen 0,1 US-Dollar (absichtlich niedrig).  Die Belohnung ist USD Gewinn / Verlust nach Abschluss eines Geschäfts zum Kauf / Verkauf von 1,0 Aktien. <br><br>  Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Finance.yahoo.com/quote/algn?ltr=1</a> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d6d/22c/540/d6d22c540514afc7261f549bea8c74ac.png" alt="Bild"><br><br>  <i>NASDAQ: ALGN</i> <br><br>  Nachdem wir einige Parameter eingestellt hatten (wobei die NN-Architektur unverändert blieb), kamen wir zu folgendem Ergebnis: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/612/008/50c/61200850c8a8c0b7963bbee12ca1a2bd.jpg" alt="Bild"><br><br>  Es stellte sich als nicht schlecht heraus, da der Agent am Ende lernte, durch Drücken von drei Tasten auf seiner Konsole Gewinn zu machen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d8d/d86/afb/d8dd86afb84b4f4625012a5c3ba9dc4e.png" alt="Bild"><br><br>  <i>roter Marker = verkaufen, grüner Marker = kaufen, grauer Marker = nichts tun.</i> <br><br>  Bitte beachten Sie, dass die durchschnittliche Belohnung pro Episode auf ihrem Höhepunkt den realistischen Transaktionswert überstieg, der im realen Handel auftreten kann. <br><br>  Es ist schade, dass Aktien aufgrund schlechter Nachrichten wie verrückt fallen ... <br><br><h3>  Schlussbemerkungen </h3><br>  Der Handel mit RL ist nicht nur schwierig, sondern auch nützlich.  Wenn Ihr Roboter es besser macht als Sie, ist es Zeit, Ihre persönliche Zeit zu verbringen, um Bildung und Gesundheit zu erlangen. <br><br>  Ich hoffe das war eine interessante Reise für dich.  Wenn Ihnen diese Geschichte gefallen hat, winken Sie mit der Hand.  Wenn großes Interesse besteht, kann ich fortfahren und Ihnen zeigen, wie die Richtlinienverlaufsmethoden unter Verwendung der R-Sprache und der Keras-API funktionieren. <br><br>  Ich möchte auch meinen Freunden, die sich für neuronale Netze interessieren, für ihren Rat danken. <br><br>  Wenn Sie noch Fragen haben, bin ich immer hier. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de433182/">https://habr.com/ru/post/de433182/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de433172/index.html">Wir verbinden den Multiplayer mit dem in C ++ geschriebenen Handyspiel "Machen Sie Wörter aus Wörtern" auf iOS und Android</a></li>
<li><a href="../de433174/index.html">Nicht alle Patches sind gleichermaßen nützlich.</a></li>
<li><a href="../de433176/index.html">Docker Remote-API für Zertifikatauthentifizierung mit Sperrüberprüfung</a></li>
<li><a href="../de433178/index.html">Wie wir eine beschädigte WAV-Datei wiederhergestellt haben</a></li>
<li><a href="../de433180/index.html">Lösen von Datentypproblemen in Ruby oder Machen Sie Daten wieder zuverlässig</a></li>
<li><a href="../de433184/index.html">ASP.NET Core 2.2 veröffentlicht. Was gibt's Neues? (2 von 3)</a></li>
<li><a href="../de433186/index.html">Es reicht nicht aus, Polygone zu zählen, um 3D-Modelle zu optimieren</a></li>
<li><a href="../de433188/index.html">Die Staatsduma legte einen Gesetzentwurf zur autonomen Arbeit von Runet vor</a></li>
<li><a href="../de433192/index.html">Kubernetes: Eine erstaunlich erschwingliche persönliche Projektlösung</a></li>
<li><a href="../de433194/index.html">Geplantes Nachtlicht</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>