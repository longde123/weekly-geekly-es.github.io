<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📗 👨🏾‍🏭 🦋 O filósofo de inteligência artificial Eliezer Yudkowsky sobre singularidade, cérebro bayesiano e duendes em um gabinete 👨🏼 👩🏽‍🤝‍👨🏼 🎻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eliezer Shlomo Yudkovsky é um especialista americano em inteligência artificial, que estuda os problemas da singularidade tecnológica e defende a cria...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O filósofo de inteligência artificial Eliezer Yudkowsky sobre singularidade, cérebro bayesiano e duendes em um gabinete</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/404137/"><img src="https://habrastorage.org/getpro/geektimes/post_images/152/af6/3ff/152af63ff689233ebebadde98fed2580.png" alt="imagem"><br><br>  <i>Eliezer Shlomo Yudkovsky é um especialista americano em inteligência artificial, que estuda os problemas da singularidade tecnológica e defende a criação da IA ​​Amigável.</i>  <i>Nos círculos não acadêmicos, ele é mais conhecido como autor da ficção de fãs de Harry Potter e Métodos de Racionalidade, sob os auspícios de Menos Errado.</i> <br><br>  Sempre fiquei impressionado com pessoas inteligentes que acreditam em coisas que me parecem absurdas.  Por exemplo, o geneticista e diretor dos Institutos Nacionais de Saúde, Francis Collins, acredita que Jesus ressuscitou dos mortos.  O teórico da IA ​​Eliezer Yudkowsky acredita que os carros ... Mas melhor eu vou dar a palavra a ele.  Em 2008, eu o entrevistei no Bloggingheads.tv, mas nada de bom aconteceu porque eu decidi que ele era um seguidor do guru <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">da</a> singularidade de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ray Kurzweil</a> .  Mas Yudkovsky não seguiu ninguém e nunca foi para a faculdade.  Ele é um teórico teimoso e original da inteligência, humana e artificial.  Seu trabalho (por exemplo, um ensaio que me ajudou a entender, ou deu a ilusão de entendimento, os teoremas de Bayes) exala a arrogância do autodidata, cujas bordas afiadas não foram polidas pela educação formal - mas isso faz parte de seu charme.  Mesmo quando te incomoda, Yudkovsky é engraçado, renovado, provocador.  Para detalhes de sua biografia, consulte seu site pessoal ou o site do Instituto para o Estudo da Inteligência de Máquinas, do qual ele participou.  E leia esta entrevista com um bônus na forma de comentários de sua esposa Briena. <br><a name="habracut"></a><br>  <b>Horgan</b> : Quando perguntado em uma festa o que você faz, o que você responde? <br><br>  <b>Yudkovsky</b> : Depende do evento.  “Sou especialista em teoria da decisão” ou “co-fundador do Instituto para o Estudo da Inteligência de Máquinas” ou, se for de um tipo diferente, falo sobre minhas obras de arte. <br><br>  <b>X:</b> Qual é o seu filme de IA favorito e por quê? <br><br>  <b>Yu: A</b> IA nos filmes é terrivelmente padrão.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ex Machina</a> chegou tão perto de uma exceção a esta regra quanto se pode esperar. <br><br>  <b>X:</b> O utilitário da faculdade é superestimado? <br><br>  <b>Yu:</b> Eu ficaria surpreso se sua utilidade fosse subestimada, dados os requisitos sociais para acabar com ela.  Até onde eu sei, não há razão para não acreditar em economistas que acreditam que a faculdade se tornou um "produto de prestígio" e que as tentativas de aumentar os empréstimos estudantis simplesmente aumentaram o custo da faculdade e o ônus da dívida estudantil. <br><br>  <b>X:</b> Por que você escreve histórias de ficção? <br><br>  <b>Yu:</b> Se você reformular os quadrinhos da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Wondermark</a> : "No começo, tentei não fazê-lo, mas não funcionou". <br><br>  Além disso, a literatura séria transmite conhecimento, enquanto a ficção transmite experiência.  Se você quiser entender as provas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">da fórmula de Bayes</a> , posso usar diagramas.  Se você quiser sentir como é usar a lógica de Bayes, preciso escrever uma história na qual o personagem a faça. <br><br>  <b>X:</b> Você é religioso em algum sentido? <br><br>  <b>Yu:</b> Não.  Ao cometer um erro, você precisa evitar a tentação de sair em defesa, tentar encontrar algum ponto de vista do qual você esteja pelo menos um pouco certo.  É muito mais sábio dizer: “Ah”, admitir que você não estava nem um pouco certo, engolir uma pílula amarga inteira e continuar a viver.  É assim que a humanidade deve se relacionar com a religião. <br><br>  <b>X:</b> Se você se tornasse o "rei do mundo", o que estaria no topo da sua lista de tarefas? <br><br>  <b>Yu: Uma</b> vez eu escrevi: “Um teste para um libertário funciona assim: imagine que você ganhou poder;  o que você vai pensar em primeiro lugar - sobre leis que você aceita ou sobre leis que você revoga? ”  Não sou 100% libertário, porque nem toda a minha lista de desejos está expressa na abolição das leis e no relaxamento das restrições.  Mas imagino como tentaria criar um mundo no qual uma pessoa desempregada pudesse oferecer uma carona para o trabalho, receber US $ 5 por 20 minutos de carro e nada de ruim aconteceria com ele por causa disso.  Ele não precisaria perder o seguro-desemprego, registrar uma permissão comercial, perder o seguro médico, passar por uma auditoria, pedir a um advogado que certificasse que seu trabalho está em conformidade com as regras da Administração de Proteção do Trabalho, etc.  Ele só teria acrescentado US $ 5. <br><br>  Eu tentaria retornar a um estado em que a contratação de um funcionário seria tão simples quanto no ano de 1900.  Talvez agora exista algum sentido em certas medidas de segurança, mas eu tentaria criar uma segurança que não restrinja uma pessoa e não produza papéis como resultado de um simples retorno de uma pessoa à economia. <br><br>  Eu tentaria fazer tudo o que economistas inteligentes têm gritado há muito tempo, e que nenhum Estado faz.  Substitua os impostos sobre investimentos e renda pelos impostos sobre consumo e imóveis.  Substitua o salário mínimo por impostos negativos sobre a folha de pagamento.  Estabeleça uma política para atingir o PIB nominal dos bancos centrais e interrompa as estruturas de apoio "grandes demais para falir".  Exigir que, durante o litígio sobre patentes, a parte vencida pague honorários legais [ <i>após a chamada</i>  <i>A regra inglesa - em contraste com as leis americanas, segundo as quais cada uma das partes adota seus próprios custos - aprox.</i>  <i>perev.</i>  ] e retorne a duração dos direitos autorais para 28 anos.  Remova os obstáculos para a construção de casas.  Copie o seguro de saúde de Cingapura.  Governo eletrônico na Estônia.  Substitua comitês e processos complexos de tomada de decisão por indivíduos específicos que tomam decisões publicamente documentadas e são responsáveis ​​por isso.  Realize experimentos controlados com várias opções para gerenciar países e leve em consideração seus resultados.  Eu posso ficar na lista por horas. <br><br>  Tudo isso pode não ter importância em duzentos milhões de anos.  Mas os ativos nominais resultantes do boom econômico podem fazer um bom trabalho enquanto tento descobrir o que faremos com a inteligência artificial.  O óbvio é o projeto de Manhattan em algum lugar da ilha, com pagamento baseado na competição entre os maiores fundos de hedge, nos quais as pessoas podem explorar o problema da inteligência artificial generalizada sem publicar os resultados de seu trabalho que automaticamente traz o fim do mundo.  E a menos que aceitemos que eu tenho habilidades mágicas ou um regime fundamentalmente irreversível, não vejo como qualquer lei que eu aceitaria atrasaria a abordagem da IA ​​bastante fortemente em um planeta onde os computadores são onipresentes. <br><br>  Mas tudo isso ainda pode ser considerado um experimento de pensamento impossível e, na vida real, a probabilidade de tal experimento é zero. <br><br>  <b>X:</b> O que há de tão bom no teorema de Bayes? <br><br>  <b>Yu:</b> Bem, por exemplo, ela é muito profunda.  Portanto, é difícil responder brevemente a essa pergunta. <br><br>  Eu poderia responder que o teorema de Bayes pode ser chamado de segunda lei da termodinâmica para a cognição.  Se você concluir que a probabilidade de alguma suposição é de 99%, seja a presença de leite no supermercado ou a causa antropogênica do aquecimento global, então você tem uma combinação de probabilidades a priori boas o suficiente e evidências razoavelmente confiáveis.  Este não é um requisito regulatório, é uma lei.  Assim como um carro não pode dirigir sem dissipar a entropia, você não pode obter uma boa imagem do mundo sem executar um processo no qual uma estrutura bayesiana exista em algum lugar interno, mesmo que as probabilidades não sejam usadas diretamente no processo. <br><br>  Pessoalmente, acho que o principal que Bayes pode nos oferecer é a existência de regras, leis de ferro que determinam se o modo de pensar funciona para marcar a realidade.  Dizem aos mórmons que reconhecem a verdade do Livro de Mórmon através de uma sensação de queimação no coração.  Conservadoramente aceite a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">probabilidade a priori do</a> Livro de Mórmon como uma em um bilhão.  Depois, avaliamos a probabilidade de o Livro de Mórmon não ser verdadeiro, e alguém experimentou uma sensação de queimação no coração depois que lhe disseram que isso era esperado.  Se você entender a fórmula de Bayes, perceberemos imediatamente que a baixa probabilidade da prova é incomensurável com a baixa probabilidade da hipótese que eles estão tentando provar com sua ajuda.  Você nem precisa criar números específicos para entender que eles não convergem - como Philip Tetlock descobriu em seu estudo de " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">super</a> preditores", eles geralmente conheciam a fórmula de Bayes, mas raramente davam números específicos.  De certa forma, é mais difícil enganá-lo se você entender que existe algum tipo de matemática com a qual você pode determinar com precisão a força da prova e entender se é suficiente para superar a baixa probabilidade da hipótese.  Você não pode apenas inventar algo e acreditar nele, porque não funciona assim. <br><br>  <b>X:</b> A hipótese do cérebro bayesiano o impressiona? <br><br>  <b>Yu:</b> Eu acho que algumas pessoas que discutem sobre esse tópico falam sobre coisas diferentes.  Perguntar se o cérebro é um algoritmo bayesiano é como perguntar se o Honda Accord é <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">alimentado por um motor térmico de Carnot</a> .  Se uma pessoa disser: “Cada carro é um processo termodinâmico que requer combustível e dissipa o calor disperso”, e outra pessoa ouve: “Se você criar um diagrama de ciclo de Carnot e mostrar sua mecânica, ele deve concordar que se parece com o interior de um Honda Accord ", Então um debate acalorado é inevitável. <br><br>  Algumas pessoas ficam muito felizes quando abrem o motor de combustão interna, encontram os cilindros e dizem: "Tenho certeza de que convertem calor em pressão e ajudam a mover o carro para a frente!"  E eles estarão certos, mas outras pessoas dirão: “Você está se concentrando no único componente de um conjunto muito maior de peças de carros.  O conversor catalítico também é muito importante e não está nos diagramas de ciclo Carnot.  E às vezes um ar condicionado funciona para nós, trabalhando exatamente o oposto de como o motor térmico funciona de acordo com suas palavras. ” <br><br>  Eu não acho que seria surpreendente se eu disser que as pessoas que falam em você: "Você claramente não está familiarizado com carros modernos;  você precisa de todo um conjunto de métodos diferentes para construir um motor, como velas e conversores catalíticos, e não apenas seus processos termodinâmicos ”, eles perdem um nível essencial de abstração. <br><br>  Mas se você quiser saber se o cérebro pode ser considerado literalmente bayesiano, e não um dispositivo que realiza trabalho cognitivo, cuja natureza podemos entender usando métodos bayesianos, então posso responder à sua pergunta: "Não, é claro".  Pode haver vários "cilindros" bayesianos neste "motor", mas muito parecerão tão estranhos quanto cintos de segurança e ar condicionado.  Mas essas adições não mudarão o fato de que, para identificar corretamente uma maçã com base em evidências sensoriais, é necessário fazer algo que possa ser interpretado como resultado de indução que possa entender o conceito de maçã e que seja atualizado com base em evidências que distingam maçãs de não maçãs. <br><br>  <b>X:</b> É possível ser muito racional? <br><br>  <b>Yu:</b> Você pode entrar no chamado  "O vale da má racionalidade."  Se antes isso você era irracional em várias coisas, equilibrando-se, então, se você se tornar racional, poderá se tornar pior do que antes.  Quanto mais você se tornar racional, pior será se escolher a direção errada para aplicar suas habilidades. <br><br>  Mas eu não recomendaria cuidar muito dessa oportunidade.  Na minha opinião, as pessoas que falam sobre o quão inteligentemente irracional podem ser são idiotas.  É difícil criar uma situação de vida realista, não exagerada, na qual você possa decidir ser irracional, e cujo resultado ainda é desconhecido para você.  Na vida real, é melhor dizer a verdade a si mesmo e não ser inteligente. <br><br>  É possível que o representante ideal do pensamento bayesiano seja incompatível com uma vida interessante e divertida.  Mas esse claramente não é um problema tão grande quanto a nossa tendência à autodestruição. <br><br>  <b>X:</b> Como o seu ponto de vista sobre a singularidade difere do de Kurzweil? <br><br>  <b>Yu:</b> <br>  • Não acho que a lei de Moore possa ser aplicada à IA.  AI é um problema de software. <br>  • Não acho que o primeiro intelecto sobre-humano apareça da fusão de máquinas com as pessoas.  Cem anos se passaram desde o advento dos carros, e agora estamos tentando fazer um exoesqueleto para um cavalo, e um carro comum ainda é mais rápido. <br>  • Eu não acho que a primeira IA forte será baseada em algoritmos da neurobiologia, assim como os aviões não foram baseados em pássaros. <br>  • Não acho que a fusão de nanotecnologias, informações e biotecnologias seja possível, inevitável, bem definida ou necessária. <br>  • Penso que de 1930 a 1970 houve mais mudanças do que de 1970 a 2010. <br>  • Penso que nos países desenvolvidos a produtividade estagna. <br>  • Acho que a extrapolação da lei de Moore para o progresso tecnológico, supostamente prevendo tudo o que será mais inteligente do que os humanos após o advento da IA, é uma coisa muito estranha.  Uma inteligência artificial inteligente arruina todos os seus gráficos. <br>  • Alguns analistas, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Illka ​​Tuomi</a> , acreditam que a lei de Moore <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">violou</a> no início dos anos 2000.  Não tenho certeza se posso me opor. <br>  • O único limiar tecnológico que me interessa é onde a IA ganha a capacidade de melhorar.  Não temos um cronograma que atinja esse limite, e não está claro qual será (embora não deva exceder em muito o nível de uma pessoa, pois ela entende ciência da computação); portanto, sua ofensiva não pode ser prevista. <br>  • Não acho que o resultado desse progresso seja bom por padrão.  Eu acho que isso pode ser bom, mas precisará ser seriamente trabalhado, e os principais números não estão interessados ​​nisso.  Dizer às pessoas que estamos em uma trajetória natural para tempos grandes e maravilhosos será uma mentira. <br>  • Penso que “singularidade” se tornou uma palavra de mala com muitos significados e detalhes incompatíveis por dentro, então parei de usá-la. <br><br>  <b>X:</b> Você provavelmente se tornará um ciborgue ultra-inteligente? <br><br>  <b>Yu:</b> A lei da conjunção de probabilidades diz que P (A&amp;B) &lt;= P (A).  A probabilidade da ocorrência simultânea dos eventos A e B é menor que a probabilidade da ocorrência de um evento A. Em experimentos em que as pessoas acreditam que P (A&amp;B)&gt; P (A) para dois eventos A e B, um " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">erro de conjunção</a> " aparece - por exemplo, em 1982, especialistas do Congresso Internacional de Previsões atribuíram uma maior probabilidade ao evento "A Rússia invadir a Polônia e os laços diplomáticos com a URSS romperem" do que a probabilidade de um evento separado "rompimento dos laços diplomáticos com a URSS", nomeado por outro grupo.  Da mesma forma, outro grupo atribuiu uma maior probabilidade ao evento "Um terremoto na Califórnia leva a uma inundação que leva a milhares de vítimas" do que outro - a probabilidade do evento "Em algum lugar da América do Norte há uma inundação com milhares de vítimas".  Embora adicionar detalhes adicionais à história claramente a torne menos provável, torna-a mais crível.  Para mim, entender esse fato, é como uma " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ponte do burro</a> " para um futurismo sério - a diferença entre o fato de você pesar cuidadosamente cada suposição individual e descobrir se você pode apoiar esse esclarecimento independentemente de todos os outros e que simplesmente compõe um maravilhoso e uma história vibrante. <br><br>  É tudo o que digo no contexto da resposta à pergunta: “Por que você está adicionando um refinamento como um ciborgue a isso?  Não quero ser um ciborgue.  É necessário tecer cuidadosamente detalhes adicionais para as declarações. <br><br>  <b>X:</b> Você tem chance de imortalidade? <br><br>  <b>Yu:</b> Literalmente?  A imortalidade literal é difícil de alcançar.  Para viver muito mais do que alguns trilhões de anos, é preciso reconsiderar o destino esperado de um universo em expansão.  Para viver mais do que os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">googolpleks</a> , é necessário cometer um erro sobre os fundamentos das leis físicas, e não apenas em detalhes. <br><br>  Mesmo que parte do raciocínio incomum se mostre verdadeiro e nosso Universo possa gerar universos filhas, isso não nos dará imortalidade.  Para viver muito mais anos no Googleplex e não se repetir, você precisará de computadores com mais elementos que o Google, e essa máquina não se encaixará na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">esfera</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Hubble</a> . <br><br>  E googolpleks não é infinito.  Parafraseando Martin Gardner, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o número de Graham</a> ainda é bastante pequeno, já que a maioria dos números finais é muito maior que ele.  Se você quer se surpreender, leia sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">hierarquias de rápido crescimento</a> e o infinito ainda será mais longo.  Somente teorias antrópicas muito estranhas e assustadoras permitirão que você viva o suficiente para assistir a uma parada na máquina Turing mais antiga, com centenas de estados. <br><br>  No entanto, não acho que, do ponto de vista emocional, eu gostaria de viver o suficiente para ver o centésimo número no jogo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">caçar um trabalhador castor</a> ".  De alguma forma, posso simpatizar comigo mesmo, que vive daqui a cem anos.  Nesse futuro, poderei simpatizar com o futuro daqui a cem anos.  E talvez em algum lugar dessa sequência haja alguém que enfrenta a perspectiva de acabar com a existência deles, e ele pode ficar muito triste com isso.  Mas não tenho certeza se consigo imaginar essa pessoa.  Quero viver outro dia.  Amanhã também vou querer viver outro dia.  Portanto, quero viver para sempre, comprovado pela indução de números inteiros positivos ".  Até meu desejo de uma vida longa em um universo fisicamente possível é uma abstração gerada pela indução.  Não consigo me imaginar em um trilhão de anos. <br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Descrevi a singularidade como uma fantasia escapista e pseudocientífica que nos distrai das mudanças climáticas, guerras, desigualdades e outros problemas sérios. Por que eu estou errado? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Porque você está tentando prever fatos empíricos através da psicanálise. Isso nunca vai funcionar. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suponha que vivamos para ver o advento da IA ​​que é inteligente o suficiente para fazer o mesmo trabalho de melhorar a IA que as pessoas. Ele pode se ajustar, programar, inventar novos algoritmos. Para melhorar. O que acontecerá a seguir - ficará mais inteligente, verá ainda mais oportunidades de aprimoramento e alcançará rapidamente um nível muito alto? Ou nada de especial vai acontecer?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pode acontecer que (A) o aprimoramento automático em um determinado delta torne a IA inteligente o suficiente para que possa olhar para trás e encontrar uma nova melhoria potencial no tamanho de k * delta, onde k&gt; 1, e isso será repetido várias vezes para levar ao rápido aprimoramento automático para nível de superinteligência. O que </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> chamou de "explosão de inteligência". Ou (B), k é menor que a unidade ou todas essas melhorias são pequenas e não levam à aparência de superinteligência, ou a superinteligência é geralmente impossível e, em vez de uma explosão, haverá um zilch. O que é verdade, A ou B? Se você criar uma IA de um certo nível e ele tentar fazê-lo, algo acontecerá no mundo real empírico, e esse evento será determinado por fatos relacionados ao cenário de algoritmos e melhorias possíveis.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Informações confiáveis ​​sobre esse evento não podem ser obtidas na psicanálise de pessoas. É como tentar ligar um carro sem combustível - é o que o teorema de Bayes nos diz. Algumas pessoas sempre serão escapistas, independentemente dos valores reais de variáveis ​​ocultas na ciência da computação, portanto, observar alguns escapistas não pode ser chamado de prova rigorosa. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este é um equívoco sobre a natureza da racionalidade - que é racional acreditar que "duendes em armários não existem" porque a fé em duendes de um armário é estúpida, imatura, desatualizada, e apenas idiotas acreditam nela. O verdadeiro princípio da racionalidade é ir verificar o armário. Portanto, naqueles universos em que os duendes vivem em armários, você acreditará em duendes e em universos em que os duendes não estão em armários, você não acreditará neles.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">É difícil, mas, em princípio, é possível tentar olhar pela porta entreaberta e perguntar: "O que seria diferente no Universo se não fosse possível obter uma boa renda com investimentos cognitivos, ou seja, uma IA tentando melhorar a si mesma terminaria não com uma explosão, mas com um zilch? Que outros fatos seriam característicos de um universo assim? ” </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Há pessoas que afirmam que a IA só pode ser aumentada para o nível de uma pessoa, já que nós próprios somos pessoas e não podemos aumentá-la mais. Parece-me que, se o nosso universo é assim, devemos observar uma diminuição na renda dos investimentos em hardware e software para xadrez de computador que excede o nível de uma pessoa - o que realmente não acontece. Além disso, a seleção natural não seria capaz de criar uma pessoa naquela época, e a mãe de Einstein deveria ter sido uma incrível física, etc.</font></font> etc. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Existem pessoas que argumentam que quanto mais complexo o algoritmo, mais ajustes ele precisa e que nossa inteligência serve como uma espécie de limitação para esse processo. Mas isso não concorda com os registros antropológicos da inteligência humana; os investimentos em ajustes e mutações do cérebro fornecem capacidades cognitivas aprimoradas. Nós sabemos, porque a genética nos diz que mutações com uma pequena resposta estatística não são corrigidas durante a evolução. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">E os hominídeos não precisavam de um cérebro exponencialmente maior que os chimpanzés. E a cabeça de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">John von Neumann</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> não </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">era</font></a><font style="vertical-align: inherit;"> exponencialmente maior que a cabeça da pessoa comum.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De um ponto de vista puramente prático, os axônios humanos transmitem informações a uma velocidade de um milhão de vezes menor que a velocidade da luz e, mesmo do ponto de vista da dissipação de calor, cada operação sináptica consome um milhão de vezes mais que a dissipação térmica mínima de uma operação binária irreversível a 300 Kelvin e assim por diante. Por que devemos assumir que o software cerebral está mais próximo do ideal do que o ferro? O privilégio da inteligência humana é que é o menor nível de inteligência capaz de criar um computador. Se fosse possível criar um computador com um nível mais baixo de inteligência, discutiríamos isso em um nível mais baixo.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas esse não é um argumento simples e, para uma descrição detalhada, envio pessoas para um de meus trabalhos antigos, "A Microeconomia da Explosão de Inteligência", que, infelizmente, ainda serve como a melhor fonte de informação. Mas são precisamente essas perguntas que precisam ser feitas para usar as evidências disponíveis para discutir se veremos uma explosão de IA na qual uma certa melhoria nas habilidades cognitivas investidas na auto-otimização aumentará em excesso essa melhoria. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quanto às oportunidades e seus preços: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">você pode imaginar um mundo sem uma explosão de inteligência e sem superinteligência. Ou um mundo onde os truques que os especialistas em aprendizado de máquina usarão para controlar a super-IA são adequados para controlar as pessoas e o regime sobre-humano. Ou um mundo onde </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">o internalismo moral</font></a><font style="vertical-align: inherit;"> funciona</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, então todas as IAs bastante avançadas são boas. Nesses mundos, ninguém precisa de todo o trabalho e todas as preocupações do Instituto de Pesquisa de Aprendizado de Máquina. E várias redes mosquiteiras foram desperdiçadas, e era melhor entregá-las ao fundo para combater a malária. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Você também pode imaginar um mundo em que lute contra a malária, lute e mantenha as emissões de carbono no nível adequado ou use soluções de geoengenharia para neutralizar os erros já cometidos. E tudo isso acaba sendo inútil, já que a civilização é incapaz de resolver o problema da moralidade da IA ​​- e todas as crianças salvas da malária com a ajuda de redes crescem apenas para que as nanomáquinas as matem em um sonho.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acho que as pessoas que estão tentando se envolver em uma caridade razoável concordam que não gostaríamos de viver em nenhum desses mundos. A única questão é qual é mais provável. O princípio central da racionalidade não é rejeitar a fé nos duendes, porque é estúpido e não prestigioso, e não acreditar nos duendes, porque é saudável e bonito. O princípio central da racionalidade é que sinais observáveis ​​e conclusões lógicas nos ajudarão a escolher um desses dois mundos.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eu acho que o primeiro mundo é improvável e o segundo é provável. Entendo que tentar convencer os outros disso é nadar contra o fluxo da fé na eterna normalidade. Crenças de que apenas nossa civilização de curto prazo, que existe há várias décadas, e apenas nossa espécie, que existe apenas um momento nas escalas evolucionárias e geológicas, fazem sentido e devem existir para sempre. E embora eu acredite que o primeiro mundo seja apenas um sonho otimista, não acho que precisamos ignorar o problema, do qual entraremos em pânico no futuro. A missão do Instituto é realizar hoje pesquisas que, segundo pessoas que vivem depois de 30 anos, deveriam ter começado há 30 anos. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sua esposa Brijena acredita na singularidade?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Brijena: Se alguém me perguntasse se eu acredito em uma singularidade, levantaria uma sobrancelha e perguntaria se eles acreditam em caminhões automáticos. Esta é uma pergunta estranha. Não sei qual será a primeira frota de caminhões automáticos ou quanto tempo levará para substituir o sistema de transporte de carga existente. E não acredito em caminhões robóticos, prevejo com segurança que o transporte não tripulado substituirá o transporte moderno pela participação de pessoas, porque estamos indo nessa direção se nada realmente estranho acontecer. Pela mesma razão, prevejo com confiança uma explosão de inteligência. Em outros significados da palavra "singularidade", não tenho tanta certeza. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Brijena deu a resposta sem ver minhas respostas. É que estamos bem adaptados um ao outro. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">É possível criar superinteligência sem entender como o cérebro funciona? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> No mesmo sentido que você pode fazer aviões sem entender como um pássaro voa. Você não precisa ser especialista em pássaros, mas, ao mesmo tempo, precisa de muito conhecimento para construir um avião, tendo obtido o que, em princípio, você já pode entender como aproximadamente um pássaro voa ou repele do ar. Portanto, estou escrevendo sobre a racionalidade humana - se você for suficientemente longe na questão da inteligência de máquinas, não poderá deixar de pensar em algumas idéias sobre como as pessoas pensam. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> O que a superinteligência pode querer? Eles terão algo como desejo sexual? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pense em um vasto espaço de possibilidades, em uma esfera multidimensional gigante. Este é um espaço de tipos de mente, um conjunto de todos os algoritmos cognitivos possíveis. Imagine que em algum lugar no fundo da esfera exista um pequeno ponto indicando todas as pessoas que já viveram. Esse é um ponto minúsculo, pois todas as pessoas têm aproximadamente o mesmo cérebro, com córtex, cerebelo, tálamo, etc. Algumas pessoas não são como as outras, por isso pode ser um ponto pontudo, mas os pontos terão a mesma escala do ponto em si. Independentemente da sua neuroatipicidade, você não trabalhará em outro algoritmo cortical.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perguntar "o que a superinteligência quer" é a pergunta errada. Superinteligências não são uma tribo estranha de pessoas que vivem do outro lado do rio e têm costumes exóticos. AI é simplesmente o nome de todo o espaço de possibilidades fora de um pequeno ponto humano. Com conhecimento suficiente, você pode entrar nesse espaço de oportunidades e sair de lá uma IA que tem desejos que podem ser descritos na linguagem humana por Wishlist, mas não porque será a Wishlist natural desses super-humanos exóticos, mas porque você isolou uma parte do espaço dos tipos mentais .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Com relação aos desejos sexuais - se você sabe exatamente o que está fazendo, resolveu o principal problema de criar IA, querendo, de forma estável, certas coisas enquanto melhora, se resolveu o principal problema de direcionar funções utilitárias da IA ​​para tarefas que parecem enganosamente simples para uma pessoa, e um problema ainda mais complicado é a construção da IA ​​usando um certo tipo de arquitetura, em que coisas como "desejos sexuais" e "felicidade do sexo" são importantes, então, talvez, você possa fazer a IA olhar para as pessoas que estão modelando sejam seus desejos, extraia essa parte deles em relação ao desejo sexual e faça com que a IA o experimente. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obviamente, você também pode, com bons conhecimentos de biologia orgânica e aerodinâmica, construir aeronaves que possam acasalar com pássaros.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas não acho que os irmãos Wright devessem ter feito essas coisas no início de suas atividades. Isso não faria sentido. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parece mais razoável resolver o problema de penetrar no espaço das mentes e extrair daí uma IA que não deseja nos desmontar em átomos livres. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Quero pensar que criaturas extremamente inteligentes professarão não-violência, porque entenderão que a violência é estúpida. Eu sou ingênuo </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Eu acho que sim. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">David hume</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eu lhe diria que você está cometendo um erro típico aplicando o predicado de “estupidez” aos valores ou operações oficiais de um indivíduo. Ações, escolhas, regras podem ser estúpidas se você tiver alguma preferência sobre o estado final do mundo. Se você é uma pessoa com meta-preferências que ainda não calculou totalmente, pode ter uma plataforma na qual pode confiar e chamar de estúpida algumas especulações sobre as preferências de objetos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Maximizador de grampos [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">experimento que demonstrou como a IA feita sem intenção maliciosa pode prejudicar a humanidade - aprox.</font></font></i>  <i>perev.</i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">] não comete um erro computacional, escolhendo aqueles casos em que o número máximo de grampos é obtido. Não está na sua plataforma de preferências, escolhendo ações erradas, e não está na sua plataforma de meta-preferências, escolhendo erroneamente as preferências. Ele calcula a resposta para outra pergunta, e não para a que você se pergunta, a pergunta "O que devo fazer?" O maximizador de grampos simplesmente executa a ação que leva ao maior número de grampos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um cenário fatal é quando a IA não te ama nem odeia, porque você é feito de átomos que ela pode usar para criar outra coisa. Teoria dos jogos e problemas de cooperação no </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dilema do prisioneiro</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">não se manifestam em todos os casos possíveis. Por exemplo, eles não aparecem quando um determinado assunto é tão mais forte que você que ele pode transformá-lo em átomos quando você deseja clicar nos botões "cooperar" ou "alterar". E quando ultrapassamos esse limite, você resolveu o problema de criar algo que não deseja prejudicá-lo ou já perdeu. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X: A</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> superinteligência </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">resolverá o difícil problema da consciência</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sim, e olhando para trás, a resposta parecerá vergonhosamente simples para nós. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X: As</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> superinteligências terão livre arbítrio? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sim, mas eles não terão a ilusão de livre arbítrio. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Como é a sua utopia? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Vou direcionar seus leitores para os meus "</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sequências da teoria do entretenimento</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">,</font></a><font style="vertical-align: inherit;"> pois ainda não consegui escrever uma história cuja ação ocorra em um mundo ideal teórico de entretenimento.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt404137/">https://habr.com/ru/post/pt404137/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt404125/index.html">A contagem regressiva acabou: restam 7 dias para o Polybius ICO</a></li>
<li><a href="../pt404127/index.html">O primeiro lançamento do Electron LV foi parcialmente bem-sucedido</a></li>
<li><a href="../pt404129/index.html">Feliz Dia dos Geeks (sim, ele é hoje)</a></li>
<li><a href="../pt404133/index.html">Até o final deste ano, o Google planeja mostrar em operação um computador quântico de 49 qubit</a></li>
<li><a href="../pt404135/index.html">O Google coleta e analisa compras offline de usuários do Android Pay</a></li>
<li><a href="../pt404139/index.html">Bitcoin na Rússia: imposto (algumas perguntas simples)</a></li>
<li><a href="../pt404141/index.html">Concorrência desleal no fornecedor</a></li>
<li><a href="../pt404143/index.html">Nuggets minúsculos: uma revisão dos registradores russos TrendVision Split e Tube</a></li>
<li><a href="../pt404147/index.html">Som, você é apenas "espaço": fones de ouvido Campfire Audio Andromeda</a></li>
<li><a href="../pt404149/index.html">Revisão do leitor impermeável de nova geração PocketBook 641 Aqua 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>