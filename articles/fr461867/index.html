<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úä ü§ô üë¶üèø Comment reconna√Ætre des images et des textes sur votre t√©l√©phone √† l'aide de ML Kit ‚úãüèª üéØ üåà</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Il y a deux ans, Sundar Pichai, le chef de Google, a d√©clar√© que la soci√©t√© du mobile-first devient d'abord l'IA et se concentre sur l'apprentissage a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment reconna√Ætre des images et des textes sur votre t√©l√©phone √† l'aide de ML Kit</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yamoney/blog/461867/"><p><img src="https://habrastorage.org/webt/6u/ek/co/6uekco-kgxxahafq0864aq6rmsw.png"></p><br><p>  Il y a deux ans, Sundar Pichai, le chef de Google, a d√©clar√© que la soci√©t√© du mobile-first devient d'abord l'IA et se concentre sur l'apprentissage automatique.  Un an plus tard, le Kit d'apprentissage machine est sorti - un ensemble d'outils avec lesquels vous pouvez utiliser efficacement ML sur iOS et Android. </p><br><p>  Il y a beaucoup de discussions sur le ML Kit aux √âtats-Unis, mais il n'y a presque aucune information en russe.  Et puisque nous l'utilisons pour certaines t√¢ches dans Yandex.Money, j'ai d√©cid√© de partager mon exp√©rience et de montrer avec des exemples comment l'utiliser pour faire des choses int√©ressantes. </p><br><p>  Je m'appelle Yura et j'ai travaill√© l'ann√©e derni√®re dans l'√©quipe Yandex.Money sur un portefeuille mobile.  Nous parlerons de l'apprentissage automatique sur mobile. </p><a name="habracut"></a><br><hr><br><p>  Remarque  R√©daction: ce billet est une nouvelle du rapport de Yuri Chechetkin ¬´Du mobile d'abord √† l'IA d'abord¬ª du m√©taphone Yandex.Money <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Android Paranoid</a> . </p><br><h2 id="chto-takoe-ml-kit">  Qu'est-ce que le kit ML? </h2><br><p>  Il s'agit du SDK mobile de Google qui facilite l'apprentissage automatique sur les appareils Android et iOS.  Il n'est pas n√©cessaire d'√™tre un expert en ML ou en intelligence artificielle, car en quelques lignes de code vous pouvez impl√©menter des choses tr√®s complexes.  De plus, il n'est pas n√©cessaire de savoir comment fonctionnent les r√©seaux de neurones ou l'optimisation des mod√®les. </p><br><h2 id="chto-zhe-mozhet-ml-kit">  Que peut faire le kit ML? </h2><br><p>  Les fonctionnalit√©s de base sont assez larges.  Par exemple, vous pouvez reconna√Ætre du texte, des visages, rechercher et suivre des objets, cr√©er des √©tiquettes pour les images et vos propres mod√®les de classification, scanner des codes-barres et des balises QR. </p><br><p>  Nous avons d√©j√† utilis√© la reconnaissance de code QR dans l'application Yandex.Money. </p><br><p>  Il y a aussi un kit ML </p><br><ol><li>  Reconnaissance historique; </li><li>  D√©finition de la langue dans laquelle le texte est √©crit; </li><li>  Traduction de textes sur l'appareil; </li><li>  R√©ponse rapide √† une lettre ou un message. </li></ol><br><p>  En plus d'un grand nombre de m√©thodes pr√™tes √† l'emploi, il existe un support pour les mod√®les personnalis√©s, ce qui donne pratiquement des possibilit√©s infinies - par exemple, vous pouvez coloriser des photographies en noir et blanc et les faire colorier. </p><br><p>  Il est important que vous n'ayez pas besoin d'utiliser de services, d'API ou de backend pour cela.  Tout peut √™tre fait directement sur l'appareil, donc nous ne chargeons pas le trafic des utilisateurs, nous n'obtenons pas un tas d'erreurs li√©es au r√©seau, nous n'avons pas √† traiter un tas de cas, par exemple, le manque d'Internet, la perte de connexion, etc.  De plus, sur l'appareil, il fonctionne beaucoup plus rapidement que via un r√©seau. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/024/fb0/78d/024fb078d1cd1b8f8b81f8be44122aad.png" alt="1"></p><br><h2 id="raspoznavanie-teksta">  Reconnaissance de texte </h2><br><p>  <strong>T√¢che: compte tenu d'une photographie, vous devez faire encercler le texte dans un rectangle.</strong> </p><br><p>  Nous commen√ßons par la d√©pendance de Gradle.  Il suffit de connecter une d√©pendance, et nous sommes pr√™ts √† travailler. </p><br><pre><code class="kotlin hljs">dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation'com.google.firebase:firebase-ml-vision:20.0.0' }</span></span></code> </pre> <br><p>  Il convient de sp√©cifier des m√©tadonn√©es indiquant que le mod√®le sera t√©l√©charg√© sur l'appareil lors du t√©l√©chargement de l'application √† partir du Play Market.  Si vous ne le faites pas et acc√©dez √† l'API sans mod√®le, nous obtiendrons une erreur et le mod√®le devra √™tre t√©l√©charg√© en arri√®re-plan.  Si vous devez utiliser plusieurs mod√®les, il est conseill√© de les sp√©cifier s√©par√©s par des virgules.  Dans notre exemple, nous utilisons le mod√®le OCR, et le nom du reste peut √™tre trouv√© dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> . </p><br><pre> <code class="kotlin hljs">&lt;application ...&gt; ... &lt;meta-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> android:name=<span class="hljs-string"><span class="hljs-string">"com.google.firebase.ml.vision.DEPENDENCIES"</span></span> android:value=<span class="hljs-string"><span class="hljs-string">"ocr"</span></span> /&gt; &lt;!-- To use multiple models: android:value=<span class="hljs-string"><span class="hljs-string">"ocr,model2,model3"</span></span> --&gt; &lt;/application&gt;</code> </pre> <br><p>  Apr√®s la configuration du projet, vous devez d√©finir les valeurs d'entr√©e.  ML Kit fonctionne avec le type FirebaseVisionImage, nous avons cinq m√©thodes, dont j'ai √©crit la signature ci-dessous.  Ils convertissent les types habituels d'Android et de Java en types de ML Kit, avec lesquels il est pratique de travailler. </p><br><pre> <code class="kotlin hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromMediaImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Image</span></span></span></span><span class="hljs-function"><span class="hljs-params">, rotation: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromBitmap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(bitmap: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Bitmap</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromFilePath</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(context: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Context</span></span></span></span><span class="hljs-function"><span class="hljs-params">, uri: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Uri</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteBuffer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( byteBuffer: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteBuffer</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteArray</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( bytes: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteArray</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage</code> </pre> <br><p>  Faites attention aux deux derniers - ils fonctionnent avec un tableau d'octets et avec un tampon d'octets, et nous devons sp√©cifier des m√©tadonn√©es pour que ML Kit comprenne comment g√©rer tout cela.  Les m√©tadonn√©es, en fait, d√©crivent le format, dans ce cas, la largeur et la hauteur, le format par d√©faut, IMAGE_FORMAT_NV21 et la rotation. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> metadata = FirebaseVisionImageMetadata.Builder() .setWidth(<span class="hljs-number"><span class="hljs-number">480</span></span>) .setHeight(<span class="hljs-number"><span class="hljs-number">360</span></span>) .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21) .setRotation(rotation) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> image = FirebaseVisionImage.fromByteBuffer(buffer, metadata)</code> </pre> <br><p>  Lorsque les donn√©es d'entr√©e sont collect√©es, cr√©ez un d√©tecteur qui reconna√Ætra le texte. </p><br><p>  Il existe deux types de d√©tecteurs, sur l'appareil et dans le cloud, ils sont cr√©√©s litt√©ralement sur une seule ligne.  Il convient de noter que le d√©tecteur de l'appareil ne fonctionne qu'avec l'anglais.  Le d√©tecteur de nuages ‚Äã‚Äãprend en charge plus de 20 langues; elles doivent √™tre sp√©cifi√©es dans la m√©thode sp√©ciale setLanguageHints. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">//  onDevice val detector = FirebaseVision.getInstance().getOnDeviceTextRecognizer() // onCloud with options val options = FirebaseVisionCloudTextRecognizerOptions.Builder() .setLanguageHints(arrayOf("en", "ru")) .build() val detector = FirebaseVision.getInstance().getCloudTextRecognizer(options)</span></span></code> </pre> <br><p>  Le nombre de langues prises en charge est sup√©rieur √† 20, elles sont toutes sur le site officiel.  Dans notre exemple, uniquement l'anglais et le russe. </p><br><p>  Apr√®s avoir entr√© et un d√©tecteur, appelez simplement la m√©thode processImage sur ce d√©tecteur.  Nous obtenons le r√©sultat sous la forme d'une t√¢che, √† laquelle nous suspendons deux rappels - pour le succ√®s et pour l'erreur.  L'exception standard vient √† une erreur, et le type FirebaseVisionText vient au succ√®s de onSuccessListener. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> result: Task&lt;FirebaseVisionText&gt; = detector.processImage(image) .addOnSuccessListener { result: FirebaseVisionText -&gt; <span class="hljs-comment"><span class="hljs-comment">// Task completed successfully // ... } .addOnFailureListener { exception: Exception -&gt; // Task failed with an exception // ... }</span></span></code> </pre> <br><h2 id="kak-rabotat-s-tipom-firebasevisiontext">  Comment travailler avec le type FirebaseVisionText? </h2><br><p>  Il se compose de blocs de texte (TextBlock), ceux-ci se composent √† leur tour de lignes (Line) et de lignes d'√©l√©ments (Element).  Ils sont imbriqu√©s les uns dans les autres. </p><br><p>  De plus, chacune de ces classes poss√®de cinq m√©thodes qui renvoient des donn√©es diff√©rentes sur l'objet.  Un rectangle est la zone o√π se trouve le texte, la confiance est la pr√©cision du texte reconnu, les points d'angle sont les points d'angle dans le sens des aiguilles d'une montre, √† partir du coin sup√©rieur gauche, les langues reconnues et le texte lui-m√™me. </p><br><pre> <code class="kotlin hljs">FirebaseVisionText contains a list of FirebaseVisionText.TextBlock which contains a list of FirebaseVisionText.Line which <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> composed of a list of FirebaseVisionText.Element. <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getBoundingBox</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>: Rect <span class="hljs-comment"><span class="hljs-comment">// axis-aligned bounding rectangle of the detected text fun getConfidence(): Float // confidence of the recognized text fun getCornerPoints(): Array&lt;Point&gt; // four corner points in clockwise direction fun getRecognizedLanguages(): List&lt;RecognizedLanguage&gt; // a list of recognized languages fun getText(): String //recognized text as a string</span></span></code> </pre> <br><h2 id="dlya-chego-eto-nuzhno">  √Ä quoi √ßa sert? </h2><br><p>  Nous pouvons reconna√Ætre √† la fois le texte entier dans l'image et ses paragraphes, morceaux, lignes ou simplement des mots.  Et √† titre d'exemple, nous pouvons parcourir, √† chaque √©tape, prendre un texte, prendre les bordures de ce texte et dessiner.  Tr√®s confortable. </p><br><p>  Nous pr√©voyons d'utiliser cet outil dans notre application de reconnaissance des cartes bancaires, dont les √©tiquettes sont situ√©es non standard.  Toutes les biblioth√®ques de reconnaissance de cartes ne fonctionnent pas bien, et pour les cartes personnalis√©es, le kit ML serait tr√®s utile.  Puisqu'il y a peu de texte, il est tr√®s facile de traiter de cette fa√ßon. </p><br><h2 id="raspoznavanie-obektov-na-foto">  Reconnaissance d'objets sur la photo </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/766/526/977/766526977ec9b1a83419e2aa6bdac37f.png" alt="2"></p><br><p>  En utilisant l'outil suivant comme exemple, je voudrais montrer que le principe de fonctionnement est √† peu pr√®s le m√™me.  Dans ce cas, reconnaissance de ce qui est repr√©sent√© sur l'objet.  Nous cr√©ons √©galement deux d√©tecteurs, l'un sur l'appareil, l'autre sur le cloud, nous pouvons sp√©cifier la pr√©cision minimale comme param√®tres.  La valeur par d√©faut est 0,5, indiqu√©e 0,7 et pr√™te √† l'emploi.  Nous obtenons √©galement le r√©sultat sous la forme de FirebaseImageLabel, c'est une liste d'√©tiquettes, chacune contenant un ID, une description et une pr√©cision. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">// onDevice val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getOnDeviceImageLabeler() // onCloud with minimum confidence val options = FirebaseVisionCloudImageLabelerOptions.Builder() .setConfidenceThreshold(0.7f) .build() val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getCloudImageLabeler(options)</span></span></code> </pre> <br><h2 id="garold-skryvayuschiy-schaste">  Harold cache le bonheur </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ec4/223/a1c/ec4223a1c8dcab56e489c6b0a4a4ca5b.jpg" alt="3"></p><br><p>  Vous pouvez essayer de comprendre √† quel point Harold cache la douleur et s'il est heureux en m√™me temps.  Nous utilisons un outil de reconnaissance faciale qui, en plus de reconna√Ætre les traits du visage, peut dire √† quel point une personne est heureuse.  Il s'est av√©r√© que Harold est heureux √† 93%.  Ou il cache tr√®s bien la douleur. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/237/352/dee/237352dee824e16c69ba131fe814fc8b.png" alt="4"></p><br><h2 id="ot-legkogo-k-legkomu-no-chut-bolee-slozhnomu-kastomnye-modeli">  De facile √† facile, mais un peu plus compliqu√©.  Mod√®les personnalis√©s. </h2><br><p>  <strong>T√¢che: classification de ce qui est repr√©sent√© sur la photo.</strong> </p><br><p>  J'ai pris une photo de l'ordinateur portable et reconnu le modem, l'ordinateur de bureau et le clavier.  Sonne comme la v√©rit√©.  Il y a mille classificateurs, et il en prend trois qui d√©crivent le mieux cette photo. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5fb/0d1/cf1/5fb0d1cf1b1ae43bdb0bd2151937229a.png" alt="7"></p><br><p>  Lorsque vous travaillez avec des mod√®les personnalis√©s, nous pouvons √©galement travailler avec eux √† la fois sur l'appareil et via le cloud. </p><br><p>  Si nous travaillons √† travers le cloud, vous devez aller dans la console Firebase, dans l'onglet ML Kit, et appuyer sur personnalis√©, o√π nous pouvons t√©l√©charger notre mod√®le sur TensorFlow Lite, car ML Kit fonctionne avec des mod√®les avec cette r√©solution.  Si nous l'utilisons sur un appareil, nous pouvons simplement placer le mod√®le dans n'importe quelle partie du projet comme un atout. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cd6/70e/aae/cd670eaae79abc02c358dc6583c9061e.png" alt="6"></p><br><p>  Nous soulignons la d√©pendance vis-√†-vis de l'interpr√©teur, qui peut fonctionner avec des mod√®les personnalis√©s, et n'oubliez pas la permission de travailler avec Internet. </p><br><pre> <code class="kotlin hljs">&lt;uses-permission android:name=<span class="hljs-string"><span class="hljs-string">"android.permission.INTERNET"</span></span> /&gt; dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation 'com.google.firebase:firebase-ml-model-interpreter:19.0.0' }</span></span></code> </pre> <br><p>  Pour les mod√®les qui se trouvent sur l'appareil, vous devez indiquer dans Gradle que le mod√®le ne doit pas √™tre compress√©, car il peut √™tre d√©form√©. </p><br><pre> <code class="kotlin hljs">android { <span class="hljs-comment"><span class="hljs-comment">// ... aaptOptions { noCompress "tflite" // Your model's file extension: "tflite" } }</span></span></code> </pre> <br><p>  Lorsque nous avons tout configur√© dans notre environnement, nous devons d√©finir des conditions sp√©ciales, qui incluent, par exemple, l'utilisation du Wi-Fi, n√©cessitent √©galement une charge et n√©cessitent que l'appareil soit inactif sont disponibles avec Android N - ces conditions indiquent que le t√©l√©phone est en charge ou en mode veille. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conditionsBuilder: FirebaseModelDownloadConditions.Builder = FirebaseModelDownloadConditions.Builder().requireWifi() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) { <span class="hljs-comment"><span class="hljs-comment">// Enable advanced conditions on Android Nougat and newer. conditionsBuilder = conditionsBuilder .requireCharging() .requireDeviceIdle() } val conditions: FirebaseModelDownloadConditions = conditionsBuilder.build()</span></span></code> </pre> <br><p>  Lorsque nous cr√©ons un mod√®le distant, nous d√©finissons les conditions d'initialisation et de mise √† jour, ainsi que l'indicateur indiquant si notre mod√®le doit √™tre mis √† jour.  Le nom du mod√®le doit correspondre √† celui que nous avons sp√©cifi√© dans la console Firebase.  Lorsque nous avons cr√©√© le mod√®le distant, nous devons l'enregistrer dans le gestionnaire de mod√®les Firebase. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cloudSource: FirebaseRemoteModel = FirebaseRemoteModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .enableModelUpdates(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setInitialDownloadConditions(conditions) .setUpdatesDownloadConditions(conditions) .build() FirebaseModelManager.getInstance().registerRemoteModel(cloudSource)</code> </pre> <br><p>  Nous faisons les m√™mes √©tapes pour le mod√®le local, sp√©cifions son nom, le chemin d'acc√®s au mod√®le et l'enregistrons dans le gestionnaire de mod√®les Firebase. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> localSource: FirebaseLocalModel = FirebaseLocalModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .setAssetFilePath(<span class="hljs-string"><span class="hljs-string">"my_model.tflite"</span></span>) .build() FirebaseModelManager.getInstance().registerLocalModel(localSource)</code> </pre> <br><p>  Apr√®s cela, vous devez cr√©er ces options o√π nous sp√©cifions les noms de nos mod√®les, installer le mod√®le distant, installer le mod√®le local et cr√©er un interpr√©teur avec ces options.  Nous pouvons sp√©cifier soit un mod√®le distant, soit seulement un mod√®le local, et l'interpr√®te comprendra lui-m√™me avec lequel travailler. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> options: FirebaseModelOptions = FirebaseModelOptions.Builder() .setRemoteModelName(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .setLocalModelName(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> interpreter = FirebaseModelInterpreter.getInstance(options)</code> </pre> <br><p>  ML Kit ne sait rien du format des donn√©es d'entr√©e et de sortie des mod√®les personnalis√©s, vous devez donc les sp√©cifier. </p><br><p>  Les donn√©es d'entr√©e sont un tableau multidimensionnel, o√π 1 est le nombre d'images, 224x224 est la r√©solution et 3 est une image RVB √† trois canaux.  Eh bien, le type de donn√©es est en octets. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> input = intArrayOf(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-comment"><span class="hljs-comment">//one 224x224 three-channel (RGB) image val output = intArrayOf(1, 1000) val inputOutputOptions = FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.BYTE, input) .setOutputFormat(0, FirebaseModelDataType.BYTE, output) .build()</span></span></code> </pre> <br><p>  Les valeurs de sortie sont de 1000 classificateurs.  Nous d√©finissons le format des valeurs d'entr√©e et de sortie en octets avec les tableaux multidimensionnels sp√©cifi√©s.  En plus des octets, float, long, int sont √©galement disponibles. </p><br><p>  Maintenant, nous d√©finissons les valeurs d'entr√©e.  Nous prenons Bitmap, le compressons √† 224 par 224, le convertissons en ByteBuffer et cr√©ons des valeurs d'entr√©e √† l'aide de FirebaseModelInput √† l'aide d'un g√©n√©rateur sp√©cial. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bitmap = Bitmap.createScaledBitmap(yourInputImage, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> imgData = convertBitmapToByteBuffer(bitmap) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> inputs: FirebaseModelInputs = FirebaseModelInputs.Builder() .add(imageData) .build()</code> </pre> <br><p>  Et maintenant, quand il y a un interpr√©teur, le format des valeurs d'entr√©e et de sortie et les valeurs d'entr√©e elles-m√™mes, nous pouvons ex√©cuter la demande en utilisant la m√©thode run.  Nous transf√©rons tout ce qui pr√©c√®de en tant que param√®tres et, par cons√©quent, nous obtenons FirebaseModelOutput, qui contient √† l'int√©rieur un g√©n√©rique du type que nous avons sp√©cifi√©.  Dans ce cas, il s'agissait d'un tableau d'octets, apr√®s quoi nous pouvons commencer le traitement.  C'est exactement le millier de classificateurs que nous avons demand√©, et nous affichons, par exemple, le top 3 le plus adapt√©. </p><br><pre> <code class="kotlin hljs">interpreter.run(inputs, inputOutputOptions) .addOnSuccessListener { result: FirebaseModelOutputs -&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> labelProbArray = result.getOutput&lt;Array&lt;ByteArray&gt;&gt;(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">//handle labelProbArray } .addOnFailureListener( object : OnFailureListener { override fun onFailure(e: Exception) { // Task failed with an exception } })</span></span></code> </pre> <br><h2 id="realizaciya-za-odin-den">  Mise en ≈ìuvre en un jour </h2><br><p>  Tout est assez facile √† impl√©menter, et la reconnaissance d'objets avec des outils int√©gr√©s peut √™tre r√©alis√©e en une seule journ√©e.  L'outil est disponible sur iOS et Android, en plus, vous pouvez utiliser le m√™me mod√®le TensorFlow pour les deux plateformes. </p><br><p>  En outre, il existe des tonnes de m√©thodes disponibles qui peuvent couvrir de nombreux cas.  La plupart des API sont disponibles sur l'appareil, c'est-√†-dire que la reconnaissance fonctionnera m√™me sans Internet. </p><br><p>  Et surtout - la prise en charge de mod√®les personnalis√©s qui peuvent √™tre utilis√©s comme vous le souhaitez pour n'importe quelle t√¢che. </p><br><h2 id="poleznye-ssylki">  Liens utiles </h2><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Documentation du kit ML</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Projet de d√©monstration du kit Github ML</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprentissage automatique pour mobile avec Firebase (Google I / O'19)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SDK Machine Learning pour les d√©veloppeurs mobiles (Google I / O'18)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Cr√©ation d'un scanner de carte de cr√©dit √† l'aide du kit Firebase ML (Medium.com)</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr461867/">https://habr.com/ru/post/fr461867/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr461851/index.html">Blockchain et √©lectricit√©</a></li>
<li><a href="../fr461855/index.html">Salaires en informatique au premier semestre 2019: selon le calculateur de salaire My Circle</a></li>
<li><a href="../fr461859/index.html">Vous ne savez rien de la technologie alimentaire</a></li>
<li><a href="../fr461861/index.html">Office 365 Cloud Security: Check Point CloudGuard SaaS Testing</a></li>
<li><a href="../fr461865/index.html">Cours vid√©o ¬´Introduction √† l'inversion √† partir de z√©ro avec IDA PRO. Chapitre 1</a></li>
<li><a href="../fr461871/index.html">101 conseils pour devenir un bon programmeur (et humain)</a></li>
<li><a href="../fr461873/index.html">ViewPager 2 - nouvelle fonctionnalit√© dans l'ancien wrapper</a></li>
<li><a href="../fr461875/index.html">5 nm vs 3 nm</a></li>
<li><a href="../fr461877/index.html">Java vs Kotlin pour Android: avis des d√©veloppeurs</a></li>
<li><a href="../fr461879/index.html">Le livre "Linux en action"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>