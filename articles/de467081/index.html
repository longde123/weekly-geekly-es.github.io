<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üê¥ ‚öúÔ∏è üö£üèΩ Analyse der emotionalen F√§rbung von Rezensionen von Kinopoisk üè° ‚ôäÔ∏è üë©‚Äçüëß‚Äçüëß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eintrag 
 Natural Language Processing (NLP) ist ein beliebter und wichtiger Bereich des maschinellen Lernens. In diesem Hub werde ich mein erstes Proj...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analyse der emotionalen F√§rbung von Rezensionen von Kinopoisk</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467081/"><h3>  Eintrag </h3><br>  Natural Language Processing (NLP) ist ein beliebter und wichtiger Bereich des maschinellen Lernens.  In diesem Hub werde ich mein erstes Projekt beschreiben, das sich mit der Analyse der emotionalen F√§rbung von in Python geschriebenen Filmkritiken befasst.  Die Aufgabe der sentimentalen Analyse ist unter denen, die die Grundkonzepte von NLP beherrschen wollen, weit verbreitet und kann in diesem Bereich zu einem Analogon der ‚ÄûHallo Welt‚Äú werden. <br><br>  In diesem Artikel werden wir alle Hauptphasen des Data Science-Prozesses durchlaufen: von der Erstellung Ihres eigenen Datensatzes √ºber die Verarbeitung und das Extrahieren von Funktionen mithilfe der NLTK-Bibliothek bis hin zum Lernen und Optimieren des Modells mithilfe von Scikit-Learn.  Die Aufgabe selbst besteht darin, Bewertungen in drei Klassen einzuteilen: negativ, neutral und positiv. <br><a name="habracut"></a><br><h3>  Datenkorpusbildung </h3><br>  Um dieses Problem zu l√∂sen, k√∂nnte man einen vorgefertigten und kommentierten Datenk√∂rper mit Bewertungen von IMDB verwenden, von denen es viele auf GitHub gibt.  Es wurde jedoch beschlossen, Ihre eigenen Bewertungen in russischer Sprache von Kinopoisk zu erstellen.  Um sie nicht manuell zu kopieren, schreiben wir einen Webparser.  Ich werde die <i>Anforderungsbibliothek</i> verwenden, um http- <i>Anforderungen</i> zu senden, und <i>BeautifulSoup</i> , um HTML-Dateien zu verarbeiten.  Definieren wir zun√§chst eine Funktion, die einen Link zu Filmkritiken enth√§lt und diese abruft.  Damit Kinopoisk den Bot in uns nicht erkennt, m√ºssen Sie das <i>Header-</i> Argument in der Funktion <i>request.get</i> angeben, das den Browser simuliert.  Es ist erforderlich, ein W√∂rterbuch mit den Schl√ºsseln User-Agent, Accept-language und Accept zu √ºbergeben, deren Werte in den Browser-Entwicklertools zu finden sind.  Als N√§chstes wird ein Parser erstellt und Bewertungen von der Seite abgerufen, die in der HTML-Markup-Klasse _reachbanner_ gespeichert sind. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url)</span></span></span><span class="hljs-function">:</span></span> r = requests.get(url, headers = headers) <span class="hljs-comment"><span class="hljs-comment">#  http  soup = BeautifulSoup(r.text, 'html.parser')#  html  reviews = soup.find_all(class_='_reachbanner_')#    reviews_clean = [] for review in reviews:#    html  reviews_clean.append(review.find_all(text=True)) return reviews_clean</span></span></code> </pre> <br>  Wir haben das HTML-Markup entfernt, unsere Bewertungen sind jedoch immer noch <i>BeautifulSoup-</i> Objekte, aber wir m√ºssen sie in Zeichenfolgen konvertieren.  Die <i>Konvertierungsfunktion</i> macht genau das.  Wir werden auch eine Funktion schreiben, die den Namen des Films abruft und sp√§ter zum Speichern von Rezensionen verwendet wird. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(reviews)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     review_converted = [] for review in reviews: for i in review: map(str, i) review = ''.join(review) review_converted.append(review) return review_converted def get_name(url): #    r = requests.get(url, headers = headers) soup = BeautifulSoup(r.text, 'html.parser') name = soup.find(class_='alternativeHeadline') name_clean = name.find_all(text = True) #   , . .     return str(name_clean[0])</span></span></code> </pre><br>  Die letzte Funktion des Parsers enth√§lt einen Link zur Hauptseite des Films, eine √úberpr√ºfungsklasse und eine M√∂glichkeit zum Speichern von √úberpr√ºfungen.  Die Funktion definiert auch <i>Verz√∂gerungen</i> zwischen Anforderungen, die erforderlich sind, um ein Verbot zu vermeiden.  Die Funktion enth√§lt eine Schleife, die Bewertungen ab der ersten Seite abruft und speichert, bis sie auf eine nicht vorhandene Seite st√∂√üt, von der die Funktion <i>load_data</i> eine leere Liste extrahiert und die Schleife <i>unterbrochen</i> wird. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parsing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url, status, path)</span></span></span><span class="hljs-function">:</span></span> page = <span class="hljs-number"><span class="hljs-number">1</span></span> delays = [<span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">13</span></span>, <span class="hljs-number"><span class="hljs-number">11.5</span></span>, <span class="hljs-number"><span class="hljs-number">12.5</span></span>, <span class="hljs-number"><span class="hljs-number">13.5</span></span>, <span class="hljs-number"><span class="hljs-number">11.2</span></span>, <span class="hljs-number"><span class="hljs-number">12.3</span></span>, <span class="hljs-number"><span class="hljs-number">11.8</span></span>] name = get_name(url) time.sleep(np.random.choice(delays)) <span class="hljs-comment"><span class="hljs-comment">#    while True: loaded_data = load_data(url + 'reviews/ord/date/status/{}/perpage/200/page/{}/'.format(status, page)) if loaded_data == []: break else: # E     ,    if not os.path.exists(path + r'\{}'.format(status)): os.makedirs(path + r'\{}'.format(status)) converted_data = convert(loaded_data) #   for i, review in enumerate(converted_data): with open(path + r'\{}\{}_{}_{}.txt'.format(status, name, page, i), 'w', encoding = 'utf-8') as output: output.write(review) page += 1 time.sleep(np.random.choice(delays))</span></span></code> </pre><br>  Anschlie√üend k√∂nnen Sie mithilfe des folgenden Zyklus Rezensionen aus Filmen extrahieren, die in der <i>Urles-</i> Liste enthalten sind.  Eine Liste der Filme muss manuell erstellt werden.  Es w√§re zum Beispiel m√∂glich, eine Liste von Links zu Filmen zu erhalten, indem eine Funktion geschrieben wird, die sie aus den 250 besten Filmen einer Filmsuche extrahiert, um dies nicht manuell zu tun. 15 bis 20 Filme w√ºrden jedoch ausreichen, um einen kleinen Datensatz mit tausend Rezensionen f√ºr jede Klasse zu bilden.  Wenn Sie ein Verbot erhalten, zeigt das Programm au√üerdem an, f√ºr welchen Film und welche Klasse der Parser angehalten hat, um nach dem Verbot an derselben Stelle fortzufahren. <br><br><pre> <code class="python hljs">path = <span class="hljs-comment"><span class="hljs-comment">#    urles = #    statuses = ['good', 'bad', 'neutral'] delays = [15, 20, 13, 18, 12.5, 13.5, 25, 12.3, 23] for url in urles: for status in statuses: try: parsing(url = url, status = status, path=path) print('one category done') time.sleep(np.random.choice(delays)) #       AttributeError except AttributeError: print(' : {}, {}'.format(url, status)) break #  else  ,      #    ,     else: print('one url done') continue break</span></span></code> </pre><br><h3>  Vorbehandlung </h3><br>  Nachdem ich den Parser geschrieben und zuf√§llige Filme f√ºr ihn und mehrere Verbote aus der Filmsuche zur√ºckgerufen hatte, mischte ich die Rezensionen in Ordnern und w√§hlte 900 Rezensionen aus jeder Klasse f√ºr das Training und den Rest f√ºr die Kontrollgruppe aus.  Jetzt ist es notwendig, das Geh√§use vorzuverarbeiten, n√§mlich es zu tokenisieren und zu normalisieren.  Tokenisieren bedeutet, den Text in Komponenten, in diesem Fall in W√∂rter, zu zerlegen, da wir die Darstellung einer Worttasche verwenden.  Und Normalisierung besteht darin, W√∂rter in Kleinbuchstaben umzuwandeln, Stoppw√∂rter und √ºberm√§√üiges Rauschen zu entfernen, zu stottern und andere Tricks anzuwenden, die dazu beitragen, den Platz f√ºr Zeichen zu verringern. <br><br>  Wir importieren die notwendigen Bibliotheken. <br><br><div class="spoiler">  <b class="spoiler_title">Versteckter Text</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.corpus <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PlaintextCorpusReader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SnowballStemmer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.probability <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FreqDist <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RegexpTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> bigrams <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pos_tag <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OrderedDict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> classification_report, accuracy_score <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MultinomialNB <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pool <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.sparse <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csr_matrix</code> </pre><br></div></div><br>  Wir definieren zun√§chst einige kleine Funktionen f√ºr die Textvorverarbeitung.  Der erste mit dem Namen <i>lower_pos_tag</i> nimmt eine Liste mit W√∂rtern, konvertiert sie in Kleinbuchstaben und speichert jedes Token in einem Tupel mit seinem Wortbestandteil.  Die Operation zum Hinzuf√ºgen eines Teils der Sprache zu einem Wort wird als POS-Tagging (Part of Speech) bezeichnet und wird in NLP h√§ufig zum Extrahieren von Entit√§ten verwendet.  In unserem Fall verwenden wir Wortarten in der folgenden Funktion, um W√∂rter zu filtern. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lower_pos_tag</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> lower_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: lower_words.append(i.lower()) pos_words = pos_tag(lower_words, lang=<span class="hljs-string"><span class="hljs-string">'rus'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> pos_words</code> </pre><br>  Die Texte enthalten eine gro√üe Anzahl von W√∂rtern, die zu oft gefunden werden, um f√ºr das Modell n√ºtzlich zu sein (die sogenannten Stoppw√∂rter).  Grunds√§tzlich handelt es sich dabei um Pr√§positionen, Konjunktionen und Pronomen, anhand derer nicht bestimmt werden kann, auf welche Klassenerinnerung sich bezieht.  Die Funktion <i>clean</i> hinterl√§sst nur Substantive, Adjektive, Verben und Adverbien.  Beachten Sie, dass Teile der Sprache entfernt werden, da sie f√ºr das Modell selbst nicht ben√∂tigt werden.  Sie k√∂nnen auch feststellen, dass diese Funktion Stamming verwendet, dessen Kern darin besteht, Suffixe und Pr√§fixe aus W√∂rtern zu entfernen.  Auf diese Weise k√∂nnen Sie die Dimension von Zeichen reduzieren, da W√∂rter mit unterschiedlichen Gattungen und Gro√ü- / Kleinschreibung auf dasselbe Token reduziert werden.  Es gibt ein leistungsf√§higeres Analogon zum Stottern - die Lemmatisierung, mit der Sie die urspr√ºngliche Form des Wortes wiederherstellen k√∂nnen.  Es funktioniert jedoch langsamer als das Stottern, und au√üerdem verf√ºgt NLTK nicht √ºber einen russischen Lemmatisator. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> stemmer = SnowballStemmer(<span class="hljs-string"><span class="hljs-string">"russian"</span></span>) cleaned_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'S'</span></span>, <span class="hljs-string"><span class="hljs-string">'A'</span></span>, <span class="hljs-string"><span class="hljs-string">'V'</span></span>, <span class="hljs-string"><span class="hljs-string">'ADV'</span></span>]: cleaned_words.append(stemmer.stem(i[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cleaned_words</code> </pre><br>  Als n√§chstes schreiben wir die letzte Funktion, die die Klassenbezeichnung √ºbernimmt und alle Bewertungen mit dieser Klasse abruft.  Um den Fall zu lesen, verwenden wir die <i>Raw-</i> Methode des <i>PlaintextCorpusReader-</i> Objekts, mit der Sie Text aus der angegebenen Datei extrahieren k√∂nnen.  Als n√§chstes wird die Tokenisierung RegexpTokenizer verwendet, die auf der Grundlage eines regul√§ren Ausdrucks arbeitet.  Zus√§tzlich zu einzelnen W√∂rtern habe ich dem Modell Bigrams hinzugef√ºgt, die Kombinationen aller benachbarten W√∂rter sind.  Diese Funktion verwendet auch das <i>FreqDist-</i> Objekt, das die H√§ufigkeit des Auftretens von W√∂rtern zur√ºckgibt.  Es wird hier verwendet, um W√∂rter zu entfernen, die in allen √úberpr√ºfungen einer bestimmten Klasse nur einmal vorkommen (sie werden auch als Hapaks bezeichnet).  Somit gibt die Funktion ein W√∂rterbuch zur√ºck, das Dokumente enth√§lt, die als eine Tasche von W√∂rtern und eine Liste aller W√∂rter f√ºr eine bestimmte Klasse dargestellt werden. <br><br><pre> <code class="python hljs">corpus_root = <span class="hljs-comment"><span class="hljs-comment">#    def process(label): # Wordmatrix -     # All words -    data = {'Word_matrix': [], 'All_words': []} #      templist_allwords = [] #        corpus = PlaintextCorpusReader(corpus_root + '\\' + label, '.*', encoding='utf-8') #       names = corpus.fileids() #   tokenizer = RegexpTokenizer(r'\w+|[^\w\s]+') for i in range(len(names)): #   bag_words = tokenizer.tokenize(corpus.raw(names[i])) lower_words = lower_pos_tag(bag_words) cleaned_words = clean(lower_words) finalist = list(bigrams(cleaned_words)) + cleaned_words data['Word_matrix'].append(final_words) templist_allwords.extend(cleaned_words) #   templistfreq = FreqDist(templist_allwords) hapaxes = templistfreq.hapaxes() #    for word in templist_allwords: if word not in hapaxes: data['All_words'].append(word) return {label: data}</span></span></code> </pre><br>  Die Vorverarbeitungsphase ist die l√§ngste, daher ist es sinnvoll, die Bearbeitung unseres Falls zu parallelisieren.  Dies kann mit dem <i>Multiprocessing-</i> Modul erfolgen.  Im n√§chsten Programmcode starte ich drei Prozesse, die gleichzeitig drei Ordner mit unterschiedlichen Klassen verarbeiten.  Als n√§chstes werden die Ergebnisse in einem W√∂rterbuch gesammelt.  Diese Vorverarbeitung ist abgeschlossen. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: data = {} labels = [<span class="hljs-string"><span class="hljs-string">'neutral'</span></span>, <span class="hljs-string"><span class="hljs-string">'bad'</span></span>, <span class="hljs-string"><span class="hljs-string">'good'</span></span>] p = Pool(<span class="hljs-number"><span class="hljs-number">3</span></span>) result = p.map(process, labels) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> result: data.update(i) p.close()</code> </pre><br><h3>  Vektorisierung </h3><br>  Nachdem wir den Fall vorverarbeitet haben, haben wir ein W√∂rterbuch, in dem f√ºr jedes Klassenlabel eine Liste mit Bewertungen enthalten ist, die wir mit Bigrams versehen, normalisiert und angereichert haben, sowie eine Liste mit W√∂rtern aus allen Bewertungen dieser Klasse.  Da das Modell die nat√ºrliche Sprache nicht so wahrnehmen kann wie wir, besteht die Aufgabe nun darin, unsere √úbersichten in numerischer Form darzustellen.  Zu diesem Zweck erstellen wir ein gemeinsames Vokabular, das aus eindeutigen Token besteht, und vektorisieren damit jede √úberpr√ºfung. <br><br>  Zun√§chst erstellen wir eine Liste, die Bewertungen aller Klassen zusammen mit ihren Bezeichnungen enth√§lt.  Als n√§chstes erstellen wir ein gemeinsames Vokabular, das aus jeder Klasse 10.000 der gebr√§uchlichsten W√∂rter mit der Methode <i>most_common</i> desselben <i>FreqDist entnimmt</i> .  Als Ergebnis erhielt ich einen Wortschatz, der aus ungef√§hr 17.000 W√∂rtern bestand. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     : # [([  ], _)] labels = ['neutral', 'bad', 'good'] labeled_data = [] for label in labels: for document in data[label]['Word_matrix']: labeled_data.append((document, label)) #      all_words = [] for label in labels: frequency = FreqDist(data[label]['All_words'] common_words = frequency.most_common(10000) words = [i[0] for i in common_words] all_words.extend(words) #    unique_words = list(OrderedDict.fromkeys(all_words))</span></span></code> </pre><br>  Es gibt verschiedene M√∂glichkeiten, Text zu vektorisieren.  Die beliebtesten von ihnen: TF-IDF, Direkt- und Frequenzcodierung.  Ich habe die Frequenzcodierung verwendet, deren Kern darin besteht, jede √úberpr√ºfung als Vektor darzustellen, deren Elemente die Anzahl der Vorkommen jedes Wortes aus dem Vokabular sind.  <i>NLTK</i> hat seine eigenen Klassifikatoren, Sie k√∂nnen sie verwenden, aber sie arbeiten langsamer als ihre Gegenst√ºcke aus <i>Scikit-Learn</i> und haben weniger Einstellungen.  Unten finden Sie den Code f√ºr die Codierung f√ºr <i>NLTK</i> .  Ich werde jedoch das Naive Bayes-Modell von <i>scikit-learn verwenden</i> und die √úberpr√ºfungen codieren, wobei die Attribute in einer sp√§rlichen Matrix von <i>SciPy</i> und die Klassenbezeichnungen in einem separaten <i>NumPy-</i> Array gespeichert werden. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     nltk  : # # [({ : -   },  )] prepared_data = [] for x in labeled_data: d = defaultdict(int) for word in unique_words: if word in x[0]: d[word] += 1 if word not in x[0]: d[word] = 0 prepared_data.append((d, x[1])) #     scikit-learn #     matrix_vec = csr_matrix((len(labeled_data), len(unique_words)), dtype=np.int8).toarray() #     target = np.zeros(len(labeled_data), 'str') for index_doc, document in enumerate(labeled_data): for index_word, word in enumerate(unique_words): #  -     matrix_vec[index_doc, index_word] = document[0].count(word) target[index_doc] = document[1] #   X, Y = shuffle(matrix_vec, target)</span></span></code> </pre><br>  Da im Datensatz die Bewertungen mit bestimmten Tags nacheinander ablaufen, dh zuerst alle neutral, dann alle negativ usw., m√ºssen Sie sie mischen.  Dazu k√∂nnen Sie die <i>Shuffle-</i> Funktion von <i>scikit-learn verwenden</i> .  Es ist nur f√ºr Situationen geeignet, in denen sich Zeichen und Klassenbezeichnungen in unterschiedlichen Arrays befinden, da Sie zwei Arrays gleichzeitig mischen k√∂nnen. <br><br><h3>  Modelltraining </h3><br>  Jetzt muss das Modell trainiert und seine Genauigkeit in der Kontrollgruppe √ºberpr√ºft werden.  Als Modell verwenden wir das Modell des Naive Bayes-Klassifikators.  <i>Scikit-learn</i> verf√ºgt je nach Datenverteilung √ºber drei Naive Bayes-Modelle: bin√§r, diskret und kontinuierlich.  Da die Verteilung unserer Funktionen diskret ist, w√§hlen wir <i>MultinomialNB</i> . <br><br>  Der Bayes'sche Klassifikator verf√ºgt √ºber den <i>Alpha-</i> Hyper- <i>Parameter</i> , der f√ºr die Gl√§ttung des Modells verantwortlich ist.  Naive Bayes berechnet die Wahrscheinlichkeiten jeder √úberpr√ºfung, die zu allen Klassen geh√∂rt, wobei die bedingten Wahrscheinlichkeiten f√ºr das Auftreten aller √úberpr√ºfungsw√∂rter multipliziert werden, sofern sie zu einer bestimmten Klasse geh√∂ren.  Wenn jedoch im Trainingsdatensatz kein √úberpr√ºfungswort gefunden wurde, ist seine bedingte Wahrscheinlichkeit gleich Null, wodurch die Wahrscheinlichkeit aufgehoben wird, dass die √úberpr√ºfung zu einer Klasse geh√∂rt.  Um dies zu vermeiden, wird standardm√§√üig allen bedingten Wortwahrscheinlichkeiten eine Einheit hinzugef√ºgt, d. H. <i>Alpha ist</i> gleich eins.  Dieser Wert ist jedoch m√∂glicherweise nicht optimal.  Sie k√∂nnen versuchen, <i>Alpha</i> mithilfe der Rastersuche und der Kreuzvalidierung auszuw√§hlen. <br><br><pre> <code class="python hljs">parameter = [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.0001</span></span>] param_grid = {<span class="hljs-string"><span class="hljs-string">'alpha'</span></span>: parameter} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>) grid_search.fit(X, Y) Alpha, best_score = grid_search.best_params_, grid_search.best_score_</code> </pre><br>  In meinem Fall gibt der Gitterherd den optimalen Wert des Hyperparameters gleich 0 mit einer Genauigkeit von 0,965 an.  Ein solcher Wert ist jedoch offensichtlich nicht optimal f√ºr den Kontrolldatensatz, da eine gro√üe Anzahl von W√∂rtern vorhanden ist, die zuvor nicht im Trainingssatz gefunden wurden.  F√ºr einen Referenzdatensatz hat dieses Modell eine Genauigkeit von 0,598.  Wenn Sie jedoch <i>Alpha</i> auf 0,1 erh√∂hen, sinkt die Genauigkeit der Trainingsdaten auf 0,82 und der Kontrolldaten auf 0,62.  Bei einem gr√∂√üeren Datensatz ist der Unterschied h√∂chstwahrscheinlich signifikanter. <br><br><pre> <code class="python hljs">model = MultinomialNB(<span class="hljs-number"><span class="hljs-number">0.1</span></span>) model.fit(X, Y) <span class="hljs-comment"><span class="hljs-comment"># X_control, Y_control   ,   X  Y #        predicted = model.predict(X_control) #     score_test = accuracy_score(Y_control, predicted) #   report = classification_report(Y_control, predicted)</span></span></code> </pre><br><br><h3>  Fazit </h3><br>  Es wird davon ausgegangen, dass das Modell verwendet werden sollte, um Bewertungen vorherzusagen, deren W√∂rter nicht zur Bildung eines Vokabulars verwendet wurden.  Daher kann die Qualit√§t des Modells anhand seiner Genauigkeit im Steuerteil der Daten bewertet werden, die 0,62 betr√§gt.  Dies ist fast doppelt so gut wie nur zu raten, aber die Genauigkeit ist immer noch ziemlich niedrig. <br><br>  Dem Klassifizierungsbericht zufolge ist klar, dass das Modell mit Bewertungen mit einer neutralen Farbe am schlechtesten abschneidet (Genauigkeit 0,47 gegen√ºber 0,68 f√ºr positiv und 0,76 f√ºr negativ).  In der Tat enthalten neutrale Bewertungen W√∂rter, die sowohl f√ºr positive als auch f√ºr negative Bewertungen charakteristisch sind.  Wahrscheinlich kann die Genauigkeit des Modells durch Erh√∂hen des Datensatzvolumens verbessert werden, da der dreitausendste Datensatz eher bescheiden ist.  Es w√§re auch m√∂glich, das Problem auf eine bin√§re Klassifizierung von Bewertungen in positive und negative zu reduzieren, was ebenfalls die Genauigkeit erh√∂hen w√ºrde. <br><br>  Danke f√ºrs Lesen. <br><br>  PS Wenn Sie sich selbst √ºben m√∂chten, kann mein Datensatz unter dem Link heruntergeladen werden. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link zum Datensatz</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de467081/">https://habr.com/ru/post/de467081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de467061/index.html">Babylonische Reihe: 5 Sicherheitsprobleme im Baugesch√§ft</a></li>
<li><a href="../de467063/index.html">Kraftstoff√ºberwachung f√ºr Dieselgeneratoren im Rechenzentrum - wie und warum ist das so wichtig?</a></li>
<li><a href="../de467065/index.html">Archiv der Olympiadenprobleme in der Physik f√ºr Sch√ºler</a></li>
<li><a href="../de467073/index.html">‚ÄûIm Westen gibt es keine Art Direktoren unter 40 Jahren. Bei uns kann es bis zu 30 sein. " Wie ist es, Designer in der IT zu sein?</a></li>
<li><a href="../de467079/index.html">CSS und Javascript Ant Carousel</a></li>
<li><a href="../de467083/index.html">Wie die seltsame Popcount-Anweisung in modernen Prozessoren verwendet wird</a></li>
<li><a href="../de467085/index.html">C-, C ++ - und DotNet-Dekompilierung sind die Grundlagen der Umkehrung. L√∂sen von Problemen beim Umkehren mit r0ot-mi. Teil 1</a></li>
<li><a href="../de467087/index.html">Wie ich mich auf die Oracle Database SQL-Zertifizierung (1Z0-071) vorbereitet und diese bestanden habe</a></li>
<li><a href="../de467089/index.html">Patched Exim - Patch erneut. Neue Remote-Befehlsausf√ºhrung in Exim 4.92 in einer Anforderung</a></li>
<li><a href="../de467091/index.html">Eine kurze Einf√ºhrung in Svelte aus der Perspektive eines Angular-Entwicklers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>