<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐴 ⚜️ 🚣🏽 Analyse der emotionalen Färbung von Rezensionen von Kinopoisk 🏡 ♊️ 👩‍👧‍👧</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eintrag 
 Natural Language Processing (NLP) ist ein beliebter und wichtiger Bereich des maschinellen Lernens. In diesem Hub werde ich mein erstes Proj...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analyse der emotionalen Färbung von Rezensionen von Kinopoisk</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467081/"><h3>  Eintrag </h3><br>  Natural Language Processing (NLP) ist ein beliebter und wichtiger Bereich des maschinellen Lernens.  In diesem Hub werde ich mein erstes Projekt beschreiben, das sich mit der Analyse der emotionalen Färbung von in Python geschriebenen Filmkritiken befasst.  Die Aufgabe der sentimentalen Analyse ist unter denen, die die Grundkonzepte von NLP beherrschen wollen, weit verbreitet und kann in diesem Bereich zu einem Analogon der „Hallo Welt“ werden. <br><br>  In diesem Artikel werden wir alle Hauptphasen des Data Science-Prozesses durchlaufen: von der Erstellung Ihres eigenen Datensatzes über die Verarbeitung und das Extrahieren von Funktionen mithilfe der NLTK-Bibliothek bis hin zum Lernen und Optimieren des Modells mithilfe von Scikit-Learn.  Die Aufgabe selbst besteht darin, Bewertungen in drei Klassen einzuteilen: negativ, neutral und positiv. <br><a name="habracut"></a><br><h3>  Datenkorpusbildung </h3><br>  Um dieses Problem zu lösen, könnte man einen vorgefertigten und kommentierten Datenkörper mit Bewertungen von IMDB verwenden, von denen es viele auf GitHub gibt.  Es wurde jedoch beschlossen, Ihre eigenen Bewertungen in russischer Sprache von Kinopoisk zu erstellen.  Um sie nicht manuell zu kopieren, schreiben wir einen Webparser.  Ich werde die <i>Anforderungsbibliothek</i> verwenden, um http- <i>Anforderungen</i> zu senden, und <i>BeautifulSoup</i> , um HTML-Dateien zu verarbeiten.  Definieren wir zunächst eine Funktion, die einen Link zu Filmkritiken enthält und diese abruft.  Damit Kinopoisk den Bot in uns nicht erkennt, müssen Sie das <i>Header-</i> Argument in der Funktion <i>request.get</i> angeben, das den Browser simuliert.  Es ist erforderlich, ein Wörterbuch mit den Schlüsseln User-Agent, Accept-language und Accept zu übergeben, deren Werte in den Browser-Entwicklertools zu finden sind.  Als Nächstes wird ein Parser erstellt und Bewertungen von der Seite abgerufen, die in der HTML-Markup-Klasse _reachbanner_ gespeichert sind. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url)</span></span></span><span class="hljs-function">:</span></span> r = requests.get(url, headers = headers) <span class="hljs-comment"><span class="hljs-comment">#  http  soup = BeautifulSoup(r.text, 'html.parser')#  html  reviews = soup.find_all(class_='_reachbanner_')#    reviews_clean = [] for review in reviews:#    html  reviews_clean.append(review.find_all(text=True)) return reviews_clean</span></span></code> </pre> <br>  Wir haben das HTML-Markup entfernt, unsere Bewertungen sind jedoch immer noch <i>BeautifulSoup-</i> Objekte, aber wir müssen sie in Zeichenfolgen konvertieren.  Die <i>Konvertierungsfunktion</i> macht genau das.  Wir werden auch eine Funktion schreiben, die den Namen des Films abruft und später zum Speichern von Rezensionen verwendet wird. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(reviews)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     review_converted = [] for review in reviews: for i in review: map(str, i) review = ''.join(review) review_converted.append(review) return review_converted def get_name(url): #    r = requests.get(url, headers = headers) soup = BeautifulSoup(r.text, 'html.parser') name = soup.find(class_='alternativeHeadline') name_clean = name.find_all(text = True) #   , . .     return str(name_clean[0])</span></span></code> </pre><br>  Die letzte Funktion des Parsers enthält einen Link zur Hauptseite des Films, eine Überprüfungsklasse und eine Möglichkeit zum Speichern von Überprüfungen.  Die Funktion definiert auch <i>Verzögerungen</i> zwischen Anforderungen, die erforderlich sind, um ein Verbot zu vermeiden.  Die Funktion enthält eine Schleife, die Bewertungen ab der ersten Seite abruft und speichert, bis sie auf eine nicht vorhandene Seite stößt, von der die Funktion <i>load_data</i> eine leere Liste extrahiert und die Schleife <i>unterbrochen</i> wird. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parsing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url, status, path)</span></span></span><span class="hljs-function">:</span></span> page = <span class="hljs-number"><span class="hljs-number">1</span></span> delays = [<span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">13</span></span>, <span class="hljs-number"><span class="hljs-number">11.5</span></span>, <span class="hljs-number"><span class="hljs-number">12.5</span></span>, <span class="hljs-number"><span class="hljs-number">13.5</span></span>, <span class="hljs-number"><span class="hljs-number">11.2</span></span>, <span class="hljs-number"><span class="hljs-number">12.3</span></span>, <span class="hljs-number"><span class="hljs-number">11.8</span></span>] name = get_name(url) time.sleep(np.random.choice(delays)) <span class="hljs-comment"><span class="hljs-comment">#    while True: loaded_data = load_data(url + 'reviews/ord/date/status/{}/perpage/200/page/{}/'.format(status, page)) if loaded_data == []: break else: # E     ,    if not os.path.exists(path + r'\{}'.format(status)): os.makedirs(path + r'\{}'.format(status)) converted_data = convert(loaded_data) #   for i, review in enumerate(converted_data): with open(path + r'\{}\{}_{}_{}.txt'.format(status, name, page, i), 'w', encoding = 'utf-8') as output: output.write(review) page += 1 time.sleep(np.random.choice(delays))</span></span></code> </pre><br>  Anschließend können Sie mithilfe des folgenden Zyklus Rezensionen aus Filmen extrahieren, die in der <i>Urles-</i> Liste enthalten sind.  Eine Liste der Filme muss manuell erstellt werden.  Es wäre zum Beispiel möglich, eine Liste von Links zu Filmen zu erhalten, indem eine Funktion geschrieben wird, die sie aus den 250 besten Filmen einer Filmsuche extrahiert, um dies nicht manuell zu tun. 15 bis 20 Filme würden jedoch ausreichen, um einen kleinen Datensatz mit tausend Rezensionen für jede Klasse zu bilden.  Wenn Sie ein Verbot erhalten, zeigt das Programm außerdem an, für welchen Film und welche Klasse der Parser angehalten hat, um nach dem Verbot an derselben Stelle fortzufahren. <br><br><pre> <code class="python hljs">path = <span class="hljs-comment"><span class="hljs-comment">#    urles = #    statuses = ['good', 'bad', 'neutral'] delays = [15, 20, 13, 18, 12.5, 13.5, 25, 12.3, 23] for url in urles: for status in statuses: try: parsing(url = url, status = status, path=path) print('one category done') time.sleep(np.random.choice(delays)) #       AttributeError except AttributeError: print(' : {}, {}'.format(url, status)) break #  else  ,      #    ,     else: print('one url done') continue break</span></span></code> </pre><br><h3>  Vorbehandlung </h3><br>  Nachdem ich den Parser geschrieben und zufällige Filme für ihn und mehrere Verbote aus der Filmsuche zurückgerufen hatte, mischte ich die Rezensionen in Ordnern und wählte 900 Rezensionen aus jeder Klasse für das Training und den Rest für die Kontrollgruppe aus.  Jetzt ist es notwendig, das Gehäuse vorzuverarbeiten, nämlich es zu tokenisieren und zu normalisieren.  Tokenisieren bedeutet, den Text in Komponenten, in diesem Fall in Wörter, zu zerlegen, da wir die Darstellung einer Worttasche verwenden.  Und Normalisierung besteht darin, Wörter in Kleinbuchstaben umzuwandeln, Stoppwörter und übermäßiges Rauschen zu entfernen, zu stottern und andere Tricks anzuwenden, die dazu beitragen, den Platz für Zeichen zu verringern. <br><br>  Wir importieren die notwendigen Bibliotheken. <br><br><div class="spoiler">  <b class="spoiler_title">Versteckter Text</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.corpus <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PlaintextCorpusReader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SnowballStemmer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.probability <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FreqDist <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RegexpTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> bigrams <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pos_tag <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OrderedDict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> classification_report, accuracy_score <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MultinomialNB <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pool <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.sparse <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csr_matrix</code> </pre><br></div></div><br>  Wir definieren zunächst einige kleine Funktionen für die Textvorverarbeitung.  Der erste mit dem Namen <i>lower_pos_tag</i> nimmt eine Liste mit Wörtern, konvertiert sie in Kleinbuchstaben und speichert jedes Token in einem Tupel mit seinem Wortbestandteil.  Die Operation zum Hinzufügen eines Teils der Sprache zu einem Wort wird als POS-Tagging (Part of Speech) bezeichnet und wird in NLP häufig zum Extrahieren von Entitäten verwendet.  In unserem Fall verwenden wir Wortarten in der folgenden Funktion, um Wörter zu filtern. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lower_pos_tag</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> lower_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: lower_words.append(i.lower()) pos_words = pos_tag(lower_words, lang=<span class="hljs-string"><span class="hljs-string">'rus'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> pos_words</code> </pre><br>  Die Texte enthalten eine große Anzahl von Wörtern, die zu oft gefunden werden, um für das Modell nützlich zu sein (die sogenannten Stoppwörter).  Grundsätzlich handelt es sich dabei um Präpositionen, Konjunktionen und Pronomen, anhand derer nicht bestimmt werden kann, auf welche Klassenerinnerung sich bezieht.  Die Funktion <i>clean</i> hinterlässt nur Substantive, Adjektive, Verben und Adverbien.  Beachten Sie, dass Teile der Sprache entfernt werden, da sie für das Modell selbst nicht benötigt werden.  Sie können auch feststellen, dass diese Funktion Stamming verwendet, dessen Kern darin besteht, Suffixe und Präfixe aus Wörtern zu entfernen.  Auf diese Weise können Sie die Dimension von Zeichen reduzieren, da Wörter mit unterschiedlichen Gattungen und Groß- / Kleinschreibung auf dasselbe Token reduziert werden.  Es gibt ein leistungsfähigeres Analogon zum Stottern - die Lemmatisierung, mit der Sie die ursprüngliche Form des Wortes wiederherstellen können.  Es funktioniert jedoch langsamer als das Stottern, und außerdem verfügt NLTK nicht über einen russischen Lemmatisator. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> stemmer = SnowballStemmer(<span class="hljs-string"><span class="hljs-string">"russian"</span></span>) cleaned_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'S'</span></span>, <span class="hljs-string"><span class="hljs-string">'A'</span></span>, <span class="hljs-string"><span class="hljs-string">'V'</span></span>, <span class="hljs-string"><span class="hljs-string">'ADV'</span></span>]: cleaned_words.append(stemmer.stem(i[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cleaned_words</code> </pre><br>  Als nächstes schreiben wir die letzte Funktion, die die Klassenbezeichnung übernimmt und alle Bewertungen mit dieser Klasse abruft.  Um den Fall zu lesen, verwenden wir die <i>Raw-</i> Methode des <i>PlaintextCorpusReader-</i> Objekts, mit der Sie Text aus der angegebenen Datei extrahieren können.  Als nächstes wird die Tokenisierung RegexpTokenizer verwendet, die auf der Grundlage eines regulären Ausdrucks arbeitet.  Zusätzlich zu einzelnen Wörtern habe ich dem Modell Bigrams hinzugefügt, die Kombinationen aller benachbarten Wörter sind.  Diese Funktion verwendet auch das <i>FreqDist-</i> Objekt, das die Häufigkeit des Auftretens von Wörtern zurückgibt.  Es wird hier verwendet, um Wörter zu entfernen, die in allen Überprüfungen einer bestimmten Klasse nur einmal vorkommen (sie werden auch als Hapaks bezeichnet).  Somit gibt die Funktion ein Wörterbuch zurück, das Dokumente enthält, die als eine Tasche von Wörtern und eine Liste aller Wörter für eine bestimmte Klasse dargestellt werden. <br><br><pre> <code class="python hljs">corpus_root = <span class="hljs-comment"><span class="hljs-comment">#    def process(label): # Wordmatrix -     # All words -    data = {'Word_matrix': [], 'All_words': []} #      templist_allwords = [] #        corpus = PlaintextCorpusReader(corpus_root + '\\' + label, '.*', encoding='utf-8') #       names = corpus.fileids() #   tokenizer = RegexpTokenizer(r'\w+|[^\w\s]+') for i in range(len(names)): #   bag_words = tokenizer.tokenize(corpus.raw(names[i])) lower_words = lower_pos_tag(bag_words) cleaned_words = clean(lower_words) finalist = list(bigrams(cleaned_words)) + cleaned_words data['Word_matrix'].append(final_words) templist_allwords.extend(cleaned_words) #   templistfreq = FreqDist(templist_allwords) hapaxes = templistfreq.hapaxes() #    for word in templist_allwords: if word not in hapaxes: data['All_words'].append(word) return {label: data}</span></span></code> </pre><br>  Die Vorverarbeitungsphase ist die längste, daher ist es sinnvoll, die Bearbeitung unseres Falls zu parallelisieren.  Dies kann mit dem <i>Multiprocessing-</i> Modul erfolgen.  Im nächsten Programmcode starte ich drei Prozesse, die gleichzeitig drei Ordner mit unterschiedlichen Klassen verarbeiten.  Als nächstes werden die Ergebnisse in einem Wörterbuch gesammelt.  Diese Vorverarbeitung ist abgeschlossen. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: data = {} labels = [<span class="hljs-string"><span class="hljs-string">'neutral'</span></span>, <span class="hljs-string"><span class="hljs-string">'bad'</span></span>, <span class="hljs-string"><span class="hljs-string">'good'</span></span>] p = Pool(<span class="hljs-number"><span class="hljs-number">3</span></span>) result = p.map(process, labels) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> result: data.update(i) p.close()</code> </pre><br><h3>  Vektorisierung </h3><br>  Nachdem wir den Fall vorverarbeitet haben, haben wir ein Wörterbuch, in dem für jedes Klassenlabel eine Liste mit Bewertungen enthalten ist, die wir mit Bigrams versehen, normalisiert und angereichert haben, sowie eine Liste mit Wörtern aus allen Bewertungen dieser Klasse.  Da das Modell die natürliche Sprache nicht so wahrnehmen kann wie wir, besteht die Aufgabe nun darin, unsere Übersichten in numerischer Form darzustellen.  Zu diesem Zweck erstellen wir ein gemeinsames Vokabular, das aus eindeutigen Token besteht, und vektorisieren damit jede Überprüfung. <br><br>  Zunächst erstellen wir eine Liste, die Bewertungen aller Klassen zusammen mit ihren Bezeichnungen enthält.  Als nächstes erstellen wir ein gemeinsames Vokabular, das aus jeder Klasse 10.000 der gebräuchlichsten Wörter mit der Methode <i>most_common</i> desselben <i>FreqDist entnimmt</i> .  Als Ergebnis erhielt ich einen Wortschatz, der aus ungefähr 17.000 Wörtern bestand. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     : # [([  ], _)] labels = ['neutral', 'bad', 'good'] labeled_data = [] for label in labels: for document in data[label]['Word_matrix']: labeled_data.append((document, label)) #      all_words = [] for label in labels: frequency = FreqDist(data[label]['All_words'] common_words = frequency.most_common(10000) words = [i[0] for i in common_words] all_words.extend(words) #    unique_words = list(OrderedDict.fromkeys(all_words))</span></span></code> </pre><br>  Es gibt verschiedene Möglichkeiten, Text zu vektorisieren.  Die beliebtesten von ihnen: TF-IDF, Direkt- und Frequenzcodierung.  Ich habe die Frequenzcodierung verwendet, deren Kern darin besteht, jede Überprüfung als Vektor darzustellen, deren Elemente die Anzahl der Vorkommen jedes Wortes aus dem Vokabular sind.  <i>NLTK</i> hat seine eigenen Klassifikatoren, Sie können sie verwenden, aber sie arbeiten langsamer als ihre Gegenstücke aus <i>Scikit-Learn</i> und haben weniger Einstellungen.  Unten finden Sie den Code für die Codierung für <i>NLTK</i> .  Ich werde jedoch das Naive Bayes-Modell von <i>scikit-learn verwenden</i> und die Überprüfungen codieren, wobei die Attribute in einer spärlichen Matrix von <i>SciPy</i> und die Klassenbezeichnungen in einem separaten <i>NumPy-</i> Array gespeichert werden. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     nltk  : # # [({ : -   },  )] prepared_data = [] for x in labeled_data: d = defaultdict(int) for word in unique_words: if word in x[0]: d[word] += 1 if word not in x[0]: d[word] = 0 prepared_data.append((d, x[1])) #     scikit-learn #     matrix_vec = csr_matrix((len(labeled_data), len(unique_words)), dtype=np.int8).toarray() #     target = np.zeros(len(labeled_data), 'str') for index_doc, document in enumerate(labeled_data): for index_word, word in enumerate(unique_words): #  -     matrix_vec[index_doc, index_word] = document[0].count(word) target[index_doc] = document[1] #   X, Y = shuffle(matrix_vec, target)</span></span></code> </pre><br>  Da im Datensatz die Bewertungen mit bestimmten Tags nacheinander ablaufen, dh zuerst alle neutral, dann alle negativ usw., müssen Sie sie mischen.  Dazu können Sie die <i>Shuffle-</i> Funktion von <i>scikit-learn verwenden</i> .  Es ist nur für Situationen geeignet, in denen sich Zeichen und Klassenbezeichnungen in unterschiedlichen Arrays befinden, da Sie zwei Arrays gleichzeitig mischen können. <br><br><h3>  Modelltraining </h3><br>  Jetzt muss das Modell trainiert und seine Genauigkeit in der Kontrollgruppe überprüft werden.  Als Modell verwenden wir das Modell des Naive Bayes-Klassifikators.  <i>Scikit-learn</i> verfügt je nach Datenverteilung über drei Naive Bayes-Modelle: binär, diskret und kontinuierlich.  Da die Verteilung unserer Funktionen diskret ist, wählen wir <i>MultinomialNB</i> . <br><br>  Der Bayes'sche Klassifikator verfügt über den <i>Alpha-</i> Hyper- <i>Parameter</i> , der für die Glättung des Modells verantwortlich ist.  Naive Bayes berechnet die Wahrscheinlichkeiten jeder Überprüfung, die zu allen Klassen gehört, wobei die bedingten Wahrscheinlichkeiten für das Auftreten aller Überprüfungswörter multipliziert werden, sofern sie zu einer bestimmten Klasse gehören.  Wenn jedoch im Trainingsdatensatz kein Überprüfungswort gefunden wurde, ist seine bedingte Wahrscheinlichkeit gleich Null, wodurch die Wahrscheinlichkeit aufgehoben wird, dass die Überprüfung zu einer Klasse gehört.  Um dies zu vermeiden, wird standardmäßig allen bedingten Wortwahrscheinlichkeiten eine Einheit hinzugefügt, d. H. <i>Alpha ist</i> gleich eins.  Dieser Wert ist jedoch möglicherweise nicht optimal.  Sie können versuchen, <i>Alpha</i> mithilfe der Rastersuche und der Kreuzvalidierung auszuwählen. <br><br><pre> <code class="python hljs">parameter = [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.0001</span></span>] param_grid = {<span class="hljs-string"><span class="hljs-string">'alpha'</span></span>: parameter} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>) grid_search.fit(X, Y) Alpha, best_score = grid_search.best_params_, grid_search.best_score_</code> </pre><br>  In meinem Fall gibt der Gitterherd den optimalen Wert des Hyperparameters gleich 0 mit einer Genauigkeit von 0,965 an.  Ein solcher Wert ist jedoch offensichtlich nicht optimal für den Kontrolldatensatz, da eine große Anzahl von Wörtern vorhanden ist, die zuvor nicht im Trainingssatz gefunden wurden.  Für einen Referenzdatensatz hat dieses Modell eine Genauigkeit von 0,598.  Wenn Sie jedoch <i>Alpha</i> auf 0,1 erhöhen, sinkt die Genauigkeit der Trainingsdaten auf 0,82 und der Kontrolldaten auf 0,62.  Bei einem größeren Datensatz ist der Unterschied höchstwahrscheinlich signifikanter. <br><br><pre> <code class="python hljs">model = MultinomialNB(<span class="hljs-number"><span class="hljs-number">0.1</span></span>) model.fit(X, Y) <span class="hljs-comment"><span class="hljs-comment"># X_control, Y_control   ,   X  Y #        predicted = model.predict(X_control) #     score_test = accuracy_score(Y_control, predicted) #   report = classification_report(Y_control, predicted)</span></span></code> </pre><br><br><h3>  Fazit </h3><br>  Es wird davon ausgegangen, dass das Modell verwendet werden sollte, um Bewertungen vorherzusagen, deren Wörter nicht zur Bildung eines Vokabulars verwendet wurden.  Daher kann die Qualität des Modells anhand seiner Genauigkeit im Steuerteil der Daten bewertet werden, die 0,62 beträgt.  Dies ist fast doppelt so gut wie nur zu raten, aber die Genauigkeit ist immer noch ziemlich niedrig. <br><br>  Dem Klassifizierungsbericht zufolge ist klar, dass das Modell mit Bewertungen mit einer neutralen Farbe am schlechtesten abschneidet (Genauigkeit 0,47 gegenüber 0,68 für positiv und 0,76 für negativ).  In der Tat enthalten neutrale Bewertungen Wörter, die sowohl für positive als auch für negative Bewertungen charakteristisch sind.  Wahrscheinlich kann die Genauigkeit des Modells durch Erhöhen des Datensatzvolumens verbessert werden, da der dreitausendste Datensatz eher bescheiden ist.  Es wäre auch möglich, das Problem auf eine binäre Klassifizierung von Bewertungen in positive und negative zu reduzieren, was ebenfalls die Genauigkeit erhöhen würde. <br><br>  Danke fürs Lesen. <br><br>  PS Wenn Sie sich selbst üben möchten, kann mein Datensatz unter dem Link heruntergeladen werden. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link zum Datensatz</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de467081/">https://habr.com/ru/post/de467081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de467061/index.html">Babylonische Reihe: 5 Sicherheitsprobleme im Baugeschäft</a></li>
<li><a href="../de467063/index.html">Kraftstoffüberwachung für Dieselgeneratoren im Rechenzentrum - wie und warum ist das so wichtig?</a></li>
<li><a href="../de467065/index.html">Archiv der Olympiadenprobleme in der Physik für Schüler</a></li>
<li><a href="../de467073/index.html">„Im Westen gibt es keine Art Direktoren unter 40 Jahren. Bei uns kann es bis zu 30 sein. " Wie ist es, Designer in der IT zu sein?</a></li>
<li><a href="../de467079/index.html">CSS und Javascript Ant Carousel</a></li>
<li><a href="../de467083/index.html">Wie die seltsame Popcount-Anweisung in modernen Prozessoren verwendet wird</a></li>
<li><a href="../de467085/index.html">C-, C ++ - und DotNet-Dekompilierung sind die Grundlagen der Umkehrung. Lösen von Problemen beim Umkehren mit r0ot-mi. Teil 1</a></li>
<li><a href="../de467087/index.html">Wie ich mich auf die Oracle Database SQL-Zertifizierung (1Z0-071) vorbereitet und diese bestanden habe</a></li>
<li><a href="../de467089/index.html">Patched Exim - Patch erneut. Neue Remote-Befehlsausführung in Exim 4.92 in einer Anforderung</a></li>
<li><a href="../de467091/index.html">Eine kurze Einführung in Svelte aus der Perspektive eines Angular-Entwicklers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>