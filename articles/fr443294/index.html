<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçõ üíè ‚òÅÔ∏è Qu'est-ce qui est autoris√© par Jupyter? üë©‚Äçüåæ üòà üí™üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Notre histoire a commenc√© par une t√¢che apparemment simple. Il √©tait n√©cessaire de mettre en place des outils d'analyse pour les sp√©cialistes de la sc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Qu'est-ce qui est autoris√© par Jupyter?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/vtb/blog/443294/">  Notre histoire a commenc√© par une t√¢che apparemment simple.  Il √©tait n√©cessaire de mettre en place des outils d'analyse pour les sp√©cialistes de la science des donn√©es et uniquement les analystes de donn√©es.  Cette t√¢che nous a √©t√© confi√©e par des coll√®gues des divisions Risques Retail et CRM, o√π la concentration de sp√©cialistes en data science est historiquement √©lev√©e.  Les clients avaient un simple d√©sir: √©crire du code Python, importer des biblioth√®ques avanc√©es (xgboost, pytorch, tensorflow, etc.) et ex√©cuter des algorithmes sur les donn√©es g√©n√©r√©es √† partir du cluster hdfs. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/793/c16/22e/793c1622e8423e8cae171263790ab234.png"><br><br>  Tout semble simple et clair.  Mais il y avait tellement d'emb√ªches que nous avons d√©cid√© d'√©crire un article √† ce sujet et de publier la solution pr√™te √† l'emploi sur GitHub. <br><a name="habracut"></a><br>  Tout d'abord, quelques d√©tails sur l'infrastructure source: <br><br><ul><li>  HDFS Data Warehouse (12 n≈ìuds Oracle Big Data Appliance, distribution Cloudera).  Au total, l'entrep√¥t dispose de 130 To de donn√©es provenant de divers syst√®mes internes de la banque; il existe √©galement des informations h√©t√©rog√®nes provenant de sources externes. <br></li><li>  Deux serveurs d'applications sur lesquels le d√©ploiement d'outils analytiques √©tait suppos√©.  Il convient de mentionner que non seulement les t√¢ches d'analyse avanc√©es "tournent" sur ces serveurs, mais l'une des exigences √©tait l'utilisation d'outils de conteneurisation (Docker) pour g√©rer les ressources du serveur, utiliser divers environnements et les configurer. <br></li></ul><br>  En tant qu'environnement principal pour le travail des analystes, ils ont d√©cid√© de choisir JupyterHub, qui de facto est d√©j√† devenu l'un des standards pour travailler avec les donn√©es et d√©velopper des mod√®les d'apprentissage automatique.  En savoir plus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  √Ä l'avenir, nous avons d√©j√† imagin√© JupyterLab. <br><br>  Il semblerait que tout soit simple: vous devez prendre et configurer un tas de Python + Anaconda + Spark.  Installez Jupyter Hub sur le serveur d'applications, int√©grez avec LDAP, connectez Spark ou connectez-vous aux donn√©es dans hdfs de toute autre mani√®re et allez-y - cr√©ez des mod√®les! <br>  Si vous explorez toutes les donn√©es source et les exigences, voici une liste plus d√©taill√©e: <br><br><ul><li>  Ex√©cution de JupyterHub dans Docker (syst√®me d'exploitation de base - Oracle Linux 7) <br></li><li> Cloudera CDH cluster 5.15.1 + Spark 2.3.0 avec authentification Kerberos dans la configuration Active Directory + Kerberos MIT d√©di√© dans le cluster (voir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Cluster MIT KDC d√©di√© avec Active Directory</a> ), Oracle Linux 6 <br></li><li>  Int√©gration Active Directory <br></li><li>  Authentification transparente dans Hadoop et Spark <br></li><li>  Prise en charge de Python 2 et 3 <br></li><li>  Spark 1 et 2 (avec la possibilit√© d'utiliser des ressources de cluster pour former des mod√®les et parall√©liser le traitement des donn√©es √† l'aide de pyspark) <br></li><li>  Capacit√© √† limiter les ressources de l'h√¥te <br></li><li>  Ensemble de biblioth√®que <br></li></ul><br>  Ce poste est con√ßu pour les professionnels de l'informatique confront√©s √† la n√©cessit√© de r√©soudre de tels probl√®mes. <br><br><h2>  Description de la solution </h2><br><h3>  Lancement dans Docker + Cloudera Cluster Integration </h3><br>  Il n'y a rien d'inhabituel ici.  Les clients des produits JupyterHub et Cloudera sont install√©s dans le conteneur (comme - voir ci-dessous), et les fichiers de configuration sont mont√©s √† partir de la machine h√¥te: <br><br>  <b>start-hub.sh</b> <br><br><pre><code class="plaintext hljs">VOLUMES="-v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z -v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre> <br><br><h3>  Int√©gration Active Directory </h3><br>  Pour l'int√©gration avec le fer Active Directory / Kerberos et pas tr√®s h√¥tes, la norme dans notre soci√©t√© est le produit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PBIS Open</a> .  Techniquement, ce produit est un ensemble de services qui communiquent avec Active Directory, avec lequel, √† leur tour, les clients travaillent via des sockets de domaine Unix.  Ce produit s'int√®gre √† Linux PAM et NSS. <br><br>  Nous avons utilis√© la m√©thode Docker standard - les sockets de domaine unix des services h√¥tes ont √©t√© mont√©s dans un conteneur (les sockets ont √©t√© trouv√©s empiriquement par de simples manipulations avec la commande lsof): <br><br>  <b>start-hub.sh</b> <br><br><pre> <code class="plaintext hljs">VOLUMES="-v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z &lt;b&gt;-v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro &lt;/b&gt; -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre><br>  √Ä leur tour, les packages PBIS sont install√©s √† l'int√©rieur du conteneur, mais sans ex√©cuter la section de post-installation.  Nous ne mettons donc que des fichiers ex√©cutables et des biblioth√®ques, mais ne d√©marrons pas de services √† l'int√©rieur du conteneur - c'est superflu pour nous.  Les commandes d'int√©gration PAM et NSS Linux sont ex√©cut√©es manuellement. <br><br>  <b>Dockerfile:</b> <br><br><pre> <code class="plaintext hljs"># Install PAM itself and standard PAM configuration packages. RUN yum install -y pam util-linux \ # Here we just download PBIS RPM packages then install them omitting scripts. # We don't need scripts since they start PBIS services, which are not used - we connect to the host services instead. &amp;&amp; find /var/yum/localrepo/ -type f -name 'pbis-open*.rpm' | xargs rpm -ivh --noscripts \ # Enable PBIS PAM integration. &amp;&amp; domainjoin-cli configure --enable pam \ # Make pam_loginuid.so module optional (Docker requirement) and add pam_mkhomedir.so to have home directories created automatically. &amp;&amp; mv /etc/pam.d/login /tmp \ &amp;&amp; awk '{ if ($1 == "session" &amp;&amp; $2 == "required" &amp;&amp; $3 == "pam_loginuid.so") { print "session optional pam_loginuid.so"; print "session required pam_mkhomedir.so skel=/etc/skel/ umask=0022";} else { print $0; } }' /tmp/login &gt; /etc/pam.d/login \ &amp;&amp; rm /tmp/login \ # Enable PBIS nss integration. &amp;&amp; domainjoin-cli configure --enable nsswitch</code> </pre><br>  Il s'av√®re que les clients du conteneur PBIS communiquent avec les services h√¥tes PBIS.  JupyterHub utilise un authentificateur PAM, et avec PBIS correctement configur√© sur l'h√¥te, tout fonctionne hors de la bo√Æte. <br><br>  Afin d'emp√™cher tous les utilisateurs d'AD d'entrer dans JupyterHub, vous pouvez utiliser le param√®tre qui restreint les utilisateurs √† des groupes AD sp√©cifiques. <br><br>  <b>exemple-config / jupyterhub / jupyterhub_config.py</b> <br><br><pre> <code class="plaintext hljs">c.DSAIAuthenticator.group_whitelist = ['COMPANY\\domain^users']</code> </pre><br><h3>  Authentification transparente dans Hadoop et Spark </h3><br>  Lors de la connexion √† JupyterHub, PBIS met en cache le ticket Kerberos de l'utilisateur dans un fichier sp√©cifique du r√©pertoire / tmp.  Pour une authentification transparente de cette mani√®re, il suffit de monter le r√©pertoire host / tmp dans le conteneur et de d√©finir la variable KRB5CCNAME √† la valeur souhait√©e (cela se fait dans notre classe d'authentificateur). <br><br>  <b>start-hub.sh</b> <br><br><pre> <code class="plaintext hljs">VOLUMES="-v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z -v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre> <br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">env['KRB5CCNAME'] = '/host/tmp/krb5cc_%d' % pwd.getpwnam(self.user.name).pw_uid</code> </pre> <br>  Gr√¢ce au code ci-dessus, l'utilisateur JupyterHub peut ex√©cuter des commandes hdfs √† partir du terminal Jupyter et ex√©cuter des travaux Spark sans √©tapes d'authentification suppl√©mentaires.  Monter le r√©pertoire / tmp entier de l'h√¥te dans le conteneur n'est pas s√ªr - nous sommes conscients de ce probl√®me, mais sa solution est toujours en cours de d√©veloppement. <br><br><h3>  Versions Python 2 et 3 </h3><br>  Ici, il semblerait, tout est simple: vous devez installer les versions n√©cessaires de Python et les int√©grer √† Jupyter, cr√©ant le noyau n√©cessaire.  Cette question a d√©j√† √©t√© abord√©e dans de nombreux endroits.  Conda est utilis√© pour g√©rer les environnements Python.  La raison pour laquelle toute simplicit√© n'est qu'apparente sera claire dans la section suivante.  Exemple de noyau pour Python 3.6 (ce fichier n'est pas dans git - tous les fichiers du noyau sont g√©n√©r√©s par du code): <br><br>  <b>/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/share/jupyter/kernels/python3.6.6/kernel.json</b> <br><br><pre> <code class="plaintext hljs">{   "argv": [      "/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/bin/python",       "-m",       "ipykernel_launcher",       "-f",      "{connection_file}"   ],   "display_name": "Python 3",   "language": "python" }</code> </pre><br><h3>  Spark 1 et 2 </h3><br>  Pour s'int√©grer aux clients SPARK, vous devez √©galement cr√©er des noyaux.  Exemple de noyau pour Python 3.6 et SPARK 2. <br><br>  <b>/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/share/jupyter/kernels/python3.6.6-pyspark2/kernel.json</b> <br><br><pre> <code class="plaintext hljs">{   "argv": [       "/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/bin/python",       "-m",       "ipykernel_launcher",       "-f",      "{connection_file}"   ],   "display_name": "Python 3 + PySpark 2",   "language": "python",   "env": {       "JAVA_HOME": "/usr/java/default/",       "SPARK_HOME": "/opt/cloudera/parcels/SPARK2/lib/spark2/",       "PYTHONSTARTUP": "/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/shell.py",       "PYTHONPATH": "/opt/cloudera/parcels/SPARK2/lib/spark2/python/:/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip",       "PYSPARK_PYTHON": "/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/bin/python"   } }</code> </pre><br>  Notez simplement que l'exigence de prise en charge de Spark 1 s'est d√©velopp√©e historiquement.  Cependant, il est possible que quelqu'un soit confront√© √† des restrictions similaires - vous ne pouvez pas, par exemple, installer Spark 2 dans un cluster.  Par cons√©quent, nous d√©crivons ici les pi√®ges que nous avons rencontr√©s sur le chemin de la mise en ≈ìuvre. <br>  Premi√®rement, Spark 1.6.1 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ne fonctionne pas</a> avec Python 3.6.  Fait int√©ressant, dans CDH 5.12.1 cela a √©t√© corrig√©, mais dans 5.15.1 - pour une raison quelconque non).  Au d√©but, nous voulions r√©soudre ce probl√®me en appliquant simplement le correctif appropri√©.  Cependant, √† l'avenir, cette id√©e a d√ª √™tre abandonn√©e, car cette approche n√©cessite l'installation d'un Spark modifi√© dans un cluster, ce qui √©tait inacceptable pour nous.  La solution a √©t√© trouv√©e en cr√©ant un environnement Conda s√©par√© avec Python 3.5. <br><br>  Le deuxi√®me probl√®me emp√™che Spark 1 de fonctionner √† l'int√©rieur de Docker.  Le pilote Spark ouvre un port sp√©cifique par lequel Worker se connecte au pilote - pour cela, le pilote lui envoie son adresse IP.  Dans le cas de Docker Worker, il essaie de se connecter au pilote via l'IP du conteneur, et lorsque vous utilisez network = bridge, cela ne fonctionne pas tout √† fait naturellement. <br><br>  La solution √©vidente est d'envoyer non pas l'IP du conteneur, mais l'IP de l'h√¥te, qui a √©t√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">impl√©ment√©e</a> dans Spark 2 en ajoutant les param√®tres de configuration appropri√©s.  Ce correctif a √©t√© repens√© de fa√ßon cr√©ative et appliqu√© √† Spark 1. Le Spark modifi√© de cette mani√®re n'a pas besoin d'√™tre plac√© sur les h√¥tes du cluster, il n'y a donc pas de probl√®me similaire √† l'incompatibilit√© avec Python 3.6. <br><br>  Quelle que soit la version de Spark, pour sa fonctionnalit√©, il est n√©cessaire d'avoir les m√™mes versions Python dans le cluster que dans le conteneur.  Pour installer Anaconda en contournant directement Cloudera Manager, nous avons d√ª apprendre √† faire deux choses: <br><br><ul><li>  construisez votre colis avec Anaconda et tous les bons environnements <br></li><li>  installez-le dans Docker (par souci de coh√©rence) <br></li></ul><br><h3>  Colis de montage Anaconda </h3><br>  Cela s'est av√©r√© √™tre une t√¢che assez simple.  Tout ce dont vous avez besoin est: <br><br><ol><li>  Pr√©parez le contenu des colis en installant les versions requises de l'environnement Anaconda et Python <br></li><li>  Cr√©ez des fichiers de m√©tadonn√©es et placez-les dans le r√©pertoire des m√©tadonn√©es <br></li><li>  Cr√©er un colis avec du goudron simple <br></li><li>  Valider l'utilitaire de colis de Cloudera <br></li></ol><br>  Le processus est d√©crit plus en d√©tail sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub</a> , il y a aussi un code de validation l√†-bas.  Nous avons emprunt√© des m√©tadonn√©es dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">colis</a> officiel Anaconda pour Cloudera, en les retravaillant de mani√®re cr√©ative. <br><br><h3>  Installer un colis dans Docker </h3><br>  Cette pratique s'est av√©r√©e utile pour deux raisons: <br><br><ul><li>  assurer l'op√©rabilit√© de Spark - il est impossible de placer Anaconda dans un cluster sans colis <br></li><li>  Spark 2 est distribu√© uniquement sous forme de colis - vous pouvez bien s√ªr l'installer dans un conteneur juste sous forme de fichiers jar, mais cette approche a √©t√© rejet√©e <br></li></ul><br>  En prime, suite √† la r√©solution des probl√®mes ci-dessus, nous avons re√ßu: <br><br><ul><li>  facilit√© de configuration des clients Hadoop et Spark - lors de l'installation des m√™mes parcelles dans Docker et dans le cluster, les chemins d'acc√®s sur le cluster et dans le conteneur sont les m√™mes <br></li><li>  facilit√© de maintien d'un environnement uniforme dans le conteneur et dans le cluster - lors de la mise √† jour du cluster, l'image Docker est simplement reconstruite avec les m√™mes parcelles que celles install√©es dans le cluster. <br></li></ul><br>  Pour installer le colis dans Docker, Cloudera Manager est d'abord install√© √† partir des packages RPM.  Pour l'installation r√©elle du colis, le code Java est utilis√©.  Le client en Java sait ce que le client en Python ne peut pas faire, j'ai donc d√ª utiliser Java et perdre une certaine uniformit√©), qui appelle l'API. <br><br>  <b>assets / install-parcels / src / InstallParcels.java</b> <br><br><pre> <code class="plaintext hljs">ParcelsResourceV5 parcels = clusters.getParcelsResource(clusterName); for (int i = 1; i &lt; args.length; i += 2) {   result = installParcel(api, parcels, args[i], args[i + 1], pause);   if (!result) {       System.exit(1);   } }</code> </pre><br><h3>  Limitation des ressources de l'h√¥te </h3><br>  Pour g√©rer les ressources de la machine h√¥te, une combinaison de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DockerSpawner</a> est utilis√©e - un composant qui ex√©cute les utilisateurs finaux de Jupyter dans un conteneur Docker distinct - et des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cgroups</a> - un m√©canisme de gestion des ressources sous Linux.  DockerSpawner utilise l'API Docker, qui vous permet de d√©finir le groupe de contr√¥le parent pour le conteneur.  Il n'y a pas une telle possibilit√© dans le DockerSpawner normal, nous avons donc √©crit un code simple qui nous permet de d√©finir la correspondance entre les entit√©s AD et le groupe de contr√¥le parent dans la configuration. <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">def set_extra_host_config(self):       extra_host_config = {}       if self.user.name in self.user_cgroup_parent:           cgroup_parent = self.user_cgroup_parent[self.user.name]       else:           pw_name = pwd.getpwnam(self.user.name).pw_name           group_found = False           for g in grp.getgrall():               if pw_name in g.gr_mem and g.gr_name in self.group_cgroup_parent:                   cgroup_parent = self.group_cgroup_parent[g.gr_name]                   group_found = True                   break           if not group_found:               cgroup_parent = self.cgroup_parent extra_host_config['cgroup_parent'] = cgroup_parent</code> </pre><br>  Une petite modification a √©galement √©t√© introduite qui lance Jupyter √† partir de la m√™me image √† partir de laquelle JupyterHub est lanc√©.  Par cons√©quent, il n'est pas n√©cessaire d'utiliser plusieurs images. <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">current_container = None host_name = socket.gethostname() for container in self.client.containers():   if container['Id'][0:12] == host_name:       current_container = container       break self.image = current_container['Image']</code> </pre><br>  Ce qui doit √™tre ex√©cut√© exactement dans le conteneur, Jupyter ou JupyterHub, est d√©termin√© dans le script de d√©marrage par des variables d'environnement: <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">#!/bin/bash ANACONDA_PATH="/opt/cloudera/parcels/Anaconda/" DEFAULT_ENV=`cat ${ANACONDA_PATH}/envs/default` source activate ${DEFAULT_ENV} if [ -z "${JUPYTERHUB_CLIENT_ID}" ]; then   while true; do       jupyterhub -f /etc/jupyterhub/jupyterhub_config.py   done else   HOME=`su ${JUPYTERHUB_USER} -c 'echo ~'`   cd ~   su ${JUPYTERHUB_USER} -p -c "jupyterhub-singleuser --KernelSpecManager.ensure_native_kernel=False --ip=0.0.0.0" fi</code> </pre><br>  La possibilit√© de d√©marrer des conteneurs Docker Jupyter √† partir du conteneur Docker JupyterHub est obtenue en montant le socket du d√©mon Docker dans le conteneur JupyterHub. <br><br>  <b>start-hub.sh</b> <br><br><pre> <code class="plaintext hljs">VOLUMES="-&lt;b&gt;v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z&lt;/b&gt; -v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre><br>  √Ä l'avenir, il est pr√©vu d'abandonner cette d√©cision au profit, par exemple, de ssh. <br><br>  Lors de l'utilisation de DockerSpawner en conjonction avec Spark, un autre probl√®me se pose: le pilote Spark ouvre des ports al√©atoires, par lesquels les travailleurs √©tablissent ensuite une connexion externe.  Nous pouvons contr√¥ler la plage de num√©ros de port parmi lesquels des nombres al√©atoires sont s√©lectionn√©s en d√©finissant ces plages dans la configuration Spark.  Cependant, ces plages doivent √™tre diff√©rentes pour diff√©rents utilisateurs, car nous ne pouvons pas ex√©cuter les conteneurs Jupyter avec les m√™mes ports publi√©s.  Pour r√©soudre ce probl√®me, un code a √©t√© √©crit qui g√©n√®re simplement des plages de ports par ID utilisateur √† partir de la base de donn√©es JupyterHub et lance le conteneur Docker et Spark avec la configuration appropri√©e: <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">def set_extra_create_kwargs(self):       user_spark_driver_port, user_spark_blockmanager_port, user_spark_ui_port, user_spark_max_retries = self.get_spark_ports()       if user_spark_driver_port == 0 or user_spark_blockmanager_port == 0 or user_spark_ui_port == 0 or user_spark_max_retries == 0:           return       ports = {}       for p in range(user_spark_driver_port, user_spark_driver_port + user_spark_max_retries):           ports['%d/tcp' % p] = None       for p in range(user_spark_blockmanager_port, user_spark_blockmanager_port + user_spark_max_retries):           ports['%d/tcp' % p] = None       for p in range(user_spark_ui_port, user_spark_ui_port + user_spark_max_retries):           ports['%d/tcp' % p] = None self.extra_create_kwargs = { 'ports' : ports }</code> </pre><br>  L'inconv√©nient de cette solution est que lorsque vous red√©marrez le conteneur avec JupyterHub, tout cesse de fonctionner en raison d'une perte de base de donn√©es.  Par cons√©quent, lorsque vous red√©marrez le JupyterHub pour, par exemple, un changement de configuration, nous ne touchons pas le conteneur lui-m√™me, mais red√©marrons uniquement le processus JupyterHub √† l'int√©rieur. <br><br>  <b>restart-hub.sh</b> <br><br><pre> <code class="plaintext hljs">#!/bin/bash docker ps | fgrep 'dsai1.2' | fgrep -v 'jupyter-' | awk '{ print $1; }' | while read ID; do docker exec $ID /bin/bash -c "kill \$( cat /root/jupyterhub.pid )"; done</code> </pre><br>  Les groupes de contr√¥le eux-m√™mes sont cr√©√©s par des outils Linux standard, la correspondance entre les entit√©s AD et les groupes de contr√¥le dans la configuration ressemble √† ceci. <br><br><pre> <code class="plaintext hljs">&lt;b&gt;config-example/jupyterhub/jupyterhub_config.py&lt;/b&gt; c.DSAISpawner.user_cgroup_parent = {   'bank\\user1'    : '/jupyter-cgroup-1', # user 1   'bank\\user2'    : '/jupyter-cgroup-1', # user 2   'bank\\user3'    : '/jupyter-cgroup-2', # user 3 } c.DSAISpawner.cgroup_parent = '/jupyter-cgroup-3'</code> </pre><br><h3>  Code Git </h3><br>  Notre solution est accessible au public sur GitHub: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/DS-AI/dsai/</a> (DSAI - Data Science and Artificial Intelligence).  Tout le code est organis√© dans des r√©pertoires avec des num√©ros de s√©rie - le code de chaque r√©pertoire suivant peut utiliser des artefacts du pr√©c√©dent.  Le r√©sultat du code du dernier r√©pertoire sera une image Docker. <br><br>  Chaque r√©pertoire contient des fichiers: <br><br><ul><li>  assets.sh - cr√©ation d'artefacts n√©cessaires √† l'assemblage (t√©l√©chargement √† partir d'Internet ou copie √† partir des r√©pertoires des √©tapes pr√©c√©dentes) <br></li><li>  build.sh - construire <br></li><li>  clean.sh - nettoyage des artefacts n√©cessaires pour l'assemblage <br></li></ul><br>  Afin de reconstruire compl√®tement l'image Docker, il est n√©cessaire d'ex√©cuter clean.sh, assets.sh, build.sh √† partir des r√©pertoires en fonction de leurs num√©ros de s√©rie. <br><br>  Pour l'assemblage, nous utilisons une machine avec Linux RedHat 7.4, Docker 17.05.0-ce.  La machine poss√®de 8 c≈ìurs, 32 Go de RAM et 250 Go d'espace disque.  Il est fortement recommand√© de ne pas utiliser d'h√¥te avec les pires param√®tres de RAM et de disque dur pour le cr√©er. <br><br>  Voici l'aide pour les noms utilis√©s: <br><br><ul><li>  01-spark-patched - RPM Spark 1.6.1 avec deux patchs SPARK-4563 et SPARK-19019 appliqu√©s. <br></li><li>  02-validateur - validateur de colis <br></li><li>  03-anaconda-dsai-parcel-1.0 - parcelle Anaconda avec le bon Python (2, 3.5 et 3.6) <br></li><li>  04-cloudera-manager-api - Biblioth√®ques d'API Cloudera Manager <br></li><li>  05-dsai1.2-offline - image finale <br></li></ul><br>  H√©las, l'assemblage peut se bloquer pour des raisons que nous n'avons pas pu r√©soudre (par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tar est abandonn√©</a> pendant l'assemblage de la parcelle. Dans ce cas, en r√®gle g√©n√©rale, il vous suffit de red√©marrer l'assemblage, mais cela n'aide pas toujours (par exemple, l'assemblage Spark d√©pend de ressources externes Cloudera, qui peut ne plus √™tre disponible, etc.). <br><br>  Un autre inconv√©nient est que l'assemblage de colis est irr√©productible.  Comme les biblioth√®ques sont constamment mises √† jour, la r√©p√©tition de l'assembly peut donner un r√©sultat diff√©rent du pr√©c√©dent. <br><br><h2>  Grande finale </h2><br>  Maintenant que les utilisateurs utilisent avec succ√®s les outils, leur nombre a d√©pass√© plusieurs dizaines et continue de cro√Ætre.  √Ä l'avenir, nous pr√©voyons d'essayer JupyterLab et envisageons de connecter le GPU au cluster, car maintenant les ressources informatiques de deux serveurs d'applications assez puissants ne suffisent plus. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr443294/">https://habr.com/ru/post/fr443294/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr443284/index.html">Index dans PostgreSQL - 4 (Btree)</a></li>
<li><a href="../fr443286/index.html">TDMS Fairway. M√©canisme de remplissage automatique pour les principales inscriptions sur les dessins et les d√©tails des documents</a></li>
<li><a href="../fr443288/index.html">Navigation dans les projets multi-modules</a></li>
<li><a href="../fr443290/index.html">Zen Erlang [et Elixir - env. traducteur]</a></li>
<li><a href="../fr443292/index.html">Nous √©tudions le principe de fonctionnement des unit√©s em √† l'aide de l'exemple de la t√¢che ¬´Agencement d'un pr√©chargeur flexible¬ª</a></li>
<li><a href="../fr443298/index.html">Recharge sans fil. Comment √ßa marche en pratique</a></li>
<li><a href="../fr443300/index.html">Comment se d√©roule le d√©veloppement chez United Traders</a></li>
<li><a href="../fr443302/index.html">Comment Apple se pr√©pare pour une √®re apr√®s l'iPhone</a></li>
<li><a href="../fr443304/index.html">√ätre technophobe est inutile, m√™me si la technophobie est justifi√©e</a></li>
<li><a href="../fr443306/index.html">Huit lois de d√©nomination dans la conception UX (partie 1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>