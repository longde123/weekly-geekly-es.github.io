<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî¥ üë©üèæ‚Äçü§ù‚Äçüë©üèª üö£üèΩ Por que as TPUs s√£o t√£o boas para o aprendizado profundo? üì¥ üë©‚Äçüë©‚Äçüëß üöÆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Processador tensor de 3¬™ gera√ß√£o 

 O processador tensor do Google √© um circuito integrado de prop√≥sito espec√≠fico ( ASIC ) desenvolvido desde o in√≠ci...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Por que as TPUs s√£o t√£o boas para o aprendizado profundo?</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422317/"><img src="https://habrastorage.org/webt/hc/p7/cd/hcp7cda1npc6ylbq16nwwcsyxd4.jpeg"><br>  <i>Processador tensor de 3¬™ gera√ß√£o</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O processador tensor do Google</a> √© um circuito integrado de prop√≥sito espec√≠fico ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ASIC</a> ) desenvolvido desde o in√≠cio pelo Google para executar tarefas de aprendizado de m√°quina.  Ele trabalha em v√°rios produtos principais do Google, incluindo Traduzir, Fotos, Assistente de Pesquisa e Gmail.  O Cloud TPU oferece os benef√≠cios da escalabilidade e facilidade de uso a todos os desenvolvedores e cientistas de dados que lan√ßam modelos de aprendizado de m√°quina de ponta no Google Cloud.  No Google Next '18, anunciamos que o Cloud TPU v2 agora est√° dispon√≠vel para todos os usu√°rios, incluindo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">contas de avalia√ß√£o gratuita</a> , e o Cloud TPU v3 est√° dispon√≠vel para teste alfa. <br><a name="habracut"></a><br><img src="https://habrastorage.org/getpro/habr/post_images/f34/b73/cee/f34b73ceecf379f47dd446f0cc8b1a7c.jpg"><br><br>  Mas muitas pessoas perguntam - qual √© a diferen√ßa entre CPU, GPU e TPU?  Criamos um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">site de demonstra√ß√£o</a> onde est√£o localizadas a apresenta√ß√£o e a anima√ß√£o que respondem a essa pergunta.  Neste post, gostaria de abordar alguns recursos do conte√∫do deste site. <br><br><h2>  Como as redes neurais funcionam? </h2><br>  Antes de come√ßar a comparar CPU, GPU e TPU, vamos ver que tipo de c√°lculos s√£o necess√°rios para o aprendizado de m√°quina - e especificamente, para redes neurais. <br><br>  Imagine, por exemplo, que usamos uma rede neural de camada √∫nica para reconhecer n√∫meros manuscritos, conforme mostrado no diagrama a seguir: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2d6/bf4/585/2d6bf4585be28678f66652fd77ecb2f8.png"><br><br>  Se a imagem for uma grade de 28x28 pixels em escala de cinza, ela poder√° ser convertida em um vetor de 784 valores (medidas).  Um neur√¥nio que reconhece o n√∫mero 8 pega esses valores e os multiplica pelos valores dos par√¢metros (linhas vermelhas no diagrama). <br><br>  O par√¢metro funciona como um filtro, extraindo recursos dos dados que indicam a semelhan√ßa de imagem e forma 8: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/020/141/559/0201415596ab1132ba07a3b430a2fa34.gif"><br><br>  Esta √© a explica√ß√£o mais simples da classifica√ß√£o de dados por redes neurais.  Multiplica√ß√£o de dados com os par√¢metros correspondentes a eles (colora√ß√£o de pontos) e sua adi√ß√£o (soma de pontos √† direita).  O resultado mais alto indica a melhor correspond√™ncia entre os dados inseridos e o par√¢metro correspondente, que provavelmente ser√° a resposta correta. <br><br>  Simplificando, as redes neurais precisam fazer um grande n√∫mero de multiplica√ß√µes e adi√ß√µes de dados e par√¢metros.  Freq√ºentemente, n√≥s os organizamos na forma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">multiplica√ß√£o</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">matrizes</a> , que voc√™ pode encontrar em √°lgebra na escola.  Portanto, o problema √© realizar um grande n√∫mero de multiplica√ß√µes de matrizes o mais r√°pido poss√≠vel, gastando o m√≠nimo de energia poss√≠vel. <br><br><h2>  Como uma CPU funciona? </h2><br>  Como a CPU aborda essa tarefa?  CPU √© um processador de uso geral baseado na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arquitetura von Neumann</a> .  Isso significa que a CPU trabalha com software e mem√≥ria da seguinte maneira: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/963/029/620/96302962031a0a23ecf34b868d194161.gif"><br><br>  A principal vantagem da CPU √© a flexibilidade.  Gra√ßas √† arquitetura von Neumann, voc√™ pode baixar softwares completamente diferentes para milh√µes de finalidades diferentes.  A CPU pode ser usada para processamento de texto, controle de mecanismo de foguete, transa√ß√µes banc√°rias, classifica√ß√£o de imagens usando uma rede neural. <br><br>  Mas como a CPU √© muito flex√≠vel, o equipamento nem sempre sabe com anteced√™ncia qual ser√° a pr√≥xima opera√ß√£o at√© que leia a pr√≥xima instru√ß√£o do software.  A CPU precisa armazenar os resultados de cada c√°lculo na mem√≥ria localizada dentro da CPU (os chamados registradores ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cache L1</a> ).  O acesso a essa mem√≥ria torna-se menos a arquitetura da CPU, conhecida como gargalo da arquitetura von Neumann.  E embora uma enorme quantidade de c√°lculos para redes neurais torne previs√≠veis as etapas futuras, cada CPU <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">l√≥gica aritm√©tica</a> (ALU, um componente que armazena e controla multiplicadores e somadores) executa opera√ß√µes sequencialmente, acessando a mem√≥ria sempre, o que limita a taxa de transfer√™ncia geral e consome uma quantidade significativa de energia . <br><br><h2>  Como funciona a GPU </h2><br>  Para aumentar a taxa de transfer√™ncia em compara√ß√£o com a CPU, a GPU usa uma estrat√©gia simples: por que n√£o integrar milhares de ALUs no processador?  A GPU moderna cont√©m cerca de 2500 - 5000 ALU no processador, o que torna poss√≠vel realizar milhares de multiplica√ß√µes e adi√ß√µes por vez. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fbb/bcb/17b/fbbbcb17b7732e20d2658d5e76023beb.gif"><br><br>  Essa arquitetura funciona bem com aplicativos que exigem paraleliza√ß√£o massiva, como, por exemplo, multiplica√ß√£o de matrizes em uma rede neural.  Com uma carga de treinamento t√≠pica de aprendizado profundo (GO), a taxa de transfer√™ncia nesse caso aumenta em uma ordem de magnitude em compara√ß√£o com a CPU.  Portanto, hoje a GPU √© a arquitetura de processador mais popular do GO. <br><br>  Mas a GPU continua sendo um processador de uso geral que deve suportar um milh√£o de aplicativos e software diferentes.  E isso nos leva de volta ao problema fundamental do gargalo da arquitetura von Neumann.  Para cada c√°lculo em milhares de ALUs, GPUs, √© necess√°rio recorrer a registradores ou mem√≥ria compartilhada para ler e salvar resultados de c√°lculos intermedi√°rios.  Como a GPU executa mais computa√ß√£o paralela em milhares de suas ALUs, tamb√©m gasta proporcionalmente mais energia no acesso √† mem√≥ria e ocupa uma grande √°rea. <br><br><h2>  Como o TPU funciona? </h2><br>  Quando desenvolvemos o TPU no Google, constru√≠mos uma arquitetura projetada para uma tarefa espec√≠fica.  Em vez de desenvolver um processador de uso geral, desenvolvemos um processador de matriz especializado para trabalhar com redes neurais.  O TPU n√£o poder√° trabalhar com um processador de texto, controlar motores de foguetes ou realizar transa√ß√µes banc√°rias, mas pode processar um grande n√∫mero de multiplica√ß√µes e adi√ß√µes para redes neurais a uma velocidade incr√≠vel, consumindo muito menos energia e ajustando-se a um volume f√≠sico menor. <br><br>  A principal coisa que lhe permite fazer isso √© a elimina√ß√£o radical do gargalo da arquitetura von Neumann.  Como a principal tarefa do TPU √© o processamento matricial, os desenvolvedores de circuitos estavam familiarizados com todas as etapas de c√°lculo necess√°rias.  Portanto, eles foram capazes de colocar milhares de multiplicadores e somadores e conect√°-los fisicamente, formando uma grande matriz f√≠sica.  Isso √© chamado de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arquitetura de matriz em pipeline</a> .  No caso do Cloud TPU v2, duas matrizes de pipeline de 128 x 128 s√£o usadas, o que totaliza 32.768 ALUs para valores de ponto flutuante de 16 bits em um processador. <br><br>  Vamos ver como um array em pipeline realiza c√°lculos para uma rede neural.  Primeiro, o TPU carrega os par√¢metros da mem√≥ria em uma matriz de multiplicadores e somadores. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ec/de3/fc6/9ecde3fc6d69116db89aacd83bdf15e5.gif"><br><br>  O TPU carrega os dados da mem√≥ria.  Ap√≥s a conclus√£o de cada multiplica√ß√£o, o resultado √© transmitido aos seguintes fatores, ao executar adi√ß√µes.  Portanto, a sa√≠da ser√° a soma de todas as multiplica√ß√µes dos dados e par√¢metros.  Durante todo o processo de computa√ß√£o volum√©trica e transfer√™ncia de dados, o acesso √† mem√≥ria √© completamente desnecess√°rio. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/04a/ef8/b31/04aef8b31b8eb550ba093df4eb811d58.gif"><br><br>  Portanto, o TPU demonstra maior rendimento ao calcular redes neurais, consumindo muito menos energia e ocupando menos espa√ßo. <br><br><h2>  Vantagem: 5 vezes menos custo </h2><br>  Quais s√£o os benef√≠cios da arquitetura TPU?  Custo.  Aqui est√° o custo do Cloud TPU v2 para agosto de 2018, no momento da reda√ß√£o deste documento: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e2f/340/d86/e2f340d861ef00ac9ceac0e7d6dc5f24.png"><br>  Custo normal e TPU do trabalho para diferentes regi√µes do Google Cloud <br><br>  A Universidade de Stanford est√° distribuindo um conjunto de testes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DAWNBench</a> que medem o desempenho de sistemas de aprendizado profundo.  L√° voc√™ pode ver v√°rias combina√ß√µes de tarefas, modelos e plataformas de computa√ß√£o, bem como os resultados dos testes correspondentes. <br><br>  No final da competi√ß√£o, em abril de 2018, o custo m√≠nimo de treinamento em processadores com arquitetura diferente de TPU era de US $ 72,40 (para o treinamento do ResNet-50 com precis√£o de 93% no ImageNet em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">inst√¢ncias spot</a> ).  Com o Cloud TPU v2, esse treinamento pode ser realizado por US $ 12,87.  Isso √© menos de 1/5 do custo.  Esse √© o poder da arquitetura projetada especificamente para redes neurais. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt422317/">https://habr.com/ru/post/pt422317/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt422303/index.html">Melhor SQL Builder - use o jOOQ no Android</a></li>
<li><a href="../pt422305/index.html">Distribui√ß√£o do n√∫mero de trabalhadores russos por sal√°rio com base em uma grande pesquisa on-line em uma plataforma n√£o especializada</a></li>
<li><a href="../pt422309/index.html">Como proteger dados em redes neurais na nuvem - √© proposto um novo m√©todo de criptografia</a></li>
<li><a href="../pt422311/index.html">Interessante e utilidade do python. Parte 2</a></li>
<li><a href="../pt422315/index.html">Como sobreviver a um ca√ßador de insetos: luta di√°ria por renda</a></li>
<li><a href="../pt422319/index.html">Pela primeira vez, a equipe russa entrou no maior acelerador cient√≠fico IndieBio</a></li>
<li><a href="../pt422321/index.html">Otimiza√ß√£o do trabalho com prot√≥tipos em mecanismos JavaScript</a></li>
<li><a href="../pt422323/index.html">Hackers: R√∫ssia e China</a></li>
<li><a href="../pt422325/index.html">DevDay sobre os testes: relaxe. Teste f√°cil</a></li>
<li><a href="../pt422327/index.html">Cronograma do projeto x atraso: batalha sem chances</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>