<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçñ üë©üèø‚Äçüè≠ üíÜüèΩ Como implementamos o cache no banco de dados Tarantool üèä üë®üèº‚Äçüç≥ üèÜ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bom dia 

 Quero compartilhar com voc√™ uma hist√≥ria sobre a implementa√ß√£o do cache no banco de dados Tarantool e meus recursos de trabalho. 
 Eu traba...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como implementamos o cache no banco de dados Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441686/"> Bom dia <br><br>  Quero compartilhar com voc√™ uma hist√≥ria sobre a implementa√ß√£o do cache no banco de dados Tarantool e meus recursos de trabalho. <br>  Eu trabalho como desenvolvedor Java em uma empresa de telecomunica√ß√µes.  A principal tarefa: a implementa√ß√£o da l√≥gica de neg√≥cios da plataforma que a empresa comprou do fornecedor.  Dos primeiros recursos, esse √© um trabalho de sab√£o e a quase completa aus√™ncia de armazenamento em cache, exceto na mem√≥ria da JVM.  Tudo isso √© bom at√© que o n√∫mero de inst√¢ncias de aplicativos exceda duas d√∫zias ... <br><br>  No decorrer do trabalho e no surgimento de um entendimento dos recursos da plataforma, foi feita uma tentativa de fazer cache.  Naquela √©poca, o MongoDB j√° havia sido lan√ßado e, como resultado, n√£o obtivemos resultados positivos especiais como no teste. <br><br>  Em uma busca adicional por alternativas e conselhos do meu amigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">mr_elzor</a> , foi decidido tentar o banco de dados Tarantool. <br><a name="habracut"></a><br>  Em um estudo superficial, apenas a d√∫vida apareceu na lua, uma vez que n√£o havia escrito sobre ela com a palavra "completamente".  Mas afastando todas as d√∫vidas, ele come√ßou a instalar.  Sobre redes fechadas e firewalls, acho que poucas pessoas est√£o interessadas, mas aconselho voc√™ a tentar contorn√°-las e colocar tudo de fontes p√∫blicas. <br><br>  Servidores de teste com configura√ß√£o: 8 Cpu, 16 GB de RAM, 100 Gb de disco r√≠gido, Debian 9.4. <br><br>  A instala√ß√£o foi realizada de acordo com as instru√ß√µes do site.  E ent√£o eu tenho uma op√ß√£o de exemplo.  A id√©ia surgiu imediatamente de uma interface visual com a qual o suporte funcionaria convenientemente.  Durante uma pesquisa r√°pida, encontrei e configurei o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tarantool-admin</a> .  Trabalha no Docker e abrange tarefas de suporte 100%, pelo menos por enquanto. <br><br>  Mas vamos falar sobre mais interessante. <br><br>  O pr√≥ximo pensamento foi configurar minha vers√£o na configura√ß√£o mestre - escravo no mesmo servidor, pois a documenta√ß√£o cont√©m apenas exemplos com dois servidores diferentes. <br><br>  Depois de passar algum tempo entendendo lua e descrevendo a configura√ß√£o, inicio o assistente. <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@master Job for tarantool@master.service failed because the control process exited with error code. See "systemctl status tarantool@master.service" and "journalctl -xe" for details.</span></span></code> </pre> <br>  Imediatamente ca√≠ em um estupor e n√£o entendo por que o erro ocorre, mas vejo que ele est√° no status "carregando". <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@master ‚óè tarantool@master.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: activating (start) since Tue 2019-02-19 17:03:24 MSK; 17s ago Docs: man:tarantool(1) Process: 20111 ExecStop=/usr/bin/tarantoolctl stop master (code=exited, status=0/SUCCESS) Main PID: 20120 (tarantool) Status: "loading" Tasks: 5 (limit: 4915) CGroup: /system.slice/system-tarantool.slice/tarantool@master.service ‚îî‚îÄ20120 tarantool master.lua &lt;loading&gt; Feb 19 17:03:24 tarantuldb-tst4 systemd[1]: Starting Tarantool Database Server... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Starting instance master... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Run console at unix/:/var/run/tarantool/master.control Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: started</span></span></code> </pre><br>  Eu corro escravo: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@slave2 Job for tarantool@slave2.service failed because the control process exited with error code. See "systemctl status tarantool@slave2.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  E eu vejo o mesmo erro.  Aqui, geralmente come√ßo a me esfor√ßar e n√£o entendo o que est√° acontecendo, pois n√£o h√° nada na documenta√ß√£o sobre isso ... Mas, ao verificar o status, vejo que ele n√£o come√ßou, apesar de dizer que o status est√° "em execu√ß√£o": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@slave2 ‚óè tarantool@slave2.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2019-02-19 17:04:52 MSK; 27s ago Docs: man:tarantool(1) Process: 20258 ExecStop=/usr/bin/tarantoolctl stop slave2 (code=exited, status=0/SUCCESS) Process: 20247 ExecStart=/usr/bin/tarantoolctl start slave2 (code=exited, status=1/FAILURE) Main PID: 20247 (code=exited, status=1/FAILURE) Status: "running" Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Service hold-off time over, scheduling restart. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Stopped Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Start request repeated too quickly. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Failed to start Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'.</span></span></code> </pre><br>  Mas, ao mesmo tempo, o mestre come√ßou a trabalhar: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20158 1 0 17:04 ? 00:00:00 tarantool master.lua &lt;running&gt; root 20268 2921 0 17:06 pts/1 00:00:00 grep taran</span></span></code> </pre><br>  Reiniciar o escravo n√£o ajuda.  Eu me pergunto por que? <br><br>  Eu paro o mestre.  E execute as a√ß√µes na ordem inversa. <br><br>  Eu vejo que o escravo est√° tentando come√ßar. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20399 1 0 17:09 ? 00:00:00 tarantool slave2.lua &lt;loading&gt;</span></span></code> </pre><br>  Inicio o assistente e vejo que ele n√£o aumentou e geralmente mudou para o status de √≥rf√£o, enquanto o escravo geralmente caiu. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20428 1 0 17:09 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Torna-se ainda mais interessante. <br><br>  Vejo nos logs do escravo que ele at√© viu o mestre e tentou sincronizar. <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: binding to 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto/101/main I&gt; binary: bound to 0.0.0.0:3302 2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: listening on 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main/101/slave2 I&gt; connecting to 1 replicas 2019-02-19 17:13:45.113 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECT 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t I&gt; remote master 825af7c3-f8df-4db0-8559-a866b8310077 at 10.78.221.74:3301 running Tarantool 1.10.2 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECTED 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; connected to 1 replicas 2019-02-19 17:13:45.114 [20751] coio V&gt; loading vylog 14 2019-02-19 17:13:45.114 [20751] coio V&gt; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span> loading vylog 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovery start 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovering from `/var/lib/tarantool/cache_slave2/00000000000000000014.snap<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(47) = 0x7f99a4000080 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; cluster uuid 4035b563-67f8-4e85-95cc-e03429f1fa4d 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(11) = 0x7f99a4004080 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(17) = 0x7f99a4008068</span></span></code> </pre><br>  E a tentativa foi bem sucedida: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a40004c0 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 1 to replica 825af7c3-f8df-4db0-8559-a866b8310077 2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a4000500 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 2 to replica 403c0323-5a9b-480d-9e71-5ba22d4ccf1b 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; recover from `/var/lib/tarantool/slave2/00000000000000000014.xlog<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; done `/var/lib/tarantool/slave2/00000000000000000014.xlog'</span></span></code> </pre><br>  At√© come√ßou: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.119 [20751] main/101/slave2 D&gt; systemd: sending message <span class="hljs-string"><span class="hljs-string">'STATUS=running'</span></span></code> </pre><br>  Mas, por raz√µes desconhecidas, ele perdeu a conex√£o e caiu: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.129 [20751] main/101/slave2 D&gt; SystemError at /build/tarantool-1.10.2.146/src/coio_task.c:416 2019-02-19 17:13:45.129 [20751] main/101/slave2 tarantoolctl:532 E&gt; Start failed: /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/share/lua/5.1/http/server.lua:1146: Can<span class="hljs-string"><span class="hljs-string">'t create tcp_server: Input/output error</span></span></code> </pre><br>  Tentar iniciar o escravo novamente n√£o ajuda. <br><br>  Agora exclua os arquivos criados pelas inst√¢ncias.  No meu caso, excluo tudo do diret√≥rio / var / lib / tarantool. <br><br>  Eu come√ßo o escravo primeiro, e s√≥ depois o mestre.  E eis que ... <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 17:20 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 20933 1 1 17:21 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  N√£o encontrei nenhuma explica√ß√£o para esse comportamento, exceto como um "recurso deste software". <br>  Essa situa√ß√£o aparecer√° sempre que o servidor tiver sido completamente reiniciado. <br><br>  Ap√≥s uma an√°lise mais aprofundada da arquitetura deste software, verifica-se que est√° planejado usar apenas uma vCPU para uma inst√¢ncia e muitos outros recursos permanecem livres. <br><br>  Na ideologia de n vCPU, podemos criar o mestre e n-2 escravos para leitura. <br><br>  Dado que no servidor de teste 8 vCPU, podemos aumentar o mestre e 6 inst√¢ncias para leitura. <br>  Copio o arquivo para escravo, corrijo as portas e corro, ou seja,  mais alguns escravos s√£o adicionados. <br><br>  Importante!  Ao adicionar outra inst√¢ncia, voc√™ deve registr√°-la no assistente. <br>  Mas voc√™ deve primeiro iniciar um novo escravo e s√≥ depois reiniciar o mestre. <br><br><h4>  Exemplo </h4><br>  Eu j√° tinha uma configura√ß√£o em execu√ß√£o com um assistente e dois escravos. <br><br>  Eu decidi adicionar um terceiro escravo. <br><br>  Registrei-o no mestre e reiniciei o mestre primeiro, e foi o que vi: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:29 tarantool slave3.lua &lt;running&gt; taranto+ 21519 1 0 09:16 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  I.e.  nosso mestre se tornou um solit√°rio e a replica√ß√£o se desfez. <br><br>  Iniciar um novo escravo n√£o ajudar√° mais e resultar√° em um erro: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl restart tarantool@slave4 Job for tarantool@slave4.service failed because the control process exited with error code. See "systemctl status tarantool@slave4.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  E nos logs, vi uma pequena entrada informativa: <br><br><pre> <code class="bash hljs">2019-02-20 09:20:10.616 [21601] main/101/slave4 I&gt; bootstrapping replica from 3c77eb9d-2fa1-4a27-885f-e72defa5cd96 at 10.78.221.74:3301 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t I&gt; can<span class="hljs-string"><span class="hljs-string">'t join/subscribe 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t xrow.c:896 E&gt; ER_READONLY: Can'</span></span>t modify data because this instance is <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-built_in"><span class="hljs-built_in">read</span></span>-only mode. 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; STOPPED 2019-02-20 09:20:10.617 [21601] main/101/slave4 xrow.c:896 E&gt; ER_READONLY: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode. 2019-02-20 09:20:10.617 [21601] main/101/slave4 F&gt; can'</span></span>t initialize storage: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode.</span></span></code> </pre><br>  Paramos o mago e come√ßamos um novo escravo.  Tamb√©m haver√° um erro, como no primeiro in√≠cio, mas veremos que ele est√° carregando o status. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21659 1 0 09:23 ? 00:00:00 tarantool slave4.lua &lt;loading&gt;</span></span></code> </pre><br>  Mas quando voc√™ inicia o master, o novo escravo trava e o master n√£o acompanha o status de execu√ß√£o. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21670 1 0 09:23 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Nesta situa√ß√£o, h√° apenas uma sa√≠da.  Como escrevi anteriormente, excluo os arquivos criados por inst√¢ncias e executo os escravos primeiro e depois os domino. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tarantool taranto+ 21892 1 0 09:30 ? 00:00:00 tarantool slave4.lua &lt;running&gt; taranto+ 21907 1 0 09:30 ? 00:00:00 tarantool slave3.lua &lt;running&gt; taranto+ 21922 1 0 09:30 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 21931 1 0 09:30 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Tudo come√ßou com sucesso. <br><br>  Foi assim que, por tentativa e erro, descobri como configurar e iniciar a replica√ß√£o corretamente. <br><br>  Como resultado, a seguinte configura√ß√£o foi montada: <br><br>  <i>2 servidores.</i> <i><br></i>  <i>2 mestre.</i>  <i>Reserva quente.</i> <i><br></i>  <i>12 escravos.</i>  <i>Todos est√£o ativos.</i> <br><br>  Na l√≥gica do tarantool, o http.server foi usado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://github.com/tarantool/">para</a> n√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://github.com/tarantool/">bloquear o</a> adaptador adicional (lembre-se do fornecedor, plataforma e sab√£o) ou fixar a biblioteca a cada processo de neg√≥cios. <br><br>  Para evitar uma discrep√¢ncia entre os mestres, no balanceador (NetScaler, HAProxy ou qualquer outro favorito), definimos a regra de reserva, ou seja,  opera√ß√µes de inser√ß√£o, atualiza√ß√£o e exclus√£o v√£o apenas para o primeiro mestre ativo. <br><br>  Nesse momento, o segundo simplesmente replica os registros do primeiro.  Os pr√≥prios escravos s√£o conectados ao primeiro mestre especificado a partir da configura√ß√£o, que √© o que precisamos nesta situa√ß√£o. <br><br>  Em lua, implementamos opera√ß√µes CRUD para valor-chave.  No momento, isso √© suficiente para resolver o problema. <br><br>  Em vista dos recursos do trabalho com sab√£o, foi implementado um processo de neg√≥cios de proxy, no qual foi estabelecida a l√≥gica de trabalhar com uma tar√¢ntula via http. <br><br>  Se os dados principais estiverem presentes, eles ser√£o retornados imediatamente.  Caso contr√°rio, uma solicita√ß√£o √© enviada ao sistema mestre e armazenada no banco de dados Tarantool. <br><br>  Como resultado, um processo comercial nos testes processa at√© 4k solicita√ß√µes.  Nesse caso, o tempo de resposta da tar√¢ntula √© de ~ 1ms.  O tempo m√©dio de resposta √© de at√© 3 ms. <br><br>  Aqui est√£o algumas informa√ß√µes dos testes: <br><br><img src="https://habrastorage.org/webt/n6/ae/lg/n6aelg4tipin2jgomzrgsw_8nie.png"><br><br>  Havia 50 processos de neg√≥cios que v√£o para 4 sistemas principais e armazenam dados em cache em sua mem√≥ria.  Duplica√ß√£o de informa√ß√µes em pleno crescimento em cada inst√¢ncia.  Dado que o java j√° adora mem√≥ria ... a perspectiva n√£o √© a melhor. <br><br><h4>  Agora </h4><br>  50 processos de neg√≥cios solicitam informa√ß√µes atrav√©s do cache.  Agora, as informa√ß√µes de 4 inst√¢ncias do assistente s√£o armazenadas em um local e n√£o s√£o armazenadas em cache na mem√≥ria em cada inst√¢ncia.  Foi poss√≠vel reduzir significativamente a carga no sistema mestre, n√£o h√° duplicatas de informa√ß√µes e o consumo de mem√≥ria nas inst√¢ncias com l√≥gica de neg√≥cios diminuiu. <br><br>  Um exemplo do tamanho do armazenamento de informa√ß√µes na mem√≥ria da tar√¢ntula: <br><br><img src="https://habrastorage.org/webt/es/93/ex/es93exozhrhnihbnq6-zpzjobma.png"><br><br>  No final do dia, esses n√∫meros podem dobrar, mas n√£o h√° "rebaixamento" no desempenho. <br><br>  Na batalha, a vers√£o atual cria solicita√ß√µes de 2 a 2,5k por segundo de carga real.  O tempo m√©dio de resposta √© semelhante a testes de at√© 3ms. <br><br>  Se voc√™ olhar o htop em um dos servidores com tarantool, veremos que eles est√£o "esfriando": <br><br><img src="https://habrastorage.org/webt/jt/mt/vf/jtmtvfat0ohxhugounnve47l7ju.png"><br><br><h4>  Sum√°rio </h4><br>  Apesar de todas as sutilezas e nuances do banco de dados Tarantool, voc√™ pode obter um √≥timo desempenho. <br><br>  Espero que este projeto se desenvolva e que esses momentos desconfort√°veis ‚Äã‚Äãsejam resolvidos. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt441686/">https://habr.com/ru/post/pt441686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt441676/index.html">Como fazer amigos PLUTO e HDSDR</a></li>
<li><a href="../pt441678/index.html">F√≠sica dos tornados de jogos: como a aerodin√¢mica √© implementada em Just Cause 4 (tr√°fego)</a></li>
<li><a href="../pt441680/index.html">Programa de confer√™ncias Lua In Moscow 2019</a></li>
<li><a href="../pt441682/index.html">HyperX Fury 3D - SSD com um pedigree claro</a></li>
<li><a href="../pt441684/index.html">Previs√µes: as nuvens mudar√£o 2019</a></li>
<li><a href="../pt441688/index.html">Jogos mudam o mundo: como Hellblade chama a aten√ß√£o para os problemas das pessoas com doen√ßas mentais</a></li>
<li><a href="../pt441690/index.html">Voc√™ n√£o precisa de Blockchain: oito casos de uso conhecidos e por que eles n√£o funcionam</a></li>
<li><a href="../pt441692/index.html">Como cobrir as faixas na blockchain? Nosso conceito de misturador de transa√ß√µes</a></li>
<li><a href="../pt441694/index.html">Por que os gr√°ficos de tr√°fego "mentem"</a></li>
<li><a href="../pt441696/index.html">A hist√≥ria do LiveJournal cir√≠lico: como a administra√ß√£o russa esmagou a ascens√£o dos blogs em l√≠ngua russa</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>