<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍🎨 📸 🐼 Evolusi Infrastruktur Basis Data: Dari Basis Data dan Aplikasi pada Satu Server hingga Streaming Replikasi ☑️ 🏇🏻 💡</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! 

 Nama saya Anton Markelov, saya seorang insinyur ops di United Traders. Kami terlibat dalam proyek dengan satu atau lain cara yang terka...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Evolusi Infrastruktur Basis Data: Dari Basis Data dan Aplikasi pada Satu Server hingga Streaming Replikasi</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/utex/blog/443530/"><img src="https://habrastorage.org/webt/9o/uy/bk/9ouybky85gzjccvemkc882t32ws.png"><br><br>  Halo, Habr! <br><br>  Nama saya Anton Markelov, saya seorang insinyur ops di United Traders.  Kami terlibat dalam proyek dengan satu atau lain cara yang terkait dengan investasi, pertukaran dan masalah keuangan lainnya.  Kami bukan perusahaan yang sangat besar, sekitar 30 insinyur pengembangan, timbangannya sesuai - sedikit kurang dari seratus server.  Selama pertumbuhan kuantitatif dan kualitatif infrastruktur kami, solusi klasik “kami menyimpan aplikasi dan databasenya di server yang sama” tidak lagi sesuai dengan kami dalam hal keandalan dan kecepatan.  Pada bagian analis, ada kebutuhan untuk membuat pertanyaan lintas-basis data, departemen operasi sudah bosan bermain-main dengan cadangan dan memantau sejumlah besar server database.  Selain itu, menyimpan status pada mesin yang sama dengan aplikasi itu sendiri sangat mengurangi fleksibilitas perencanaan sumber daya dan ketahanan infrastruktur. <br><br>  Proses transisi ke arsitektur saat ini adalah evolusi, berbagai solusi diuji baik untuk menyediakan antarmuka yang nyaman bagi pengembang dan analis, dan untuk meningkatkan keandalan dan pengelolaan seluruh ekonomi ini.  Saya ingin berbicara tentang tahap-tahap utama modernisasi DBMS kami, apa yang telah kami lakukan dan keputusan apa yang kami ambil, sebagai akibatnya, lingkungan independen yang toleran terhadap kesalahan yang menyediakan cara interaksi yang nyaman bagi para insinyur operasi, pengembang dan analis.  Saya berharap pengalaman kami akan bermanfaat bagi para insinyur dari perusahaan skala kami. <br><br>  Artikel ini adalah ringkasan dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">laporan</a> saya di konferensi UPTIMEDAY, mungkin format video akan lebih nyaman bagi seseorang, meskipun penulisnya sedikit lebih baik dengan tangan saya daripada pembicara mulut. <br><br>  "Manusia Kepingan Salju" dengan KDPV <i>dipinjam</i> tanpa malu-malu dari Maxim Dorofeev. <br><a name="habracut"></a><br><h2>  Penyakit pertumbuhan </h2><br>  Kami memiliki arsitektur microservice, layanan ditulis terutama di Jawa atau Kotlin menggunakan kerangka Spring.  Di samping setiap microservice adalah basis PostgreSQL, semuanya ditutupi oleh nginx di atas untuk menyediakan akses.  Layanan microser tipikal adalah aplikasi pada Boot Spring yang menulis datanya ke PostgreSQL (bagian dari aplikasi pada saat yang sama dan untuk ClickHouse), berkomunikasi dengan tetangga melalui Kafka dan memiliki beberapa titik akhir REST atau GraphQL untuk komunikasi dengan dunia luar. <br><br><img src="https://habrastorage.org/webt/43/s1/aq/43s1aq-9ggmakbk3dygl8ox9bna.png"><br><br>  Sebelumnya, ketika kami masih sangat kecil, kami hanya menyimpan beberapa server di DigitalOcean, Kafka belum ada di sana, semua komunikasi dilakukan melalui REST.  Artinya, kami mengambil tetesan, menginstal Java, PostgreSQL, nginx di sana, meluncurkan Zabbix di sana sehingga memonitor sumber daya server dan ketersediaan titik akhir layanan.  Mereka mengerahkan segala sesuatu dengan bantuan Ansible, kami memiliki buku pedoman standar, empat hingga lima peran diluncurkan seluruh layanan.  Selama kami memiliki, secara relatif, 6 server pada produksi dan 3 pada tes - Anda entah bagaimana bisa hidup dengannya. <br>  Kemudian fase pengembangan aktif dimulai, jumlah aplikasi bertambah, sepuluh layanan Microsoft berubah menjadi empat puluh, fungsionalitasnya mulai berubah, ditambah integrasi dengan sistem eksternal seperti CRM, situs klien dan sejenisnya muncul.  Kami merasakan sakit pertama.  Beberapa aplikasi mulai mengkonsumsi lebih banyak sumber daya, berhenti masuk ke server yang ada, kami mendapat tetesan, menyeret aplikasi bolak-balik, mengambil banyak tangan.  Sangat menyakitkan - tidak ada yang suka pekerjaan mekanik bodoh, - Saya ingin memutuskan dengan cepat.  Jadi kami langsung saja - kami hanya mengambil 3 server khusus yang besar alih-alih 10 tetesan awan.  Ini menutup masalah untuk sementara waktu, tetapi menjadi jelas bahwa sudah waktunya untuk mencari opsi untuk beberapa jenis orkestrasi dan penyeimbangan ulang server.  Kami mulai mencermati solusi seperti DC / OS dan Kubernetes, dan secara bertahap meningkatkan keahlian kami di bidang ini. <br><br>  Sekitar waktu yang sama, kami memiliki departemen analitis, yang perlu secara teratur membuat permintaan yang sulit, menyiapkan laporan, memiliki dasbor yang indah, dan ini membuat kami kesakitan kedua.  Pertama, analis banyak memuat pangkalan, dan kedua, mereka membutuhkan pertanyaan lintas basis data, karena  setiap microservice menyimpan irisan data yang agak sempit.  Kami menguji beberapa sistem, pada awalnya kami mencoba menyelesaikannya semua melalui replikasi tingkat-tabel (kembali ke PostgreSQL kesembilan, tidak ada replikasi logis di luar kotak), tetapi kerajinan yang dihasilkan berdasarkan pglogical, Presto, Slony-I dan Bucardo sepenuhnya tidak diatur.  Sebagai contoh, pglogical tidak mendukung migrasi - versi baru dari microservice diluncurkan, struktur database berubah, Java sendiri mengubah struktur menggunakan Flyway, dan pada replika di pglogical semuanya perlu diubah secara manual.  Kalau tidak, ada sesuatu yang hilang, atau itu terlalu sulit. <br><br><h2>  Budak super </h2><br>  Sebagai hasil dari penelitian, lahirlah solusi sederhana dan brutal yang disebut Superslave: kami mengambil server terpisah, mengkonfigurasinya menjadi budak untuk setiap server produksi pada port yang berbeda, dan menciptakan basis data virtual yang menggabungkan basis data dari para budak melalui postgres_fdw (pembungkus data asing).  Yaitu, semua ini dilaksanakan dengan cara standar postgres tanpa memperkenalkan entitas tambahan, secara sederhana dan andal: dengan satu permintaan, dimungkinkan untuk mendapatkan data dari beberapa basis data.  Kami memberikan crossbase virtual ini kepada analis.  Nilai tambah tambahan adalah bahwa replika hanya-baca, bahkan dengan kesalahan dengan hak akses, tidak dapat menulis apa pun di sana. <br><br><img src="https://habrastorage.org/webt/uf/j7/2i/ufj72i581hu3b_s5cep3cy_dk60.png"><br><br>  Kami mengambil <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Redash</a> untuk visualisasi, ia tahu cara menggambar grafik, menjalankan permintaan sesuai jadwal, misalnya, sekali sehari, dan memiliki sistem hak yang berat, jadi kami membiarkan analis dan pengembang pergi ke sana. <br><br><img src="https://habrastorage.org/webt/wq/jo/gj/wqjogjc5ovjcfxz9b6nqa_g71nm.png"><br><br>  Secara paralel, pertumbuhan terus berlanjut, Kafka muncul di infrastruktur sebagai bus dan ClickHouse untuk penyimpanan analisis.  Mereka mudah berkerumun di luar kotak, budak super kami dengan latar belakang mereka tampak seperti fosil canggung.  Plus, PostgreSQL, pada kenyataannya, tetap menjadi satu-satunya negara yang perlu diseret setelah aplikasi (jika masih harus ditransfer ke server lain), dan kami benar-benar ingin mendapatkan aplikasi stateless untuk terlibat dalam eksperimen dengan Kubernetes dan dia platform serupa. <br><br>  Kami mulai mencari solusi yang memenuhi persyaratan berikut: <br><br><ul><li>  toleransi kesalahan: ketika N server jatuh, gugus terus bekerja; </li><li>  untuk aplikasi, semuanya harus tetap seperti sebelumnya, tidak ada perubahan dalam kode; </li><li>  kemudahan penyebaran dan manajemen; </li><li>  lebih sedikit lapisan abstraksi di atas PostgreSQL biasa; </li><li>  idealnya, memuat penyeimbangan agar tidak semua permintaan masuk ke satu server; </li><li>  Idealnya, ini ditulis dalam bahasa yang akrab. </li></ul><br>  Tidak banyak kandidat: <br><br><ul><li>  replikasi streaming standar (repmgr, Patroni, Stolon); </li><li>  replikasi berbasis pemicu (Londiste, Slony); </li><li>  replikasi kueri lapisan tengah (pgpool-II); </li><li>  replikasi sinkron dengan beberapa server inti (Bucardo). </li></ul><br>  Dengan sebagian besar, kami sudah memiliki pengalaman buruk selama pembangunan crossbase, sehingga Patroni dan Stolon tetap.  Patroni ditulis dalam Python, Stolon in Go, kami memiliki keahlian yang cukup dalam kedua bahasa.  Selain itu, mereka memiliki arsitektur dan fungsionalitas yang serupa, sehingga pilihan dibuat untuk alasan subyektif: Patroni dikembangkan oleh Zalando, dan kami pernah mencoba untuk bekerja dengan proyek Nakadi mereka (API REST untuk Kafka), di mana kami menemukan kurangnya dokumentasi. <br><br><h2>  Stolon </h2><br><img src="https://habrastorage.org/webt/z5/af/hf/z5afhfobk43vpt0x2dmryf0gt50.png"><br><br>  Arsitektur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Stolon</a> cukup sederhana: ada N server, dengan bantuan etcd / consul pemimpin dipilih, PostgreSQL diluncurkan di dalamnya dalam mode wizard dan direplikasi ke server lain.  Kemudian proksi stolon pergi ke master PostgreSQL ini, berpura-pura menjadi aplikasi dengan postgres biasa, dan klien pergi ke proksi ini.  Dalam hal hilangnya master, pemilihan ulang dilakukan, orang lain menjadi master, sisanya menjadi siaga.  Ada beberapa lapisan abstraksi, PostgreSQL diinstal seperti biasa, satu-satunya peringatan adalah bahwa konfigurasi PostgreSQL disimpan di etcd, dan dikonfigurasi agak berbeda. <br><br>  Saat menguji cluster, kami menemukan beberapa masalah: <br><br><ul><li>  Stolon tidak tahu cara bekerja di atas ZooKeeper, hanya konsul atau etcd; </li><li>  etcd sangat sensitif terhadap IO.  Jika Anda menyimpan PostgreSQL dan etcd di server yang sama, Anda pasti membutuhkan SSD cepat; </li><li>  bahkan pada SSD perlu untuk mengkonfigurasi timeout etcd, jika tidak semuanya akan rusak di bawah beban - cluster akan berpikir bahwa master telah jatuh dan terus-menerus memutuskan koneksi; </li><li>  Secara default, max_connections pada PostgreSQL kecil (200), Anda perlu meningkatkannya sesuai kebutuhan Anda; </li><li>  sekelompok tiga etcd akan selamat dari kematian hanya satu server, idealnya Anda perlu memiliki konfigurasi, misalnya 5 etcd + 3 Stolon; </li><li>  di luar kotak, semua koneksi menuju ke master, budak tidak dapat diakses ke koneksi. </li></ul><br>  Karena semua koneksi ke PostgreSQL masuk ke wizard, kami kembali mengalami masalah dengan permintaan analitik yang berat.  etcd kadang-kadang dengan susah payah bereaksi terhadap beban tinggi pada master dan mengubahnya.  Dan beralih wizard selalu memutus koneksi.  Permintaan telah dimulai kembali, semuanya dimulai dari awal lagi.  Untuk solusinya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sebuah skrip Python</a> ditulis yang meminta alamat stolonctl dari budak hidup dan menghasilkan konfigurasi untuk HAProxy, mengarahkan permintaan kepada mereka. <br><br><img src="https://habrastorage.org/webt/k6/er/ds/k6erds-_gyudfmyruuzpmw0oudm.png"><br><br>  Gambaran berikut ternyata: permintaan dari aplikasi pergi ke porta stolon-proxy, yang mengarahkan mereka ke master, dan permintaan dari analis (mereka selalu hanya-baca) pergi ke port HAProxy, yang melemparkannya ke beberapa budak. <br><br>  Juga, secara harfiah hari ini, PR diadopsi di Stolon, yang memungkinkan pengiriman informasi tentang contoh Stolon ke penemuan layanan pihak ketiga. <br><br><img src="https://habrastorage.org/webt/o-/al/k_/o-alk_xqmlva498z1yav5nbooze.png"><br><br>  Sejauh dilihat dari metrik kecepatan respons aplikasi, transisi ke cluster jarak jauh tidak memiliki dampak signifikan pada kinerja, waktu respons rata-rata tidak berubah.  Latensi jaringan yang dihasilkan, tampaknya, dikompensasi oleh fakta bahwa database sekarang berada di server khusus. <br><br>  Stolon tanpa masalah selamat dari penyihir crash (kehilangan server, kehilangan jaringan, kehilangan disk), ketika server menjadi hidup - secara otomatis me-reset replika.  Titik terlemah di Stolon adalah etcd, kegagalan di dalamnya menempatkan cluster.  Kami mengalami kecelakaan yang khas: sekelompok tiga node dll, dua ditebang.  Semuanya, kuorumnya rusak, dll berubah menjadi tidak sehat, gugus Stolon tidak menerima koneksi apa pun, termasuk permintaan dari stolonctl.  Skema pemulihan: ubah etcd di server yang masih hidup menjadi satu simpul tunggal, kemudian tambahkan anggota kembali.  Kesimpulan: untuk bertahan dari kematian dua server, Anda harus memiliki setidaknya 5 instance etcd. <br><br><h2>  Memantau dan menangkap kesalahan </h2><br>  Dengan pertumbuhan infrastruktur dan kompleksitas layanan-layanan mikro, saya ingin mengumpulkan lebih banyak informasi tentang apa yang terjadi di dalam aplikasi dan mesin Java.  Kami tidak dapat mengadaptasi Zabbix dengan lingkungan baru: sangat tidak nyaman dalam kondisi infrastruktur yang berubah.  Saya harus menggiling kruk melalui API-nya, atau naik ke dalam dengan tangan saya, yang bahkan lebih buruk.  Basis datanya kurang beradaptasi dengan beban berat, dan secara umum sangat tidak nyaman untuk memasukkan semua ini ke dalam basis data relasional. <br><br>  Akibatnya, kami memilih Prometheus untuk pemantauan.  Dia memiliki aktuator di luar kotak untuk aplikasi Spring untuk menyediakan metrik, untuk Kafka mereka mengacaukan JMX Eksportir, yang juga menyediakan metrik dengan cara yang nyaman.  Para eksportir yang tidak ditemukan "di dalam kotak", kami menulis sendiri dengan Python, ada sekitar sepuluh dari mereka.  Kami memvisualisasikan Grafana, mengumpulkan log dengan Graylog (karena dia sekarang mendukung Beats). <br><br>  Kami menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Sentry</a> untuk mengumpulkan kesalahan.  Dia menulis semuanya dalam bentuk terstruktur, menggambar grafik, menunjukkan apa yang terjadi lebih sering, lebih jarang.  Biasanya, pengembang segera pergi ke Sentry segera setelah penyebaran, melihat apakah ada puncak, atau sangat perlu digulung kembali.  Ternyata dengan cepat menangkap kesalahan tanpa mengambil log. <br><br>  Itu saja untuk saat ini, jika format artikel sesuai dengan pembaca, kami akan terus berbicara tentang infrastruktur kami lebih lanjut, masih ada banyak kesenangan: Kafka dan solusi analitik untuk peristiwa yang melewatinya, saluran CI / CD untuk aplikasi Windows dan petualangan dengan Openshift. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id443530/">https://habr.com/ru/post/id443530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id443520/index.html">Memilih mobil untuk spesialis IT, atau tips untuk poci teh dari poci teh</a></li>
<li><a href="../id443522/index.html">Hosting: opsi, perbandingan, statistik pengguna</a></li>
<li><a href="../id443524/index.html">Animasi Flash mandiri di Unity3D. Bagian Satu, Liris</a></li>
<li><a href="../id443526/index.html">Dari Algoritma ke Kanker: Ceramah dari School of Bioinformatics</a></li>
<li><a href="../id443528/index.html">Amazon merilis Open Distro untuk Elasticsearch</a></li>
<li><a href="../id443532/index.html">5 fitur serbuk logam untuk pencetakan 3D</a></li>
<li><a href="../id443534/index.html">Compute Express Link - Interkoneksi untuk Data Besar</a></li>
<li><a href="../id443542/index.html">Titik Periksa Gratis Memulai Kursus Gratis R80.20</a></li>
<li><a href="../id443544/index.html">Memory Profileing pada STM32 dan Mikrokontroler Lainnya: Analisis Ukuran Stack Statis</a></li>
<li><a href="../id443546/index.html">Toyota dan JAXA berencana untuk memiliki penjelajah berawak di bulan pada tahun 2029</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>