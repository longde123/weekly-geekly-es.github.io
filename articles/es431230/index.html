<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äçüíª üöú üõë Almacenamiento para infraestructura HPC, o c√≥mo recolectamos 65 PB de almacenamiento en el Centro de Investigaci√≥n RIKEN Jap√≥n üåÖ üë®‚Äç‚öñÔ∏è üèÑ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="datacenterknowledge.com 

 El a√±o pasado, se implement√≥ la instalaci√≥n de almacenamiento basada en RAIDIX m√°s grande en este momento. Se implement√≥ un...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Almacenamiento para infraestructura HPC, o c√≥mo recolectamos 65 PB de almacenamiento en el Centro de Investigaci√≥n RIKEN Jap√≥n</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/raidix/blog/431230/"><img src="https://habrastorage.org/webt/gt/hj/ib/gthjibaqdw82ss1jxo2tmhpa97o.jpeg"><br>  <i><font color="#99999">datacenterknowledge.com</font></i> <br><br>  El a√±o pasado, se implement√≥ la instalaci√≥n de almacenamiento basada en RAIDIX m√°s grande en este momento.  Se implement√≥ un sistema de 11 cl√∫steres de conmutaci√≥n por error en el Instituto RIKEN de Ciencias de la Computaci√≥n (Jap√≥n).  El objetivo principal del sistema es el HPC Infrastructure Storage (HPCI), que se implementa como parte del proyecto de intercambio de informaci√≥n de intercambio acad√©mico a gran escala Academic Cloud (basado en la red SINET). <br><br>  Una caracter√≠stica importante de este proyecto es su volumen total de 65 PB, de los cuales el volumen utilizable del sistema es 51.4 PB.  Para comprender mejor este valor, agregamos que se trata de 6512 discos de 10 TB cada uno (el m√°s moderno en el momento de la instalaci√≥n).  Esto es mucho <br><a name="habracut"></a><br>  El trabajo en el proyecto continu√≥ durante todo el a√±o, despu√©s de lo cual el monitoreo de la estabilidad del sistema continu√≥ durante aproximadamente un a√±o.  Los indicadores obtenidos cumplieron con los requisitos establecidos, y ahora podemos hablar sobre el √©xito de este registro y un proyecto significativo para nosotros. <br><br><h2>  Supercomputadora en el Centro de Computaci√≥n del Instituto RIKEN </h2><br>  Para la industria de las TIC, el Instituto RIKEN es conocido principalmente por su legendaria "computadora K" (del japon√©s "kei", que significa 10 billones), que en el momento del lanzamiento (junio de 2011) era considerada la supercomputadora m√°s poderosa del mundo. <br><br><div class="spoiler">  <b class="spoiler_title">Lea sobre la computadora K</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">www.nytimes.com/2011/06/20/technology/20computer.html</a> <br></div></div><br>  La supercomputadora ayuda al Centro de Ciencias Computacionales en la implementaci√≥n de investigaciones complejas a gran escala: permite modelar el clima, las condiciones clim√°ticas y el comportamiento molecular, calcular y analizar reacciones en f√≠sica nuclear, predicci√≥n de terremotos y mucho m√°s.  Las capacidades de la supercomputadora tambi√©n se utilizan para una investigaci√≥n m√°s "diaria" y aplicada, para buscar campos petroleros y pronosticar tendencias en los mercados burs√°tiles. <br><br>  Dichos c√°lculos y experimentos generan una gran cantidad de datos, cuyo valor y significado no pueden ser sobreestimados.  Para aprovechar al m√°ximo esto, los cient√≠ficos japoneses han desarrollado un concepto para un √∫nico espacio de informaci√≥n en el que los profesionales de HPC de diferentes centros de investigaci√≥n tendr√°n acceso a los recursos de HPC recibidos. <br><br><h2>  Infraestructura inform√°tica de alto rendimiento (HPCI) </h2><br>  HPCI opera sobre la base de SINET (The Science Information Network), una red troncal para el intercambio de datos cient√≠ficos entre universidades y centros de investigaci√≥n japoneses.  Actualmente, SINET re√∫ne a unos 850 institutos y universidades, creando enormes oportunidades para el intercambio de informaci√≥n en investigaciones que afectan la f√≠sica nuclear, la astronom√≠a, la geodesia, la sismolog√≠a y la inform√°tica. <br><br>  HPCI es un proyecto de infraestructura √∫nico que forma un sistema unificado de intercambio de informaci√≥n en el campo de la inform√°tica de alto rendimiento entre universidades y centros de investigaci√≥n en Jap√≥n. <br><br>  Al combinar las capacidades de la supercomputadora "K" y otros centros de investigaci√≥n en una forma accesible, la comunidad cient√≠fica recibe beneficios obvios por trabajar con datos valiosos creados por la inform√°tica de la supercomputadora. <br><br>  Con el fin de proporcionar un acceso efectivo de los usuarios conjuntos al entorno HPCI, se impusieron altos requisitos de almacenamiento para la velocidad de acceso.  Y gracias a la "hiperproductividad" de la computadora K, se calcul√≥ que el cl√∫ster de almacenamiento en el Centro de Ciencias Computacionales del Instituto RIKEN se cre√≥ con un volumen de trabajo de al menos 50 PB. <br><br>  La infraestructura del proyecto HPCI se construy√≥ sobre la base del sistema de archivos Gfarm, que permiti√≥ proporcionar un alto nivel de rendimiento y combinar grupos de almacenamiento dispares en un solo espacio compartido. <br><br><h2>  Sistema de archivos Gfarm </h2><br>  Gfarm es un sistema de archivos distribuidos de c√≥digo abierto desarrollado por ingenieros japoneses.  Gfarm es el fruto del desarrollo del Instituto de Ciencia y Tecnolog√≠a Industrial Avanzada (AIST), y el nombre del sistema se refiere a la arquitectura utilizada por Grid Data Farm. <br><br>  Este sistema de archivos combina una serie de propiedades aparentemente incompatibles: <br><br><ul><li>  Alta escalabilidad en volumen y rendimiento. </li><li>  Distribuci√≥n de redes de larga distancia con soporte para un solo espacio de nombres para varios centros de investigaci√≥n diversos. </li><li>  Soporte de API POSIX </li><li>  Alto rendimiento requerido para computaci√≥n paralela </li><li>  Seguridad de almacenamiento de datos </li></ul><br>  Gfarm crea un sistema de archivos virtual utilizando recursos de almacenamiento de m√∫ltiples servidores.  El servidor de metadatos distribuye los datos y el esquema de distribuci√≥n en s√≠ est√° oculto para los usuarios.  Debo decir que Gfarm consiste no solo en un cl√∫ster de almacenamiento, sino tambi√©n en una cuadr√≠cula computacional que utiliza los recursos de los mismos servidores.  El principio de funcionamiento del sistema se parece a Hadoop: el trabajo enviado se "baja" al nodo donde se encuentran los datos. <br><br>  La arquitectura del sistema de archivos es asim√©trica.  Los roles est√°n claramente asignados: Servidor de almacenamiento, Servidor de metadatos, Cliente.  Pero al mismo tiempo, los tres roles pueden ser realizados por la misma m√°quina.  Los servidores de almacenamiento almacenan muchas copias de archivos y los servidores de metadatos funcionan en modo maestro-esclavo. <br><br><h2>  Proyecto de trabajo </h2><br>  Core Micro Systems, un socio estrat√©gico y proveedor exclusivo de RAIDIX en Jap√≥n, implement√≥ la implementaci√≥n en el Instituto RIKEN del Centro de Ciencias de la Computaci√≥n.  Para implementar el proyecto, se necesitaron unos 12 meses de trabajo minucioso, en el que no solo los empleados de Core Micro Systems, sino tambi√©n los especialistas t√©cnicos del equipo de Reydix tomaron parte activa. <br><br>  Al mismo tiempo, la transici√≥n a otro sistema de almacenamiento parec√≠a poco probable: el sistema existente ten√≠a muchos enlaces t√©cnicos que complicaban la transici√≥n a cualquier nueva marca. <br><br>  Durante largas pruebas, comprobaciones y mejoras, RAIDIX ha demostrado un alto rendimiento y una eficiencia consistentes al trabajar con una cantidad de datos tan impresionante. <br><br>  Sobre las mejoras vale la pena contar un poco m√°s.  Era necesario no solo crear la integraci√≥n de los sistemas de almacenamiento con el sistema de archivos Gfarm, sino tambi√©n expandir algunas caracter√≠sticas funcionales del software.  Por ejemplo, para cumplir con los requisitos establecidos de las especificaciones t√©cnicas, era necesario desarrollar e implementar la tecnolog√≠a de escritura autom√°tica lo antes posible. <br><br>  El despliegue del sistema en s√≠ fue sistem√°tico.  Los ingenieros de Core Micro Systems realizaron con cuidado y precisi√≥n cada etapa de la prueba, aumentando gradualmente la escala del sistema. <br><br>  En agosto de 2017, la primera fase de implementaci√≥n se complet√≥ cuando el volumen del sistema alcanz√≥ 18 PB.  En octubre del mismo a√±o, se implement√≥ la segunda fase, en la cual el volumen aument√≥ a un r√©cord de 51 PB. <br><br><h2>  Arquitectura de soluciones </h2><br>  La soluci√≥n se cre√≥ a trav√©s de la integraci√≥n de los sistemas de almacenamiento RAIDIX y el sistema de archivos distribuido Gfarm.  En conjunto con Gfarm, la capacidad de crear almacenamiento escalable usando 11 sistemas RAIDIX de doble controlador. <br><br>  La conexi√≥n a los servidores de Gfarm se realiza a trav√©s de 8 x SAS 12G. <br><br><img src="https://habrastorage.org/webt/ii/3x/wk/ii3xwkawuyvwht0pczwonxbumnu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">1. Imagen de un cl√∫ster con un servidor de datos separado para cada nodo</font></i> <br><br>  (1) conexiones de malla SAN de 48 Gbps √ó 8;  ancho de banda: 384 Gbps <br>  (2) conexiones de TELA de malla de 48 Gbps √ó 40;  ancho de banda: 1920Gbps <br><br><h3>  Configuraci√≥n de plataforma de controlador dual </h3><br><table><tbody><tr><td>  CPU </td><td>  Intel Xeon E5-2637 - 4 piezas </td></tr><tr><td>  Placa base </td><td>  Compatible con el modelo de procesador compatible con PCI Express 3.0 x8 / x16 </td></tr><tr><td>  Cach√© interna </td><td>  256 GB para cada nodo </td></tr><tr><td>  Chasis </td><td>  2U </td></tr><tr><td>  Controladores SAS para conectar estanter√≠as de discos, servidores y sincronizaci√≥n de cach√© de escritura </td><td>  Broadcom 9305 16e, 9300 8e </td></tr><tr><td>  HDD </td><td>  HGST Helium 10TB SAS HDD </td></tr><tr><td>  Sincronizaci√≥n de latidos </td><td>  Ethernet 1 GbE </td></tr><tr><td>  CacheSync Sync </td><td>  6 x SAS 12G </td></tr></tbody></table><br>  Ambos nodos del cl√∫ster de conmutaci√≥n por error est√°n conectados a 10 JBOD (60 discos de 10 TB cada uno) a trav√©s de 20 puertos SAS 12G para cada nodo.  En estos estantes de disco, se crearon 58 matrices RAID6 de 10 TB (8 discos de datos (D) + 2 discos de paridad (P)) y se asignaron 12 discos para ‚Äúintercambio en caliente‚Äù. <br><br>  10 JBOD =&gt; 58 √ó RAID6 (8 discos de datos (D) + 2 discos de paridad (P)), LUN de 580 HDD + 12 HDD para ‚Äúintercambio en caliente‚Äù (2.06% del volumen total) <br><br>  592 HDD (10TB SAS / 7.2k HDD) por cluster * HDD: HGST (MTBF: 2 500 000 horas) <br><br><img src="https://habrastorage.org/webt/qv/vc/0e/qvvc0egbrc1txvsztmx3o6yl918.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">2. Cl√∫ster de conmutaci√≥n por error con diagrama de conexi√≥n 10 JBOD</font></i> <br><br><h3>  Sistema general y diagrama de conexi√≥n </h3><br><img src="https://habrastorage.org/webt/6m/ju/xv/6mjuxvlhx5zlcnqb1eopx9nknfu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">3. Imagen de un solo cl√∫ster dentro del sistema HPCI</font></i> <br><br><h2>  Indicadores clave del proyecto </h2><br><blockquote>  Capacidad utilizable por cl√∫ster: <b>4.64 PB</b> ((RAID6 / 8D + 2P) LUN √ó 58) <br><br>  La capacidad √∫til total de todo el sistema: <b>51.04 PB</b> (4.64 PB √ó 11 grupos). <br><br>  Capacidad total del sistema: <b>65 PB</b> . <br><br>  El rendimiento del sistema fue: <b>17 GB / s</b> para escritura, <b>22 GB / s</b> para lectura. <br><br>  El rendimiento total del subsistema de disco del cl√∫ster en 11 sistemas de almacenamiento RAIDIX: <b>250 GB / s</b> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es431230/">https://habr.com/ru/post/es431230/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es431218/index.html">C√≥mo hice un juego de c√≥mics de Lovecraft</a></li>
<li><a href="../es431220/index.html">La mirada de un bi√≥logo a las ra√≠ces de nuestro envejecimiento.</a></li>
<li><a href="../es431222/index.html">Archivo de sitios web</a></li>
<li><a href="../es431226/index.html">El juego Snake para FPGA Cyclone IV (con joystick VGA y SPI)</a></li>
<li><a href="../es431228/index.html">Obst√°culo para la luz: cristales l√≠quidos para ayudar</a></li>
<li><a href="../es431232/index.html">Generamos marcadores de posici√≥n SVG hermosos en Node.js</a></li>
<li><a href="../es431234/index.html">11 de diciembre, Mosc√∫ - Alfa JS MeetUp</a></li>
<li><a href="../es431236/index.html">C√≥mo escribir en Objective-C en 2018. Parte 1</a></li>
<li><a href="../es431238/index.html">El resumen de eventos para profesionales de recursos humanos en el campo de TI para diciembre de 2018</a></li>
<li><a href="../es431242/index.html">TLS y certificados web</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>