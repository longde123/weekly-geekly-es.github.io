<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçå üêµ üôÖüèª An√°lise de desempenho da VM no VMware vSphere. Parte 2: Mem√≥ria ü§π ü§ûüèø üï¥üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parte 1. Sobre a CPU 
 Parte 3. Sobre o armazenamento 

 Neste artigo, falaremos sobre os contadores de desempenho de RAM no vSphere. 
 Parece que a m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>An√°lise de desempenho da VM no VMware vSphere. Parte 2: Mem√≥ria</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dataline/blog/455820/"><img src="https://habrastorage.org/webt/el/am/7y/elam7yyhc6vrowmmrt5ofxgj-r0.png"><br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1. Sobre a CPU</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 3. Sobre o armazenamento</a> <br><br>  Neste artigo, falaremos sobre os contadores de desempenho de RAM no vSphere. <br>  Parece que a mem√≥ria √© cada vez mais inequ√≠voca do que com o processador: se houver problemas de desempenho na VM, √© dif√≠cil n√£o not√°-los.  Mas se eles aparecerem, lidar com eles √© muito mais dif√≠cil.  Mas as primeiras coisas primeiro. <a name="habracut"></a><br><br><h3>  Pouco de teoria </h3><br>  A RAM das m√°quinas virtuais √© obtida da mem√≥ria do servidor no qual as VMs est√£o sendo executadas.  Isso √© bastante √≥bvio :).  Se a RAM do servidor n√£o for suficiente para todos, o ESXi come√ßar√° a aplicar t√©cnicas de recupera√ß√£o de mem√≥ria.  Caso contr√°rio, os sistemas operacionais da VM travariam com erros de acesso √† RAM. <br><br>  Quais t√©cnicas de uso do ESXi decidem dependendo da carga de RAM: <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Status da mem√≥ria</b> <br></td><td>  <b>A fronteira</b> <br></td><td>  <b>Ac√ß√µes</b> <br></td></tr><tr><td>  Alta <br></td><td>  400% de minFree <br></td><td> Ap√≥s atingir o limite superior, grandes p√°ginas de mem√≥ria s√£o divididas em pequenas (o TPS funciona no modo padr√£o). <br></td></tr><tr><td>  Limpar <br></td><td>  100% de minFree <br></td><td>  P√°ginas grandes de mem√≥ria s√£o divididas em pequenas, o TPS funciona √† for√ßa. <br></td></tr><tr><td>  Suave <br></td><td>  64% de minFree <br></td><td>  TPS + bal√£o <br></td></tr><tr><td>  Dif√≠cil <br></td><td>  32% de min <br></td><td>  TPS + Compactar + Trocar <br></td></tr><tr><td>  Baixo <br></td><td>  16% de minFree <br></td><td>  Comprimir + Trocar + Bloquear <br></td></tr></tbody></table></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fonte</a> <br><br>  minFree √© a RAM necess√°ria para o hypervisor funcionar. <br><br>  Antes do ESXi 4.1, inclusive, o minFree era corrigido por padr√£o - 6% da RAM do servidor (a porcentagem podia ser alterada atrav√©s da op√ß√£o Mem.MinFreePct no ESXi).  Nas vers√µes posteriores, devido ao aumento dos volumes de mem√≥ria nos servidores minFree, ele come√ßou a ser calculado com base no tamanho da mem√≥ria do host e n√£o como um valor percentual fixo. <br><br>  O valor minFree (padr√£o) √© calculado da seguinte maneira: <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Porcentagem de mem√≥ria reservada para minFree</b> <br></td><td>  <b>Faixa de mem√≥ria</b> <br></td></tr><tr><td>  6% <br></td><td>  0-4 GB <br></td></tr><tr><td>  4% <br></td><td>  4-12 GB <br></td></tr><tr><td>  2% <br></td><td>  12-28 GB <br></td></tr><tr><td>  1% <br></td><td>  Mem√≥ria restante <br></td></tr></tbody></table></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fonte</a> <br><br>  Por exemplo, para um servidor com 128 GB de RAM, o valor MinFree seria: <br>  MinFree = 245,76 + 327,68 + 327,68 + 1024 = 1925,12 MB = 1,88 GB <br>  O valor real pode diferir em algumas centenas de MB, depende do servidor e da RAM. <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Porcentagem de mem√≥ria reservada para minFree</b> <br></td><td>  <b>Faixa de mem√≥ria</b> <br></td><td>  <b>Valor para 128 GB</b> <br></td></tr><tr><td>  6% <br></td><td>  0-4 GB <br></td><td>  245,76 MB <br></td></tr><tr><td>  4% <br></td><td>  4-12 GB <br></td><td>  327,68 MB <br></td></tr><tr><td>  2% <br></td><td>  12-28 GB <br></td><td>  327,68 MB <br></td></tr><tr><td>  1% <br></td><td>  Mem√≥ria restante (100 GB) <br></td><td>  1024 MB <br></td></tr></tbody></table></div><br><br>  Normalmente, para estandes produtivos, apenas Alto pode ser considerado normal.  Para bancadas de teste e desenvolvimento, condi√ß√µes Clear / Soft podem ser aceit√°veis.  Se houver menos de 64% de MinFree de RAM restante no host, as VMs em execu√ß√£o nele definitivamente enfrentar√£o problemas de desempenho. <br><br>  Em cada estado, certas t√©cnicas de recupera√ß√£o de mem√≥ria s√£o aplicadas come√ßando com o TPS, o que praticamente n√£o afeta o desempenho da VM, terminando com Swapping.  Vou falar mais sobre eles. <br><br>  <b>Compartilhamento de p√°gina transparente (TPS).</b>  O TPS √©, grosso modo, a desduplica√ß√£o das p√°ginas de RAM das m√°quinas virtuais em um servidor. <br><br>  O ESXi pesquisa por p√°ginas id√™nticas da RAM da m√°quina virtual, contando e comparando a soma de hash de p√°ginas e remove p√°ginas duplicadas, substituindo-as por links para a mesma p√°gina na mem√≥ria f√≠sica do servidor.  Como resultado, o consumo de mem√≥ria f√≠sica √© reduzido e √© poss√≠vel obter uma nova subscri√ß√£o de mem√≥ria com pouca ou nenhuma perda de desempenho. <br><br><img src="https://habrastorage.org/webt/ul/fz/1i/ulfz1i0bomyhsarceziylov-o6i.jpeg"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fonte</a> <br><br>  Este mecanismo funciona apenas para p√°ginas de 4 kB (p√°ginas pequenas).  P√°ginas com tamanho de 2 MB (p√°ginas grandes), o hipervisor nem tenta deduplicar: a chance de encontrar p√°ginas id√™nticas desse tamanho n√£o √© grande. <br><br>  Por padr√£o, o ESXi aloca mem√≥ria para p√°ginas grandes.  A quebra de p√°ginas grandes em pequenas come√ßa quando o limite do estado Alto √© atingido e √© for√ßado quando o estado Limpar √© atingido (consulte a tabela de estados do hypervisor). <br><br>  Se voc√™ deseja que o TPS comece a trabalhar sem aguardar o preenchimento da RAM do host, nas Op√ß√µes avan√ßadas ESXi, defina o valor <i>‚ÄúMem.AllocGuestLargePage‚Äù</i> como 0 (o padr√£o √© 1).  A aloca√ß√£o de p√°ginas grandes de mem√≥ria para m√°quinas virtuais ser√° desativada. <br><br>  Desde dezembro de 2014, em todas as vers√µes do ESXi, o TPS entre VMs foi desativado por padr√£o, pois foi encontrada uma vulnerabilidade que teoricamente permite acessar a RAM de outra VM a partir de uma VM.  Detalhes aqui.  Informa√ß√µes sobre a implementa√ß√£o pr√°tica da explora√ß√£o da vulnerabilidade do TPS que n√£o conheci. <br><br>  A pol√≠tica do TPS √© controlada pela op√ß√£o avan√ßada <i>‚ÄúMem.ShareForceSalting‚Äù</i> no ESXi: <br>  0 - TPS entre VMs.  O TPS funciona para p√°ginas de diferentes VMs; <br>  1 - TPS para VMs com o mesmo valor ‚Äúsched.mem.pshare.salt‚Äù no VMX; <br>  2 (padr√£o) - Intra-VM TPS.  O TPS funciona para p√°ginas dentro de uma VM. <br><br>  Definitivamente, faz sentido desativar p√°ginas grandes e ativar o Inter-VM TPS em bancos de teste.  Tamb√©m pode ser usado para estandes com um grande n√∫mero de VMs do mesmo tipo.  Por exemplo, em estandes com VDI, a economia de mem√≥ria f√≠sica pode atingir dezenas de por cento. <br><br>  <b>Bal√£o de mem√≥ria.</b>  O bal√£o n√£o √© mais uma t√©cnica t√£o inofensiva e transparente para o sistema operacional da VM como o TPS.  Mas, com o uso adequado do bal√£o, voc√™ pode viver e at√© trabalhar. <br><br>  Juntamente com o Vmware Tools, um driver especial √© instalado na VM, chamado Balloon Driver (tamb√©m conhecido como vmmemctl).  Quando o hipervisor come√ßa a ficar sem mem√≥ria f√≠sica e entra no estado Soft, o ESXi solicita √† VM que retorne a RAM n√£o utilizada atrav√©s desse driver de bal√£o.  O driver, por sua vez, trabalha no n√≠vel do sistema operacional e solicita mem√≥ria livre dele.  O hypervisor v√™ quais p√°ginas da mem√≥ria f√≠sica o Balloon Driver utilizou, pega a mem√≥ria da m√°quina virtual e a retorna ao host.  N√£o h√° problemas com a opera√ß√£o do sistema operacional, pois no n√≠vel do sistema operacional a mem√≥ria √© ocupada pelo driver do bal√£o.  Por padr√£o, o Balloon Driver pode ocupar at√© 65% da mem√≥ria da VM. <br><br>  Se o VMware Tools n√£o estiver instalado na VM ou o Ballooning estiver desativado (eu n√£o recomendo, mas h√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">KB</a> :), o hipervisor muda imediatamente para m√©todos mais rigorosos de remo√ß√£o de mem√≥ria.  Conclus√£o: verifique se o VMware Tools na VM est√°. <br><br><img src="https://habrastorage.org/webt/ey/pm/s1/eypms1ugdkmhr0r4odhga1locao.png"><br>  <i>A opera√ß√£o do driver do bal√£o pode ser verificada no sistema operacional via VMware Tools</i> . <br><br>  <b>Compress√£o de mem√≥ria</b>  Essa t√©cnica √© usada quando o ESXi atinge Hard.  Como o nome sugere, o ESXi est√° tentando compactar 4 KB de p√°ginas de RAM para 2 KB e, assim, liberar algum espa√ßo na mem√≥ria f√≠sica do servidor.  Essa t√©cnica aumenta significativamente o tempo de acesso ao conte√∫do das p√°ginas da mem√≥ria RAM da VM, pois a p√°gina deve ser limpa primeiro.  √Äs vezes, nem todas as p√°ginas podem ser compactadas e o processo em si leva algum tempo.  Portanto, essa t√©cnica n√£o √© muito eficaz na pr√°tica. <br><br>  <b>Troca de mem√≥ria.</b>  Ap√≥s uma fase curta, o Memory Compression ESXi quase inevitavelmente (se as VMs n√£o foram para outros hosts ou desligaram) passa para Swapping.  E se houver muito pouca mem√≥ria restante (estado baixo), o hipervisor tamb√©m interromper√° a aloca√ß√£o de p√°ginas de mem√≥ria da VM, o que pode causar problemas nas VMs convidadas. <br><br>  √â assim que o Swapping funciona.  Quando voc√™ liga a m√°quina virtual, um arquivo com a extens√£o .vswp √© criado para ela.  Em tamanho, √© igual √† RAM n√£o reservada da VM: esta √© a diferen√ßa entre a mem√≥ria configurada e a reservada.  Ao trabalhar com troca, o ESXi descarrega as p√°ginas de mem√≥ria da m√°quina virtual nesse arquivo e come√ßa a trabalhar com ela em vez da mem√≥ria f√≠sica do servidor.  Obviamente, essa mem√≥ria "RAM" √© v√°rias ordens de magnitude mais lenta que a mem√≥ria real, mesmo que .vswp esteja em armazenamento r√°pido. <br><br>  Ao contr√°rio do Balonismo, quando p√°ginas n√£o usadas s√£o selecionadas em uma VM, as p√°ginas que s√£o ativamente usadas pelo SO ou aplicativos dentro da VM podem ir para o disco durante a Troca.  Como resultado, o desempenho da VM diminui at√© travar.  A VM funciona formalmente e pelo menos pode ser desabilitada corretamente no sistema operacional.  Se voc√™ for paciente;) <br><br>  Se as VMs foram para Trocar, √© uma situa√ß√£o anormal que √© melhor evitar, se poss√≠vel. <br><br><h3>  Contadores b√°sicos de desempenho de mem√≥ria da m√°quina virtual </h3><br>  Ent√£o chegamos √† coisa principal.  Para monitorar o status da mem√≥ria na VM, os seguintes contadores est√£o dispon√≠veis: <br><br>  <b>Ativo</b> - mostra a quantidade de RAM (Kbytes) √† qual a VM obteve acesso no per√≠odo de medi√ß√£o anterior. <br><br>  <b>O uso</b> √© o mesmo que Ativo, mas como uma porcentagem da mem√≥ria da VM configurada.  √â calculado usando a seguinte f√≥rmula: ativo size tamanho da mem√≥ria configurada da m√°quina virtual. <br>  Alto uso e ativo, respectivamente, nem sempre s√£o indicativos de problemas de desempenho da VM.  Se uma VM usa agressivamente a mem√≥ria (pelo menos obt√©m acesso a ela), isso n√£o significa que n√£o h√° mem√≥ria suficiente.  Pelo contr√°rio, esta √© uma ocasi√£o para ver o que est√° acontecendo no sistema operacional. <br>  H√° um alarme padr√£o no uso de mem√≥ria para VMs: <br><br><img src="https://habrastorage.org/webt/8u/ca/n8/8ucan84mevajwvlnr-4ov9boyro.png"><br><br>  <b>Compartilhada</b> - a quantidade de RAM em uma VM deduplicada usando o TPS (dentro de uma VM ou entre VMs). <br><br>  <b>Concedido</b> - a quantidade de mem√≥ria f√≠sica do host (Kbytes) fornecida √† VM.  Inclui compartilhado. <br><br>  <b>Consumida</b> (concedida - compartilhada) - a quantidade de mem√≥ria f√≠sica (Kbytes) que a VM consome do host.  N√£o inclui compartilhado. <br><br>  Se parte da mem√≥ria da VM n√£o for alocada a partir da mem√≥ria f√≠sica do host, mas do arquivo de permuta ou a mem√≥ria tiver sido retirada da VM atrav√©s do Driver Balloon, esse valor n√£o ser√° levado em considera√ß√£o em Concedido e Consumido. <br>  Valores altos de Concedido e Consumido s√£o perfeitamente normais.  O sistema operacional gradualmente retira a mem√≥ria do hypervisor e n√£o retribui.  Com o tempo, com uma VM trabalhando ativamente, os valores desses contadores se aproximam da quantidade de mem√≥ria configurada e permanecem l√°. <br><br>  <b>Zero</b> - a quantidade de RAM na VM (Kbytes), que cont√©m zeros.  Essa mem√≥ria √© considerada hypervisor livre e pode ser fornecida a outras m√°quinas virtuais.  Depois que o SO convidado o recebeu, ele gravou algo na mem√≥ria nula, vai para Consumido e n√£o retorna. <br><br>  <b>Sobrecarga reservada</b> - a quantidade de RAM na VM (Kbytes) reservada pelo hypervisor para a VM funcionar.  Essa √© uma quantidade pequena, mas deve estar dispon√≠vel no host, caso contr√°rio, a VM n√£o ser√° iniciada. <br><br>  <b>Bal√£o</b> - a quantidade de RAM (KB) capturada da VM usando o Driver de bal√£o. <br><br>  <b>Compactado</b> - a quantidade de RAM (KB) que p√¥de ser compactada. <br><br>  <b>Trocado</b> - a quantidade de RAM (Kbytes) que, por falta de mem√≥ria f√≠sica no servidor, foi movida para o disco. <br>  Os contadores de bal√£o e outras t√©cnicas de recupera√ß√£o de mem√≥ria s√£o zero. <br><br>  √â assim que o gr√°fico se parece com os contadores de mem√≥ria de uma VM normalmente funcionando com 150 GB de RAM. <br><br><img src="https://habrastorage.org/webt/0l/pp/w3/0lppw3nz9iqzcnuergtxiseb67s.png"><br><br>  No gr√°fico abaixo, a VM tem problemas √≥bvios.  O gr√°fico mostra que para esta VM foram usadas todas as t√©cnicas descritas para trabalhar com RAM.  O bal√£o desta VM √© muito maior que o Consumido.  De fato, a VM provavelmente est√° mais morta do que viva. <br><br><img src="https://habrastorage.org/webt/f4/ic/xk/f4icxkpxpykxua_gp-sirlzuu_u.png"><br><br><h3>  ESXTOP </h3><br>  Assim como na CPU, se voc√™ deseja avaliar rapidamente a situa√ß√£o no host, bem como sua din√¢mica com um intervalo de at√© 2 segundos, vale a pena usar o ESXTOP. <br><br>  A tela ESXTOP Memory √© chamada com a tecla ‚Äúm‚Äù e se parece com isso (campos B, D, H, J, K, L, O selecionados): <br><br><img src="https://habrastorage.org/webt/rm/wj/4k/rmwj4krvvizdtcizkrid0zjuml8.png"><br><br>  Os seguintes par√¢metros ser√£o interessantes para n√≥s: <br><br>  <b>Mem overcommit avg</b> - o valor m√©dio da assinatura <b>excessiva</b> de mem√≥ria no host por 1, 5 e 15 minutos.  Se acima de zero, ent√£o √© uma ocasi√£o para ver o que acontece, mas nem sempre um indicador da presen√ßa de problemas. <br><br>  Nas linhas <b>PMEM / MB</b> e <b>VMKMEM / MB</b> - informa√ß√µes sobre a mem√≥ria f√≠sica do servidor e a mem√≥ria dispon√≠vel para o VMkernel.  A partir do interessante aqui voc√™ pode ver o valor minfree (em MB), o estado do host da mem√≥ria (no nosso caso, alto). <br><br>  Na linha <b>NUMA / MB,</b> voc√™ pode ver a distribui√ß√£o da RAM pelos n√≥s NUMA (soquetes).  Neste exemplo, a distribui√ß√£o √© desigual, o que, em princ√≠pio, n√£o √© muito bom. <br><br>  A seguir, √© apresentado um resumo das estat√≠sticas do servidor para t√©cnicas de recupera√ß√£o de mem√≥ria: <br><br>  <b>PSHARE / MB</b> s√£o estat√≠sticas do TPS; <br><br>  <b>SWAP / MB</b> - estat√≠sticas sobre o uso de Swap; <br><br>  <b>ZIP / MB</b> - estat√≠sticas de compacta√ß√£o de p√°ginas de mem√≥ria; <br><br>  <b>MEMCTL / MB</b> - Estat√≠sticas de uso do driver de bal√£o. <br><br>  Para VMs individuais, podemos estar interessados ‚Äã‚Äãnas seguintes informa√ß√µes.  Eu escondi os nomes das VMs para n√£o embara√ßar o p√∫blico :).  Se a m√©trica ESXTOP for a mesma do contador no vSphere, cito o contador correspondente. <br><br>  <b>MEMSZ</b> √© a quantidade de mem√≥ria configurada na VM (MB). <br>  MEMSZ = GRANT + MCTLSZ + SWCUR + intocado. <br><br>  <b>CONCESS√ÉO</b> - Concedida em MB. <br><br>  <b>TCHD</b> - Ativo em MB. <br><br>  <b>MCTL?</b>  - est√° instalado no VM Balloon Driver. <br><br>  <b>MCTLSZ</b> - Bal√£o em MB. <br><br>  <b>MCTLGT</b> √© a quantidade de RAM (MB) que o ESXi deseja remover da VM por meio do driver de bal√£o (Memctl Target). <br><br>  <b>MCTLMAX</b> - a quantidade m√°xima de RAM (MB) que o ESXi pode remover da VM atrav√©s do driver de bal√£o. <br><br>  <b>SWCUR</b> - a quantidade atual de RAM (MB) fornecida √† VM a partir do arquivo Swap. <br><br>  <b>SWGT</b> - a quantidade de RAM (MB) que o ESXi deseja fornecer √†s VMs de um arquivo de troca (destino de troca). <br><br>  Tamb√©m atrav√©s do ESXTOP, voc√™ pode ver informa√ß√µes mais detalhadas sobre a topologia da NUMA VM.  Para fazer isso, selecione os campos D, G: <br><br><img src="https://habrastorage.org/webt/ff/7y/zd/ff7yzdsjedyntnpj4duwv0c731m.png"><br><br>  N√≥s <b>NHN</b> - NUMA nos quais a VM est√° localizada.  Aqui voc√™ pode notar imediatamente uma ampla vm que n√£o se encaixa em um n√≥ NUMA. <br><br>  <b>NRMEM</b> - quantos megabytes de mem√≥ria a VM retira do n√≥ NUMA remoto. <br><br>  <b>NLMEM</b> - quantos megabytes de mem√≥ria a VM retira do n√≥ NUMA local. <br><br>  <b>N% L</b> - porcentagem de mem√≥ria da VM no n√≥ NUMA local (se for menor que 80%, podem ocorrer problemas de desempenho). <br><br><h3>  Mem√≥ria no hypervisor </h3><br>  Se os contadores de CPU no hipervisor geralmente n√£o s√£o de interesse especial, a situa√ß√£o √© o oposto na mem√≥ria.  Um alto uso de mem√≥ria na VM nem sempre indica um problema de desempenho, mas um alto uso de mem√≥ria no hipervisor apenas inicia o t√©cnico de gerenciamento de mem√≥ria e causa problemas com o desempenho da VM.  Os alarmes de uso da mem√≥ria do host devem ser monitorados e as VMs n√£o podem entrar no Swap. <br><br><img src="https://habrastorage.org/webt/h2/x_/59/h2x_59kddpe84yzudcq03fq1rmc.png"><br><br><img src="https://habrastorage.org/webt/oc/w7/c9/ocw7c9vmbrqogjhmtpbotng4-6y.png"><br><br><h3>  Desfazer trocas </h3><br>  Se a VM entrou no Swap, seu desempenho √© bastante reduzido.  Os tra√ßos de bal√£o e compacta√ß√£o desaparecem rapidamente ap√≥s o aparecimento de RAM livre no host, mas a m√°quina virtual n√£o tem pressa em retornar do Swap para a RAM do servidor. <br>  Antes do ESXi 6.0, a √∫nica maneira r√°pida e confi√°vel de tirar as VMs do Swap era pela reinicializa√ß√£o (mais precisamente, desligando / ligando o cont√™iner).  A partir do ESXi 6.0, apareceu uma maneira n√£o t√£o oficial, mas funcional e confi√°vel de tirar VMs do Swap.  Em uma das confer√™ncias, consegui conversar com um dos engenheiros da VMware respons√°vel pelo CPU Scheduler.  Ele confirmou que o m√©todo √© bastante funcional e seguro.  Em nossa experi√™ncia, problemas com ele tamb√©m n√£o foram percebidos. <br><br>  Os comandos reais para a sa√≠da de VMs do Swap foram <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descritos por</a> Duncan Epping.  N√£o repetirei a descri√ß√£o detalhada, apenas d√™ um exemplo de seu uso.  Como pode ser visto na captura de tela, algum tempo ap√≥s a execu√ß√£o dos comandos Swap especificados na VM desaparece. <br><br><img src="https://habrastorage.org/webt/e5/lm/7e/e5lm7e0e6i_yxrrlm7dfwixptv0.png"><br><br><h3>  Dicas para gerenciar RAM no ESXi </h3><br>  Concluindo, darei algumas dicas para ajudar a evitar problemas com o desempenho da VM devido √† RAM: <br><br><ul><li>  Evite o excesso de inscri√ß√µes na RAM em clusters produtivos.  √â sempre aconselh√°vel ter ~ 20 a 30% de mem√≥ria livre no cluster, para que o DRS (e o administrador) tenha espa√ßo de manobra e as VMs n√£o sejam trocadas durante a migra√ß√£o.  Tamb√©m n√£o esque√ßa a margem para toler√¢ncia a falhas.  √â desagrad√°vel quando, quando um servidor falha e a VM √© reinicializada usando HA, algumas das m√°quinas tamb√©m v√£o para Swap. </li><li>  Em infraestruturas altamente consolidadas, tente N√ÉO criar VMs com mais da metade da mem√≥ria do host.  Isso, novamente, ajudar√° o DRS a distribuir m√°quinas virtuais entre os servidores de cluster sem problemas.  Esta regra, √© claro, n√£o √© universal :). </li><li>  Cuidado com o alarme de uso de mem√≥ria do host. </li><li>  N√£o se esque√ßa de colocar o VMware Tools na VM e n√£o desative o bal√£o. </li><li>  Considere ativar o TPS entre VMs e desativar p√°ginas grandes em ambientes de teste e VDI. </li><li>  Se a VM estiver com problemas de desempenho, verifique se est√° usando mem√≥ria de um n√≥ NUMA remoto. </li><li>  Tire VMs do Swap o mais r√°pido poss√≠vel!  Entre outras coisas, se a VM estiver em Swap, por raz√µes √≥bvias, o sistema de armazenamento sofre. </li></ul><br>  Isso √© tudo para RAM.  Abaixo est√£o artigos relacionados para quem deseja se aprofundar nos detalhes.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O pr√≥ximo artigo</a> ser√° dedicado √† hist√≥ria. <br><br><div class="spoiler">  <b class="spoiler_title">Links √∫teis</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">http://www.yellow-bricks.com/2015/03/02/what-happens-at-which-vsphere-memory-state/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">http://www.yellow-bricks.com/2013/06/14/how-does-mem-minfreepct-work-with-vsphere-5-0-and-up/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://www.vladan.fr/vmware-transparent-page-sharing-tps-explained/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">http://www.yellow-bricks.com/2016/06/02/memory-pages-swapped-can-unswap/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://kb.vmware.com/s/article/1002586</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://www.vladan.fr/what-is-vmware-memory-ballooning/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://kb.vmware.com/s/article/2080735</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://kb.vmware.com/s/article/2017642</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://labs.vmware.com/vmtj/vmware-esx-memory-resource-management-swap</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://blogs.vmware.com/vsphere/2013/10/understanding-vsphere-active-memory.html</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://www.vmware.com/support/developer/converter-sdk/conv51_apireference/memory_counters.html</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://docs.vmware.com/en/VMware-vSphere/6.5/vsphere-esxi-vcenter-server-65-monitoring-performance-guide.pdf</a> <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt455820/">https://habr.com/ru/post/pt455820/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt455800/index.html">Matrix 1.0 - Libera√ß√£o descentralizada do protocolo de mensagens</a></li>
<li><a href="../pt455806/index.html">Nascimento e morte de um √°lbum: entendemos como os formatos musicais mudaram nos √∫ltimos 100 anos</a></li>
<li><a href="../pt455808/index.html">Obtenha extratos do registro no site do STF usando python</a></li>
<li><a href="../pt455812/index.html">Construindo uma arquitetura de microsservi√ßo em Golang e gRPC, parte 2 (janela de encaixe)</a></li>
<li><a href="../pt455816/index.html">Como criar uma a√ß√£o interessante para o Google Assistant. Lifehacks de Just AI</a></li>
<li><a href="../pt455826/index.html">Rega autom√°tica com controle remoto</a></li>
<li><a href="../pt455828/index.html">Cientistas descobriram novas formas ex√≥ticas de sincroniza√ß√£o</a></li>
<li><a href="../pt455830/index.html">Uma olhada em V√° pelos olhos de um desenvolvedor .NET. Semana # 1</a></li>
<li><a href="../pt455832/index.html">Hist√≥rico de uma √∫nica investiga√ß√£o SQL</a></li>
<li><a href="../pt455834/index.html">Benchmarks para servidores Linux: 5 ferramentas abertas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>