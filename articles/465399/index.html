<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåî üíë üë®üèø‚Äçüè≠ Implemente el almacenamiento distribuido CEPH y con√©ctelo a Kubernetes üòâ üëÜüèΩ üôÜüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parte 1 Implementamos el entorno para trabajar con microservicios. Parte 1 instalando Kubernetes HA en metal desnudo (Debian) 
 Hola, queridos lectore...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Implemente el almacenamiento distribuido CEPH y con√©ctelo a Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465399/"><p><img src="https://habrastorage.org/webt/w-/ha/t9/w-hat9zskv2ab6vdpumzdxllw8w.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1 Implementamos el entorno para trabajar con microservicios.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1 instalando Kubernetes HA en metal desnudo (Debian)</a> </p><br><h2>  Hola, queridos lectores de Habr! </h2><br><p>  En una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicaci√≥n</a> anterior <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">,</a> habl√© sobre c√≥mo implementar un cl√∫ster de conmutaci√≥n por error de Kubernetes.  Pero el hecho es que en Kubernetes es conveniente implementar aplicaciones sin estado que no necesitan mantener su estado o trabajar con datos.  Pero en la mayor√≠a de los casos, necesitamos guardar datos y no perderlos al reiniciar los hogares. <br>  Kubernetes utiliza vol√∫menes para estos fines.  Cuando trabajamos con soluciones en la nube de Kubernetes, no hay problemas particulares.  Solo necesitamos pedir el volumen requerido de Google, Amazon u otro proveedor de la nube y, guiados por la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n</a> , conectar los vol√∫menes recibidos a los pods. <br>  Cuando tratamos con metal desnudo, las cosas son un poco m√°s complicadas.  Hoy quiero hablar sobre una de las soluciones basadas en el uso de ceph. </p><br><p>  En esta publicaci√≥n dir√©: </p><br><ul><li>  c√≥mo implementar almacenamiento distribuido ceph </li><li>  C√≥mo usar Ceph cuando trabajas con Kubernetes <a name="habracut"></a></li></ul><br><h2>  Introduccion </h2><br><p>  Para empezar, me gustar√≠a explicar a qui√©n ser√° √∫til este art√≠culo.  En primer lugar, para los lectores que implementaron el cl√∫ster de acuerdo con mi primera publicaci√≥n para continuar creando una arquitectura de microservicio.  En segundo lugar, para las personas que desean intentar desplegar un cl√∫ster ceph por su cuenta y evaluar su rendimiento. </p><br><p>  En esta publicaci√≥n, no abordar√© el tema de la planificaci√≥n de cl√∫steres para ninguna necesidad, solo hablar√© sobre principios y conceptos generales.  No profundizar√© en la "sintonizaci√≥n" y la sintonizaci√≥n profunda, hay muchas publicaciones sobre este tema, incluso sobre el Habr.  El art√≠culo ser√° m√°s introductorio, pero al mismo tiempo le permitir√° obtener una soluci√≥n de trabajo que pueda adaptar a sus necesidades en el futuro. </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Lista de hosts, recursos de host, SO y versiones de software</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Estructura del c√∫mulo cef√°lico</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Configurar nodos de cl√∫ster antes de la instalaci√≥n</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Instalar ceph-deploy</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Crear un grupo ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Configuraci√≥n de red</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Instalar paquetes ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Instalaci√≥n e inicializaci√≥n de monitores.</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Agregar OSD</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Conecta ceph a kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Crear un grupo de datos</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Crear un secreto de cliente</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Implementar aprovisionador ceph rbd</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Crear una clase de almacenamiento</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Prueba de Kubernetes + ligamento cef√°lico</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Lista de materiales utilizados en la preparaci√≥n del art√≠culo.</a> </li></ol><br><a name="vm"></a><br><h2>  Lista de hosts y requisitos del sistema </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nombre</b> </th><th>  <b>Direcci√≥n IP</b> </th><th>  <b>Comentario</b> </th></tr><tr><td>  prueba ceph01 </td><td>  10.73.88.52 </td><td>  ceph-node01 </td></tr><tr><td>  prueba ceph02 </td><td>  10.73.88.53 </td><td>  ceph-node02 </td></tr><tr><td>  prueba ceph03 </td><td>  10.73.88.54 </td><td>  ceph-node03 </td></tr></tbody></table></div><br><p>  Cuando escribo un art√≠culo, uso m√°quinas virtuales con esta configuraci√≥n </p><br><p><img src="https://habrastorage.org/webt/kd/-z/bd/kd-zbdwb_76g1vvvgmtojoirseo.png"></p><br><p>  Cada uno tiene instalado un sistema operativo Debian 9.5.  Estas son m√°quinas de prueba, cada una con dos discos, el primero para el sistema operativo, el segundo para el cef OSD. </p><br><p>  Implementar√© el cl√∫ster a trav√©s de la utilidad ceph-deploy.  Puede implementar un cl√∫ster ceph en modo manual, todos los pasos se describen en la documentaci√≥n, pero el prop√≥sito de este art√≠culo es decir qu√© tan r√°pido puede implementar ceph y comenzar a usarlo en kubernetes. <br>  Ceph es bastante glot√≥n por los recursos, especialmente la RAM.  Para una buena velocidad, es recomendable utilizar unidades ssd. </p><br><p>  Puede leer m√°s sobre los requisitos en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n oficial de ceph.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><br></a> </p><br><a name="ceph"></a><br><h2>  Estructura del c√∫mulo cef√°lico </h2><br><p>  <strong>MON</strong> <br>  <em>Un monitor es un demonio que act√∫a como el coordinador desde el cual comienza el cl√∫ster.</em>  <em>Tan pronto como tengamos al menos un monitor en funcionamiento, tenemos un cl√∫ster Ceph.</em>  <em>El monitor almacena informaci√≥n sobre el estado y la condici√≥n del cl√∫ster intercambiando varias tarjetas con otros monitores.</em>  <em>Los clientes recurren a los monitores para averiguar en qu√© OSD escribir / leer datos.</em>  <em>Cuando implementa un nuevo almacenamiento, lo primero que hace es crear un monitor (o varios).</em>  <em>El cl√∫ster puede vivir en un monitor, pero se recomienda hacer 3 o 5 monitores para evitar la ca√≠da de todo el sistema debido a la ca√≠da de un solo monitor.</em>  <em>Lo principal es que el n√∫mero de estos debe ser impar para evitar situaciones de cerebro dividido.</em>  <em>Los monitores trabajan en un qu√≥rum, por lo que si cae m√°s de la mitad de los monitores, el cl√∫ster se bloquear√° para evitar inconsistencias en los datos.</em> <br>  <strong>Mons.</strong> <br>  <em>El demonio Ceph Manager funciona con el demonio monitor para proporcionar un control adicional.</em> <em><br></em>  <em>Desde la versi√≥n 12.x, el demonio ceph-mgr se ha vuelto necesario para el funcionamiento normal.</em> <em><br></em>  <em>Si el demonio mgr no se est√° ejecutando, ver√° una advertencia al respecto.</em> <br>  <strong>OSD (dispositivo de almacenamiento de objetos)</strong> <br>  <em>OSD es una unidad de almacenamiento que almacena los datos en s√≠ y procesa las solicitudes de los clientes mediante el intercambio de datos con otros OSD.</em>  <em>Esto suele ser un disco.</em>  <em>Y generalmente para cada OSD hay un demonio OSD separado que puede ejecutarse en cualquier m√°quina en la que est√© instalado este disco.</em> </p><br><p>  Los tres demonios funcionar√°n en cada m√°quina de nuestro cl√∫ster.  En consecuencia, supervise y administre demonios como servicio y demonios OSD para una unidad de nuestra m√°quina virtual. </p><br><a name="before"></a><br><h2>  Configurar nodos de cl√∫ster antes de la instalaci√≥n </h2><br><p>  La documentaci√≥n de ceph especifica el siguiente flujo de trabajo: </p><br><p><img src="https://habrastorage.org/webt/wz/9i/gu/wz9igu71hiezyom4zvtbxp6ye30.png"></p><br><p>  Trabajar√© desde el primer nodo del cl√∫ster ceph01-test, ser√° Admin Node, tambi√©n contendr√° archivos de configuraci√≥n para la utilidad ceph-deploy.  Para que la utilidad ceph-deploy funcione correctamente, todos los nodos del cl√∫ster deben ser accesibles a trav√©s de ssh con el nodo Admin.  Por conveniencia, escribir√© los nombres cortos de los hosts para el cl√∫ster </p><br><pre><code class="plaintext hljs">10.73.88.52 ceph01-test 10.73.88.53 ceph02-test 10.73.88.54 ceph03-tset</code> </pre> <br><p>  Y copie las claves a los otros hosts.  Todos los comandos que ejecutar√© desde la ra√≠z. </p><br><pre> <code class="plaintext hljs">ssh-copy-id ceph02-test ssh-copy-id ceph03-test</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Documentaci√≥n de configuraci√≥n</a> </p><br><anchor>  ceph-deploy </anchor><br><h2>  Instalar ceph-deploy </h2><br><p>  El primer paso es instalar ceph-deploy en la m√°quina ceph01-test </p><br><pre> <code class="plaintext hljs">wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -</code> </pre> <br><p>  A continuaci√≥n, debe elegir la versi√≥n que desea poner.  Pero aqu√≠ hay dificultades, actualmente ceph para Debian OS solo admite paquetes luminosos. <br>  Si desea poner una versi√≥n m√°s reciente, tendr√° que usar un espejo, por ejemplo <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://mirror.croit.io/debian-mimic/dists/</a> </p><br><p>  Agregue un repositorio con m√≠mica en los tres nodos </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-deploy</code> </pre> <br><p>  Si luminoso es suficiente para ti, entonces puedes usar los repositorios oficiales </p><br><pre> <code class="plaintext hljs">echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list apt-transport-https apt update apt install ceph-deploy</code> </pre> <br><p>  Tambi√©n instalamos NTP en los tres nodos. </p><br><div class="spoiler">  <b class="spoiler_title">ya que esta recomendaci√≥n est√° en la documentaci√≥n del ceph</b> <div class="spoiler_text"><p>  Recomendamos instalar NTP en los nodos Ceph (especialmente en los nodos Ceph Monitor) para evitar problemas derivados de la deriva del reloj. <br></p></div></div><br><pre> <code class="plaintext hljs">apt install ntp</code> </pre> <br><p>  Aseg√∫rese de habilitar el servicio NTP.  Aseg√∫rese de que cada nodo Ceph use el mismo servidor NTP.  Puedes ver m√°s detalles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> </p><br><a name="ceph-install"></a><br><h2>  Crear un grupo ceph </h2><br><p>  Cree un directorio para archivos de configuraci√≥n y archivos ceph-deploy </p><br><pre> <code class="plaintext hljs">mkdir my-cluster cd my-cluster</code> </pre> <br><p>  Creemos una nueva configuraci√≥n de cl√∫ster, al crear, indique que habr√° tres monitores en nuestro cl√∫ster </p><br><pre> <code class="plaintext hljs">ceph-deploy new ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-network"></a><br><h2>  Configuraci√≥n de red </h2><br><p>  Ahora el punto importante, es hora de hablar sobre la red para ceph.  Ceph utiliza dos redes p√∫blicas y una red de cl√∫ster para trabajar. <br><img src="https://habrastorage.org/webt/jz/dp/wz/jzdpwzxrzpdqr7k9v4u53hmc05s.png"></p><br><p>  Como puede ver en el diagrama de la red p√∫blica, este es el nivel de usuario y aplicaci√≥n, y la red de cl√∫ster es la red a trav√©s de la cual se replican los datos. <br>  Es altamente deseable separar estas dos redes entre s√≠.  Adem√°s, la red de cl√∫ster de velocidad de red es deseable al menos 10 Gb. <br>  Por supuesto, puede mantener todo en la misma red.  Pero esto est√° plagado del hecho de que tan pronto como aumenta el volumen de replicaci√≥n entre los OSD, por ejemplo, cuando los nuevos OSD (discos) caen o se agregan, la carga de la red aumentar√° MUY.  Por lo tanto, la velocidad y la estabilidad de su infraestructura depender√°n en gran medida de la red utilizada por ceph. <br>  Desafortunadamente, mi cl√∫ster de virtualizaci√≥n no tiene una red separada, y usar√© un segmento de red com√∫n. <br>  La configuraci√≥n de red para el cl√∫ster se realiza a trav√©s del archivo de configuraci√≥n, que generamos con el comando anterior. </p><br><pre> <code class="plaintext hljs">/my-cluster# cat ceph.conf [global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Como podemos ver, la implementaci√≥n de cef no cre√≥ la configuraci√≥n de red predeterminada para nosotros, por lo que agregar√© el par√°metro public network = {public-network / netmask} a la secci√≥n global de la configuraci√≥n.  Mi red es 10.73.0.0/16, as√≠ que despu√©s de agregar mi configuraci√≥n se ver√° as√≠ </p><br><pre> <code class="plaintext hljs">[global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 public network = 10.73.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Si desea separar la red del cl√∫ster del p√∫blico, agregue el par√°metro cluster network = {cluster-network / netmask} <br>  Puede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">leer</a> m√°s sobre redes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en la documentaci√≥n.</a> </p><br><a name="ceph-pack"></a><br><h2>  Instalar paquetes ceph </h2><br><p>  Usando ceph-deploy, instalamos todos los paquetes ceph que necesitamos en nuestros tres nodos. <br>  Para hacer esto, en ceph01-test, ejecute <br>  Si la versi√≥n es m√≠mica, entonces </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release mimic ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Si la versi√≥n es luminosa, entonces </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release luminous ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Y espera hasta que todo est√© establecido. </p><br><a name="ceph-mon"></a><br><h2>  Instalaci√≥n e inicializaci√≥n de monitores. </h2><br><p>  Despu√©s de instalar todos los paquetes, crearemos e iniciaremos los monitores de nuestro cl√∫ster. <br>  C ceph01-test haz lo siguiente </p><br><pre> <code class="plaintext hljs">ceph-deploy mon create-initial</code> </pre> <br><p>  Se crear√°n monitores en el proceso, se lanzar√°n demonios y ceph-deploy verificar√° el qu√≥rum. <br>  Ahora dispersa las configuraciones en los nodos del cl√∫ster. </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Y verifique el estado de nuestro cl√∫ster, si hizo todo correctamente, entonces el estado deber√≠a ser <br>  SALUD_OK </p><br><pre> <code class="plaintext hljs">~/my-cluster# ceph status cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs:</code> </pre> <br><p>  Crear mgr </p><br><pre> <code class="plaintext hljs">ceph-deploy mgr create ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Y verifica el estado nuevamente </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Deber√≠a aparecer una l√≠nea </p><br><pre> <code class="plaintext hljs">mgr: ceph01-test(active), standbys: ceph02-test, ceph03-test</code> </pre> <br><p>  Escribimos la configuraci√≥n a todos los hosts en el cl√∫ster </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-osd"></a><br><h2>  Agregar OSD </h2><br><p>  Por el momento tenemos un cl√∫ster en funcionamiento, pero todav√≠a no tiene discos (osd en terminolog√≠a ceph) para almacenar informaci√≥n. </p><br><p>  OSD se puede agregar con el siguiente comando (vista general) </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data {device} {ceph-node}</code> </pre> <br><p>  En mi banco de pruebas, el disco / dev / sdb se asigna bajo osd, por lo que en mi caso los comandos ser√°n los siguientes </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data /dev/sdb ceph01-test ceph-deploy osd create --data /dev/sdb ceph02-test ceph-deploy osd create --data /dev/sdb ceph03-test</code> </pre> <br><p>  Verifique que todos los OSD est√©n funcionando. </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Conclusi√≥n </p><br><pre> <code class="plaintext hljs"> cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: ceph01-test(active) osd: 3 osds: 3 up, 3 in</code> </pre> <br><p>  Tambi√©n puede probar algunos comandos √∫tiles para OSD. </p><br><pre> <code class="plaintext hljs">ceph osd df ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 1 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 2 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 TOTAL 15 GiB 3.0 GiB 12 GiB 20.05</code> </pre> <br><p>  y </p><br><pre> <code class="plaintext hljs">ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.01469 root default -3 0.00490 host ceph01-test 0 hdd 0.00490 osd.0 up 1.00000 1.00000 -5 0.00490 host ceph02-test 1 hdd 0.00490 osd.1 up 1.00000 1.00000 -7 0.00490 host ceph03-test 2 hdd 0.00490 osd.2 up 1.00000 1.00000</code> </pre><br><p>  Si todo est√° bien, entonces tenemos un grupo ceph en funcionamiento.  En la siguiente parte, dir√© c√≥mo usar ceph con kubernetes </p><br><a name="kubernetes"></a><br><h1>  Conecta ceph a kubernetes </h1><br><p>  Desafortunadamente, no podr√© describir en detalle la operaci√≥n de los vol√∫menes de Kubernetes en este art√≠culo, as√≠ que intentar√© encajar en un p√°rrafo. <br>  Kubernetes utiliza clases de almacenamiento para trabajar con vol√∫menes de datos, cada clase de almacenamiento tiene su propio aprovisionador, puede considerarlo como una especie de "controlador" para trabajar con diferentes vol√∫menes de almacenamiento de datos.  La lista completa que admite kubernetes se puede encontrar en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n oficial</a> . <br>  Kubernetes tambi√©n tiene soporte para trabajar con rbd, pero no hay un cliente rbd instalado en la imagen oficial de kube-controller-manager, por lo que debe usar una imagen diferente. <br>  Tambi√©n debe tenerse en cuenta que los vol√∫menes (pvc) creados como rbd solo pueden ser ReadWriteOnce (RWO) y, lo que significa que puede montar el volumen creado SOLO en un hogar. </p><br><p>  Para que nuestro cl√∫ster pueda trabajar con vol√∫menes ceph, necesitamos: <br>  en un grupo Ceph: </p><br><ul><li>  crear grupo de datos en el grupo ceph </li><li>  crear un cliente y clave de acceso al grupo de datos </li><li>  obtener secreto administrativo ceph </li></ul><br><p>  Para que nuestro cl√∫ster pueda trabajar con vol√∫menes ceph, necesitamos: <br>  en un grupo Ceph: </p><br><ul><li>  crear grupo de datos en el grupo ceph </li><li>  crear un cliente y clave de acceso al grupo de datos </li><li>  obtener secreto administrativo ceph </li></ul><br><p>  en el cl√∫ster de Kubernetes: </p><br><ul><li>  crear secreto administrativo ceph y clave de cliente ceph </li><li>  instale el aprovisionador ceph rbd o cambie la imagen de kube-controller-manager a una imagen que admita rbd </li><li>  crear secreto con clave de cliente ceph </li><li>  crear clase de almacenamiento </li><li>  instalar ceph-common en las notas de trabajo de kubernetes </li></ul><br><a name="ceph-pool"></a><br><h2>  Crear un grupo de datos </h2><br><p>  En el cl√∫ster ceph, cree un grupo para los vol√∫menes de kubernetes </p><br><pre> <code class="plaintext hljs">ceph osd pool create kube 8 8</code> </pre> <br><p>  Aqu√≠ har√© una peque√±a explicaci√≥n, los n√∫meros 8 8 al final son los n√∫meros de pg y pgs.  Estos valores dependen del tama√±o de su grupo ceph.  Hay calculadoras especiales que calculan la cantidad de pg y pgs, por ejemplo, oficial de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ceph</a> <br>  Para comenzar, recomiendo dejarlo de forma predeterminada, si en el futuro esta cantidad se puede aumentar (solo se puede reducir con la versi√≥n Nautilus). </p><br><a name="ceph-key"></a><br><h2>  Crear un cliente para un grupo de datos </h2><br><p>  Crear un cliente para el nuevo grupo </p><br><pre> <code class="plaintext hljs">ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'</code> </pre> <br><p>  Recibiremos una clave para el cliente, en el futuro la necesitaremos para crear un secreto kubernetes </p><br><pre> <code class="plaintext hljs">ceph auth get-key client.kube AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg==</code> </pre> <br><h2>  Obteniendo clave de administrador </h2><br><p>  Y obtener la clave de administrador </p><br><pre> <code class="plaintext hljs">ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '{print $3'} AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA==</code> </pre> <br><p>  <strong>En el grupo ceph, todo el trabajo se ha completado y ahora tenemos que ir a una m√°quina que tenga acceso al grupo kubernetes</strong> <br>  Trabajar√© con la prueba master01 (10.73.71.25) del cl√∫ster implementado por m√≠ en la primera publicaci√≥n. </p><br><a name="kubernetes-secrets"></a><br><h2>  Crear un secreto de cliente </h2><br><p>  Cree un archivo con el token del cliente que recibimos (no olvide reemplazarlo con su token) </p><br><pre> <code class="plaintext hljs">echo AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg== &gt; /tmp/key.client</code> </pre> <br><p>  Y crea un secreto que usaremos en el futuro </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><h2>  Crear secreto de administrador </h2><br><p>  Cree un archivo con token de administrador (no olvide reemplazarlo con su token) </p><br><pre> <code class="plaintext hljs">echo AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA== &gt; /tmp/key.admin</code> </pre> <br><p>  Despu√©s de eso, crea un secreto de administrador </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><p>  Comprueba que se han creado secretos </p><br><pre> <code class="plaintext hljs">kubectl get secret -n kube-system | grep ceph ceph-admin-secret kubernetes.io/rbd 1 8m31s ceph-secret kubernetes.io/rbd 1 7m32s</code> </pre> <br><a name="kubernetes-provisioner"></a><br><h2>  Primero, el m√©todo implementa ceph rbd provisioner </h2><br><p>  Clonamos el repositorio kubernetes-incubator / external-storage de github, tiene todo lo que necesitas para hacer que los kubernetes se agrupen con el repositorio ceph. </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/ceph/rbd/deploy/ NAMESPACE=kube-system sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml</code> </pre> <br><pre> <code class="plaintext hljs">kubectl -n $NAMESPACE apply -f ./rbac</code> </pre> <br><p>  Conclusi√≥n </p><br><pre> <code class="plaintext hljs">clusterrole.rbac.authorization.k8s.io/rbd-provisioner created clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created deployment.extensions/rbd-provisioner created role.rbac.authorization.k8s.io/rbd-provisioner created rolebinding.rbac.authorization.k8s.io/rbd-provisioner created serviceaccount/rbd-provisioner created</code> </pre> <br><h2>  M√©todo dos: Reemplace la imagen de kube-controller-manager </h2><br><p>  No hay soporte de rbd en la imagen oficial de kube-controller-manager, por lo que tendremos que cambiar la imagen del controlador-manager. <br>  Para esto, en cada uno de los asistentes de Kubernetes, debe editar el archivo kube-controller-manager.yaml y reemplazar la imagen con gcr.io/google_containers/hyperkube:v1.15.2.  Preste atenci√≥n a la versi√≥n de la imagen que debe coincidir con su versi√≥n del cl√∫ster de Kubernetes. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/manifests/kube-controller-manager.yaml</code> </pre> <br><p>  Despu√©s de eso, deber√° reiniciar kube-controller-manager </p><br><pre> <code class="plaintext hljs">ubectl get pods -A | grep manager kube-system kube-controller-manager-master01-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master02-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master03-test 1/1 Running 9111 103d</code> </pre> <br><p>  Los pods deben actualizarse autom√°ticamente, pero si por alguna raz√≥n esto no sucedi√≥, puede volver a crearlos manualmente, mediante la eliminaci√≥n. </p><br><pre> <code class="plaintext hljs">kubectl delete pod -n kube-system kube-controller-manager-master01-test kubectl delete pod -n kube-system kube-controller-manager-master02-test kubectl delete pod -n kube-system kube-controller-manager-master03-test</code> </pre> <br><p>  Comprueba que todo est√° bien </p><br><pre> <code class="plaintext hljs">kubectl describe pod -n kube-system kube-controller-manager-master02-test | grep Image: Image: gcr.io/google_containers/hyperkube:v1.15.2</code> </pre> <br><p>  - </p><br><a name="storage-class"></a><br><h2>  Crear una clase de almacenamiento </h2><br><p>  <strong>M√©todo uno: si utiliz√≥ el aprovisionador ceph.com/rbd</strong> <br>  Cree un archivo yaml con una descripci√≥n de nuestra clase de almacenamiento.  Adem√°s, todos los archivos utilizados a continuaci√≥n se pueden descargar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en mi repositorio</a> en el directorio ceph </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: ceph.com/rbd parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  E incrustarlo en nuestro grupo </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class.yaml</code> </pre> <br><p>  Comprueba que todo est√° bien </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd ceph.com/rbd 7s</code> </pre> <br><p>  <strong>M√©todo dos: si utiliz√≥ el aprovisionador kubernetes.io/rbd</strong> <br>  Crear clase de almacenamiento </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class-hyperkube.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: kubernetes.io/rbd allowVolumeExpansion: true parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  E incrustarlo en nuestro grupo </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class-hyperkube.yaml storageclass.storage.k8s.io/ceph-rbd created</code> </pre> <br><p>  Comprueba que todo est√° bien </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd kubernetes.io/rbd 107s</code> </pre> <br><a name="test"></a><br><h2>  Prueba de Kubernetes + ligamento cef√°lico </h2><br><p>  Antes de probar ceph + kubernetes, debe instalar el paquete ceph-common en CADA c√≥digo de trabajo del cl√∫ster. </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-common</code> </pre> <br><p>  Crear un archivo yaml PersistentVolumeClaim </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: ceph-rbd resources: requests: storage: 1Gi EOF</code> </pre> <br><p>  Matarlo </p><br><pre> <code class="plaintext hljs">kubectl apply -f claim.yaml</code> </pre> <br><p>  Verifique que se cree PersistentVolumeClaim. </p><br><pre> <code class="plaintext hljs">bectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO ceph-rbd 44m</code> </pre> <br><p>  Y tambi√©n cre√≥ autom√°ticamente PersistentVolume. </p><br><pre> <code class="plaintext hljs">kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO Delete Bound default/claim1 ceph-rbd 37m</code> </pre> <br><p>  Creemos un pod de prueba en el que incluyamos el pvc creado en el directorio / mnt.  Ejecute este archivo /mnt/test.txt con el texto "¬°Hola mundo!" </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./create-file-pod.yaml kind: Pod apiVersion: v1 metadata: name: create-file-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "echo Hello world! &gt; /mnt/test.txt &amp;&amp; exit 0 || exit 1" volumeMounts: - name: pvc mountPath: "/mnt" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Lo mataremos y verificaremos que ha completado su tarea. </p><br><pre> <code class="plaintext hljs">kubectl apply -f create-file-pod.yaml kubectl get pods -w</code> </pre> <br><p>  Esperemos el estado </p><br><pre> <code class="plaintext hljs">create-file-pod 0/1 Completed 0 16s</code> </pre> <br><p>  Creemos otro, conectemos nuestro volumen pero ya en / mnt / test, y luego aseg√∫rese de que el archivo creado por el primer volumen est√© en su lugar. </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./test-pod.yaml kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "sleep 600" volumeMounts: - name: pvc mountPath: "/mnt/test" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Ejecute kubectl get po -w y espere hasta que el pod se est√© ejecutando <br>  Despu√©s de eso, veamos y verifiquemos que el volumen est√© conectado y nuestro archivo en el directorio / mnt / test </p><br><pre> <code class="plaintext hljs">kubectl exec test-pod -ti sh cat /mnt/test/test.txt Helo world!</code> </pre> <br><p>  Gracias por leer hasta el final.  Perd√≥n por el retraso en la publicaci√≥n. <br>  Estoy listo para responder todas las preguntas en mensajes personales o en las redes sociales que se indican en mi perfil. <br>  En la pr√≥xima publicaci√≥n peque√±a, le dir√© c√≥mo implementar el almacenamiento S3 basado en el cl√∫ster ceph creado, y luego de acuerdo con el plan de la primera publicaci√≥n. </p><br><a name="book"></a><br><p>  Materiales utilizados para publicaci√≥n </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Documentaci√≥n oficial de Ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Presentaci√≥n del repositorio de Ceph en im√°genes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Documentaci√≥n oficial de Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">incubadora de kubernetes / de almacenamiento externo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio kubernetes-ceph-percona con archivos de muestra</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/465399/">https://habr.com/ru/post/465399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../465379/index.html">Control de bomba de insulina aut√≥nomo inal√°mbrico hecho en casa</a></li>
<li><a href="../465391/index.html">Siguiendo los pasos del movimiento ruso Scala. Parte 1</a></li>
<li><a href="../465393/index.html">Energ√≠a de la bater√≠a para dispositivos MySensors</a></li>
<li><a href="../465395/index.html">¬øCu√°l es la diferencia principal entre la inyecci√≥n de dependencia y el localizador de servicios?</a></li>
<li><a href="../465397/index.html">¬øC√≥mo apareci√≥ el traductor Nitro, que ayuda a los desarrolladores en localizaci√≥n y soporte t√©cnico?</a></li>
<li><a href="../465401/index.html">5 actividades para acelerar la resoluci√≥n de problemas en cualquier equipo de TI</a></li>
<li><a href="../465403/index.html">Achtung! Nuevas c√°maras en el camino o informaci√≥n actualizada sobre radares y detectores de radar</a></li>
<li><a href="../465407/index.html">1. Descripci√≥n general de los conmutadores Extreme Enterprise Layer</a></li>
<li><a href="../465409/index.html">Vue.js Mejores pr√°cticas para desarrollo web</a></li>
<li><a href="../465415/index.html">Hablamos de DevOps en un lenguaje comprensible.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>