<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíø üí™üèæ üë©üèΩ‚Äçüíª ¬øQu√© permite Jupyter? üíæ üë®üèø üî¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nuestra historia comenz√≥ con una tarea aparentemente simple. Era necesario establecer herramientas anal√≠ticas para especialistas en ciencia de datos y...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¬øQu√© permite Jupyter?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/vtb/blog/443294/">  Nuestra historia comenz√≥ con una tarea aparentemente simple.  Era necesario establecer herramientas anal√≠ticas para especialistas en ciencia de datos y solo analistas de datos.  Esta tarea fue dirigida a nosotros por colegas de las divisiones de riesgo minorista y CRM, donde la concentraci√≥n de especialistas en ciencia de datos es hist√≥ricamente alta.  Los clientes ten√≠an un simple deseo: escribir c√≥digo en Python, importar bibliotecas avanzadas (xgboost, pytorch, tensorflow, etc.) y ejecutar algoritmos en los datos generados desde el cl√∫ster hdfs. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/793/c16/22e/793c1622e8423e8cae171263790ab234.png"><br><br>  Todo parece ser simple y claro.  Pero hubo tantas dificultades que decidimos escribir una publicaci√≥n al respecto y publicar la soluci√≥n preparada en GitHub. <br><a name="habracut"></a><br>  Primero, algunos detalles sobre la infraestructura fuente: <br><br><ul><li>  HDFS Data Warehouse (12 nodos de Oracle Big Data Appliance, distribuci√≥n Cloudera).  En total, el almac√©n tiene 130 TB de datos de varios sistemas internos del banco; tambi√©n hay informaci√≥n heterog√©nea de fuentes externas. <br></li><li>  Dos servidores de aplicaciones en los que se supon√≠a el despliegue de herramientas anal√≠ticas.  Vale la pena mencionar que no solo las tareas anal√≠ticas avanzadas "giran" en estos servidores, por lo que uno de los requisitos era el uso de herramientas de contenedorizaci√≥n (Docker) para administrar los recursos del servidor, usar varios entornos y configurarlos. <br></li></ul><br>  Como el entorno principal para el trabajo de los analistas, decidieron elegir JupyterHub, que de hecho ya se ha convertido en uno de los est√°ndares para trabajar con datos y desarrollar modelos de aprendizaje autom√°tico.  Lea m√°s sobre esto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> .  En el futuro, ya imaginamos JupyterLab. <br><br>  Parece que todo es simple: necesitas tomar y configurar un mont√≥n de Python + Anaconda + Spark.  Instale Jupyter Hub en el servidor de aplicaciones, integre con LDAP, conecte Spark o con√©ctese a datos en hdfs de cualquier otra manera y siga adelante: ¬°cree modelos! <br>  Si profundiza en todos los datos y requisitos de origen, aqu√≠ hay una lista m√°s detallada: <br><br><ul><li>  Ejecuci√≥n de JupyterHub en Docker (sistema operativo base - Oracle Linux 7) <br></li><li> Cloudera CDH cluster 5.15.1 + Spark 2.3.0 con autenticaci√≥n Kerberos en la configuraci√≥n de Active Directory + MIT Kerberos dedicado en el cl√∫ster (consulte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">KDC MIT dedicado con cl√∫ster con Active Directory</a> ), Oracle Linux 6 <br></li><li>  Integraci√≥n de Active Directory <br></li><li>  Autenticaci√≥n transparente en Hadoop y Spark <br></li><li>  Soporte para Python 2 y 3 <br></li><li>  Spark 1 y 2 (con la capacidad de usar recursos de cl√∫ster para modelos de capacitaci√≥n y procesamiento de datos en paralelo usando pyspark) <br></li><li>  Capacidad para limitar los recursos del host <br></li><li>  Conjunto de biblioteca <br></li></ul><br>  Esta publicaci√≥n est√° dise√±ada para profesionales de TI que enfrentan la necesidad de resolver tales problemas. <br><br><h2>  Descripci√≥n de la soluci√≥n </h2><br><h3>  Lanzamiento en Docker + Cloudera Cluster Integration </h3><br>  No hay nada inusual aqu√≠.  Los clientes de productos JupyterHub y Cloudera se instalan en el contenedor (como se ve a continuaci√≥n) y los archivos de configuraci√≥n se montan desde la m√°quina host: <br><br>  <b>start-hub.sh</b> <br><br><pre><code class="plaintext hljs">VOLUMES="-v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z -v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre> <br><br><h3>  Integraci√≥n de Active Directory </h3><br>  Para la integraci√≥n con Active Directory / Kerberos iron y no muy hosts, el est√°ndar en nuestra empresa es el producto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PBIS Open</a> .  T√©cnicamente, este producto es un conjunto de servicios que se comunican con Active Directory, con el cual, a su vez, los clientes trabajan a trav√©s de sockets de dominio Unix.  Este producto se integra con Linux PAM y NSS. <br><br>  Utilizamos el m√©todo Docker est√°ndar: los sockets de dominio de Unix de los servicios de host se montaron en un contenedor (los sockets se encontraron emp√≠ricamente mediante simples manipulaciones con el comando lsof): <br><br>  <b>start-hub.sh</b> <br><br><pre> <code class="plaintext hljs">VOLUMES="-v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z &lt;b&gt;-v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro &lt;/b&gt; -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre><br>  A su vez, los paquetes PBIS se instalan dentro del contenedor, pero sin ejecutar la secci√≥n posterior a la instalaci√≥n.  Por lo tanto, colocamos solo archivos y bibliotecas ejecutables, pero no iniciamos servicios dentro del contenedor; esto es superfluo para nosotros.  Los comandos de integraci√≥n PAM y NSS Linux se ejecutan manualmente. <br><br>  <b>Dockerfile:</b> <br><br><pre> <code class="plaintext hljs"># Install PAM itself and standard PAM configuration packages. RUN yum install -y pam util-linux \ # Here we just download PBIS RPM packages then install them omitting scripts. # We don't need scripts since they start PBIS services, which are not used - we connect to the host services instead. &amp;&amp; find /var/yum/localrepo/ -type f -name 'pbis-open*.rpm' | xargs rpm -ivh --noscripts \ # Enable PBIS PAM integration. &amp;&amp; domainjoin-cli configure --enable pam \ # Make pam_loginuid.so module optional (Docker requirement) and add pam_mkhomedir.so to have home directories created automatically. &amp;&amp; mv /etc/pam.d/login /tmp \ &amp;&amp; awk '{ if ($1 == "session" &amp;&amp; $2 == "required" &amp;&amp; $3 == "pam_loginuid.so") { print "session optional pam_loginuid.so"; print "session required pam_mkhomedir.so skel=/etc/skel/ umask=0022";} else { print $0; } }' /tmp/login &gt; /etc/pam.d/login \ &amp;&amp; rm /tmp/login \ # Enable PBIS nss integration. &amp;&amp; domainjoin-cli configure --enable nsswitch</code> </pre><br>  Resulta que los clientes del contenedor PBIS se comunican con los servicios de host PBIS.  JupyterHub utiliza un autenticador PAM, y con PBIS configurado correctamente en el host, todo funciona de forma inmediata. <br><br>  Para evitar que todos los usuarios de AD ingresen a JupyterHub, puede usar la configuraci√≥n que restringe a los usuarios a grupos espec√≠ficos de AD. <br><br>  <b>config-example / jupyterhub / jupyterhub_config.py</b> <br><br><pre> <code class="plaintext hljs">c.DSAIAuthenticator.group_whitelist = ['COMPANY\\domain^users']</code> </pre><br><h3>  Autenticaci√≥n transparente en Hadoop y Spark </h3><br>  Al iniciar sesi√≥n en JupyterHub, PBIS almacena en cach√© el ticket Kerberos del usuario en un archivo espec√≠fico en el directorio / tmp.  Para una autenticaci√≥n transparente de esta manera, es suficiente montar el directorio / tmp del host en el contenedor y establecer la variable KRB5CCNAME en el valor deseado (esto se hace en nuestra clase de autenticador). <br><br>  <b>start-hub.sh</b> <br><br><pre> <code class="plaintext hljs">VOLUMES="-v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z -v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre> <br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">env['KRB5CCNAME'] = '/host/tmp/krb5cc_%d' % pwd.getpwnam(self.user.name).pw_uid</code> </pre> <br>  Gracias al c√≥digo anterior, el usuario de JupyterHub puede ejecutar comandos hdfs desde el terminal Jupyter y ejecutar trabajos de Spark sin pasos de autenticaci√≥n adicionales.  Montar todo el directorio / tmp del host en el contenedor no es seguro: somos conscientes de este problema, pero su soluci√≥n a√∫n est√° en desarrollo. <br><br><h3>  Python versiones 2 y 3 </h3><br>  Parece que aqu√≠ todo es simple: necesita instalar las versiones necesarias de Python e integrarlas con Jupyter, creando el Kernel necesario.  Este problema ya se ha cubierto en muchos lugares.  Conda se usa para administrar entornos Python.  Por qu√© toda la simplicidad solo es aparente quedar√° claro en la siguiente secci√≥n.  Ejemplo de kernel para Python 3.6 (este archivo no est√° en git; todos los archivos del kernel se generan por c√≥digo): <br><br>  <b>/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/share/jupyter/kernels/python3.6.6/kernel.json</b> <br><br><pre> <code class="plaintext hljs">{   "argv": [      "/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/bin/python",       "-m",       "ipykernel_launcher",       "-f",      "{connection_file}"   ],   "display_name": "Python 3",   "language": "python" }</code> </pre><br><h3>  Spark 1 y 2 </h3><br>  Para integrarse con clientes SPARK, tambi√©n necesita crear Kernels.  Ejemplo de kernel para Python 3.6 y SPARK 2. <br><br>  <b>/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/share/jupyter/kernels/python3.6.6-pyspark2/kernel.json</b> <br><br><pre> <code class="plaintext hljs">{   "argv": [       "/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/bin/python",       "-m",       "ipykernel_launcher",       "-f",      "{connection_file}"   ],   "display_name": "Python 3 + PySpark 2",   "language": "python",   "env": {       "JAVA_HOME": "/usr/java/default/",       "SPARK_HOME": "/opt/cloudera/parcels/SPARK2/lib/spark2/",       "PYTHONSTARTUP": "/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/shell.py",       "PYTHONPATH": "/opt/cloudera/parcels/SPARK2/lib/spark2/python/:/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip",       "PYSPARK_PYTHON": "/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/bin/python"   } }</code> </pre><br>  Solo tenga en cuenta que el requisito de tener soporte para Spark 1 se ha desarrollado hist√≥ricamente.  Sin embargo, es posible que alguien enfrente restricciones similares: no puede, por ejemplo, instalar Spark 2 en un cl√∫ster.  Por lo tanto, describimos aqu√≠ los escollos que encontramos en el camino a la implementaci√≥n. <br>  Primero, Spark 1.6.1 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">no funciona</a> con Python 3.6.  Curiosamente, en CDH 5.12.1 esto se solucion√≥, pero en 5.15.1, por alguna raz√≥n no).  Al principio, quer√≠amos resolver este problema simplemente aplicando el parche apropiado.  Sin embargo, en el futuro, esta idea tuvo que ser abandonada, ya que este enfoque requiere la instalaci√≥n de un Spark modificado en un cl√∫ster, lo que era inaceptable para nosotros.  La soluci√≥n se encontr√≥ al crear un entorno Conda separado con Python 3.5. <br><br>  El segundo problema evita que Spark 1 funcione dentro de Docker.  El controlador Spark abre un puerto espec√≠fico a trav√©s del cual Worker se conecta al controlador; para esto, el controlador le env√≠a su direcci√≥n IP.  En el caso de Docker Worker, intenta conectarse al controlador a trav√©s de la IP del contenedor, y cuando usa network = bridge no funciona de forma natural. <br><br>  La soluci√≥n obvia es enviar no la IP del contenedor, sino la IP del host, que se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">implement√≥</a> en Spark 2 agregando las configuraciones de configuraci√≥n apropiadas.  Este parche se redise√±√≥ de manera creativa y se aplic√≥ a Spark 1. No es necesario colocar Spark modificado de esta manera en los hosts del cl√∫ster, por lo tanto, no surge un problema similar a la incompatibilidad con Python 3.6. <br><br>  Independientemente de la versi√≥n de Spark, para su funcionalidad es necesario tener las mismas versiones de Python en el cl√∫ster que en el contenedor.  Para instalar Anaconda directamente sin pasar por Cloudera Manager, tuvimos que aprender a hacer dos cosas: <br><br><ul><li>  construye tu paquete con Anaconda y todos los entornos correctos <br></li><li>  inst√°lelo en Docker (por consistencia) <br></li></ul><br><h3>  Parcela Asamblea Anaconda </h3><br>  Esto result√≥ ser una tarea bastante simple.  Todo lo que necesitas es: <br><br><ol><li>  Prepare el contenido de la parcela instalando las versiones requeridas del entorno Anaconda y Python <br></li><li>  Cree archivos de metadatos y col√≥quelos en el meta directorio <br></li><li>  Crear paquete con alquitr√°n simple <br></li><li>  Validar la utilidad de paqueter√≠a de Cloudera <br></li></ol><br>  El proceso se describe con m√°s detalle en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GitHub</a> , tambi√©n hay un c√≥digo de validaci√≥n all√≠.  Pedimos prestados metadatos en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">paquete</a> oficial de Anaconda para Cloudera, redise√±√°ndolo creativamente. <br><br><h3>  Instalar paquete en Docker </h3><br>  Esta pr√°ctica ha demostrado ser √∫til por dos razones: <br><br><ul><li>  Garantizar la operatividad de Spark: es imposible colocar Anaconda en un cl√∫ster sin parcela <br></li><li>  Spark 2 se distribuye solo en forma de paquete; por supuesto, podr√≠a instalarlo en un contenedor solo en forma de archivos jar, pero este enfoque fue rechazado <br></li></ul><br>  Como beneficio adicional, como resultado de resolver los problemas anteriores, recibimos: <br><br><ul><li>  facilidad para configurar clientes Hadoop y Spark: al instalar las mismas parcelas en Docker y en el cl√∫ster, las rutas en el cl√∫ster y en el contenedor son las mismas <br></li><li>  facilidad para mantener un entorno uniforme en el contenedor y en el cl√∫ster: al actualizar el cl√∫ster, la imagen de Docker simplemente se reconstruye con las mismas parcelas que se instalaron en el cl√∫ster. <br></li></ul><br>  Para instalar el paquete en Docker, Cloudera Manager se instala primero desde los paquetes RPM.  Para la instalaci√≥n real de la parcela, se utiliza el c√≥digo Java.  El cliente en Java sabe lo que el cliente en Python no puede hacer, as√≠ que tuve que usar Java y perder algo de uniformidad), que llama a la API. <br><br>  <b>assets / install-parcels / src / InstallParcels.java</b> <br><br><pre> <code class="plaintext hljs">ParcelsResourceV5 parcels = clusters.getParcelsResource(clusterName); for (int i = 1; i &lt; args.length; i += 2) {   result = installParcel(api, parcels, args[i], args[i + 1], pause);   if (!result) {       System.exit(1);   } }</code> </pre><br><h3>  Limitaci√≥n de recursos del host </h3><br>  Para administrar los recursos de la m√°quina host, se utiliza una combinaci√≥n de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DockerSpawner</a> , un componente que ejecuta a los usuarios finales de Jupyter en un contenedor Docker separado, y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cgroups</a> , un mecanismo de administraci√≥n de recursos en Linux.  DockerSpawner utiliza la API de Docker, que le permite configurar el cgroup primario para el contenedor.  No existe tal posibilidad en el DockerSpawner regular, por lo que escribimos un c√≥digo simple que nos permite establecer la correspondencia entre las entidades de AD y el cgroup primario en la configuraci√≥n. <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">def set_extra_host_config(self):       extra_host_config = {}       if self.user.name in self.user_cgroup_parent:           cgroup_parent = self.user_cgroup_parent[self.user.name]       else:           pw_name = pwd.getpwnam(self.user.name).pw_name           group_found = False           for g in grp.getgrall():               if pw_name in g.gr_mem and g.gr_name in self.group_cgroup_parent:                   cgroup_parent = self.group_cgroup_parent[g.gr_name]                   group_found = True                   break           if not group_found:               cgroup_parent = self.cgroup_parent extra_host_config['cgroup_parent'] = cgroup_parent</code> </pre><br>  Tambi√©n se ha introducido una peque√±a modificaci√≥n que lanza Jupyter desde la misma imagen desde la que se lanza JupyterHub.  Por lo tanto, no hay necesidad de usar m√°s de una imagen. <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">current_container = None host_name = socket.gethostname() for container in self.client.containers():   if container['Id'][0:12] == host_name:       current_container = container       break self.image = current_container['Image']</code> </pre><br>  Las variables de entorno determinan exactamente qu√© ejecutar en el contenedor, Jupyter o JupyterHub: <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">#!/bin/bash ANACONDA_PATH="/opt/cloudera/parcels/Anaconda/" DEFAULT_ENV=`cat ${ANACONDA_PATH}/envs/default` source activate ${DEFAULT_ENV} if [ -z "${JUPYTERHUB_CLIENT_ID}" ]; then   while true; do       jupyterhub -f /etc/jupyterhub/jupyterhub_config.py   done else   HOME=`su ${JUPYTERHUB_USER} -c 'echo ~'`   cd ~   su ${JUPYTERHUB_USER} -p -c "jupyterhub-singleuser --KernelSpecManager.ensure_native_kernel=False --ip=0.0.0.0" fi</code> </pre><br>  La capacidad de iniciar los contenedores Jupyter Docker desde el contenedor JupyterHub Docker se logra mediante el montaje del z√≥calo del demonio Docker en el contenedor JupyterHub. <br><br>  <b>start-hub.sh</b> <br><br><pre> <code class="plaintext hljs">VOLUMES="-&lt;b&gt;v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z&lt;/b&gt; -v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre><br>  En el futuro, se planea abandonar esta decisi√≥n en favor de, por ejemplo, ssh. <br><br>  Cuando se usa DockerSpawner junto con Spark, surge otro problema: el controlador Spark abre puertos aleatorios, a trav√©s de los cuales los trabajadores establecen una conexi√≥n externa.  Podemos controlar el rango de n√∫meros de puerto de los cuales se seleccionan los aleatorios estableciendo estos rangos en la configuraci√≥n de Spark.  Sin embargo, estos rangos deben ser diferentes para diferentes usuarios, ya que no podemos ejecutar contenedores Jupyter con los mismos puertos publicados.  Para resolver este problema, se escribi√≥ un c√≥digo que simplemente genera rangos de puertos por ID de usuario de la base de datos JupyterHub y lanza el contenedor Docker y Spark con la configuraci√≥n adecuada: <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">def set_extra_create_kwargs(self):       user_spark_driver_port, user_spark_blockmanager_port, user_spark_ui_port, user_spark_max_retries = self.get_spark_ports()       if user_spark_driver_port == 0 or user_spark_blockmanager_port == 0 or user_spark_ui_port == 0 or user_spark_max_retries == 0:           return       ports = {}       for p in range(user_spark_driver_port, user_spark_driver_port + user_spark_max_retries):           ports['%d/tcp' % p] = None       for p in range(user_spark_blockmanager_port, user_spark_blockmanager_port + user_spark_max_retries):           ports['%d/tcp' % p] = None       for p in range(user_spark_ui_port, user_spark_ui_port + user_spark_max_retries):           ports['%d/tcp' % p] = None self.extra_create_kwargs = { 'ports' : ports }</code> </pre><br>  La desventaja de esta soluci√≥n es que cuando reinicia el contenedor con JupyterHub, todo deja de funcionar debido a la p√©rdida de la base de datos.  Por lo tanto, cuando reinicia JupyterHub para, por ejemplo, un cambio de configuraci√≥n, no tocamos el contenedor en s√≠, sino que solo reiniciamos el proceso de JupyterHub dentro de √©l. <br><br>  <b>reiniciar-hub.sh</b> <br><br><pre> <code class="plaintext hljs">#!/bin/bash docker ps | fgrep 'dsai1.2' | fgrep -v 'jupyter-' | awk '{ print $1; }' | while read ID; do docker exec $ID /bin/bash -c "kill \$( cat /root/jupyterhub.pid )"; done</code> </pre><br>  Los propios grupos C se crean mediante herramientas est√°ndar de Linux, la correspondencia entre las entidades AD y los grupos c en la configuraci√≥n se ve as√≠. <br><br><pre> <code class="plaintext hljs">&lt;b&gt;config-example/jupyterhub/jupyterhub_config.py&lt;/b&gt; c.DSAISpawner.user_cgroup_parent = {   'bank\\user1'    : '/jupyter-cgroup-1', # user 1   'bank\\user2'    : '/jupyter-cgroup-1', # user 2   'bank\\user3'    : '/jupyter-cgroup-2', # user 3 } c.DSAISpawner.cgroup_parent = '/jupyter-cgroup-3'</code> </pre><br><h3>  C√≥digo git </h3><br>  Nuestra soluci√≥n est√° disponible p√∫blicamente en GitHub: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/DS-AI/dsai/</a> (DSAI - Data Science and Artificial Intelligence).  Todo el c√≥digo est√° organizado en directorios con n√∫meros de serie: el c√≥digo de cada directorio posterior puede usar artefactos del anterior.  El resultado del c√≥digo del √∫ltimo directorio ser√° una imagen de Docker. <br><br>  Cada directorio contiene archivos: <br><br><ul><li>  assets.sh: creaci√≥n de artefactos necesarios para el ensamblaje (descarga de Internet o copia de los directorios de los pasos anteriores) <br></li><li>  build.sh - construir <br></li><li>  clean.sh: artefactos de limpieza necesarios para el ensamblaje <br></li></ul><br>  Para reconstruir completamente la imagen de Docker, es necesario ejecutar clean.sh, assets.sh, build.sh desde los directorios de acuerdo con sus n√∫meros de serie. <br><br>  Para el ensamblaje, utilizamos una m√°quina con Linux RedHat 7.4, Docker 17.05.0-ce.  La m√°quina tiene 8 n√∫cleos, 32 GB de RAM y 250 GB de espacio en disco.  Se recomienda encarecidamente que no utilice un host con la peor configuraci√≥n de RAM y HDD para construirlo. <br><br>  Aqu√≠ est√° la ayuda para los nombres utilizados: <br><br><ul><li>  01-spark-parcheado - RPM Spark 1.6.1 con dos parches aplicados SPARK-4563 y SPARK-19019. <br></li><li>  02-validator - validador de paquetes <br></li><li>  03-anaconda-dsai-parcel-1.0 - parcela Anaconda con el Python correcto (2, 3.5 y 3.6) <br></li><li>  04-cloudera-manager-api - Bibliotecas API de Cloudera Manager <br></li><li>  05-dsai1.2-offline - imagen final <br></li></ul><br>  Por desgracia, el ensamblaje puede bloquearse por razones que no pudimos reparar (por ejemplo, el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">alquitr√°n se cae</a> durante el ensamblaje del paquete. En este caso, por regla general, solo necesita reiniciar el ensamblaje, pero esto no siempre ayuda (por ejemplo, el ensamblaje Spark depende de recursos externos) Cloudera, que puede que ya no est√© disponible, etc.). <br><br>  Otro inconveniente es que el conjunto de paquetes es irreproducible.  Dado que las bibliotecas se actualizan constantemente, la repetici√≥n del ensamblado puede dar un resultado diferente al anterior. <br><br><h2>  Gran final </h2><br>  Ahora los usuarios utilizan con √©xito las herramientas, su n√∫mero ha superado varias docenas y contin√∫a creciendo.  En el futuro, planeamos probar JupyterLab y estamos pensando en conectar la GPU al cl√∫ster, porque ahora los recursos inform√°ticos de dos servidores de aplicaciones bastante potentes ya no son suficientes. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/443294/">https://habr.com/ru/post/443294/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443284/index.html">√çndices en PostgreSQL - 4 (Btree)</a></li>
<li><a href="../443286/index.html">TDMS Fairway. Mecanismo de autocompletar para las principales inscripciones en los dibujos y detalles de los documentos.</a></li>
<li><a href="../443288/index.html">Navegaci√≥n en proyectos de m√≥dulos m√∫ltiples</a></li>
<li><a href="../443290/index.html">Zen Erlang [y Elixir - aprox. traductor</a></li>
<li><a href="../443292/index.html">Estudiamos el principio de funcionamiento de las unidades em utilizando el ejemplo de la tarea "Dise√±o de un precargador flexible"</a></li>
<li><a href="../443298/index.html">Carga inal√°mbrica ¬øC√≥mo funciona en la pr√°ctica?</a></li>
<li><a href="../443300/index.html">¬øC√≥mo es el desarrollo en United Traders?</a></li>
<li><a href="../443304/index.html">Ser tecnof√≥bico no tiene sentido, incluso si la tecnofobia est√° justificada</a></li>
<li><a href="../443306/index.html">Ocho leyes de nombres en el dise√±o de experiencia de usuario (Parte 1)</a></li>
<li><a href="../443308/index.html">Mitos de la f√≠sica moderna. Leyes de conservaci√≥n</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>