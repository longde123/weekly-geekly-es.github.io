<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ôèÔ∏è ü§öüèæ üßóüèø Orientation longue distance de la machine gr√¢ce √† un apprentissage renforc√© üè≥Ô∏è‚Äçüåà üë®üèº‚Äç‚öñÔ∏è üë©üèº‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Aux √âtats-Unis seulement, 3 millions de personnes handicap√©es ne peuvent pas quitter leur domicile. Des robots auxiliaires capables de parcourir autom...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Orientation longue distance de la machine gr√¢ce √† un apprentissage renforc√©</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444372/"> Aux √âtats-Unis seulement, 3 millions de personnes handicap√©es ne peuvent pas quitter leur domicile.  Des robots auxiliaires capables de parcourir automatiquement de longues distances peuvent rendre ces personnes plus ind√©pendantes en leur apportant de la nourriture, des m√©dicaments et des emballages.  Des √©tudes montrent que l'apprentissage en profondeur avec renforcement (OP) est bien adapt√© pour comparer des donn√©es et des actions d'entr√©e brutes, par exemple, pour apprendre √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">capturer des objets</a> ou √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©placer des robots</a> , mais g√©n√©ralement les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">agents</a> OP ne comprennent pas les grands espaces physiques n√©cessaires pour une orientation s√ªre √† longue distance distances sans aide humaine et adaptation √† un nouvel environnement. <br><a name="habracut"></a><br>  Dans trois travaux r√©cents, ¬´La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">course d'orientation √† partir de z√©ro avec AOP</a> ¬ª, ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PRM-RL: mise en ≈ìuvre de l'orientation robotique sur de longues distances en utilisant une combinaison d'apprentissage par renforcement et de planification bas√©e sur les mod√®les</a> ¬ª et ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Orientation</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√† longue distance</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">avec PRM-RL</a> ¬ª, nous avons Nous √©tudions des robots autonomes qui s'adaptent facilement √† un nouvel environnement, combinant une OP profonde avec une planification √† long terme.  Nous enseignons aux agents de planification locaux comment effectuer les actions de base n√©cessaires √† l'orientation et comment se d√©placer sur de courtes distances sans collision avec des objets en mouvement.  Les planificateurs locaux effectuent des observations environnementales bruyantes √† l'aide de capteurs tels que des lidars unidimensionnels qui fournissent la distance √† un obstacle et fournissent des vitesses lin√©aires et angulaires pour contr√¥ler le robot.  Nous formons le planificateur local aux simulations utilisant l'apprentissage automatique par renforcement (AOP), une m√©thode qui automatise la recherche de r√©compenses pour l'OP et l'architecture du r√©seau de neurones.  Malgr√© la port√©e limit√©e de 10 √† 15 m, les planificateurs locaux s'adaptent bien √† la fois √† l'utilisation dans de vrais robots et √† de nouveaux environnements jusque-l√† inconnus.  Cela vous permet de les utiliser comme blocs de construction pour l'orientation sur de grands espaces.  Ensuite, nous construisons une feuille de route, un graphique o√π les n≈ìuds sont des sections distinctes, et les bords connectent les n≈ìuds uniquement si des planificateurs locaux, imitant bien de vrais robots utilisant des capteurs et des contr√¥les bruyants, peuvent se d√©placer entre eux. <br><br><h2>  Apprentissage automatique par renforcement (AOP) </h2><br>  Dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">notre premier travail,</a> nous formons un planificateur local dans un petit environnement statique.  Cependant, lors de l'apprentissage avec l'algorithme OP profond standard, par exemple le gradient d√©terministe profond ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DDPG</a> ), il y a plusieurs obstacles.  Par exemple, le v√©ritable objectif des planificateurs locaux est d'atteindre un objectif donn√©, gr√¢ce auquel ils re√ßoivent de rares r√©compenses.  En pratique, cela oblige les chercheurs √† consacrer un temps consid√©rable √† la mise en ≈ìuvre pas √† pas de l'algorithme et √† l'ajustement manuel des bourses.  Les chercheurs doivent √©galement prendre des d√©cisions sur l'architecture des r√©seaux de neurones sans avoir de recettes claires et r√©ussies.  Enfin, des algorithmes tels que DDPG apprennent de mani√®re instable et pr√©sentent souvent un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">oubli catastrophique</a> . <br><br>  Pour surmonter ces obstacles, nous avons automatis√© le deep learning avec renforcement.  AOP est un wrapper automatique √©volutif autour d'un OP profond, recherchant des r√©compenses et une architecture de r√©seau neuronal gr√¢ce <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√† une optimisation hyperparam√©trique</a> √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">grande √©chelle</a> .  Il fonctionne en deux temps, la recherche de r√©compenses et la recherche d'architecture.  Pendant la recherche de r√©compenses, AOP forme simultan√©ment la population d'agents DDPG pendant plusieurs g√©n√©rations, et chacun a sa propre fonction de r√©compense l√©g√®rement modifi√©e, optimis√©e pour la vraie t√¢che du planificateur local: atteindre le point final du chemin.  √Ä la fin de la phase de recherche de r√©compense, nous en s√©lectionnons une qui conduit le plus souvent les agents √† l'objectif.  Dans la phase de recherche de l'architecture du r√©seau neuronal, nous r√©p√©tons ce processus, en utilisant le prix s√©lectionn√© pour cette course et en ajustant les couches du r√©seau, en optimisant le prix cumulatif. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c96/017/74a/c9601774ae263fe9a2f333d2066e923d.png"><br>  <i>AOP √† la recherche de prix et d'architecture du r√©seau neuronal</i> <br><br>  Cependant, ce processus √©tape par √©tape rend l'AOP inefficace en termes de nombre d'√©chantillons.  La formation AOP avec 10 g√©n√©rations de 100 agents n√©cessite 5 milliards d'√©chantillons, soit 32 ans d'√©tudes!  L'avantage est qu'apr√®s AOP, le processus d'apprentissage manuel est automatis√© et DDPG n'a pas d'oubli catastrophique.  Plus important encore, la qualit√© des politiques finales est plus √©lev√©e - elles r√©sistent au bruit provenant du capteur, du lecteur et de la localisation, et sont bien g√©n√©ralis√©es √† de nouveaux environnements.  Notre meilleure politique est 26% plus efficace que les autres m√©thodes d'orientation sur nos sites de test. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/049/823/97c/04982397cb2b6b7a7d20bc9e49ee1a75.png"><br>  <i>Rouge - Succ√®s de l'AOP sur de courtes distances (jusqu'√† 10 m) dans plusieurs b√¢timents jusque-l√† inconnus.</i>  <i>Comparaison avec DDPG entra√Æn√© manuellement (rouge fonc√©), champs de potentiel artificiel (bleu), fen√™tre dynamique (bleu) et clonage de comportement (vert).</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Kq1nQAF4xeM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>La politique du planificateur AOP local fonctionne bien avec les robots dans des environnements r√©els non structur√©s</i> <br><br>  Et bien que ces politiciens ne soient capables que d'une orientation locale, ils r√©sistent aux obstacles mobiles et sont bien tol√©r√©s par de vrais robots dans des environnements non structur√©s.  Et bien qu'ils aient √©t√© form√©s aux simulations avec des objets statiques, ils g√®rent efficacement les objets en mouvement.  L'√©tape suivante consiste √† combiner les politiques AOP avec une planification bas√©e sur des √©chantillons afin d'√©largir leur domaine de travail et leur apprendre √† naviguer sur de longues distances. <br><br><h2>  Orientation longue distance avec PRM-RL </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Les planificateurs bas√©s sur des mod√®les</a> fonctionnent avec une orientation √† longue distance, rapprochant les mouvements du robot.  Par exemple, un robot cr√©e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des feuilles de route probabilistes</a> (PRM) en dessinant des chemins de transition entre les sections.  Dans notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">deuxi√®me travail</a> , qui a remport√© le prix lors de la conf√©rence <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ICRA 2018</a> , nous combinons PRM avec des programmateurs OP locaux r√©gl√©s manuellement (sans AOP) pour former des robots localement, puis les adapter √† d'autres environnements. <br><br>  Tout d'abord, pour chaque robot, nous formons la politique du planificateur local dans une simulation g√©n√©ralis√©e.  Ensuite, nous cr√©ons un PRM prenant en compte cette politique, le soi-disant PRM-RL, bas√© sur une carte de l'environnement o√π il sera utilis√©.  La m√™me carte peut √™tre utilis√©e pour tout robot que nous souhaitons utiliser dans le b√¢timent. <br><br>  Pour cr√©er un PRM-RL, nous combinons des n≈ìuds √† partir d'√©chantillons uniquement si le planificateur OP local peut se d√©placer de mani√®re fiable et r√©p√©t√©e entre eux.  Cela se fait dans une simulation Monte Carlo.  La carte r√©sultante s'adapte aux capacit√©s et √† la g√©om√©trie d'un robot particulier.  Les cartes pour robots ayant la m√™me g√©om√©trie, mais avec diff√©rents capteurs et lecteurs, auront une connectivit√© diff√©rente.  √âtant donn√© que l'agent peut tourner autour du coin, des n≈ìuds qui ne sont pas en ligne de vue directe peuvent √©galement √™tre inclus.  Cependant, les n≈ìuds adjacents aux murs et aux obstacles seront moins susceptibles d'√™tre inclus dans la carte en raison du bruit du capteur.  Au moment de l'ex√©cution, l'agent OP se d√©place sur la carte d'une section √† l'autre. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/227/75f/f50/22775ff503bbaeb6113227523d06aa8a.gif"><br>  <i>Une carte est cr√©√©e avec trois simulations de Monte Carlo pour chaque paire de n≈ìuds s√©lectionn√©e au hasard</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/546/268/9f1/5462689f131cbd48339eec89f36add51.png"><br>  <i>La plus grande carte mesurait 288 x 163 m et contenait pr√®s de 700 000 bords.</i>  <i>300 travailleurs l'ont r√©cup√©r√© pendant 4 jours, apr√®s avoir effectu√© 1,1 milliard de contr√¥les de collision.</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le troisi√®me ouvrage</a> apporte plusieurs am√©liorations au PRM-RL d'origine.  Premi√®rement, nous rempla√ßons le DDPG r√©gl√© manuellement par des ordonnanceurs AOP locaux, ce qui am√©liore l'orientation sur de longues distances.  Deuxi√®mement, des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cartes de localisation et de marquage simultan√©s</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SLAM</a> ) sont ajout√©es, que les robots utilisent au moment de l'ex√©cution comme source pour cr√©er des feuilles de route.  Les cartes SLAM sont sujettes au bruit, ce qui comble le ¬´foss√© entre le simulateur et la r√©alit√©¬ª, un probl√®me bien connu en robotique, √† cause duquel les agents form√©s aux simulations se comportent bien pire dans le monde r√©el.  Notre niveau de r√©ussite dans la simulation co√Øncide avec le niveau de r√©ussite de vrais robots.  Enfin, nous avons ajout√© des cartes de construction r√©parties, afin de pouvoir cr√©er de tr√®s grandes cartes contenant jusqu'√† 700 000 n≈ìuds. <br><br>  Nous avons √©valu√© cette m√©thode avec l'aide de notre agent AOP, qui a cr√©√© des cartes bas√©es sur des dessins de b√¢timents qui ont d√©pass√© l'environnement d'entra√Ænement de 200 fois dans la zone, y compris uniquement les c√¥tes, qui ont √©t√© r√©ussies dans 90% des cas en 20 tentatives.  Nous avons compar√© PRM-RL avec diff√©rentes m√©thodes √† des distances allant jusqu'√† 100 m, ce qui d√©passait consid√©rablement la port√©e du planificateur local.  PRM-RL a obtenu un succ√®s 2 √† 3 fois plus souvent que les m√©thodes conventionnelles gr√¢ce √† la connexion correcte des n≈ìuds, adapt√©e aux capacit√©s du robot. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa4/659/37a/fa465937a0e3a90383480a831ef4cec7.png"><br>  <i>Taux de r√©ussite du d√©placement de 100 m dans diff√©rents b√¢timents.</i>  <i>Bleu - planificateur AOP local, premier travail;</i>  <i>rouge - PMR d'origine;</i>  <i>jaune - champs potentiels artificiels;</i>  <i>le vert est le deuxi√®me emploi;</i>  <i>rouge - le troisi√®me travail, PRM avec AOP.</i> <br><br>  Nous avons test√© PRM-RL sur de nombreux robots r√©els dans de nombreux b√¢timents.  Voici l'une des suites de tests;  le robot se d√©place de mani√®re fiable presque partout, √† l'exception des endroits et des zones les plus en d√©sordre qui d√©passent la carte SLAM. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e05/773/fbd/e05773fbd5e6cd4cb41adfebf9a8d083.png"><br><br><h2>  Conclusion </h2><br>  L'orientation machine peut s√©rieusement augmenter l'ind√©pendance des personnes √† mobilit√© r√©duite.  Cela peut √™tre r√©alis√© en d√©veloppant des robots autonomes qui peuvent facilement s'adapter √† l'environnement et aux m√©thodes disponibles pour la mise en ≈ìuvre dans le nouvel environnement sur la base des informations existantes.  Cela peut √™tre fait en automatisant la formation d'orientation de base sur de courtes distances avec AOP, puis en utilisant les comp√©tences acquises avec des cartes SLAM pour cr√©er des feuilles de route.  Les feuilles de route sont constitu√©es de n≈ìuds reli√©s par des nervures, sur lesquels les robots peuvent se d√©placer de mani√®re fiable.  En cons√©quence, une politique de comportement du robot est d√©velopp√©e qui, apr√®s une formation, peut √™tre utilis√©e dans diff√©rents environnements et √©mettre des feuilles de route sp√©cialement adapt√©es pour un robot particulier. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr444372/">https://habr.com/ru/post/fr444372/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr444362/index.html">Unity et Havok travaillent sur un nouveau moteur physique</a></li>
<li><a href="../fr444364/index.html">Jeu Rust 24 heures: exp√©rience de d√©veloppement personnel</a></li>
<li><a href="../fr444366/index.html">S√©minaire "Exigences de s√©curit√© de l'information: comment les entreprises peuvent vivre avec elles"</a></li>
<li><a href="../fr444368/index.html">Nous venons d'imprimer le microphone sur une imprimante 3D dans le laboratoire - puis il y aura de la science-fiction compl√®te</a></li>
<li><a href="../fr444370/index.html">De quoi le format Mini PCI-e est-il capable?</a></li>
<li><a href="../fr444374/index.html">Effet hipster: pourquoi les non-conformistes se ressemblent souvent</a></li>
<li><a href="../fr444376/index.html">L'√©conomie de l'attention est presque morte</a></li>
<li><a href="../fr444378/index.html">USPACE - Espace unique pour les avions avec et sans pilote</a></li>
<li><a href="../fr444382/index.html">Comment visiter l'Universit√© de Cor√©e avec le syst√®me de fichiers r√©seau</a></li>
<li><a href="../fr444384/index.html">Livre "Analyse des donn√©es textuelles appliqu√©es en Python"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>