<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§µüèæ üîÇ üë©üèæ‚Äçüöí Multiprocessing Intel Neural Computer Stick Zugriff √ºber REST ü§æüèæ üí™üèª üë©‚Äçüç≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Single-Tasking-Problem 
 In der letzten Serie habe ich den Intel Neural Computer Stick 2 auf den Tank gelegt und alle Berechnungen des neuronalen Netz...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Multiprocessing Intel Neural Computer Stick Zugriff √ºber REST</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469265/"><h2>  Single-Tasking-Problem </h2><br>  In der letzten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Serie habe</a> ich den Intel Neural Computer Stick 2 auf den Tank gelegt und alle Berechnungen des neuronalen Netzwerks darauf geworfen, wobei ich Tensorflow und OpenCV-DNN aufgegeben habe. <br><br>  Es gab ein Problem, auf das ich damals bereits gesto√üen bin - die Unf√§higkeit, mit NCS aus mehreren Prozessen gleichzeitig zu arbeiten.  Damals war es nicht kritisch, aber jetzt ist es Zeit, es herauszufinden. <br><a name="habracut"></a><br>  Beim Versuch, ein Modell aus dem zweiten Prozess zu laden, begann OpenVino zu schw√∂ren: <br><br><pre><code class="bash hljs">E: [ncAPI] [ 926029] resetAll:348 Failed to connect to stalled device, rc: X_LINK_ERROR E: [ncAPI] [ 933282] ncDeviceOpen:672 Failed to find suitable device, rc: X_LINK_DEVICE_NOT_FOUND</code> </pre> <br>  Bei der Suche im Intel Support Forum wurde ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√§hnliches Problem</a> festgestellt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">.</a> <br><br>  Von dort wurden wir zu der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation weitergeleitet,</a> wo klar angegeben ist: <br><br><blockquote>  Ein einzelnes Ger√§t kann nicht f√ºr mehrere Prozesse freigegeben werden. </blockquote><br>  In diesem Experiment k√∂nnen Sie den Multiprozesszugriff minimieren und beginnen. <br><br><h2>  NCS-Service </h2><br>  Es ist v√∂llig logisch, die direkte Arbeit mit NCS in einem separaten Dienst zu platzieren und die API an alle Clients zu verteilen, √ºber die sie arbeiten werden. <br><br>  Im Allgemeinen sollte dies ein Thema √ºber den Roboter und seine neuen Errungenschaften in Bezug auf neuronale Netze sein.  Es stellte sich jedoch heraus, dass das Material zur NCS-API in einem separaten Artikel enthalten ist. <br><br><h3>  NCS-API </h3><br>  Auf niedriger Ebene ist die NCS-API sehr einfach: <br>  - Modell laden <br>  - Berechnung starten <br>  - Holen Sie sich eine Liste der Modelle <br>  - Modelleigenschaften abrufen <br><br>  Wenn beim Laden des Modells alles eindeutig ist, ist der Berechnungsauspuff ein kontextsensitiver Tensor, den der Client m√∂glicherweise nicht alle ben√∂tigt. <br><br>  Das Abrufen einer Liste von Modellen ist ebenfalls ziemlich transparent, und die Dimension des Eingangstensors f√§llt sofort ein - f√ºr den Menschen bedeutet dies, dass es sch√∂n w√§re, die Bilder im Voraus an die Netzwerkeinstellungen anzupassen. <br><br>  Dar√ºber hinaus ist ein niedriger Pegel gut, aber wenn Sie spezielle Operationen unterst√ºtzen, vereinfacht dies die Logik und die Daten. <br><br>  Daher gibt es neben der Basis eine Aufgabe, die API f√ºr die Klassifizierung, Erkennung und Segmentierung zu unterst√ºtzen. <br><br>  Leider werden die interessantesten Segmentierungsmodelle vom NCS <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nicht unterst√ºtzt</a> , sodass Sie sich auf das Einfachste mit der Stra√üe und dem Markup beschr√§nken m√ºssen. <br><br>  Jede dieser Operationen verwendet die Grundberechnung des Modells, unterscheidet sich jedoch in der Interpretation des Ausgangstensors. <br><br><h3>  Hauptschnittstelle </h3><br>  Die Hauptschnittstelle enth√§lt also Methoden: <br><br><ul><li>  POST: / load - Laden Sie das Modell </li><li>  POST: / unload / $ model - L√∂scht das Modell (aus dem Dienst kann es nicht vom Ger√§t entfernt werden) </li><li>  GET: / list - Eine Liste der Modelle abrufen </li><li>  GET: / input / shape / $ model - Ermitteln Sie die Dimension des Eingangstensors </li><li>  POST: / inference / file / $ model - Berechnen Sie mit Daten aus dem Speicher </li><li>  POST: / inference / path / $ model - Berechnen Sie mit Daten im Dateisystem </li></ul><br>  Hier sind zwei W√∂rter zu Daten aus dem Speicher und dem Dateisystem: <br><br>  Wenn der NCS-Dienst und sein Benutzer auf derselben Himbeere ausgef√ºhrt werden, ist es sinnvoll, beim √úbertragen des Bildes zu speichern und stattdessen den Pfad so zu √ºbertragen, dass der Dienst selbst die Datei liest. <br>  Befindet sich das Bild bereits im Speicher (oder ist es im Dateisystem nicht vorhanden), √ºbertragen wir es direkt von dort. <br><br>  Tests zeigen, dass die √úbertragung von Bytes aus dem Speicher erheblich langsamer ist (Messung f√ºr 1000 Versuche): <br><br>  Aus dem Speicher: 87,5 Sekunden <br>  Dateipfad: 63,3150 Sekunden <br><br>  Diese beiden Optionen werden jedoch f√ºr jede Methode unterst√ºtzt, sowohl f√ºr die allgemeine Berechnung als auch f√ºr die folgenden Sonderf√§lle. <br><br>  Im Allgemeinen nimmt die Inferenzmethode ein Bild in Form eines Numpy-Arrays als Eingabe und erzeugt einen Tensor im gleichen Format. <br>  Die Interpretation des Auspuffs ist bereits ein Kundenproblem. <br>  Um diese Aufgabe zu erleichtern, unterst√ºtzt der Dienst spezielle Methoden, die wichtige Informationen in menschlicher Form aus dem Ausgangstensor extrahieren. <br><br><h3>  Klassifizierung </h3><br>  Zur Klassifizierung erstellen wir eine separate REST-Methode, die den Ausgangstensor in eine Reihe von Paaren (Klasse, Punktzahl) konvertiert. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_class_tensor</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> ret = [] thr = <span class="hljs-number"><span class="hljs-number">0.01</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>): cls = np.argmax(data) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> data[cls] &lt; thr: <span class="hljs-keyword"><span class="hljs-keyword">break</span></span>; logging.debug((<span class="hljs-string"><span class="hljs-string">"Class"</span></span>, cls, <span class="hljs-string"><span class="hljs-string">"score"</span></span>, data[cls])) c = {<span class="hljs-string"><span class="hljs-string">"class"</span></span> : int(cls), <span class="hljs-string"><span class="hljs-string">"score"</span></span> : int(<span class="hljs-number"><span class="hljs-number">100</span></span> * data[cls])} data[cls] = <span class="hljs-number"><span class="hljs-number">0</span></span> ret.append(c) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ret <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">classify</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_id, img)</span></span></span><span class="hljs-function">:</span></span> rc, out = run_inference(model_id, img) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> rc: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> rc, out <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, get_class_tensor(out)</code> </pre><br>  Wie bei der normalen Ausgabe werden zwei Methoden unterst√ºtzt - √ºber eine Datei im Speicher und einen Pfad auf der Festplatte. <br><br><ul><li>  POST: / classify / file / $ model </li><li>  POST: / classify / path / $ model </li></ul><br><h3>  Erkennung </h3><br>  Der Detektorausgangstensor enth√§lt eine Menge (Klasse, Wahrscheinlichkeit, normalisierte Koordinaten) und sieht ziemlich umst√§ndlich aus. <br><br>  Wir verwandeln es in eine verst√§ndliche Form, w√§hrend wir unwahrscheinliche Optionen abschneiden: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_detect_from_tensor</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(t, rows, cols)</span></span></span><span class="hljs-function">:</span></span> score = int(<span class="hljs-number"><span class="hljs-number">100</span></span> * t[<span class="hljs-number"><span class="hljs-number">2</span></span>]) cls = int(t[<span class="hljs-number"><span class="hljs-number">1</span></span>]) left = int(t[<span class="hljs-number"><span class="hljs-number">3</span></span>] * cols) top = int(t[<span class="hljs-number"><span class="hljs-number">4</span></span>] * rows) right = int(t[<span class="hljs-number"><span class="hljs-number">5</span></span>] * cols) bottom = int(t[<span class="hljs-number"><span class="hljs-number">6</span></span>] * rows) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> {<span class="hljs-string"><span class="hljs-string">"class"</span></span> : cls, <span class="hljs-string"><span class="hljs-string">"score"</span></span> : score, <span class="hljs-string"><span class="hljs-string">"x"</span></span> : left, <span class="hljs-string"><span class="hljs-string">"y"</span></span> : top, <span class="hljs-string"><span class="hljs-string">"w"</span></span> : (right - left), <span class="hljs-string"><span class="hljs-string">"h"</span></span> : (bottom - top)} <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">build_detection</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, thr, rows, cols)</span></span></span><span class="hljs-function">:</span></span> T = {} <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data: score = t[<span class="hljs-number"><span class="hljs-number">2</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> score &gt; thr: cls = int(t[<span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> cls <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> T: T[cls] = get_detect_from_tensor(t, rows, cols) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: a = T[cls] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> a[<span class="hljs-string"><span class="hljs-string">"score"</span></span>] &lt; score: T[cls] = get_detect_from_tensor(t, rows, cols) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> list(T.values()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detect</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_id, img)</span></span></span><span class="hljs-function">:</span></span> rc, out = run_inference(model_id, img) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> rc: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> rc, out rows, cols = img.shape[:<span class="hljs-number"><span class="hljs-number">2</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, build_detection(out[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">0.01</span></span>, rows, cols)</code> </pre><br>  Wie √ºblich werden beide Methoden unterst√ºtzt: <br><br><ul><li>  POST: / detect / file / $ model </li><li>  POST: / detect / path / $ model </li></ul><br><h3>  Segmentierung </h3><br>  Der Segmentierungstensor enth√§lt Wahrscheinlichkeiten nach Klasse und sogar in der Dimension des neuronalen Netzwerks. <br>  Konvertieren Sie dies einfach in eine Klassenmaske: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">segment</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_id, img)</span></span></span><span class="hljs-function">:</span></span> rc, out = run_inference(model_id, img) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> rc: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> rc, out out = np.argmax(out, axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) out = cv.resize(out, (img.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], img.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]),interpolation=cv.INTER_NEAREST) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, out</code> </pre><br><ul><li>  POST: / segment / file / $ model </li><li>  POST: / segment / path / $ model </li></ul><br><h2>  Fazit </h2><br>  Wie bereits erw√§hnt, hatte ich urspr√ºnglich vor, in einem der Kapitel des Artikels √ºber den Dienst √ºber den Dienst zu sprechen, aber es stellte sich heraus, dass der Band auf einem separaten Dokument abgerufen wird. <br><br>  Auch hier verwende ich den Dienst auf dem Raspberry Pi, aber er kann auf jeder Plattform ausgef√ºhrt werden, auf der Python und OpenVino mit NCS vorhanden sind. <br><br><h2>  Referenzen </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intel OpenVino Intro</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Raspbian Installationsanleitung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenVino Model Zoo - eine Liste vorgefertigter Modelle mit Beschreibungen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ressource, in der Sie OpenVino-Modelle herunterladen k√∂nnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github Service Quellcode</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de469265/">https://habr.com/ru/post/de469265/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de469249/index.html">Nitter, ein alternatives Frontend f√ºr Twitter</a></li>
<li><a href="../de469253/index.html">Wie viel kostet die Erstellung einer App?</a></li>
<li><a href="../de469257/index.html">Wir integrieren Linux-Befehle mit PowerShell und WSL in Windows</a></li>
<li><a href="../de469259/index.html">Klimawandel: Wir analysieren die Temperatur in verschiedenen St√§dten in den letzten 100 Jahren</a></li>
<li><a href="../de469263/index.html">Warum ist Karma auf Habr√© gut?</a></li>
<li><a href="../de469267/index.html">Behandlung von Systemaufrufen mit LD_PRELOAD mit einem einzigen Einstiegspunkt</a></li>
<li><a href="../de469271/index.html">Serialisieren und Deserialisieren von .NET Core vs Go-Daten</a></li>
<li><a href="../de469275/index.html">Wie ich es an 18 US-Universit√§ten getan habe</a></li>
<li><a href="../de469277/index.html">Wer sind DevOps?</a></li>
<li><a href="../de469287/index.html">Kampfgolems von den Karten. Wie wir das Spiel in die Parobot Card League verwandelt haben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>