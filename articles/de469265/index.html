<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤵🏾 🔂 👩🏾‍🚒 Multiprocessing Intel Neural Computer Stick Zugriff über REST 🤾🏾 💪🏻 👩‍🍳</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Single-Tasking-Problem 
 In der letzten Serie habe ich den Intel Neural Computer Stick 2 auf den Tank gelegt und alle Berechnungen des neuronalen Netz...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Multiprocessing Intel Neural Computer Stick Zugriff über REST</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469265/"><h2>  Single-Tasking-Problem </h2><br>  In der letzten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Serie habe</a> ich den Intel Neural Computer Stick 2 auf den Tank gelegt und alle Berechnungen des neuronalen Netzwerks darauf geworfen, wobei ich Tensorflow und OpenCV-DNN aufgegeben habe. <br><br>  Es gab ein Problem, auf das ich damals bereits gestoßen bin - die Unfähigkeit, mit NCS aus mehreren Prozessen gleichzeitig zu arbeiten.  Damals war es nicht kritisch, aber jetzt ist es Zeit, es herauszufinden. <br><a name="habracut"></a><br>  Beim Versuch, ein Modell aus dem zweiten Prozess zu laden, begann OpenVino zu schwören: <br><br><pre><code class="bash hljs">E: [ncAPI] [ 926029] resetAll:348 Failed to connect to stalled device, rc: X_LINK_ERROR E: [ncAPI] [ 933282] ncDeviceOpen:672 Failed to find suitable device, rc: X_LINK_DEVICE_NOT_FOUND</code> </pre> <br>  Bei der Suche im Intel Support Forum wurde ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ähnliches Problem</a> festgestellt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">.</a> <br><br>  Von dort wurden wir zu der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation weitergeleitet,</a> wo klar angegeben ist: <br><br><blockquote>  Ein einzelnes Gerät kann nicht für mehrere Prozesse freigegeben werden. </blockquote><br>  In diesem Experiment können Sie den Multiprozesszugriff minimieren und beginnen. <br><br><h2>  NCS-Service </h2><br>  Es ist völlig logisch, die direkte Arbeit mit NCS in einem separaten Dienst zu platzieren und die API an alle Clients zu verteilen, über die sie arbeiten werden. <br><br>  Im Allgemeinen sollte dies ein Thema über den Roboter und seine neuen Errungenschaften in Bezug auf neuronale Netze sein.  Es stellte sich jedoch heraus, dass das Material zur NCS-API in einem separaten Artikel enthalten ist. <br><br><h3>  NCS-API </h3><br>  Auf niedriger Ebene ist die NCS-API sehr einfach: <br>  - Modell laden <br>  - Berechnung starten <br>  - Holen Sie sich eine Liste der Modelle <br>  - Modelleigenschaften abrufen <br><br>  Wenn beim Laden des Modells alles eindeutig ist, ist der Berechnungsauspuff ein kontextsensitiver Tensor, den der Client möglicherweise nicht alle benötigt. <br><br>  Das Abrufen einer Liste von Modellen ist ebenfalls ziemlich transparent, und die Dimension des Eingangstensors fällt sofort ein - für den Menschen bedeutet dies, dass es schön wäre, die Bilder im Voraus an die Netzwerkeinstellungen anzupassen. <br><br>  Darüber hinaus ist ein niedriger Pegel gut, aber wenn Sie spezielle Operationen unterstützen, vereinfacht dies die Logik und die Daten. <br><br>  Daher gibt es neben der Basis eine Aufgabe, die API für die Klassifizierung, Erkennung und Segmentierung zu unterstützen. <br><br>  Leider werden die interessantesten Segmentierungsmodelle vom NCS <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nicht unterstützt</a> , sodass Sie sich auf das Einfachste mit der Straße und dem Markup beschränken müssen. <br><br>  Jede dieser Operationen verwendet die Grundberechnung des Modells, unterscheidet sich jedoch in der Interpretation des Ausgangstensors. <br><br><h3>  Hauptschnittstelle </h3><br>  Die Hauptschnittstelle enthält also Methoden: <br><br><ul><li>  POST: / load - Laden Sie das Modell </li><li>  POST: / unload / $ model - Löscht das Modell (aus dem Dienst kann es nicht vom Gerät entfernt werden) </li><li>  GET: / list - Eine Liste der Modelle abrufen </li><li>  GET: / input / shape / $ model - Ermitteln Sie die Dimension des Eingangstensors </li><li>  POST: / inference / file / $ model - Berechnen Sie mit Daten aus dem Speicher </li><li>  POST: / inference / path / $ model - Berechnen Sie mit Daten im Dateisystem </li></ul><br>  Hier sind zwei Wörter zu Daten aus dem Speicher und dem Dateisystem: <br><br>  Wenn der NCS-Dienst und sein Benutzer auf derselben Himbeere ausgeführt werden, ist es sinnvoll, beim Übertragen des Bildes zu speichern und stattdessen den Pfad so zu übertragen, dass der Dienst selbst die Datei liest. <br>  Befindet sich das Bild bereits im Speicher (oder ist es im Dateisystem nicht vorhanden), übertragen wir es direkt von dort. <br><br>  Tests zeigen, dass die Übertragung von Bytes aus dem Speicher erheblich langsamer ist (Messung für 1000 Versuche): <br><br>  Aus dem Speicher: 87,5 Sekunden <br>  Dateipfad: 63,3150 Sekunden <br><br>  Diese beiden Optionen werden jedoch für jede Methode unterstützt, sowohl für die allgemeine Berechnung als auch für die folgenden Sonderfälle. <br><br>  Im Allgemeinen nimmt die Inferenzmethode ein Bild in Form eines Numpy-Arrays als Eingabe und erzeugt einen Tensor im gleichen Format. <br>  Die Interpretation des Auspuffs ist bereits ein Kundenproblem. <br>  Um diese Aufgabe zu erleichtern, unterstützt der Dienst spezielle Methoden, die wichtige Informationen in menschlicher Form aus dem Ausgangstensor extrahieren. <br><br><h3>  Klassifizierung </h3><br>  Zur Klassifizierung erstellen wir eine separate REST-Methode, die den Ausgangstensor in eine Reihe von Paaren (Klasse, Punktzahl) konvertiert. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_class_tensor</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> ret = [] thr = <span class="hljs-number"><span class="hljs-number">0.01</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>): cls = np.argmax(data) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> data[cls] &lt; thr: <span class="hljs-keyword"><span class="hljs-keyword">break</span></span>; logging.debug((<span class="hljs-string"><span class="hljs-string">"Class"</span></span>, cls, <span class="hljs-string"><span class="hljs-string">"score"</span></span>, data[cls])) c = {<span class="hljs-string"><span class="hljs-string">"class"</span></span> : int(cls), <span class="hljs-string"><span class="hljs-string">"score"</span></span> : int(<span class="hljs-number"><span class="hljs-number">100</span></span> * data[cls])} data[cls] = <span class="hljs-number"><span class="hljs-number">0</span></span> ret.append(c) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ret <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">classify</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_id, img)</span></span></span><span class="hljs-function">:</span></span> rc, out = run_inference(model_id, img) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> rc: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> rc, out <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, get_class_tensor(out)</code> </pre><br>  Wie bei der normalen Ausgabe werden zwei Methoden unterstützt - über eine Datei im Speicher und einen Pfad auf der Festplatte. <br><br><ul><li>  POST: / classify / file / $ model </li><li>  POST: / classify / path / $ model </li></ul><br><h3>  Erkennung </h3><br>  Der Detektorausgangstensor enthält eine Menge (Klasse, Wahrscheinlichkeit, normalisierte Koordinaten) und sieht ziemlich umständlich aus. <br><br>  Wir verwandeln es in eine verständliche Form, während wir unwahrscheinliche Optionen abschneiden: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_detect_from_tensor</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(t, rows, cols)</span></span></span><span class="hljs-function">:</span></span> score = int(<span class="hljs-number"><span class="hljs-number">100</span></span> * t[<span class="hljs-number"><span class="hljs-number">2</span></span>]) cls = int(t[<span class="hljs-number"><span class="hljs-number">1</span></span>]) left = int(t[<span class="hljs-number"><span class="hljs-number">3</span></span>] * cols) top = int(t[<span class="hljs-number"><span class="hljs-number">4</span></span>] * rows) right = int(t[<span class="hljs-number"><span class="hljs-number">5</span></span>] * cols) bottom = int(t[<span class="hljs-number"><span class="hljs-number">6</span></span>] * rows) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> {<span class="hljs-string"><span class="hljs-string">"class"</span></span> : cls, <span class="hljs-string"><span class="hljs-string">"score"</span></span> : score, <span class="hljs-string"><span class="hljs-string">"x"</span></span> : left, <span class="hljs-string"><span class="hljs-string">"y"</span></span> : top, <span class="hljs-string"><span class="hljs-string">"w"</span></span> : (right - left), <span class="hljs-string"><span class="hljs-string">"h"</span></span> : (bottom - top)} <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">build_detection</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, thr, rows, cols)</span></span></span><span class="hljs-function">:</span></span> T = {} <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data: score = t[<span class="hljs-number"><span class="hljs-number">2</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> score &gt; thr: cls = int(t[<span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> cls <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> T: T[cls] = get_detect_from_tensor(t, rows, cols) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: a = T[cls] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> a[<span class="hljs-string"><span class="hljs-string">"score"</span></span>] &lt; score: T[cls] = get_detect_from_tensor(t, rows, cols) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> list(T.values()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detect</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_id, img)</span></span></span><span class="hljs-function">:</span></span> rc, out = run_inference(model_id, img) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> rc: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> rc, out rows, cols = img.shape[:<span class="hljs-number"><span class="hljs-number">2</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, build_detection(out[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">0.01</span></span>, rows, cols)</code> </pre><br>  Wie üblich werden beide Methoden unterstützt: <br><br><ul><li>  POST: / detect / file / $ model </li><li>  POST: / detect / path / $ model </li></ul><br><h3>  Segmentierung </h3><br>  Der Segmentierungstensor enthält Wahrscheinlichkeiten nach Klasse und sogar in der Dimension des neuronalen Netzwerks. <br>  Konvertieren Sie dies einfach in eine Klassenmaske: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">segment</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_id, img)</span></span></span><span class="hljs-function">:</span></span> rc, out = run_inference(model_id, img) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> rc: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> rc, out out = np.argmax(out, axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) out = cv.resize(out, (img.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], img.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]),interpolation=cv.INTER_NEAREST) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, out</code> </pre><br><ul><li>  POST: / segment / file / $ model </li><li>  POST: / segment / path / $ model </li></ul><br><h2>  Fazit </h2><br>  Wie bereits erwähnt, hatte ich ursprünglich vor, in einem der Kapitel des Artikels über den Dienst über den Dienst zu sprechen, aber es stellte sich heraus, dass der Band auf einem separaten Dokument abgerufen wird. <br><br>  Auch hier verwende ich den Dienst auf dem Raspberry Pi, aber er kann auf jeder Plattform ausgeführt werden, auf der Python und OpenVino mit NCS vorhanden sind. <br><br><h2>  Referenzen </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intel OpenVino Intro</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Raspbian Installationsanleitung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenVino Model Zoo - eine Liste vorgefertigter Modelle mit Beschreibungen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ressource, in der Sie OpenVino-Modelle herunterladen können</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github Service Quellcode</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de469265/">https://habr.com/ru/post/de469265/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de469249/index.html">Nitter, ein alternatives Frontend für Twitter</a></li>
<li><a href="../de469253/index.html">Wie viel kostet die Erstellung einer App?</a></li>
<li><a href="../de469257/index.html">Wir integrieren Linux-Befehle mit PowerShell und WSL in Windows</a></li>
<li><a href="../de469259/index.html">Klimawandel: Wir analysieren die Temperatur in verschiedenen Städten in den letzten 100 Jahren</a></li>
<li><a href="../de469263/index.html">Warum ist Karma auf Habré gut?</a></li>
<li><a href="../de469267/index.html">Behandlung von Systemaufrufen mit LD_PRELOAD mit einem einzigen Einstiegspunkt</a></li>
<li><a href="../de469271/index.html">Serialisieren und Deserialisieren von .NET Core vs Go-Daten</a></li>
<li><a href="../de469275/index.html">Wie ich es an 18 US-Universitäten getan habe</a></li>
<li><a href="../de469277/index.html">Wer sind DevOps?</a></li>
<li><a href="../de469287/index.html">Kampfgolems von den Karten. Wie wir das Spiel in die Parobot Card League verwandelt haben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>