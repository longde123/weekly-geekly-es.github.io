<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤟 🤛🏿 👿 Larga vida a los overclockers: cómo la refrigeración líquida comenzó a dominar en los centros de datos ⚽️ 🧝🏼 🖕🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content=""Las computadoras de alta velocidad no pueden funcionar sin aire" 

 Hay un momento en la película Iron Man 2 cuando Tony Stark ve una vieja película ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Larga vida a los overclockers: cómo la refrigeración líquida comenzó a dominar en los centros de datos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442576/"><h3>  "Las computadoras de alta velocidad no pueden funcionar sin aire" </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/reQs3g5LO8E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Hay un momento en la película Iron Man 2 cuando Tony Stark ve una vieja película de su difunto padre, donde dice: "Estoy limitado por la tecnología de mi tiempo, pero un día puedes resolverlo".  Y luego cambiarás el mundo ".  Es fantástico, pero la idea que expresa es bastante real.  Las ideas de los ingenieros a menudo están muy adelantadas a su tiempo.  Siempre ha habido dispositivos en Star Trek, pero el resto del mundo ha tenido que trabajar durante décadas para crear tabletas y libros electrónicos. <br><br>  El concepto de refrigeración líquida encaja perfectamente en esta categoría.  La idea en sí ha existido desde la década de 1960, pero se mantuvo radical en comparación con una opción mucho más barata y segura para la refrigeración por aire.  Pasaron más de 40 años hasta que la refrigeración líquida comenzó a desarrollarse un poco en la década de 2000, e incluso entonces era principalmente una prerrogativa de los entusiastas de las PC que buscaban dispersar sus CPU más allá de los límites recomendados por Intel y AMD. <br><a name="habracut"></a><br>  Hoy en día, los sistemas de refrigeración líquida están ganando popularidad.  Tal sistema para PC se puede comprar por menos de $ 100, y la producción artesanal dirigida a aplicaciones industriales y centros de datos (como CoolIT, Asetek, Green Revolution Computing, Ebullient) ofrece refrigeración líquida () para servidores.  Los ZhO se usan principalmente en supercomputadoras, informática de alta velocidad u otras situaciones en las que se requiere una gran cantidad de energía de la computadora, y los procesadores funcionan con casi el 100% de carga, pero estas opciones se están volviendo más comunes. <br><br>  Hay dos tipos populares de ZhO: enfriamiento directo de chips e inmersión.  Con enfriamiento directo, el radiador se conecta a la CPU, como un enfriador estándar, pero en lugar de eso, se conectan dos tubos.  Uno viene con agua fría, un disipador térmico que absorbe el calor de la CPU, y el otro deja caliente.  Luego se enfría y vuelve a la CPU en un circuito cerrado que se asemeja al flujo sanguíneo. <br><br>  Durante el enfriamiento por inmersión, el equipo se llena con líquido que, obviamente, no debe conducir electricidad.  Este enfoque es más similar a las piscinas de enfriamiento de los reactores nucleares.  El enfriamiento sumergible sigue siendo una opción más avanzada y requiere refrigerantes más caros que una conexión directa, donde puede usar agua común.  Además, siempre existe el riesgo de fugas.  Por lo tanto, con mucho, la opción más popular es la conexión directa. <br><br>  Como uno de los principales ejemplos, tome Alphabet.  Cuando esta empresa matriz de Google presentó el AI TensorFlow 3.0 en mayo de 2018, el director Sundar Pichai dijo que estos chips eran tan potentes que "por primera vez, tuvimos que instalar refrigeración líquida en los centros de datos".  Alphabet tuvo que pagar este precio por un aumento de ocho veces en la productividad. <br><br>  Por otro lado, Skybox Datacenters anunció recientemente planes para crear una gran supercomputadora con 40,000 servidores de DownUnder GeoSolutions (DUG), diseñada para la exploración de petróleo y gas.  Este proyecto producirá 250 petaflops de potencia informática, más que cualquier otro existente, y se espera que los servidores se enfríen por fluido cuando se sumerjan en tanques llenos de fluido dieléctrico. <br><br>  En cualquier caso, "la refrigeración líquida es la refrigeración del futuro, y siempre lo será", dijo Craig Pennington, vicepresidente de diseño, operador del centro de datos de Equinix.  "Parece obvio que este es el enfoque correcto, pero nadie lo ha aplicado". <br><br>  ¿Cómo ha pasado JO del arte esotérico a la vuelta de la informática a un método casi universalmente aceptado en los centros de datos modernos?  Como todas las tecnologías, esto sucedió en parte como resultado de la evolución, prueba y error, y una gran cantidad de soluciones de ingeniería.  Sin embargo, para ZhO, los centros de datos de hoy deberían agradecer a los primeros overclockers que son héroes anónimos de este método. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2c7/137/4b2/2c71374b2b2d2158d46d7e0ddb0fdabc.png"><br>  <i>El panel de control del sistema de procesamiento de datos IBM System 360</i> <br><br><h2>  ¿Qué queremos decir con refrigeración líquida? </h2><br>  La refrigeración líquida se convirtió en una idea popular en 1964 cuando IBM estudió el tema de la refrigeración sumergible para el mainframe System 360. Era uno de los mainframes de la compañía;  Las series 700 y 7000 existieron durante más de diez años, y System / 360 "comenzó la era de la compatibilidad con computadoras, permitiendo por primera vez que diferentes máquinas de la línea de productos trabajen juntas", según escriben en IBM.  El concepto era simple: el agua enfriada tenía que fluir a través de un dispositivo que la enfriaba a una temperatura inferior a la temperatura ambiente, y luego el agua se alimentaba directamente al sistema.  El circuito utilizado por IBM ahora se conoce como enfriamiento posterior cuando el disipador térmico está montado detrás de la unidad central.  El dispositivo aspiraba aire caliente de la unidad central con ventiladores, y luego este aire se enfriaba con agua, al igual que un radiador enfría el motor de un automóvil. <br><br>  Desde entonces, los ingenieros han perfeccionado este concepto básico y han surgido dos formas dominantes de FV: inmersión y contacto directo.  La inmersión es lo que es;  La electrónica está en un baño líquido que, por razones obvias, no puede ser agua.  El líquido no debe conducir electricidad, es decir, ser un aislante (empresas como 3M incluso desarrollan específicamente líquidos para esto). <br><br>  Pero el buceo tiene muchos problemas y desventajas.  Solo se puede acceder al servidor ubicado en el líquido desde arriba.  Por lo tanto, los puertos externos deben ubicarse allí.  La ubicación del servidor de gabinetes de 1U en un bastidor no sería práctico, por lo que el servidor no puede colocarse secuencialmente.  Un dieléctrico, y generalmente uno mineral, es pequeño, muy costoso y difícil de limpiar en caso de fuga.  Se requerirán discos duros especiales, y la alteración del centro de datos requerirá importantes inversiones.  Por lo tanto, como en el caso de la supercomputadora mencionada anteriormente, la inmersión se realiza mejor en un nuevo centro de datos, en lugar de rehacer el antiguo. <br><br>  El contacto directo de JO, en cambio, es que el radiador (o intercambiador de calor) está en el chip, como un radiador normal.  En lugar de un ventilador, usa dos tuberías de agua: una que trae agua fría para enfriar y la segunda que sopla agua caliente calentada por contacto con el radiador.  Esta forma de ZhO se convirtió en la más popular, fue adoptada por fabricantes como HP Enterprise, Dell EMC e IBM, así como por fabricantes de gabinetes Chatsworth Systems y Schneider Electric. <br><br>  El enfriamiento directo usa agua, pero es muy sensible a su calidad.  No se debe usar agua del grifo sin filtrar.  Solo mira tu grifo o ducha.  Nadie necesita la acumulación de calcio en los servidores.  Al menos el enfriamiento directo requiere agua destilada pura y, a veces, su mezcla con anticongelante.  La fabricación de dicho refrigerante es una ciencia en sí misma. <br><br><h2>  Enlace de Intel </h2><br>  ¿Cómo pasamos de los radiadores de IBM a los modernos y extravagantes sistemas de enfriamiento?  De nuevo, gracias a los overclockers.  A principios de siglo, la refrigeración líquida comenzó a ganar popularidad entre los overclockers de PC y los aficionados que ensamblaron sus computadoras que querían aumentar su velocidad más allá de los límites oficiales.  Sin embargo, era un arte esotérico sin diseños estándar.  Todos hicieron algo diferente.  La persona que reunió todo esto tenía que ser tan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">generosa</a> que el ensamblaje de los productos de IKEA pareciera una completa tontería.  La mayoría de los sistemas de enfriamiento ni siquiera encajaban en los gabinetes. <br><br>  A principios de 2004, la situación comenzó a cambiar debido a cambios internos en las políticas de Intel.  Un ingeniero del centro de diseño de Hillsboro, Oregon, donde se diseñan la mayoría de los chips de la compañía, a pesar de tener su sede en Santa Clara, California, ha estado trabajando en un proyecto especial de enfriamiento durante varios años.  El proyecto le costó a la compañía $ 1 millón y tenía como objetivo crear un enfriador líquido para procesadores Intel.  Desafortunadamente, Intel estaba a punto de cerrarlo. <br><br>  El ingeniero esperaba un resultado diferente.  Para salvar el proyecto, se le ocurrió esta idea en Falcon Northwest, una compañía con sede en Portland que construyó complementos de juegos para computadoras.  "La razón fue porque la compañía pensó que el enfriamiento por líquido alentaba a las personas a hacer overclock y esta actividad estaba prohibida en ese momento", dijo Kelt Reeves, presidente de Falcon Northwest.  Y en esta posición, Intel tenía su propia lógica.  En ese momento, los minoristas sin principios de Asia estaban vendiendo PCs overclockeadas bajo la apariencia de otras más potentes, y con una refrigeración deficiente, y a los ojos del público, esto de alguna manera se convirtió en un problema de Intel.  Por lo tanto, la compañía se opuso al overclocking. <br><br>  Sin embargo, este ingeniero de Oregón creía que si lograba encontrar clientes y un mercado para ese refrigerador, Intel rendirá.  (Además, el producto Intel resultante fue mucho mejor en calidad que lo que estaba disponible en el mercado, nos dijo Reeves).  Por lo tanto, después de cierta persuasión interna y negociaciones entre las compañías, Intel permitió a Falcon vender sistemas de enfriamiento, en particular porque Intel ya los producía en miles.  El único inconveniente fue que Falcon no podía mencionar que Intel estaba involucrado.  Falcon estuvo de acuerdo, y pronto se convirtió en el primer fabricante en suministrar sistemas de PC todo en uno completamente sellados. <br><br>  Reeves señaló que esta solución de ZhO de vanguardia no era particularmente fácil de usar.  Falcon tuvo que cambiar la carcasa para que se ajustara al radiador e inventar una placa de enfriamiento para el agua.  Pero con el tiempo, los fabricantes de refrigeradores, como ThermalTake y Corsair, aprendieron lo que Intel estaba haciendo y comenzaron a realizar mejoras consistentes.  Desde entonces, han aparecido varios productos y fabricantes, por ejemplo, CoolIT y Asetek, que fabricaron específicamente ZhO para centros de datos.  Algunos de sus productos, por ejemplo, tuberías que no se rompen, no se agrietan y no tienen fugas con una garantía de hasta siete años, finalmente se otorgaron bajo licencia a los fabricantes de sistemas de refrigeración para el usuario final, y dicho intercambio de tecnología en ambas direcciones se ha convertido en la norma. <br><br>  Y a medida que este mercado crece en diferentes direcciones, incluso Intel finalmente cambió de opinión.  Ahora anuncia capacidades de overclocking para los procesadores de las series K y X, y ni siquiera le importa vender refrigeradores regulares junto con la CPU superior para los jugadores. <br><br>  "ZhO ya es una tecnología probada: todo el mundo lo está haciendo por parte del consumidor", dijo Reeves.  Intel ha dejado de suministrar refrigeradores regulares con las CPU más potentes, porque necesitan JO;  ya ha sido probado y se ha recibido una bendición de Intel.  No creo que haya alguien que diga que las soluciones completas para esto no son lo suficientemente confiables ". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad3/8c7/8a8/ad38c78a8953abdcdb563f84b7fadae7.jpg"><br>  <i>Enfriamiento por inmersión en el centro de datos.</i>  <i>Las cajas están llenas de fluido dieléctrico que fluye a través de las tuberías.</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/221/cab/4fd/221cab4fd3522b8580d2e27d394f51d9.jpg"><br>  <i>Refrigeración líquida de Skybox Datacenters con inmersión.</i>  <i>Los intercambiadores de calor se sumergen con equipos informáticos y el fluido dieléctrico no sale del tanque.</i>  <i>Un circuito de agua pasa por las habitaciones y se acerca a cada intercambiador de calor.</i> <br><br><h2>  Hechos a favor de la practicidad del enfriamiento líquido. </h2><br>  Durante mucho tiempo, los centros de datos tradicionales proporcionaron un piso elevado con pequeñas aberturas a través de las cuales el aire frío se elevó, absorbido por los servidores.  Esto se llamaba CRAC, o aire acondicionado de la sala de computadoras.  El problema es que ahora no es suficiente soplar aire frío a través de las aberturas en el piso. <br><br>  La razón principal del reciente auge del enfriamiento líquido es la necesidad.  Los procesadores de hoy se calientan demasiado y los servidores están demasiado cerca para que el aire los enfríe eficientemente, incluso Google dice.  La capacidad calorífica del agua es 3300 veces mayor que la del aire, y un sistema de enfriamiento de agua puede bombear 300 litros de agua por minuto, en comparación con 20 metros cúbicos de aire por minuto. <br><br>  En pocas palabras, el agua puede enfriarse de manera mucho más eficiente y en un espacio mucho más pequeño.  Por lo tanto, después de muchos años de intentar reducir el consumo de energía, los fabricantes de procesadores pueden dispersar la energía y torcer el voltaje para obtener el máximo rendimiento, sabiendo que la refrigeración líquida puede manejar esto. <br><br>  "Se nos pide que enfríemos los chips cuyo consumo de energía pronto superará los 500 vatios", dijo Jeff Lyon, director de CoolIT.  - Algunos procesadores que aún no han ingresado al mercado consumirán 300 vatios cada uno.  Todo esto se está desarrollando a pedido de AI y machine learning.  La velocidad de enfriamiento simplemente no es suficiente ". <br><br>  Lyon dijo que CoolIT está considerando expandir el sistema de enfriamiento a conjuntos de chips, sistemas de control de potencia, chips de red y memoria.  "No habrá nada radical al tratar con la memoria", agregó.  - Hay opciones de RAM con empaque avanzado, que consumen 18 vatios por DIMM.  Un DIMM típico consume 4-6 vatios.  Entre los sistemas con una gran cantidad de memoria, encontramos servidores con 16 o incluso 24 DIMM instalados, lo que significa mucho calor ”. <br><br>  Uno por uno, los fabricantes se enfrentan a tales solicitudes.  Equinix observa cómo la densidad promedio crece de 5 kW a 7-8 kW, y ahora a 15-16 kW, con algunos equipos que ya muestran una densidad de 40 kW.  “Entonces, la cantidad total de aire que necesita ser bombeada se vuelve demasiado grande.  No sucederá instantáneamente, pero en los próximos años habrá una adopción fundamental de refrigeración líquida ”, dijo Pennington de Equinix. <br><br><h2>  Un poco sobre enfriamiento por inmersión </h2><br>  Green Revolution Cooling se centra en el enfriamiento por inmersión, y su director Peter Poulin dice que desde una perspectiva de eficiencia energética, el enfriamiento por inmersión es mejor que el enfriamiento directo por dos razones.  Primero, los ventiladores se eliminan de todos los servidores.  Solo esto reduce el consumo de energía en un 15% en promedio.  Y un cliente de la compañía lo redujo en un 30%. <br><br>  Hay otra ventaja indirecta para eliminar a los fanáticos: el silencio.  A pesar del hecho de que a menudo se usan ventiladores muy pequeños en los servidores, los servidores son terriblemente ruidosos, y estar en el centro de datos es desagradable debido al calor y al ruido.  La refrigeración líquida hace que estos lugares sean mucho más agradables para trabajar. <br><br>  Otra ventaja es que se requiere muy poca energía para soportar el sistema de enfriamiento por inmersión.  Solo hay tres partes móviles: una bomba para hacer circular un refrigerador, una bomba para moverla a una torre de enfriamiento y un ventilador de torre de enfriamiento.  Después de reemplazar el aire refrigerado por líquido, el consumo de electricidad puede caer al 5% de lo que se gastó en aire acondicionado.  "Se obtiene una gran reducción en el consumo de energía, lo que le permite hacer muchas otras cosas", dijo Poulnin.  "Dependiendo del consumidor, el centro de datos puede ser más eficiente energéticamente o reducir las emisiones de carbono asociadas con la construcción de centros de datos". <br><br><h2>  Hechos a favor de la eficiencia energética del enfriamiento líquido </h2><br>  El consumo de energía ha sido durante mucho tiempo una preocupación para la industria del centro de datos (la Agencia de Protección Ambiental de EE. UU. Ha estado rastreando esta cifra durante al menos diez años).  Los centros de datos actuales son grandes empresas que consumen aproximadamente el 2% de toda la electricidad global y liberan tanto CO2 como la industria de las aerolíneas.  Por lo tanto, el interés en este tema no se desvanece.  Afortunadamente, la refrigeración líquida reduce las facturas de electricidad. <br><br>  Los primeros ahorros se deben a la desconexión del aire acondicionado en el centro de datos.  El segundo es la eliminación de los fanáticos.  Cada rack de servidores tiene muchos ventiladores que emiten aire, pero su número se puede reducir a un número pequeño o a cero, dependiendo de la densidad. <br><br>  Y con la tecnología de "enfriamiento en seco", en la que no hay congelación, puede lograr ahorros aún mayores.  Inicialmente, el enfriamiento conectado directamente condujo el agua a través de un refrigerador, que la enfrió a 15–25 grados Celsius.  Pero al final, resultó que los refrigeradores líquidos, que pasaban agua a través de una larga secuencia de tuberías y ventiladores, tuberías frías calentadas por agua caliente y difusión térmica natural, también enfrían el agua a una temperatura suficiente. <br><br>  "Debido a que este proceso es tan efectivo, no tiene que preocuparse por enfriar el agua a una temperatura baja", dice Pennington.  - El agua tibia aún elimina efectivamente todo el calor de los servidores.  No necesita un ciclo de compresión, solo puede usar enfriadores secos ". <br><br>  Los enfriadores secos también ahorran agua.  Un gran centro de datos que usa refrigeradores puede consumir millones de litros de agua por año, pero un centro de datos con enfriadores secos no consume agua.  Esto ahorra energía y agua, lo que puede ser muy útil si el centro de datos se encuentra dentro de la ciudad. <br><br>  "No consumimos mucha agua", dijo Pennington.  - Si diseñas todo cuidadosamente, obtienes un sistema cerrado.  El agua no se vierte y no se vierte, solo necesita agregar agua aproximadamente una vez al año para mantener los tanques llenos.  No agrega agua constantemente a su automóvil, este es el caso con nosotros ". <br><br><h2>  La aceptación sigue a la efectividad </h2><br>  Un ejemplo del mundo real: Dell, al cambiar a refrigeración líquida, ha aumentado la eficiencia energética ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PUE</a> ) en un 56%, según Brian Payne, vicepresidente de gestión de productos y marketing de PowerEdge Dell EMC.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PUE es la relación entre la energía que se debe gastar para enfriar el sistema y la energía necesaria para operar el sistema [de hecho, esta es la relación entre la energía total utilizada por el centro de datos y la energía gastada directamente en alimentar la infraestructura de TI / aprox. perev]. PUE de 3 significa que se gasta 2 veces más energía en enfriar un sistema que en energía del sistema, y ​​PUE = 2 significa que tanto la energía como el enfriamiento se consumen por igual. PUE no puede ser igual a 1, ya que el enfriamiento es necesario, pero los operadores de centros de datos están obsesionados con tratar de acercar la cifra a 1.0 como sea posible.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Además de mejorar PUE, el aumento de la potencia informática que reciben los clientes de Dell puede ser de hasta un 23%, y esto no sobrecarga el sistema más allá de toda medida. "En función de las inversiones en infraestructura necesarias, predecimos el rendimiento anual del sistema", dice Payne. - Lo compararía con la compra de un acondicionador de aire más eficiente en energía para el hogar. Invierte un poco, pero con el tiempo siente los beneficios de las facturas de electricidad ".</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como un adherente completamente diferente del enfriamiento líquido, tome el centro de supercomputadoras en Ohio, OSC. Este clúster emplea 1800 nodos. Después de cambiar a JO, como dijo Doug Johnson, arquitecto jefe de sistemas, el centro alcanzó PUE = 1.5. OSC utiliza un circuito externo, por lo que el agua se elimina del edificio y se enfría a temperatura ambiente, que en promedio es de 30 ° C en verano y mucho menos en invierno. Las virutas alcanzan los 70 ° C, e incluso si el agua se calienta hasta 40 ° C, sigue siendo mucho más fría que las virutas y cumple su propósito.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como muchos de los primeros en adoptar la nueva tecnología, para OSC todo es nuevo. Hace cinco años, el centro no usaba ZhO en absoluto, y hoy ocupa el 25%. El centro espera que en tres años la barra crecerá al 75%, y después de unos años cambiarán completamente a ZhO. Pero incluso en el estado actual, según Johnson, enfriar el centro requiere cuatro veces menos energía que antes de la transición a ZhO, y en general, esta solución redujo el consumo total de energía en 2/3. "Creo que el porcentaje aumentará cuando comencemos a integrar la GPU en el sistema de enfriamiento".</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Desde el punto de vista del cliente, se necesita tiempo y energía para evaluar una nueva tecnología, por eso una gran empresa como Dell acordó trabajar con CoolIT para anunciar ZhO. </font><font style="vertical-align: inherit;">No es sorprendente que, en primer lugar, entre las preocupaciones de los clientes siga existiendo la posibilidad de fugas. </font><font style="vertical-align: inherit;">Sin embargo, a pesar de todas las fluctuaciones, resulta que en este momento tienen pocas opciones si quieren lograr el mejor rendimiento. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Siempre ha habido miedo a las filtraciones", dice Lyon de CoolIT. </font><font style="vertical-align: inherit;">- La situación ha cambiado, y ahora simplemente no hay otras opciones. </font><font style="vertical-align: inherit;">Las computadoras de alta velocidad no pueden hacer exactamente eso ".</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/442576/">https://habr.com/ru/post/442576/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../442566/index.html">Igual que en la luna: ingeniería inversa de un módulo de amplificador operacional híbrido</a></li>
<li><a href="../442568/index.html">Semana de la seguridad 10: Vulnerabilidades del controlador NVIDIA</a></li>
<li><a href="../442570/index.html">Sigma gobierna. Arte o nuevo estándar para SOC</a></li>
<li><a href="../442572/index.html">Usar la herramienta de configuración de Datapath</a></li>
<li><a href="../442574/index.html">Se crea la base para una teoría generalizada de las redes neuronales.</a></li>
<li><a href="../442578/index.html">Lanzamiento de Linux 5.0</a></li>
<li><a href="../442580/index.html">Ingeniería inversa de formato binario utilizando archivos Korg .SNG como ejemplo</a></li>
<li><a href="../442582/index.html">Cómo tratamos de mobbing</a></li>
<li><a href="../442584/index.html">Documentos sobre el edificio: pequeñas alegrías de la automatización en el ejemplo de la Torre Oscura</a></li>
<li><a href="../442586/index.html">La vulnerabilidad en Telegram permite omitir la contraseña del código local de cualquier longitud</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>