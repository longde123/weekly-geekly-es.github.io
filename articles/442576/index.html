<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ü ü§õüèø üëø Larga vida a los overclockers: c√≥mo la refrigeraci√≥n l√≠quida comenz√≥ a dominar en los centros de datos ‚öΩÔ∏è üßùüèº üñïüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content=""Las computadoras de alta velocidad no pueden funcionar sin aire" 

 Hay un momento en la pel√≠cula Iron Man 2 cuando Tony Stark ve una vieja pel√≠cula ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Larga vida a los overclockers: c√≥mo la refrigeraci√≥n l√≠quida comenz√≥ a dominar en los centros de datos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442576/"><h3>  "Las computadoras de alta velocidad no pueden funcionar sin aire" </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/reQs3g5LO8E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Hay un momento en la pel√≠cula Iron Man 2 cuando Tony Stark ve una vieja pel√≠cula de su difunto padre, donde dice: "Estoy limitado por la tecnolog√≠a de mi tiempo, pero un d√≠a puedes resolverlo".  Y luego cambiar√°s el mundo ".  Es fant√°stico, pero la idea que expresa es bastante real.  Las ideas de los ingenieros a menudo est√°n muy adelantadas a su tiempo.  Siempre ha habido dispositivos en Star Trek, pero el resto del mundo ha tenido que trabajar durante d√©cadas para crear tabletas y libros electr√≥nicos. <br><br>  El concepto de refrigeraci√≥n l√≠quida encaja perfectamente en esta categor√≠a.  La idea en s√≠ ha existido desde la d√©cada de 1960, pero se mantuvo radical en comparaci√≥n con una opci√≥n mucho m√°s barata y segura para la refrigeraci√≥n por aire.  Pasaron m√°s de 40 a√±os hasta que la refrigeraci√≥n l√≠quida comenz√≥ a desarrollarse un poco en la d√©cada de 2000, e incluso entonces era principalmente una prerrogativa de los entusiastas de las PC que buscaban dispersar sus CPU m√°s all√° de los l√≠mites recomendados por Intel y AMD. <br><a name="habracut"></a><br>  Hoy en d√≠a, los sistemas de refrigeraci√≥n l√≠quida est√°n ganando popularidad.  Tal sistema para PC se puede comprar por menos de $ 100, y la producci√≥n artesanal dirigida a aplicaciones industriales y centros de datos (como CoolIT, Asetek, Green Revolution Computing, Ebullient) ofrece refrigeraci√≥n l√≠quida () para servidores.  Los ZhO se usan principalmente en supercomputadoras, inform√°tica de alta velocidad u otras situaciones en las que se requiere una gran cantidad de energ√≠a de la computadora, y los procesadores funcionan con casi el 100% de carga, pero estas opciones se est√°n volviendo m√°s comunes. <br><br>  Hay dos tipos populares de ZhO: enfriamiento directo de chips e inmersi√≥n.  Con enfriamiento directo, el radiador se conecta a la CPU, como un enfriador est√°ndar, pero en lugar de eso, se conectan dos tubos.  Uno viene con agua fr√≠a, un disipador t√©rmico que absorbe el calor de la CPU, y el otro deja caliente.  Luego se enfr√≠a y vuelve a la CPU en un circuito cerrado que se asemeja al flujo sangu√≠neo. <br><br>  Durante el enfriamiento por inmersi√≥n, el equipo se llena con l√≠quido que, obviamente, no debe conducir electricidad.  Este enfoque es m√°s similar a las piscinas de enfriamiento de los reactores nucleares.  El enfriamiento sumergible sigue siendo una opci√≥n m√°s avanzada y requiere refrigerantes m√°s caros que una conexi√≥n directa, donde puede usar agua com√∫n.  Adem√°s, siempre existe el riesgo de fugas.  Por lo tanto, con mucho, la opci√≥n m√°s popular es la conexi√≥n directa. <br><br>  Como uno de los principales ejemplos, tome Alphabet.  Cuando esta empresa matriz de Google present√≥ el AI TensorFlow 3.0 en mayo de 2018, el director Sundar Pichai dijo que estos chips eran tan potentes que "por primera vez, tuvimos que instalar refrigeraci√≥n l√≠quida en los centros de datos".  Alphabet tuvo que pagar este precio por un aumento de ocho veces en la productividad. <br><br>  Por otro lado, Skybox Datacenters anunci√≥ recientemente planes para crear una gran supercomputadora con 40,000 servidores de DownUnder GeoSolutions (DUG), dise√±ada para la exploraci√≥n de petr√≥leo y gas.  Este proyecto producir√° 250 petaflops de potencia inform√°tica, m√°s que cualquier otro existente, y se espera que los servidores se enfr√≠en por fluido cuando se sumerjan en tanques llenos de fluido diel√©ctrico. <br><br>  En cualquier caso, "la refrigeraci√≥n l√≠quida es la refrigeraci√≥n del futuro, y siempre lo ser√°", dijo Craig Pennington, vicepresidente de dise√±o, operador del centro de datos de Equinix.  "Parece obvio que este es el enfoque correcto, pero nadie lo ha aplicado". <br><br>  ¬øC√≥mo ha pasado JO del arte esot√©rico a la vuelta de la inform√°tica a un m√©todo casi universalmente aceptado en los centros de datos modernos?  Como todas las tecnolog√≠as, esto sucedi√≥ en parte como resultado de la evoluci√≥n, prueba y error, y una gran cantidad de soluciones de ingenier√≠a.  Sin embargo, para ZhO, los centros de datos de hoy deber√≠an agradecer a los primeros overclockers que son h√©roes an√≥nimos de este m√©todo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2c7/137/4b2/2c71374b2b2d2158d46d7e0ddb0fdabc.png"><br>  <i>El panel de control del sistema de procesamiento de datos IBM System 360</i> <br><br><h2>  ¬øQu√© queremos decir con refrigeraci√≥n l√≠quida? </h2><br>  La refrigeraci√≥n l√≠quida se convirti√≥ en una idea popular en 1964 cuando IBM estudi√≥ el tema de la refrigeraci√≥n sumergible para el mainframe System 360. Era uno de los mainframes de la compa√±√≠a;  Las series 700 y 7000 existieron durante m√°s de diez a√±os, y System / 360 "comenz√≥ la era de la compatibilidad con computadoras, permitiendo por primera vez que diferentes m√°quinas de la l√≠nea de productos trabajen juntas", seg√∫n escriben en IBM.  El concepto era simple: el agua enfriada ten√≠a que fluir a trav√©s de un dispositivo que la enfriaba a una temperatura inferior a la temperatura ambiente, y luego el agua se alimentaba directamente al sistema.  El circuito utilizado por IBM ahora se conoce como enfriamiento posterior cuando el disipador t√©rmico est√° montado detr√°s de la unidad central.  El dispositivo aspiraba aire caliente de la unidad central con ventiladores, y luego este aire se enfriaba con agua, al igual que un radiador enfr√≠a el motor de un autom√≥vil. <br><br>  Desde entonces, los ingenieros han perfeccionado este concepto b√°sico y han surgido dos formas dominantes de FV: inmersi√≥n y contacto directo.  La inmersi√≥n es lo que es;  La electr√≥nica est√° en un ba√±o l√≠quido que, por razones obvias, no puede ser agua.  El l√≠quido no debe conducir electricidad, es decir, ser un aislante (empresas como 3M incluso desarrollan espec√≠ficamente l√≠quidos para esto). <br><br>  Pero el buceo tiene muchos problemas y desventajas.  Solo se puede acceder al servidor ubicado en el l√≠quido desde arriba.  Por lo tanto, los puertos externos deben ubicarse all√≠.  La ubicaci√≥n del servidor de gabinetes de 1U en un bastidor no ser√≠a pr√°ctico, por lo que el servidor no puede colocarse secuencialmente.  Un diel√©ctrico, y generalmente uno mineral, es peque√±o, muy costoso y dif√≠cil de limpiar en caso de fuga.  Se requerir√°n discos duros especiales, y la alteraci√≥n del centro de datos requerir√° importantes inversiones.  Por lo tanto, como en el caso de la supercomputadora mencionada anteriormente, la inmersi√≥n se realiza mejor en un nuevo centro de datos, en lugar de rehacer el antiguo. <br><br>  El contacto directo de JO, en cambio, es que el radiador (o intercambiador de calor) est√° en el chip, como un radiador normal.  En lugar de un ventilador, usa dos tuber√≠as de agua: una que trae agua fr√≠a para enfriar y la segunda que sopla agua caliente calentada por contacto con el radiador.  Esta forma de ZhO se convirti√≥ en la m√°s popular, fue adoptada por fabricantes como HP Enterprise, Dell EMC e IBM, as√≠ como por fabricantes de gabinetes Chatsworth Systems y Schneider Electric. <br><br>  El enfriamiento directo usa agua, pero es muy sensible a su calidad.  No se debe usar agua del grifo sin filtrar.  Solo mira tu grifo o ducha.  Nadie necesita la acumulaci√≥n de calcio en los servidores.  Al menos el enfriamiento directo requiere agua destilada pura y, a veces, su mezcla con anticongelante.  La fabricaci√≥n de dicho refrigerante es una ciencia en s√≠ misma. <br><br><h2>  Enlace de Intel </h2><br>  ¬øC√≥mo pasamos de los radiadores de IBM a los modernos y extravagantes sistemas de enfriamiento?  De nuevo, gracias a los overclockers.  A principios de siglo, la refrigeraci√≥n l√≠quida comenz√≥ a ganar popularidad entre los overclockers de PC y los aficionados que ensamblaron sus computadoras que quer√≠an aumentar su velocidad m√°s all√° de los l√≠mites oficiales.  Sin embargo, era un arte esot√©rico sin dise√±os est√°ndar.  Todos hicieron algo diferente.  La persona que reuni√≥ todo esto ten√≠a que ser tan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">generosa</a> que el ensamblaje de los productos de IKEA pareciera una completa tonter√≠a.  La mayor√≠a de los sistemas de enfriamiento ni siquiera encajaban en los gabinetes. <br><br>  A principios de 2004, la situaci√≥n comenz√≥ a cambiar debido a cambios internos en las pol√≠ticas de Intel.  Un ingeniero del centro de dise√±o de Hillsboro, Oregon, donde se dise√±an la mayor√≠a de los chips de la compa√±√≠a, a pesar de tener su sede en Santa Clara, California, ha estado trabajando en un proyecto especial de enfriamiento durante varios a√±os.  El proyecto le cost√≥ a la compa√±√≠a $ 1 mill√≥n y ten√≠a como objetivo crear un enfriador l√≠quido para procesadores Intel.  Desafortunadamente, Intel estaba a punto de cerrarlo. <br><br>  El ingeniero esperaba un resultado diferente.  Para salvar el proyecto, se le ocurri√≥ esta idea en Falcon Northwest, una compa√±√≠a con sede en Portland que construy√≥ complementos de juegos para computadoras.  "La raz√≥n fue porque la compa√±√≠a pens√≥ que el enfriamiento por l√≠quido alentaba a las personas a hacer overclock y esta actividad estaba prohibida en ese momento", dijo Kelt Reeves, presidente de Falcon Northwest.  Y en esta posici√≥n, Intel ten√≠a su propia l√≥gica.  En ese momento, los minoristas sin principios de Asia estaban vendiendo PCs overclockeadas bajo la apariencia de otras m√°s potentes, y con una refrigeraci√≥n deficiente, y a los ojos del p√∫blico, esto de alguna manera se convirti√≥ en un problema de Intel.  Por lo tanto, la compa√±√≠a se opuso al overclocking. <br><br>  Sin embargo, este ingeniero de Oreg√≥n cre√≠a que si lograba encontrar clientes y un mercado para ese refrigerador, Intel rendir√°.  (Adem√°s, el producto Intel resultante fue mucho mejor en calidad que lo que estaba disponible en el mercado, nos dijo Reeves).  Por lo tanto, despu√©s de cierta persuasi√≥n interna y negociaciones entre las compa√±√≠as, Intel permiti√≥ a Falcon vender sistemas de enfriamiento, en particular porque Intel ya los produc√≠a en miles.  El √∫nico inconveniente fue que Falcon no pod√≠a mencionar que Intel estaba involucrado.  Falcon estuvo de acuerdo, y pronto se convirti√≥ en el primer fabricante en suministrar sistemas de PC todo en uno completamente sellados. <br><br>  Reeves se√±al√≥ que esta soluci√≥n de ZhO de vanguardia no era particularmente f√°cil de usar.  Falcon tuvo que cambiar la carcasa para que se ajustara al radiador e inventar una placa de enfriamiento para el agua.  Pero con el tiempo, los fabricantes de refrigeradores, como ThermalTake y Corsair, aprendieron lo que Intel estaba haciendo y comenzaron a realizar mejoras consistentes.  Desde entonces, han aparecido varios productos y fabricantes, por ejemplo, CoolIT y Asetek, que fabricaron espec√≠ficamente ZhO para centros de datos.  Algunos de sus productos, por ejemplo, tuber√≠as que no se rompen, no se agrietan y no tienen fugas con una garant√≠a de hasta siete a√±os, finalmente se otorgaron bajo licencia a los fabricantes de sistemas de refrigeraci√≥n para el usuario final, y dicho intercambio de tecnolog√≠a en ambas direcciones se ha convertido en la norma. <br><br>  Y a medida que este mercado crece en diferentes direcciones, incluso Intel finalmente cambi√≥ de opini√≥n.  Ahora anuncia capacidades de overclocking para los procesadores de las series K y X, y ni siquiera le importa vender refrigeradores regulares junto con la CPU superior para los jugadores. <br><br>  "ZhO ya es una tecnolog√≠a probada: todo el mundo lo est√° haciendo por parte del consumidor", dijo Reeves.  Intel ha dejado de suministrar refrigeradores regulares con las CPU m√°s potentes, porque necesitan JO;  ya ha sido probado y se ha recibido una bendici√≥n de Intel.  No creo que haya alguien que diga que las soluciones completas para esto no son lo suficientemente confiables ". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad3/8c7/8a8/ad38c78a8953abdcdb563f84b7fadae7.jpg"><br>  <i>Enfriamiento por inmersi√≥n en el centro de datos.</i>  <i>Las cajas est√°n llenas de fluido diel√©ctrico que fluye a trav√©s de las tuber√≠as.</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/221/cab/4fd/221cab4fd3522b8580d2e27d394f51d9.jpg"><br>  <i>Refrigeraci√≥n l√≠quida de Skybox Datacenters con inmersi√≥n.</i>  <i>Los intercambiadores de calor se sumergen con equipos inform√°ticos y el fluido diel√©ctrico no sale del tanque.</i>  <i>Un circuito de agua pasa por las habitaciones y se acerca a cada intercambiador de calor.</i> <br><br><h2>  Hechos a favor de la practicidad del enfriamiento l√≠quido. </h2><br>  Durante mucho tiempo, los centros de datos tradicionales proporcionaron un piso elevado con peque√±as aberturas a trav√©s de las cuales el aire fr√≠o se elev√≥, absorbido por los servidores.  Esto se llamaba CRAC, o aire acondicionado de la sala de computadoras.  El problema es que ahora no es suficiente soplar aire fr√≠o a trav√©s de las aberturas en el piso. <br><br>  La raz√≥n principal del reciente auge del enfriamiento l√≠quido es la necesidad.  Los procesadores de hoy se calientan demasiado y los servidores est√°n demasiado cerca para que el aire los enfr√≠e eficientemente, incluso Google dice.  La capacidad calor√≠fica del agua es 3300 veces mayor que la del aire, y un sistema de enfriamiento de agua puede bombear 300 litros de agua por minuto, en comparaci√≥n con 20 metros c√∫bicos de aire por minuto. <br><br>  En pocas palabras, el agua puede enfriarse de manera mucho m√°s eficiente y en un espacio mucho m√°s peque√±o.  Por lo tanto, despu√©s de muchos a√±os de intentar reducir el consumo de energ√≠a, los fabricantes de procesadores pueden dispersar la energ√≠a y torcer el voltaje para obtener el m√°ximo rendimiento, sabiendo que la refrigeraci√≥n l√≠quida puede manejar esto. <br><br>  "Se nos pide que enfr√≠emos los chips cuyo consumo de energ√≠a pronto superar√° los 500 vatios", dijo Jeff Lyon, director de CoolIT.  - Algunos procesadores que a√∫n no han ingresado al mercado consumir√°n 300 vatios cada uno.  Todo esto se est√° desarrollando a pedido de AI y machine learning.  La velocidad de enfriamiento simplemente no es suficiente ". <br><br>  Lyon dijo que CoolIT est√° considerando expandir el sistema de enfriamiento a conjuntos de chips, sistemas de control de potencia, chips de red y memoria.  "No habr√° nada radical al tratar con la memoria", agreg√≥.  - Hay opciones de RAM con empaque avanzado, que consumen 18 vatios por DIMM.  Un DIMM t√≠pico consume 4-6 vatios.  Entre los sistemas con una gran cantidad de memoria, encontramos servidores con 16 o incluso 24 DIMM instalados, lo que significa mucho calor ‚Äù. <br><br>  Uno por uno, los fabricantes se enfrentan a tales solicitudes.  Equinix observa c√≥mo la densidad promedio crece de 5 kW a 7-8 kW, y ahora a 15-16 kW, con algunos equipos que ya muestran una densidad de 40 kW.  ‚ÄúEntonces, la cantidad total de aire que necesita ser bombeada se vuelve demasiado grande.  No suceder√° instant√°neamente, pero en los pr√≥ximos a√±os habr√° una adopci√≥n fundamental de refrigeraci√≥n l√≠quida ‚Äù, dijo Pennington de Equinix. <br><br><h2>  Un poco sobre enfriamiento por inmersi√≥n </h2><br>  Green Revolution Cooling se centra en el enfriamiento por inmersi√≥n, y su director Peter Poulin dice que desde una perspectiva de eficiencia energ√©tica, el enfriamiento por inmersi√≥n es mejor que el enfriamiento directo por dos razones.  Primero, los ventiladores se eliminan de todos los servidores.  Solo esto reduce el consumo de energ√≠a en un 15% en promedio.  Y un cliente de la compa√±√≠a lo redujo en un 30%. <br><br>  Hay otra ventaja indirecta para eliminar a los fan√°ticos: el silencio.  A pesar del hecho de que a menudo se usan ventiladores muy peque√±os en los servidores, los servidores son terriblemente ruidosos, y estar en el centro de datos es desagradable debido al calor y al ruido.  La refrigeraci√≥n l√≠quida hace que estos lugares sean mucho m√°s agradables para trabajar. <br><br>  Otra ventaja es que se requiere muy poca energ√≠a para soportar el sistema de enfriamiento por inmersi√≥n.  Solo hay tres partes m√≥viles: una bomba para hacer circular un refrigerador, una bomba para moverla a una torre de enfriamiento y un ventilador de torre de enfriamiento.  Despu√©s de reemplazar el aire refrigerado por l√≠quido, el consumo de electricidad puede caer al 5% de lo que se gast√≥ en aire acondicionado.  "Se obtiene una gran reducci√≥n en el consumo de energ√≠a, lo que le permite hacer muchas otras cosas", dijo Poulnin.  "Dependiendo del consumidor, el centro de datos puede ser m√°s eficiente energ√©ticamente o reducir las emisiones de carbono asociadas con la construcci√≥n de centros de datos". <br><br><h2>  Hechos a favor de la eficiencia energ√©tica del enfriamiento l√≠quido </h2><br>  El consumo de energ√≠a ha sido durante mucho tiempo una preocupaci√≥n para la industria del centro de datos (la Agencia de Protecci√≥n Ambiental de EE. UU. Ha estado rastreando esta cifra durante al menos diez a√±os).  Los centros de datos actuales son grandes empresas que consumen aproximadamente el 2% de toda la electricidad global y liberan tanto CO2 como la industria de las aerol√≠neas.  Por lo tanto, el inter√©s en este tema no se desvanece.  Afortunadamente, la refrigeraci√≥n l√≠quida reduce las facturas de electricidad. <br><br>  Los primeros ahorros se deben a la desconexi√≥n del aire acondicionado en el centro de datos.  El segundo es la eliminaci√≥n de los fan√°ticos.  Cada rack de servidores tiene muchos ventiladores que emiten aire, pero su n√∫mero se puede reducir a un n√∫mero peque√±o o a cero, dependiendo de la densidad. <br><br>  Y con la tecnolog√≠a de "enfriamiento en seco", en la que no hay congelaci√≥n, puede lograr ahorros a√∫n mayores.  Inicialmente, el enfriamiento conectado directamente condujo el agua a trav√©s de un refrigerador, que la enfri√≥ a 15‚Äì25 grados Celsius.  Pero al final, result√≥ que los refrigeradores l√≠quidos, que pasaban agua a trav√©s de una larga secuencia de tuber√≠as y ventiladores, tuber√≠as fr√≠as calentadas por agua caliente y difusi√≥n t√©rmica natural, tambi√©n enfr√≠an el agua a una temperatura suficiente. <br><br>  "Debido a que este proceso es tan efectivo, no tiene que preocuparse por enfriar el agua a una temperatura baja", dice Pennington.  - El agua tibia a√∫n elimina efectivamente todo el calor de los servidores.  No necesita un ciclo de compresi√≥n, solo puede usar enfriadores secos ". <br><br>  Los enfriadores secos tambi√©n ahorran agua.  Un gran centro de datos que usa refrigeradores puede consumir millones de litros de agua por a√±o, pero un centro de datos con enfriadores secos no consume agua.  Esto ahorra energ√≠a y agua, lo que puede ser muy √∫til si el centro de datos se encuentra dentro de la ciudad. <br><br>  "No consumimos mucha agua", dijo Pennington.  - Si dise√±as todo cuidadosamente, obtienes un sistema cerrado.  El agua no se vierte y no se vierte, solo necesita agregar agua aproximadamente una vez al a√±o para mantener los tanques llenos.  No agrega agua constantemente a su autom√≥vil, este es el caso con nosotros ". <br><br><h2>  La aceptaci√≥n sigue a la efectividad </h2><br>  Un ejemplo del mundo real: Dell, al cambiar a refrigeraci√≥n l√≠quida, ha aumentado la eficiencia energ√©tica ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PUE</a> ) en un 56%, seg√∫n Brian Payne, vicepresidente de gesti√≥n de productos y marketing de PowerEdge Dell EMC.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PUE es la relaci√≥n entre la energ√≠a que se debe gastar para enfriar el sistema y la energ√≠a necesaria para operar el sistema [de hecho, esta es la relaci√≥n entre la energ√≠a total utilizada por el centro de datos y la energ√≠a gastada directamente en alimentar la infraestructura de TI / aprox. perev]. PUE de 3 significa que se gasta 2 veces m√°s energ√≠a en enfriar un sistema que en energ√≠a del sistema, y ‚Äã‚ÄãPUE = 2 significa que tanto la energ√≠a como el enfriamiento se consumen por igual. PUE no puede ser igual a 1, ya que el enfriamiento es necesario, pero los operadores de centros de datos est√°n obsesionados con tratar de acercar la cifra a 1.0 como sea posible.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adem√°s de mejorar PUE, el aumento de la potencia inform√°tica que reciben los clientes de Dell puede ser de hasta un 23%, y esto no sobrecarga el sistema m√°s all√° de toda medida. "En funci√≥n de las inversiones en infraestructura necesarias, predecimos el rendimiento anual del sistema", dice Payne. - Lo comparar√≠a con la compra de un acondicionador de aire m√°s eficiente en energ√≠a para el hogar. Invierte un poco, pero con el tiempo siente los beneficios de las facturas de electricidad ".</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como un adherente completamente diferente del enfriamiento l√≠quido, tome el centro de supercomputadoras en Ohio, OSC. Este cl√∫ster emplea 1800 nodos. Despu√©s de cambiar a JO, como dijo Doug Johnson, arquitecto jefe de sistemas, el centro alcanz√≥ PUE = 1.5. OSC utiliza un circuito externo, por lo que el agua se elimina del edificio y se enfr√≠a a temperatura ambiente, que en promedio es de 30 ¬∞ C en verano y mucho menos en invierno. Las virutas alcanzan los 70 ¬∞ C, e incluso si el agua se calienta hasta 40 ¬∞ C, sigue siendo mucho m√°s fr√≠a que las virutas y cumple su prop√≥sito.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como muchos de los primeros en adoptar la nueva tecnolog√≠a, para OSC todo es nuevo. Hace cinco a√±os, el centro no usaba ZhO en absoluto, y hoy ocupa el 25%. El centro espera que en tres a√±os la barra crecer√° al 75%, y despu√©s de unos a√±os cambiar√°n completamente a ZhO. Pero incluso en el estado actual, seg√∫n Johnson, enfriar el centro requiere cuatro veces menos energ√≠a que antes de la transici√≥n a ZhO, y en general, esta soluci√≥n redujo el consumo total de energ√≠a en 2/3. "Creo que el porcentaje aumentar√° cuando comencemos a integrar la GPU en el sistema de enfriamiento".</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Desde el punto de vista del cliente, se necesita tiempo y energ√≠a para evaluar una nueva tecnolog√≠a, por eso una gran empresa como Dell acord√≥ trabajar con CoolIT para anunciar ZhO. </font><font style="vertical-align: inherit;">No es sorprendente que, en primer lugar, entre las preocupaciones de los clientes siga existiendo la posibilidad de fugas. </font><font style="vertical-align: inherit;">Sin embargo, a pesar de todas las fluctuaciones, resulta que en este momento tienen pocas opciones si quieren lograr el mejor rendimiento. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Siempre ha habido miedo a las filtraciones", dice Lyon de CoolIT. </font><font style="vertical-align: inherit;">- La situaci√≥n ha cambiado, y ahora simplemente no hay otras opciones. </font><font style="vertical-align: inherit;">Las computadoras de alta velocidad no pueden hacer exactamente eso ".</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/442576/">https://habr.com/ru/post/442576/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../442566/index.html">Igual que en la luna: ingenier√≠a inversa de un m√≥dulo de amplificador operacional h√≠brido</a></li>
<li><a href="../442568/index.html">Semana de la seguridad 10: Vulnerabilidades del controlador NVIDIA</a></li>
<li><a href="../442570/index.html">Sigma gobierna. Arte o nuevo est√°ndar para SOC</a></li>
<li><a href="../442572/index.html">Usar la herramienta de configuraci√≥n de Datapath</a></li>
<li><a href="../442574/index.html">Se crea la base para una teor√≠a generalizada de las redes neuronales.</a></li>
<li><a href="../442578/index.html">Lanzamiento de Linux 5.0</a></li>
<li><a href="../442580/index.html">Ingenier√≠a inversa de formato binario utilizando archivos Korg .SNG como ejemplo</a></li>
<li><a href="../442582/index.html">C√≥mo tratamos de mobbing</a></li>
<li><a href="../442584/index.html">Documentos sobre el edificio: peque√±as alegr√≠as de la automatizaci√≥n en el ejemplo de la Torre Oscura</a></li>
<li><a href="../442586/index.html">La vulnerabilidad en Telegram permite omitir la contrase√±a del c√≥digo local de cualquier longitud</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>