<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßê ‚ÜñÔ∏è üéöÔ∏è Una nueva realizaci√≥n de la curiosidad en IA. Entrenamiento con una recompensa que depende de la dificultad para predecir el resultado üññüèª üè∏ üë©üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El progreso en el juego "La venganza de Montezuma" fue considerado por muchos como un sin√≥nimo de logros en el estudio de entornos desconocidos 

 Hem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Una nueva realizaci√≥n de la curiosidad en IA. Entrenamiento con una recompensa que depende de la dificultad para predecir el resultado</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428776/"><img src="https://habrastorage.org/getpro/habr/post_images/49b/e3e/fbf/49be3efbf10821888431e9529873176a.svg" width="780"><br>  <i><font color="gray">El progreso en el juego "La venganza de Montezuma" fue considerado por muchos como un sin√≥nimo de logros en el estudio de entornos desconocidos</font></i> <br><br>  Hemos desarrollado un m√©todo predictivo de destilaci√≥n de red aleatoria (RND) que alienta a los agentes de aprendizaje reforzados a explorar el entorno a trav√©s de la curiosidad.  Este m√©todo excedi√≥ por primera vez los resultados humanos promedio en el juego de computadora <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"La venganza de Montezuma"</a> (a excepci√≥n de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aplicaci√≥n</a> an√≥nima en el ICLR, donde el resultado es peor que el nuestro).  <b>RND demuestra eficiencia ultramoderna, encuentra peri√≥dicamente las 24 habitaciones y pasa el primer nivel sin demostraci√≥n preliminar y sin acceso al estado b√°sico del juego.</b> <br><a name="habracut"></a><br>  El m√©todo RND estimula la transici√≥n de un agente a estados desconocidos al medir la complejidad de predecir el resultado de superponer una red neuronal aleatoria aleatoria en datos de estado.  Si la condici√≥n no es familiar, entonces el resultado final es dif√≠cil de predecir, lo que significa que la recompensa es alta.  El m√©todo se puede aplicar a cualquier algoritmo de aprendizaje de refuerzo; es simple de implementar y efectivo para escalar.  A continuaci√≥n hay un enlace a la implementaci√≥n de RND, que reproduce los resultados de nuestro art√≠culo. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Texto de un art√≠culo cient√≠fico</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo.</a> </blockquote><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/40VZeFppDEM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h1>  Resultados en la venganza de Montezuma </h1><br>  Para lograr la meta deseada, el agente primero debe estudiar qu√© acciones son posibles en el entorno y qu√© constituye el progreso hacia la meta.  Muchas se√±ales de recompensa en los juegos proporcionan un plan de estudios, por lo que incluso las estrategias de investigaci√≥n simples son suficientes para lograr el objetivo.  En el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">trabajo inicial con la presentaci√≥n de DQN</a> , la venganza de Montezuma fue el <b>√∫nico juego donde DQN mostr√≥ el resultado del 0% del puntaje humano promedio (4700)</b> .  Es poco probable que las estrategias de inteligencia simples obtengan recompensas y no encuentren m√°s que unas pocas habitaciones en el nivel.  Desde entonces, el progreso en el juego La venganza de Montezuma ha sido visto por muchos como sin√≥nimo de avances en el estudio de entornos desconocidos. <br><br>  Se logr√≥ un progreso significativo en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2016</a> al combinar DQN con una bonificaci√≥n en el mostrador, como resultado de lo cual el agente logr√≥ encontrar 15 habitaciones y obtener la puntuaci√≥n m√°s alta de 6600 con un promedio de aproximadamente 3700. Desde entonces, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">las</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mejoras</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">significativas</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en el</a> resultado se logran solo a trav√©s de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">demostraciones</a> de personas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">expertas</a> o accediendo a los estados base del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">emulador</a> . <br><br>  Llevamos a cabo un experimento RND a gran escala con 1024 trabajadores, obteniendo un <b>resultado promedio de 10,000 en 9 inicios</b> y un <b>mejor resultado promedio de 14,500</b> .  En cada caso, el agente encontr√≥ 20-22 habitaciones.  Adem√°s, en un lanzamiento m√°s peque√±o, pero m√°s largo (de 10), el <b>resultado m√°ximo es 17,500, que corresponde a pasar el primer nivel y encontrar las 24 habitaciones</b> .  El siguiente gr√°fico compara estos dos experimentos, mostrando el valor promedio dependiendo de los par√°metros de actualizaci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cde/262/bde/cde262bde2a497752d59599ba524d41b.svg" width="780"><br><br>  La siguiente visualizaci√≥n muestra el progreso del experimento a menor escala.  El agente, bajo la influencia de la curiosidad, abre nuevas salas y encuentra formas de sumar puntos. Durante el entrenamiento, esta recompensa externa lo obliga a regresar a estas salas m√°s tarde. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/animated-pyramid_10-29e.mp4" type="video/mp4"></video></div></div></div><br>  <i><font color="gray">Las habitaciones descubiertas por el agente y el resultado promedio durante el entrenamiento.</font></i>  <i><font color="gray">El grado de transparencia de la habitaci√≥n corresponde a cu√°ntas veces de 10 pases del agente se detect√≥.</font></i>  <i><font color="gray"><a href="">Video</a></font></i> <br><br><h1>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Estudio de aprendizaje a gran escala basado en la curiosidad.</a> </h1><br>  Antes de desarrollar RND, nosotros, junto con el personal de la Universidad de California en Berkeley, exploramos el aprendizaje sin ninguna recompensa ambiental.  La curiosidad proporciona una manera m√°s f√°cil de ense√±ar a los agentes a interactuar con <i>cualquier</i> entorno, en lugar de utilizar una funci√≥n de recompensa especialmente dise√±ada para una tarea espec√≠fica, que a√∫n no es un hecho que corresponda a la soluci√≥n del problema.  En proyectos como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ALE</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Universe</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Malmo</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Gym</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Gym Retro</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Unity</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepMind Lab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CommAI</a> , se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">abre</a> una gran cantidad de entornos simulados para el agente a trav√©s de una interfaz estandarizada.  Un agente que utiliza una funci√≥n de recompensa generalizada que no es espec√≠fica de un entorno particular puede adquirir un nivel b√°sico de competencia en una amplia gama de entornos.  Esto le permite determinar un comportamiento √∫til incluso en ausencia de recompensas elaboradas. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Texto de un art√≠culo cient√≠fico</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo.</a> </blockquote><br>  En entornos de entrenamiento est√°ndar con refuerzo en cada paso de tiempo discreto, el agente env√≠a la acci√≥n al entorno, y reacciona, brind√°ndole al agente una nueva observaci√≥n, una recompensa por la transici√≥n y un indicador del final del episodio.  En nuestro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo anterior,</a> configuramos el entorno <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">para producir</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">solo la</a> siguiente observaci√≥n.  All√≠, el agente estudia el modelo predictivo del siguiente estado en funci√≥n de su experiencia y utiliza el error de predicci√≥n como una recompensa interna.  Como resultado, se siente atra√≠do por la imprevisibilidad.  Por ejemplo, un cambio en la cuenta del juego se recompensa solo si la cuenta se muestra en la pantalla y el cambio es dif√≠cil de predecir.  Un agente, por regla general, encuentra interacciones √∫tiles con nuevos objetos, ya que los resultados de tales interacciones suelen ser m√°s dif√≠ciles de predecir que otros aspectos del entorno. <br><br>  Al igual que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">otros</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">investigadores</a> , tratamos de evitar modelar todos los aspectos del entorno, independientemente de si son relevantes o no, eligiendo las caracter√≠sticas de observaci√≥n para el modelado.  Sorprendentemente, encontramos que incluso las funciones aleatorias funcionan bien. <br><br><h1>  ¬øQu√© hacen los agentes curiosos? </h1><br>  Probamos nuestro agente en m√°s de 50 entornos diferentes y observamos una gama de competencias desde acciones aparentemente aleatorias hasta interacci√≥n consciente con el entorno.  Para nuestra sorpresa, en algunos casos, el agente logr√≥ pasar el juego, aunque no se le inform√≥ el objetivo a trav√©s de una recompensa externa. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Retribuci√≥n interna al inicio de la formaci√≥n.</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">El salto en recompensa interna en el primer paso del nivel.</font></i> <br><br>  <b>Breakout</b> : salta en la recompensa interna cuando el agente ve una nueva configuraci√≥n de bloques en una etapa temprana de entrenamiento y cuando el nivel pasa por primera vez despu√©s de entrenar durante varias horas. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/BowlingSmaller.mp4" type="video/mp4"></video></div></div></div><br>  <b>Pong</b> : entrenamos al agente para controlar ambas plataformas simult√°neamente, y aprendi√≥ a mantener la pelota en el juego, lo que condujo a peleas prolongadas.  Incluso cuando entrenaba contra la IA en el juego, el agente intent√≥ maximizar el juego y no ganar. <br><br>  <b><a href="">Bolos</a></b> : el agente aprendi√≥ a jugar mejor que otros agentes que fueron entrenados directamente para maximizar la recompensa externa.  Creemos que esto sucede porque el agente se siente atra√≠do por el parpadeo apenas predecible del marcador despu√©s de los lanzamientos. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Mario.mp4" type="video/mp4"></video></div></div></div><br>  <b>Mario</b> : la recompensa interna est√° particularmente bien alineada con el objetivo del juego: progresi√≥n de nivel.  El agente es recompensado por buscar nuevas √°reas, ya que los detalles del √°rea reci√©n encontrada no se pueden predecir.  Como resultado, el agente descubri√≥ 11 niveles, encontr√≥ habitaciones secretas e incluso derrot√≥ a jefes. <br><br><h1>  Problema ruidoso de TV </h1><br>  Como jugador en una m√°quina tragamonedas, atra√≠do por resultados aleatorios, el agente a veces cae en la trampa de su curiosidad como resultado del "problema ruidoso de la televisi√≥n".  El agente encuentra una fuente de aleatoriedad en el entorno y contin√∫a observ√°ndola, siempre experimentando una alta recompensa interna por tales transiciones.  Un ejemplo de esa trampa es mirar un televisor que produce ruido est√°tico.  Demostramos esto literalmente al poner al agente en el laberinto de Unity con un televisor que reproduce canales aleatorios. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agente en un laberinto con una televisi√≥n ruidosa</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withoutTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agente en un laberinto sin televisi√≥n ruidosa</font></i> <br><br>  Te√≥ricamente, el problema de un televisor ruidoso es realmente grave, pero a√∫n esper√°bamos que en entornos mucho m√°s deterministas como la venganza de Montezuma, la curiosidad har√≠a que el agente encontrara habitaciones e interactuara con objetos.  Probamos varias opciones para predecir el siguiente estado basado en la curiosidad, combinando un bono de investigaci√≥n con una cuenta de juego. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/montezuma.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/pitfall.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/privateeye.mp4" type="video/mp4"></video></div></div></div><br>  En estos experimentos, el agente controla el entorno a trav√©s de un controlador de ruido, que con cierta probabilidad repite la √∫ltima acci√≥n en lugar de la actual.  Este escenario con acciones repetidas "pegajosas" se ha <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">propuesto</a> como una mejor pr√°ctica para entrenar agentes en juegos totalmente deterministas, como Atari, para evitar la memorizaci√≥n.  Las acciones "pegajosas" hacen que la transici√≥n de una habitaci√≥n a otra sea impredecible. <br><br><h1>  Destilaci√≥n de red aleatoria </h1><br>  Dado que predecir el siguiente estado es inherentemente susceptible al problema de un televisor ruidoso, hemos identificado las siguientes fuentes relevantes de errores de predicci√≥n: <br><br><ul><li>  <b>Factor 1</b> .  El error de pron√≥stico es alto si el predictor no puede generalizar a partir de los ejemplos considerados anteriormente.  La nueva experiencia corresponde a un error de predicci√≥n alto. </li><li>  <b>Factor 2</b>  El error de pron√≥stico es alto debido al objetivo de pron√≥stico estoc√°stico. </li><li>  <b>Factor 3</b> .  El error de pron√≥stico es alto debido a la falta de informaci√≥n necesaria para el pron√≥stico, o porque la clase del modelo predictor es demasiado limitada para ajustarse a la complejidad de la funci√≥n objetivo. </li></ul><br>  Determinamos que el factor 1 es una fuente √∫til de errores porque cuantifica la novedad de la experiencia, mientras que los factores 2 y 3 conducen al problema de un televisor ruidoso.  Para evitar los factores 2 y 3, desarrollamos RND, un nuevo bono de investigaci√≥n basado en la <b>predicci√≥n de la emisi√≥n de una red neuronal constante y aleatoriamente inicializada en el siguiente estado, teniendo en cuenta el siguiente estado</b> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db6/ac9/7fc/db6ac97fc37b0914e1a62145f855820c.svg" width="780"><br><br>  La intuici√≥n sugiere que los modelos predictivos tienen un bajo error al predecir las condiciones en las que fue entrenada.  En particular, las predicciones del agente sobre la emisi√≥n de una red neuronal inicializada aleatoriamente ser√°n menos precisas en los nuevos estados que en los estados que el agente sol√≠a conocer antes.  La ventaja de usar el problema de pron√≥stico sint√©tico es que puede ser determinista (omitiendo el factor 2), y dentro de la clase de funciones, el predictor puede elegir un predictor de la misma arquitectura que la red objetivo (omitiendo el factor 3).  Esto elimina el problema RND de un televisor ruidoso. <br><br>  Combinamos el bono de investigaci√≥n con recompensas externas a trav√©s de una forma de optimizaci√≥n de la pol√≠tica m√°s cercana: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Optimizaci√≥n de pol√≠tica proximal</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PPO</a> ), que utiliza <b>dos valores de valor para dos flujos de recompensa</b> .  Esto le permite usar diferentes descuentos para diferentes recompensas y combinar recompensas epis√≥dicas y no epis√≥dicas.  <b>Debido a esa flexibilidad adicional, nuestro mejor agente a menudo encuentra 22 de 24 habitaciones en el primer nivel en la venganza de Montezuma, y ‚Äã‚Äãa veces pasa el primer nivel despu√©s de encontrar las dos habitaciones restantes.</b>  El mismo m√©todo demuestra un rendimiento r√©cord en los juegos Venture y Gravitar. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/043/51a/ee8/04351aee8d6be917caf1994b968e04b9.svg" width="780"><br>  La siguiente visualizaci√≥n muestra un gr√°fico de la recompensa interna en el episodio de Venganza de Montezuma, donde el agente encuentra por primera vez la antorcha. <br><br><img src="https://habrastorage.org/webt/hu/vg/px/huvgpxpbzzc3-reechovxyhjcjs.gif"><br><br><h1>  La implementaci√≥n competente es importante </h1><br>  Para seleccionar un buen algoritmo, es importante tener en cuenta consideraciones generales, como la susceptibilidad al problema de un televisor ruidoso.  Sin embargo, descubrimos que cambios aparentemente muy peque√±os en nuestro algoritmo simple afectan en gran medida su efectividad: desde un agente que no puede salir de la primera sala hasta un agente que pasa por el primer nivel.  Para agregar estabilidad al entrenamiento, evitamos la saturaci√≥n de rasgos y llevamos las recompensas internas a un rango predecible.  Tambi√©n notamos <b>mejoras significativas en la efectividad de RND cada vez que encontramos y reparamos un error</b> (nuestro favorito inclu√≠a la puesta a cero aleatoria de la matriz, lo que llev√≥ al hecho de que las recompensas externas se consideraban no epis√≥dicas; nos dimos cuenta de esto solo despu√©s de pensar en la funci√≥n de valor externo , que parec√≠a sospechosamente peri√≥dico).  Corregir estos detalles se ha convertido en una parte importante para lograr un alto rendimiento incluso cuando se utilizan algoritmos conceptualmente similares al trabajo anterior.  Esta es una de las razones por las cuales es mejor elegir algoritmos simples siempre que sea posible. <br><br><h1>  Trabajo futuro </h1><br>  Ofrecemos las siguientes √°reas para futuras investigaciones: <br><br><ul><li>  An√°lisis de las ventajas de diferentes m√©todos de investigaci√≥n y b√∫squeda de nuevas formas de combinarlos. </li><li>  Capacitar a un agente curioso en muchos entornos diferentes sin recompensas y aprender a transferir a un entorno objetivo con recompensas. </li><li>  Inteligencia global, incluidas soluciones coordinadas a lo largo de horizontes a largo plazo. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es428776/">https://habr.com/ru/post/es428776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es428766/index.html">El resumen de materiales frescos del mundo del front-end para la √∫ltima semana No. 337 (29 de octubre - 4 de noviembre de 2018)</a></li>
<li><a href="../es428768/index.html">En tres art√≠culos sobre m√≠nimos cuadrados: programa educativo sobre teor√≠a de probabilidad</a></li>
<li><a href="../es428770/index.html">Macros de teclado para tareas cotidianas</a></li>
<li><a href="../es428772/index.html">Democratizaci√≥n de los datos de Uber.</a></li>
<li><a href="../es428774/index.html">Firewall GPS para centros de datos: por qu√© es necesario y c√≥mo funciona</a></li>
<li><a href="../es428778/index.html">Ver lo invisible. Infrarrojo cercano (0.9-1.7Œºm)</a></li>
<li><a href="../es428786/index.html">Procesador cu√°ntico basado en resonancia de giro y manipulaciones con un sistema singlete-triplete</a></li>
<li><a href="../es428788/index.html">Bajo el cap√≥ de Bitfury Clarke: c√≥mo funciona nuestro nuevo chip de miner√≠a</a></li>
<li><a href="../es428790/index.html">Estamos escribiendo un chat bot para VKontakte en python usando longpoll. Segunda parte Dobles lazos, excepciones y otras herej√≠as</a></li>
<li><a href="../es428792/index.html">El nuevo chip Apple T2 hace que sea dif√≠cil escuchar a trav√©s del micr√≥fono incorporado de la computadora port√°til</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>