<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèø‚Äçüî¨ üë∏üèø üç§ LLVM f√ºr Tensorflow oder Moore's Law End Compiler üë®üèø‚Äçüöí ü§≤üèº ‚ûó</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das TensorFlow-√ñkosystem enth√§lt eine Reihe von Compilern und Optimierern, die auf verschiedenen Ebenen des Software- und Hardware-Stacks arbeiten. F√º...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>LLVM f√ºr Tensorflow oder Moore's Law End Compiler</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/457826/">  Das TensorFlow-√ñkosystem enth√§lt eine Reihe von Compilern und Optimierern, die auf verschiedenen Ebenen des Software- und Hardware-Stacks arbeiten.  F√ºr diejenigen, die Tensorflow t√§glich verwenden, kann dieser mehrstufige Stapel schwer verst√§ndliche Fehler sowohl bei der Kompilierung als auch bei der Laufzeit verursachen, die mit der Verwendung verschiedener Arten von Hardware (GPU, TPU, mobile Plattformen usw.) verbunden sind. <br><br>  Diese Komponenten, beginnend mit dem Tensorflow-Diagramm, k√∂nnen in Form eines solchen Diagramms dargestellt werden: <br><br><img src="https://habrastorage.org/webt/ly/cf/0y/lycf0y0ld6eld56mnuba9iyhjii.png"><br><br>  <i>Es ist tats√§chlich schwieriger</i> <br><a name="habracut"></a><br>  In diesem Diagramm sehen wir, dass Tensorflow-Diagramme auf verschiedene Arten ausgef√ºhrt werden k√∂nnen. <br><br><div class="spoiler">  <b class="spoiler_title">eine Notiz</b> <div class="spoiler_text">  In TensorFlow 2.0 k√∂nnen Diagramme implizit sein, die gierige Ausf√ºhrung kann Vorg√§nge einzeln, in Gruppen oder in einem vollst√§ndigen Diagramm ausf√ºhren.  Diese Diagramme oder Fragmente des Diagramms m√ºssen optimiert und ausgef√ºhrt werden. <br></div></div><br>  Zum Beispiel: <br><br><ul><li>  Wir senden die Grafiken an den Tensorflow-Executor, der spezialisierte handgeschriebene Kernel aufruft </li><li>  Konvertieren Sie sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">XLA</a> HLO (XLA High-Level Optimizer-Darstellung) - eine allgemeine Darstellung des XLA-Optimierers, der wiederum den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LLVM-</a> Compiler f√ºr die CPU oder GPU aufrufen oder XLA f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TPU</a> weiterhin verwenden oder kombinieren kann. </li><li>  Wir konvertieren sie in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TensorRT</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nGraph</a> oder ein anderes Format f√ºr einen speziellen Befehlssatz, der in Hardware implementiert ist. </li><li>  Wir konvertieren sie in das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TensorFlow Lite-</a> Format, f√ºhren sie in der TensorFlow Lite-Laufzeit aus oder konvertieren sie in Code, um sie auf der GPU oder dem DSP √ºber die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Android Neural Networks API</a> (NNAPI) oder dergleichen auszuf√ºhren. </li></ul><br>  Es gibt auch komplexere Methoden, einschlie√ülich vieler Optimierungsdurchl√§ufe auf jeder Ebene, wie beispielsweise im Grappler-Framework, das die Operationen in TensorFlow optimiert. <br><br>  Obwohl diese verschiedenen Implementierungen von Compilern und Zwischendarstellungen die Leistung verbessern, stellt ihre Vielfalt f√ºr Endbenutzer ein Problem dar, z. B. verwirrende Fehlermeldungen beim Koppeln dieser Subsysteme.  Au√üerdem m√ºssen die Entwickler neuer Software- und Hardware-Stacks die Optimierungs- und Konvertierungspassagen f√ºr jeden neuen Fall anpassen. <br><br>  Aufgrund all dessen freuen wir uns, MLIR, eine mehrstufige Zwischenvertretung, bekannt zu geben.  Dies ist ein Zwischenansichtsformat und Kompilierungsbibliotheken zur Verwendung zwischen einer Modellansicht und einem Compiler auf niedriger Ebene, der hardwareabh√§ngigen Code generiert.  Mit der Einf√ºhrung von MLIR m√∂chten wir neuen Forschungsergebnissen bei der Entwicklung der Optimierung von Compilern und der Implementierung von Compilern auf der Basis industrieller Qualit√§tskomponenten Platz machen. <br><br>  Wir erwarten, dass MLIR f√ºr viele Gruppen von Interesse ist, darunter: <br><br><ul><li>  Compilerforscher sowie Praktiker, die die Leistung und den Speicherverbrauch von Modellen f√ºr maschinelles Lernen optimieren m√∂chten; </li><li>  Hardwarehersteller, die nach einer M√∂glichkeit suchen, ihre Hardware mit Tensorflow zu kombinieren, z. B. TPUs, mobile Neuroprozessoren in Smartphones und andere benutzerdefinierte ASICs; </li><li>  Personen, die den Programmiersprachen die Vorteile der Optimierung von Compilern und Hardwarebeschleunigern bieten m√∂chten; </li></ul><br><h3>  Was ist MLIR? </h3><br>  MLIR ist im Wesentlichen eine flexible Infrastruktur f√ºr moderne Optimierungscompiler.  Dies bedeutet, dass es aus einer IR-Spezifikation (Intermediate Representation) und einer Reihe von Werkzeugen zum Transformieren dieser Darstellung besteht.  Wenn wir √ºber Compiler sprechen, wird der √úbergang von einer √ºbergeordneten Ansicht zu einer untergeordneten Ansicht als Verringern bezeichnet, und wir werden diesen Begriff in Zukunft verwenden. <br><br>  MLIR wurde unter dem Einfluss von LLVM gebaut und leiht sich schamlos viele gute Ideen daraus.  Es verf√ºgt √ºber ein flexibles Typsystem und dient zur Darstellung, Analyse und Transformation von Graphen, wobei viele Abstraktionsebenen in einer Kompilierungsebene kombiniert werden.  Diese Abstraktionen umfassen Tensorflow-Operationen, verschachtelte polyedrische Schleifenbereiche, LLVM-Anweisungen sowie Festpunktoperationen und -typen. <br><br><h3>  Dialekte von MLIR </h3><br>  Um die verschiedenen Software- und Hardwareziele zu trennen, hat MLIR ‚ÄûDialekte‚Äú, darunter: <br><br><ul><li>  TensorFlow IR, das alles enth√§lt, was in TensorFlow-Diagrammen getan werden kann </li><li>  XLA HLO IR, entwickelt, um alle Vorteile des XLA-Compilers zu nutzen, dessen Ausgabe wir nicht nur Code f√ºr TPU erhalten k√∂nnen. </li><li>  Ein experimenteller Affinit√§tsdialekt, der speziell f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">polyedrische</a> Darstellungen und Optimierungen entwickelt wurde </li><li>  LLVM IR, 1: 1, das mit der nativen LLVM-Ansicht √ºbereinstimmt, sodass MLIR mithilfe von LLVM Code f√ºr die GPU und die CPU generieren kann. </li><li>  TensorFlow Lite wurde entwickelt, um Code f√ºr mobile Plattformen zu generieren </li></ul><br>  Jeder Dialekt enth√§lt eine Reihe spezifischer Operationen, bei denen Invarianten verwendet werden, z. B.: "Es handelt sich um einen bin√§ren Operator, dessen Eingabe und Ausgabe vom gleichen Typ sind." <br><br><h3>  Erweiterungen MLIR </h3><br>  MLIR verf√ºgt nicht √ºber eine feste und integrierte Liste globaler intrinsischer Operationen.  Dialekte k√∂nnen vollst√§ndig benutzerdefinierte Typen definieren, und so kann MLIR Dinge wie das LLVM-IR-Typsystem (mit erstklassigen Aggregaten), Dom√§nensprachenabstraktionen wie quantisierte Typen, die f√ºr ML-optimierte Beschleuniger wichtig sind, und in Zukunft auch modellieren. sogar ein Swift- oder Clang-System. <br><br>  Wenn Sie diesem System einen neuen Low-Level-Compiler hinzuf√ºgen m√∂chten, k√∂nnen Sie einen neuen Dialekt erstellen und vom Dialekt des TensorFlow-Diagramms zu Ihrem Dialekt absteigen.  Dies vereinfacht den Pfad f√ºr Hardwareentwickler und Compilerentwickler.  Sie k√∂nnen den Dialekt auf verschiedene Ebenen desselben Modells ausrichten. Optimierer auf hoher Ebene sind f√ºr bestimmte Teile der IR verantwortlich. <br><br>  F√ºr Compiler-Forscher und Framework-Entwickler k√∂nnen Sie mit MLIR Transformationen auf jeder Ebene erstellen. Sie k√∂nnen Ihre eigenen Operationen und Abstraktionen im IR definieren und so Ihre Anwendungsaufgaben besser modellieren.  Somit ist MLIR mehr als eine reine Compiler-Infrastruktur, die LLVM ist. <br><br>  Obwohl MLIR als Compiler f√ºr ML arbeitet, erm√∂glicht es auch den Einsatz von Technologien f√ºr maschinelles Lernen!  Dies ist sehr wichtig f√ºr Ingenieure, die numerische Bibliotheken entwickeln, und kann nicht die gesamte Vielfalt der ML-Modelle und -Hardware unterst√ºtzen.  Die Flexibilit√§t von MLIR erleichtert das Erforschen von Strategien f√ºr den Code-Abstieg beim Wechsel zwischen Abstraktionsebenen. <br><br><h3>  Was weiter </h3><br>  Wir haben ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub-Repository</a> ge√∂ffnet und laden alle Interessierten ein (siehe unseren Leitfaden!).  Wir werden in den kommenden Monaten etwas mehr als diese Toolbox ver√∂ffentlichen - die Dialektspezifikationen TensorFlow und TF Lite.  Wir k√∂nnen Ihnen mehr erz√§hlen, um mehr zu erfahren, sehen Sie sich die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pr√§sentation von Chris Luttner</a> und unsere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">README auf Github an</a> . <br><br>  Wenn Sie √ºber alle Aspekte von MLIR auf dem Laufenden bleiben m√∂chten, nehmen Sie an unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neuen Mailingliste teil</a> , die sich in K√ºrze auf Ank√ºndigungen zuk√ºnftiger Versionen unseres Projekts konzentrieren wird.  Bleib bei uns! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de457826/">https://habr.com/ru/post/de457826/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de457812/index.html">‚ÄûWarte! Wer kommt? " Video√ºberwachung auf der Veranda</a></li>
<li><a href="../de457814/index.html">N√§chste Schritte 2</a></li>
<li><a href="../de457816/index.html">Juju auf einen Blick</a></li>
<li><a href="../de457820/index.html">Wie Sie einen Evangelisten f√ºr Ihr Unternehmen entwickeln</a></li>
<li><a href="../de457824/index.html">Infekti√∂ser Stress: Interspezifische Synchronisation des Cortisolspiegels am Beispiel von Hunden und ihren Besitzern</a></li>
<li><a href="../de457830/index.html">Wie repariere ich ein Hinterhof-Schwimmbad in 7 Stunden mit der Methode des kritischen Pfades?</a></li>
<li><a href="../de457836/index.html">Was ich durch das Erstellen von Dribbble gelernt habe</a></li>
<li><a href="../de457838/index.html">EDR-Technologie als Element der SOC-Nuklear-Triade</a></li>
<li><a href="../de457842/index.html">Sojus-TM Raumfahrzeug-Bewegungssteuerungssystem</a></li>
<li><a href="../de457844/index.html">Sieben Kerngewohnheiten f√ºr remote arbeitende Entwicklungsteams</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>