<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñïüèª ü¶ê üö∞ PNL. Les bases. Techniques. D√©veloppement personnel. Partie 1 üçú üéÖüèº üë©üèæ‚Äçüî¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Table des mati√®res   PNL. Les bases. Techniques. D√©veloppement personnel. Partie 2: NER 
 
 Salut Je m'appelle Ivan Smurov et je dirige le groupe de r...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>PNL. Les bases. Techniques. D√©veloppement personnel. Partie 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/437008/"><div class="spoiler">  <b class="spoiler_title">Table des mati√®res</b> <div class="spoiler_text"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/nf/p0/ws/nfp0wsz4wap5qwx33ulwimmaid8.png" alt="image"></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PNL.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Les bases.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Techniques.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">D√©veloppement personnel.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 2: NER</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><br></a> </div></div><br>  Salut  Je m'appelle Ivan Smurov et je dirige le groupe de recherche PNL chez ABBYY.  Vous pouvez lire ce que notre groupe fait <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  J'ai r√©cemment donn√© une conf√©rence sur le traitement automatique du langage naturel (NLP) √† la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">School of Deep Learning</a> - c'est un groupe √† la PhysTech School of Applied Mathematics and Computer Science au MIPT pour les √©tudiants seniors int√©ress√©s par la programmation et les math√©matiques.  Peut-√™tre que les th√®ses de ma conf√©rence seront utiles √† quelqu'un, alors je les partagerai avec Habr. <br><br>  Puisque tout ne peut pas √™tre saisi √† la fois, nous diviserons l'article en deux parties.  Aujourd'hui, je vais parler de la fa√ßon dont les r√©seaux de neurones (ou l'apprentissage en profondeur) sont utilis√©s dans la PNL.  Dans la deuxi√®me partie de l'article, nous nous concentrerons sur l'une des t√¢ches PNL les plus courantes - la t√¢che d'extraction d'entit√©s nomm√©es (reconnaissance d'entit√© nomm√©e, NER) et analyser en d√©tail l'architecture de ses solutions. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cn/5o/dx/cn5odxj0tvmrhaf4jstaz7ptars.png"></div><br><a name="habracut"></a><br><h2>  Qu'est-ce que la PNL? <br></h2><br>  Il s'agit d'un large √©ventail de t√¢ches pour le traitement de textes dans une langue naturelle (c'est-√†-dire la langue que les gens parlent et √©crivent).  Il existe un ensemble de t√¢ches PNL classiques, dont la solution est pratique. <br><br><ul><li>  La premi√®re et la plus importante t√¢che historique est la traduction automatique.  Il est pratiqu√© depuis tr√®s longtemps et il y a d'√©normes progr√®s.  Mais la t√¢che d'obtenir une traduction enti√®rement automatique de haute qualit√© (FAHQMT) n'est toujours pas r√©solue.  D'une certaine mani√®re, c'est le moteur NLP, l'une des plus grandes t√¢ches que vous pouvez faire. <br><br><img src="https://habrastorage.org/webt/bj/r0/og/bjr0ogf9-tarmd10sntfy-c_mnq.png" alt="image"><br></li><li>  La deuxi√®me t√¢che est la classification des textes.  Un ensemble de textes est donn√© et la t√¢che consiste √† classer ces textes en cat√©gories.  Lequel?  C'est une question pour le corps. <br><br>  Le premier et l'un des moyens les plus pratiques de l'appliquer d'un point de vue pratique est la classification des lettres en spam et en rogne (pas en spam). <br><br>  Une autre option classique est la classification multiclasse des nouvelles en cat√©gories (rubrication) - politique √©trang√®re, sport, chapiteau, etc. Ou, disons, vous recevez des lettres et vous souhaitez s√©parer les commandes de la boutique en ligne des billets d'avion et des r√©servations d'h√¥tel. <br><br>  La troisi√®me application classique du probl√®me de classification des textes est l'analyse sentimentale.  Par exemple, la classification des avis comme positifs, n√©gatifs et neutres. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/my/c1/co/myc1cop8u7adcvhhh0qgw-dmtse.png"></div><br>  √âtant donn√© qu'il existe de nombreuses cat√©gories possibles dans lesquelles vous pouvez diviser des textes, la classification des textes est l'une des t√¢ches pratiques les plus populaires de la PNL. </li><li>  La troisi√®me t√¢che consiste √† r√©cup√©rer les entit√©s nomm√©es, NER.  Nous s√©lectionnons dans les sections de texte qui correspondent √† un ensemble pr√©s√©lectionn√© d'entit√©s, par exemple, vous devez trouver tous les emplacements, personnes et organisations dans le texte.  Dans le texte ¬´Ostap Bender - Directeur du bureau¬´ Horns and Hooves ¬ª¬ª, vous devez comprendre que Ostap Bender est une personne et que ¬´Horns and Hooves¬ª est une organisation.  Pourquoi cette t√¢che est n√©cessaire dans la pratique et comment la r√©soudre, nous parlerons dans la deuxi√®me partie de notre article. </li><li><img src="https://habrastorage.org/webt/op/d2/lv/opd2lvphnvm4j1e7o6s6-u34pie.png" align="right">  La quatri√®me t√¢che est li√©e √† la troisi√®me - la t√¢che d'extraire des faits et des relations (extraction de relations).  Par exemple, il y a une attitude de travail (Profession).  D'apr√®s le texte ¬´Ostap Bender - Directeur du bureau¬´ Horns and Hooves ¬ª¬ª, il est clair que notre h√©ros est li√© aux relations professionnelles avec ¬´Horns and Hooves¬ª.  La m√™me chose peut √™tre dite de bien d'autres fa√ßons: ¬´Le bureau d'Ostap Bender est dirig√© par le bureau¬´ Horns and Hooves ¬ª, ou¬´ Ostap Bender est pass√© du simple fils du lieutenant Schmidt au chef du bureau ¬´Horns and Hooves¬ª.  Ces phrases diff√®rent non seulement par leur pr√©dicat, mais aussi par leur structure. <br><br>  Des exemples d'autres relations qui sont souvent mises en √©vidence sont l'achat et la vente, la propri√©t√©, le fait de la naissance avec des attributs - date, lieu, etc. (naissance) et quelques autres. <br><br>  La t√¢che ne semble pas avoir d'application pratique √©vidente, mais elle est n√©anmoins utilis√©e dans la structuration d'informations non structur√©es.  De plus, il est important dans les syst√®mes de questions-r√©ponses et de dialogue, dans les moteurs de recherche - toujours lorsque vous devez analyser une question et comprendre √† quel type elle se rapporte, ainsi que les restrictions qui existent sur la r√©ponse. <br><br><img src="https://habrastorage.org/webt/eh/y7/tl/ehy7tl9hbhhxlsy57j21q8xpa4w.png" alt="image"><br></li><li>  Les deux t√¢ches suivantes sont probablement les plus hype.  Ce sont des syst√®mes de questions-r√©ponses et de dialogue (chat bots).  Amazon Alexa, Alice sont des exemples classiques de syst√®mes conversationnels.  Pour qu'ils fonctionnent correctement, de nombreuses t√¢ches PNL doivent √™tre r√©solues.  Par exemple, la classification de texte permet de d√©terminer si nous tombons dans l'un des sc√©narios de chatbot orient√©s objectif.  Supposons que "la question des taux de change".  L'extraction des relations est n√©cessaire pour identifier les espaces r√©serv√©s pour le mod√®le de script, et la t√¢che de mener un dialogue sur des sujets communs (¬´orateurs¬ª) nous aidera dans une situation o√π nous ne sommes tomb√©s dans aucun des sc√©narios. <br><br>  Les syst√®mes de questions-r√©ponses sont √©galement une chose compr√©hensible et utile.  Vous posez une question √† une voiture, la voiture cherche une r√©ponse dans une base de donn√©es ou un corps de texte.  IBM Watson ou Wolfram Alpha sont des exemples de tels syst√®mes. </li><li>  Un autre exemple du probl√®me classique de la PNL est la sammarisation.  L'√©nonc√© du probl√®me est simple: le syst√®me d'entr√©e accepte un texte volumineux et la sortie est un texte plus petit, refl√©tant en quelque sorte le contenu d'un texte volumineux.  Par exemple, une machine est n√©cessaire pour g√©n√©rer une nouvelle narration d'un texte, de son nom ou de son annotation. <br><br><img src="https://habrastorage.org/webt/m-/mj/uq/m-mjuqawoktinjvjcrkcmlnt5xi.png" alt="image"><br></li><li>  Une autre t√¢che populaire est l'exploration d'argumentation, la recherche de justification dans le texte.  On vous donne un fait et un texte, vous devez trouver une justification √† ce fait dans le texte. </li></ul><br>  Ce n'est en aucun cas la liste compl√®te des t√¢ches PNL.  Il y en a des dizaines.  Dans l'ensemble, tout ce qui peut √™tre fait avec du texte dans une langue naturelle peut √™tre attribu√© aux t√¢ches de la PNL, seuls les sujets √©num√©r√©s sont √† l'oreille et ils ont les applications pratiques les plus √©videntes. <br><br><h2>  Pourquoi est-il difficile de r√©soudre des t√¢ches PNL? <br></h2><br>  La formulation des t√¢ches n'est pas tr√®s compliqu√©e, mais les t√¢ches elles-m√™mes ne sont pas du tout simples, car nous travaillons avec le langage naturel.  Les ph√©nom√®nes de polys√©mie (les mots polys√©miques ont une signification initiale commune) et d'homonymie (les mots ayant des sens diff√©rents sont prononc√©s et √©crits de la m√™me mani√®re) sont caract√©ristiques de tout langage naturel.  Et si un locuteur natif du russe comprend bien que <i>l'accueil chaleureux a</i> peu de choses en commun avec la <i>technique de combat</i> , d'une part, et la <i>bi√®re chaude</i> , d'autre part, le syst√®me automatique doit l'apprendre depuis longtemps.  Pourquoi est-il pr√©f√©rable de traduire ¬´ <i>Appuyez sur la barre d'espace pour continuer</i> ¬ª en ennuyeux ¬´ <i>Pour continuer, appuyez sur la barre d'espace</i> ¬ª que ¬´La <i>barre de presse d'espace continuera de fonctionner</i> ¬ª. <br><br><ul><li>  Polys√©mie: arr√™t (processus ou b√¢timent), table (organisation ou objet), pic (oiseau ou personne). </li><li>  Homonymie: cl√©, arc, serrure, po√™le. <br><br><img src="https://habrastorage.org/webt/5i/-h/c1/5i-hc1p3hrtjf1duq5zxwajegyy.png" alt="image"><br></li><li>  Un autre exemple classique de la complexit√© du langage est le pronom anaphore.  Par exemple, donnons-nous le texte " <i>Concierge deux heures de neige, il √©tait m√©content</i> ".  Le pronom ¬´il¬ª peut d√©signer √† la fois le concierge et la neige.  Par contexte, on comprend facilement que c'est un concierge, pas de la neige.  Mais pour que l'ordinateur le comprenne aussi facilement, ce n'est pas facile.  Le probl√®me du pronom anaphore n'est pas encore tr√®s bien r√©solu; les tentatives actives pour am√©liorer la qualit√© des d√©cisions se poursuivent. </li><li>  Une autre complexit√© suppl√©mentaire est l'ellipse.  Par exemple, " <i>Petya a mang√© une pomme verte et Masha en a mang√© une rouge</i> ."  Nous comprenons que Masha a mang√© une pomme rouge.  Cependant, il n'est pas facile de faire comprendre cela √† la machine.  Maintenant, la t√¢che de restaurer les ellipses est r√©solue sur de minuscules cas (plusieurs centaines de phrases), et sur eux la qualit√© de la restauration compl√®te est franchement faible (de l'ordre de 0,5).  Il est clair que pour des applications pratiques, une telle qualit√© n'est pas bonne. </li></ul><br>  Soit dit en passant, cette ann√©e, lors de la conf√©rence <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dialogue</a> , des pistes auront lieu √† la fois sur l'anaphore et sur les espaces (un type d'ellipse) pour la langue russe.  Pour les deux t√¢ches, les caisses ont √©t√© assembl√©es avec un volume plusieurs fois sup√©rieur aux volumes des b√¢timents existants (de plus, pour les espaces, le volume de la valise est d'un ordre de grandeur sup√©rieur aux volumes des valises, non seulement pour le russe, mais pour toutes les langues en g√©n√©ral).  Si vous souhaitez participer √† des comp√©titions dans ces b√¢timents, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cliquez ici (avec inscription, mais sans SMS)</a> . <br><br><h2>  Comment les t√¢ches PNL sont r√©solues <br></h2><br>  Contrairement au traitement d'image, vous pouvez toujours trouver des articles sur la PNL qui d√©crivent des solutions qui utilisent des algorithmes classiques tels que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SVM</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Xgboost</a> , pas des r√©seaux de neurones, et qui montrent des r√©sultats qui ne sont pas trop inf√©rieurs aux solutions de pointe. <br><br>  Cependant, il y a plusieurs ann√©es, les r√©seaux de neurones ont commenc√© √† vaincre les mod√®les classiques.  Il est important de noter que pour la plupart des t√¢ches, les solutions bas√©es sur des m√©thodes classiques √©taient uniques, en r√®gle g√©n√©rale, pas similaires √† la r√©solution d'autres probl√®mes √† la fois dans l'architecture et dans la fa√ßon dont la collecte et le traitement des attributs se produisent. <br><br>  Cependant, les architectures de r√©seaux de neurones sont beaucoup plus g√©n√©rales.  L'architecture du r√©seau lui-m√™me, tr√®s probablement, est √©galement diff√©rente, mais beaucoup plus petite, il y a une tendance √† l'universalisation compl√®te.  Cependant, avec quelles fonctionnalit√©s et comment nous travaillons exactement, c'est d√©j√† presque la m√™me chose pour la plupart des t√¢ches PNL.  Seules les derni√®res couches des r√©seaux de neurones diff√®rent.  Ainsi, nous pouvons supposer qu'un seul pipeline NLP a √©t√© form√©.  Sur la fa√ßon dont il est organis√©, nous allons maintenant vous en dire plus. <br><br><h2>  Pipeline nlp </h2><br>  Cette fa√ßon de travailler avec les enseignes, qui est plus ou moins la m√™me pour toutes les t√¢ches. <br><br>  En ce qui concerne la langue, l'unit√© de base avec laquelle nous travaillons est le mot.  Ou plus formellement un "jeton".  Nous utilisons ce terme parce qu'il n'est pas tr√®s clair ce qu'est 2128506 - est-ce un mot ou pas?  La r√©ponse n'est pas √©vidente.  Le jeton est g√©n√©ralement s√©par√© des autres jetons par des espaces ou des signes de ponctuation.  Et comme vous pouvez le comprendre √† partir des difficult√©s que nous avons d√©crites ci-dessus, le contexte de chaque jeton est tr√®s important.  Il existe diff√©rentes approches, mais dans 95% des cas, le contexte pris en compte lors de l'ex√©cution du mod√®le est la proposition, y compris le jeton d'origine. <br><br>  De nombreuses t√¢ches sont g√©n√©ralement r√©solues au niveau de la proposition.  Par exemple, la traduction automatique.  Plus souvent qu'autrement, nous traduisons simplement une phrase et n'utilisons pas du tout un contexte plus large.  Il y a des t√¢ches o√π ce n'est pas le cas, par exemple, les syst√®mes de dialogue.  Il est important de se rappeler de quoi le syst√®me a √©t√© question auparavant afin qu'il puisse r√©pondre aux questions.  Cependant, l'offre est √©galement l'unit√© principale avec laquelle nous travaillons. <br><br>  Par cons√©quent, les deux premi√®res √©tapes du pipeline qui sont effectu√©es pour r√©soudre presque toutes les t√¢ches sont la segmentation (division du texte en phrases) et la tokenisation (division des phrases en jetons, c'est-√†-dire des mots individuels).  Cela se fait avec des algorithmes simples. <br><br>  Ensuite, vous devez calculer les caract√©ristiques de chaque jeton.  En r√®gle g√©n√©rale, cela se produit en deux √©tapes.  La premi√®re consiste √† calculer des attributs de jeton ind√©pendants du contexte.  Il s'agit d'un ensemble de signes qui ne d√©pendent en aucun cas d'autres mots entourant notre jeton.  Les attributs communs ind√©pendants du contexte sont: <br><br><ul><li>  plongements </li><li>  signes symboliques </li><li>  fonctionnalit√©s suppl√©mentaires sp√©cifiques √† une t√¢che ou une langue particuli√®re </li></ul><br>  Nous parlerons des encastrements et des signes symboliques plus en d√©tail ci-dessous (√† propos des signes symboliques - pas aujourd'hui, mais dans la deuxi√®me partie de notre article), mais pour l'instant donnons des exemples possibles de signes suppl√©mentaires. <br><br>  L'une des fonctionnalit√©s les plus couramment utilis√©es est la partie de la parole ou la balise POS (partie de la parole).  Ces fonctionnalit√©s peuvent √™tre importantes pour r√©soudre de nombreux probl√®mes, par exemple les t√¢ches d'analyse.  Pour les langues √† morphologie complexe, comme la langue russe, les caract√®res morphologiques sont √©galement importants: par exemple, dans quel cas est le nom, quel type d'adjectif.  Nous pouvons en tirer diff√©rentes conclusions sur la structure de la proposition.  De plus, la morphologie est n√©cessaire pour la lemmatisation (r√©duction des mots aux formes initiales), √† l'aide de laquelle nous pouvons r√©duire la dimension de l'espace attributaire, et donc l'analyse morphologique est activement utilis√©e pour la plupart des probl√®mes de PNL. <br><br>  Lorsque nous r√©solvons un probl√®me o√π l'interaction entre diff√©rents objets est importante (par exemple, dans la t√¢che d'extraction de relation ou lors de la cr√©ation d'un syst√®me de questions-r√©ponses), nous devons en savoir beaucoup sur la structure de la proposition.  Cela n√©cessite une analyse.  √Ä l'√©cole, tout le monde a analys√© une phrase pour un sujet, un pr√©dicat, un ajout, etc. L'analyse syntaxique est quelque chose dans cet esprit, mais plus compliqu√©. <br><br>  Un autre exemple de fonctionnalit√© suppl√©mentaire est la position du jeton dans le texte.  On peut savoir a priori qu'une entit√© se retrouve le plus souvent au d√©but du texte ou vice versa √† la fin. <br><br>  Tous ensemble - encastrements, signes symboliques et additionnels - forment un vecteur de signes symboliques qui ne d√©pend pas du contexte. <br><br><h2>  Fonctionnalit√©s contextuelles </h2><br>  Les signes de jeton contextuels sont un ensemble de signes qui contient des informations non seulement sur le jeton lui-m√™me, mais aussi sur ses voisins.  Il existe diff√©rentes fa√ßons de calculer ces sympt√¥mes.  Dans les algorithmes classiques, les gens marchaient souvent simplement par la ¬´fen√™tre¬ª: ils prenaient plusieurs (par exemple, trois) jetons √† l'original et plusieurs jetons apr√®s, puis calculaient tous les signes dans une telle fen√™tre.  Cette approche n'est pas fiable, car des informations importantes pour l'analyse peuvent √™tre √† une distance sup√©rieure √† la fen√™tre, respectivement, nous pouvons manquer quelque chose. <br><br>  Par cons√©quent, toutes les fonctionnalit√©s contextuelles sont d√©sormais calcul√©es au niveau de la proposition de mani√®re standard: en utilisant des r√©seaux de neurones r√©currents bidirectionnels LSTM ou GRU.  Pour obtenir des attributs de jeton contextuels √† partir d'ind√©pendants du contexte, les attributs ind√©pendants du contexte de tous les jetons d'offre sont soumis au RNN bidirectionnel (simple ou multicouche).  La sortie du RNN bidirectionnel au i-√®me instant dans le temps est un signe contextuel du i-token, qui contient des informations sur les deux jetons pr√©c√©dents (puisque ces informations sont contenues dans la i-√®me valeur du RNN direct), et sur les suivants (t .k. cette information est contenue dans la valeur correspondante du RNN inverse). <br><br>  De plus, pour chaque t√¢che individuelle, nous faisons quelque chose de diff√©rent, mais les premi√®res couches - jusqu'au RNN bidirectionnel, peuvent √™tre utilis√©es pour presque toutes les t√¢ches. <br><br>  Cette m√©thode d'obtention de fonctionnalit√©s est appel√©e pipeline NLP. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mn/sz/0h/mnsz0hwhtiects6ksgru_y9hw2o.png"></div><br><br>  Il convient de noter qu'au cours des 2 derni√®res ann√©es, les chercheurs ont activement essay√© d'am√©liorer le pipeline NLP - √† la fois en termes de vitesse (par exemple, le transformateur - une architecture bas√©e sur l'auto-attention qui ne contient pas de RNN et est donc capable d'apprendre et d'appliquer plus rapidement), et avec point de vue des signes utilis√©s (maintenant ils utilisent activement des signes bas√©s sur des mod√®les de langage pr√©-form√©s, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ELMo</a> , ou ils utilisent les premi√®res couches du mod√®le de langage pr√©-form√© et les recyclent dans le cas disponible pour la t√¢che - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ULMFit</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">BERT</a> ). <br><br><h2>  Int√©grations sous forme de mots <br></h2><br>  Examinons de plus pr√®s ce qu'est l'int√©gration.  En gros, l'incorporation est une repr√©sentation concise du contexte d'un mot.  Pourquoi est-il important de conna√Ætre le contexte d'un mot?  Parce que nous croyons en une hypoth√®se de distribution - que des mots qui ont un sens similaire sont utilis√©s dans des contextes similaires. <br><br>  Essayons maintenant de donner une d√©finition rigoureuse de l'incorporation.  L'incorporation est une cartographie d'un vecteur discret de caract√©ristiques cat√©gorielles dans un vecteur continu avec une dimension pr√©d√©termin√©e. <br><br>  Un exemple canonique d'int√©gration est l'int√©gration de mots (int√©gration sous forme de mots). <br><br>  Qu'est-ce qui agit habituellement comme vecteur d'entit√©s discr√®tes?  Un vecteur bool√©en correspondant √† toutes les valeurs possibles d'une certaine cat√©gorie (par exemple, toutes les parties possibles du discours ou tous les mots possibles d'un dictionnaire limit√©). <br><br>  Pour les int√©grations sous forme de mots, cette cat√©gorie est g√©n√©ralement l'index du mot dans le dictionnaire.  Disons qu'il existe un dictionnaire avec une dimension de 100 000.  En cons√©quence, chaque mot a un vecteur caract√©ristique discret - un vecteur bool√©en de dimension 100 000, o√π √† un endroit (l'index du mot dans notre dictionnaire) est un, et les autres sont des z√©ros. <br><br>  Pourquoi voulons-nous mapper nos vecteurs d'entit√©s discr√®tes √† des dimensions donn√©es continues?  Parce que les vecteurs avec une dimension de 100 000 ne sont pas tr√®s pratiques √† utiliser pour les calculs, mais les vecteurs d'entiers de dimensions 100, 200 ou, par exemple, 300, sont beaucoup plus pratiques. <br><br>  En principe, nous ne pouvons pas essayer d'imposer des restrictions suppl√©mentaires sur une telle cartographie.  Mais puisque nous construisons un tel mappage, essayons de nous assurer que les vecteurs de mots de m√™me signification sont √©galement proches dans un certain sens.  Cela se fait √† l'aide d'un simple r√©seau neuronal √† action directe. <br><br><h2>  Int√©grer la formation <br></h2><br>  Comment les plongements sont-ils form√©s?  Nous essayons de r√©soudre le probl√®me de la restauration d'un mot par contexte (ou vice versa, la restauration d'un contexte par mot).  Dans le cas le plus simple, nous obtenons l'index dans le dictionnaire du mot pr√©c√©dent (le vecteur bool√©en de la dimension du dictionnaire) en entr√©e et essayons de d√©terminer l'index dans le dictionnaire de notre mot.  Cela se fait √† l'aide d'une grille avec une architecture extr√™mement simple: deux couches enti√®rement connect√©es.  Vient d'abord une couche enti√®rement connect√©e du vecteur bool√©en de la dimension du dictionnaire √† la couche cach√©e de la dimension d'incorporation (c'est-√†-dire multipliant simplement le vecteur bool√©en par la matrice de la dimension souhait√©e).  Et puis vice versa, une couche enti√®rement connect√©e avec softmax √† partir d'une couche cach√©e de dimension incorpor√©e dans un vecteur de dimension de dictionnaire.  Gr√¢ce √† la fonction d'activation softmax, nous obtenons la distribution de probabilit√© de notre mot et pouvons choisir l'option la plus probable. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vn/fu/ld/vnfuldlmtgik5rihivtpcivmnla.png"></div><br>  <i>L'incorporation du i√®me mot est simplement la i√®me ligne dans la matrice de transition W.</i> <br><br>  Dans les mod√®les utilis√©s en pratique, l'architecture est plus complexe, mais pas beaucoup.  La principale diff√©rence est que nous utilisons non pas un vecteur du contexte pour d√©finir notre mot, mais plusieurs (par exemple, tout dans une fen√™tre de taille 3).  Une option l√©g√®rement plus populaire consiste √† pr√©dire non pas un mot par contexte, mais plut√¥t un contexte par mot.  Cette approche est appel√©e Skip-gram. <br><br>  Donnons un exemple de l'application d'une t√¢che qui est r√©solue lors de la formation des plongements (dans la variante CBOW, pr√©dictions de mots par contexte).  Par exemple, supposons qu'un contexte de jeton se compose de 2 mots pr√©c√©dents.                ‚Äú ‚Äù, ,  ,       ‚Äú‚Äù. <br><br>   ,          (    ),       ,      . <br><br>      ,    ,      ,        (,        ,        ).   ‚Äî      . <br><br>  ,  ,         .      ,      ,    ,   . <br><br>         ,   ‚Äî ELMo, ULMFit, BERT.       ,         (  , , ,    ). <br><br><h2>   ? <br></h2><br>    ,     2  . <br><br><ul><li> -,     ,           ,   -   100 .     ‚Äì   :    ,    ,     . </li><li> -,      .      -.        .        .    ,        ,    .  ,    ,    .          .         ,       ,    . </li></ul><br><img src="https://habrastorage.org/webt/dh/w6/w2/dhw6w2s41xbc8y08jgszsupa-z8.png" alt="image"><br><br>    ,      .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ,    ,  ,        ,    ,       .       ,     ,    ,     ,     . <br><br>          NER.    ,    ,            .     ,        ,       ,    ,    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr437008/">https://habr.com/ru/post/fr437008/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr436998/index.html">Protection des micropuces contre l'ing√©nierie inverse et l'entr√©e non autoris√©e</a></li>
<li><a href="../fr437000/index.html">Comment apprendre aux gens √† utiliser git</a></li>
<li><a href="../fr437002/index.html">ASP.NET Core valide</a></li>
<li><a href="../fr437004/index.html">Les programmeurs YML r√™vent-ils de tests ansibles?</a></li>
<li><a href="../fr437006/index.html">Test de l'imprimante 3D Wanhao Duplicator 10</a></li>
<li><a href="../fr437010/index.html">√âchos du pass√©: l'exp√©rience de Young √† la base de la nouvelle m√©thode de spectroscopie aux rayons X</a></li>
<li><a href="../fr437014/index.html">La t√¢che de N corps ou comment faire sauter une galaxie sans quitter la cuisine</a></li>
<li><a href="../fr437018/index.html">Quelques pi√®ges de la saisie statique en Python</a></li>
<li><a href="../fr437020/index.html">Quel est le probl√®me avec l'apprentissage par renforcement?</a></li>
<li><a href="../fr437022/index.html">Bit de s√©curit√© contre le bruit 0x22 (attaques par injection de d√©fauts, 35C3 et Wallet.fail)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>