<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçü§ù‚Äçüë®üèª üßì üö¥ Redes neurais e aprendizado profundo, cap√≠tulo 3, parte 2: por que a regulariza√ß√£o ajuda a reduzir o treinamento? üë®üèæ üîó üë®üèæ‚Äçü§ù‚Äçüë®üèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Conte√∫do 

- Cap√≠tulo 1: usando redes neurais para reconhecer n√∫meros manuscritos 
- Cap√≠tulo 2: como o algoritmo de retropropaga√ß√£o funciona 
- Cap√≠t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neurais e aprendizado profundo, cap√≠tulo 3, parte 2: por que a regulariza√ß√£o ajuda a reduzir o treinamento?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/459816/"><div class="spoiler">  <b class="spoiler_title">Conte√∫do</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 1: usando redes neurais para reconhecer n√∫meros manuscritos</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 2: como o algoritmo de retropropaga√ß√£o funciona</a> </li><li>  Cap√≠tulo 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1: aprimorando o m√©todo de treinamento de redes neurais</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2: Por que a regulariza√ß√£o ajuda a reduzir a reciclagem?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 3: como escolher hiperpar√¢metros de redes neurais?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 4: prova visual de que as redes neurais s√£o capazes de computar qualquer fun√ß√£o</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 5: por que as redes neurais profundas s√£o t√£o dif√≠ceis de treinar?</a> </li><li>  Cap√≠tulo 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1: Aprendizado Profundo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2: progresso recente no reconhecimento de imagens</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Posf√°cio: existe um algoritmo simples para criar intelig√™ncia?</a> </li></ul></div></div><br>  Empiricamente, vimos que a regulariza√ß√£o ajuda a reduzir a reciclagem.  Isso √© inspirador - mas, infelizmente, n√£o √© √≥bvio por que a regulariza√ß√£o ajuda.  Geralmente, as pessoas explicam de alguma maneira: em certo sentido, pesos menores t√™m menos complexidade, o que fornece uma explica√ß√£o mais simples e eficiente dos dados, portanto, eles devem ser os preferidos.  No entanto, essa √© uma explica√ß√£o muito curta e algumas partes dela podem parecer d√∫bias ou misteriosas.  Vamos desdobrar esta hist√≥ria e examin√°-la com um olhar cr√≠tico.  Para fazer isso, suponha que tenhamos um conjunto de dados simples para o qual queremos criar um modelo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/0f/h2/4p/0fh24p1sl8wmgoov1ewqmqbv900.png"></div><a name="habracut"></a><br>  Em termos de significado, aqui estudamos o fen√¥meno do mundo real, e x e y denotam dados reais.  Nosso objetivo √© construir um modelo que nos permita prever y em fun√ß√£o de x.  Poder√≠amos tentar usar uma rede neural para criar esse modelo, mas sugiro algo mais simples: tentarei modelar y como um polin√¥mio em x.  Farei isso em vez de redes neurais, pois o uso de polin√¥mios torna a explica√ß√£o especialmente clara.  Assim que lidarmos com o caso do polin√¥mio, passaremos para a Assembl√©ia Nacional.  Existem dez pontos no gr√°fico acima, o que significa que podemos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">encontrar um polin√¥mio exclusivo de</a> 9¬™ ordem y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... + a <sub>9</sub> que se ajusta exatamente aos dados.  E aqui est√° o gr√°fico desse polin√¥mio. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o6/lj/6j/o6lj6j82dn6rly8xnv-fmzvvwn0.png"></div><br>  Acerto perfeito.  Mas podemos obter uma boa aproxima√ß√£o usando o modelo linear y = 2x <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-3/qk/sq/-3qksq4rmlcq54obwxcxy4_dvtm.png"></div><br>  Qual √© o melhor?  Qual √© mais prov√°vel de ser verdade?  Qual ser√° melhor generalizado para outros exemplos do mesmo fen√¥meno do mundo real? <br><br>  Perguntas dif√≠ceis.  E eles n√£o podem ser respondidos exatamente sem informa√ß√µes adicionais sobre o fen√¥meno subjacente do mundo real.  Contudo, vejamos duas possibilidades: (1) um modelo com um polin√¥mio de 9¬™ ordem realmente descreve o fen√¥meno do mundo real e, portanto, generaliza perfeitamente;  (2) o modelo correto √© y = 2x, mas temos ru√≠do adicional associado ao erro de medi√ß√£o; portanto, o modelo n√£o se encaixa perfeitamente. <br><br>  A priori, n√£o se pode dizer qual das duas possibilidades est√° correta (ou que n√£o existe uma terceira).  Logicamente, qualquer um deles pode se tornar verdadeiro.  E a diferen√ßa entre eles √© n√£o trivial.  Sim, com base nos dados dispon√≠veis, pode-se dizer que h√° apenas uma pequena diferen√ßa entre os modelos.  Mas suponha que desejamos prever o valor de y correspondente a algum valor grande de x, muito maior que qualquer um dos mostrados no gr√°fico.  Se tentarmos fazer isso, uma enorme diferen√ßa aparecer√° entre as previs√µes dos dois modelos, j√° que o termo x <sup>9</sup> domina no polin√¥mio de 9¬™ ordem, e o modelo linear permanece linear. <br><br>  Um ponto de vista do que est√° acontecendo √© afirmar que uma explica√ß√£o mais simples deve ser usada na ci√™ncia, se poss√≠vel.  Quando encontramos um modelo simples que explica muitos pontos de refer√™ncia, queremos apenas gritar: "Eureka!"  Afinal, √© improv√°vel que uma explica√ß√£o simples apare√ßa puramente por acidente.  Suspeitamos que o modelo deva produzir alguma verdade associada ao fen√¥meno.  Nesse caso, o modelo y = 2x + ru√≠do parece muito mais simples que y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... Seria surpreendente se a simplicidade surgisse por acaso, portanto suspeitamos que y = 2x + ru√≠do expresse alguma verdade subjacente.  Desse ponto de vista, o modelo de 9¬™ ordem simplesmente estuda o efeito do ru√≠do local.  Embora o modelo de 9¬™ ordem funcione perfeitamente para esses pontos de refer√™ncia espec√≠ficos, ele n√£o pode generalizar para outros pontos, como resultado do qual o modelo linear com ru√≠do ter√° melhores recursos preditivos. <br><br>  Vamos ver o que esse ponto de vista significa para redes neurais.  Suponha que, em nossa rede, haja principalmente pesos baixos, como √© geralmente o caso em redes regularizadas.  Devido ao seu pequeno peso, o comportamento da rede n√£o muda muito quando v√°rias entradas aleat√≥rias s√£o alteradas aqui e ali.  Como resultado, a rede regularizada √© dif√≠cil de aprender os efeitos do ru√≠do local presente nos dados.  Isso √© semelhante ao desejo de garantir que as evid√™ncias individuais n√£o afetem grandemente a sa√≠da da rede como um todo.  Em vez disso, a rede regularizada √© treinada para responder √†s evid√™ncias frequentemente encontradas nos dados de treinamento.  Por outro lado, uma rede com grandes pesos pode mudar seu comportamento bastante fortemente em resposta a pequenas altera√ß√µes nos dados de entrada.  Portanto, uma rede irregular pode usar grandes pesos para treinar um modelo complexo que cont√©m muitas informa√ß√µes de ru√≠do nos dados de treinamento.  Em resumo, as limita√ß√µes das redes regularizadas permitem que eles criem modelos relativamente simples com base em padr√µes frequentemente encontrados nos dados de treinamento e s√£o resistentes a desvios causados ‚Äã‚Äãpor ru√≠do nos dados de treinamento.  H√° uma esperan√ßa de que isso fa√ßa com que nossas redes estudem o fen√¥meno em si e generalizem melhor o conhecimento adquirido. <br><br>  Com tudo isso dito, a id√©ia de dar prefer√™ncia a explica√ß√µes mais simples deve deix√°-lo nervoso.  √Äs vezes, as pessoas chamam essa id√©ia de "navalha de Occam" e a aplicam com zelo, como se tivesse o status de um princ√≠pio cient√≠fico geral.  Mas isso, √© claro, n√£o √© um princ√≠pio cient√≠fico geral.  N√£o existe uma raz√£o l√≥gica priorit√°ria para preferir explica√ß√µes simples a explica√ß√µes complexas.  √Äs vezes, uma explica√ß√£o mais complicada est√° correta. <br><br>  Deixe-me descrever dois exemplos de como uma explica√ß√£o mais complexa se mostrou correta.  Na d√©cada de 1940, o f√≠sico Marcel Shane anunciou a descoberta de uma nova part√≠cula.  A empresa para a qual ele trabalhou, General Electric, ficou encantada e distribuiu amplamente a publica√ß√£o deste evento.  No entanto, o f√≠sico Hans Bethe era c√©tico.  Bethe visitou Shane e estudou as placas com tra√ßos da nova part√≠cula de Shane.  Shane mostrou placa Beta ap√≥s placa, mas Bete encontrou em cada um deles um problema que indicava a necessidade de recusar esses dados.  Finalmente, Shane mostrou a Beta um registro que parecia adequado.  Bethe disse que provavelmente era apenas um desvio estat√≠stico.  Shane: "Sim, mas as chances de isso ocorrer devido √†s estat√≠sticas, mesmo pela sua pr√≥pria f√≥rmula, s√£o de uma em cinco".  Bethe: "No entanto, eu j√° olhei para cinco registros."  Por fim, Shane disse: "Mas voc√™ explicou cada um dos meus registros, toda boa imagem com alguma outra teoria, e eu tenho uma hip√≥tese que explica todos os registros de uma s√≥ vez, e da√≠ resulta que estamos falando de uma nova part√≠cula".  Bethe respondeu: ‚ÄúA √∫nica diferen√ßa entre minhas explica√ß√µes e a sua √© que as suas est√£o erradas e as minhas est√£o corretas.  Sua √∫nica explica√ß√£o est√° incorreta e todas as minhas explica√ß√µes est√£o corretas.  Posteriormente, descobriu-se que a natureza concordava com Bethe, e a part√≠cula de Shane evaporou. <br><br>  No segundo exemplo, em 1859, o astr√¥nomo Urbain Jean Joseph Le Verrier descobriu que a forma da √≥rbita de Merc√∫rio n√£o corresponde √† teoria da gravita√ß√£o universal de Newton.  Houve um pequeno desvio dessa teoria e, em seguida, foram propostas v√°rias op√ß√µes para resolver o problema, que se resumiram ao fato de que a teoria de Newton como um todo est√° correta e requer apenas uma pequena altera√ß√£o.  E em 1916, Einstein mostrou que esse desvio pode ser bem explicado usando sua teoria geral da relatividade, radicalmente diferente da gravidade newtoniana e baseada em matem√°tica muito mais complexa.  Apesar dessa complexidade adicional, √© geralmente aceito hoje que a explica√ß√£o de Einstein est√° correta, e a gravidade newtoniana est√° incorreta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mesmo de forma modificada</a> .  Isso acontece, em particular, porque hoje sabemos que a teoria de Einstein explica muitos outros fen√¥menos com os quais a teoria de Newton teve dificuldades.  Al√©m disso, o mais surpreendente √© que a teoria de Einstein prediz com precis√£o v√°rios fen√¥menos que a gravidade newtoniana n√£o previu de maneira alguma.  No entanto, essas qualidades impressionantes n√£o eram √≥bvias no passado.  A julgar com base na mera simplicidade, ent√£o alguma forma modificada da teoria newtoniana pareceria mais atraente. <br><br>  Tr√™s moralidades podem ser extra√≠das dessas hist√≥rias.  Primeiro, √†s vezes √© bastante dif√≠cil decidir qual das duas explica√ß√µes ser√° "mais f√°cil".  Em segundo lugar, mesmo que tenhamos tomado essa decis√£o, a simplicidade deve ser guiada com muito cuidado!  Terceiro, o verdadeiro teste do modelo n√£o √© a simplicidade, mas qu√£o bem ele prediz novos fen√¥menos em novas condi√ß√µes de comportamento. <br><br>  Considerando tudo isso e tendo cuidado, aceitaremos um fato emp√≠rico - os SNs regularizados geralmente s√£o mais generalizados do que os irregulares.  Portanto, mais adiante neste livro, frequentemente usaremos a regulariza√ß√£o.  As hist√≥rias mencionadas s√£o necess√°rias apenas para explicar por que ningu√©m ainda desenvolveu uma explica√ß√£o te√≥rica completamente convincente sobre por que a regulariza√ß√£o ajuda as redes a generalizar.  Os pesquisadores continuam publicando trabalhos nos quais tentam abordagens diferentes para regulariza√ß√£o, comparando-os, analisando o que funciona melhor e tentando entender por que abordagens diferentes funcionam pior ou melhor.  Portanto, a regulariza√ß√£o pode ser tratada como uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nuvem</a> .  Quando isso ajuda com frequ√™ncia, n√£o temos uma compreens√£o sist√™mica completamente satisfat√≥ria do que est√° acontecendo - apenas regras heur√≠sticas e pr√°ticas incompletas. <br><br>  Aqui reside um conjunto mais profundo de problemas que chegam ao cora√ß√£o da ci√™ncia.  Este √© um problema de generaliza√ß√£o.  A regulariza√ß√£o pode nos dar uma varinha m√°gica computacional que ajuda nossas redes a generalizar melhor os dados, mas n√£o fornece um entendimento b√°sico de como a generaliza√ß√£o funciona e qual √© a melhor abordagem para ela. <br><br>  Esses problemas remontam ao <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">problema da indu√ß√£o</a> , cuja interpreta√ß√£o bem conhecida foi realizada pelo fil√≥sofo escoc√™s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">David Hume</a> no livro " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Um estudo sobre cogni√ß√£o humana</a> " (1748).  O problema da indu√ß√£o √© o tema do ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">teorema da aus√™ncia de refei√ß√µes gratuitas</a> ‚Äù de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">David Walpert e William Macredie</a> (1977). <br><br>  E isso √© especialmente irritante, porque na vida comum as pessoas s√£o fenomenalmente capazes de generalizar dados.  Mostre algumas imagens do elefante √† crian√ßa, e ele aprender√° rapidamente a reconhecer outros elefantes.  √â claro que ele √†s vezes pode cometer um erro, por exemplo, confundir um rinoceronte com um elefante, mas, em geral, esse processo funciona surpreendentemente com precis√£o.  Agora, temos um sistema - o c√©rebro humano - com uma enorme quantidade de par√¢metros livres.  E depois que ele recebe uma ou mais imagens de treinamento, o sistema aprende a generaliz√°-las para outras imagens.  Nosso c√©rebro, em certo sentido, √© incrivelmente bom em regularizar!  Mas como fazemos isso?  No momento, isso √© desconhecido para n√≥s.  Penso que, no futuro, desenvolveremos tecnologias de regulariza√ß√£o mais poderosas em redes neurais artificiais, t√©cnicas que finalmente permitir√£o √† Assembl√©ia Nacional generalizar dados com base em conjuntos de dados ainda menores. <br><br>  De fato, nossas redes j√° est√£o generalizando muito melhor do que se poderia esperar a priori.  Uma rede com 100 neur√¥nios ocultos possui quase 80.000 par√¢metros.  Temos apenas 50.000 imagens em dados de treinamento.  √â o mesmo que tentar esticar um polin√¥mio de 80.000 pedidos para mais de 50.000 pontos de refer√™ncia.  Por todas as indica√ß√µes, nossa rede deve treinar muito.  E, no entanto, como vimos, essa rede realmente generaliza bastante.  Por que isso est√° acontecendo?  Isto n√£o est√° totalmente claro.  Foi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">levantada</a> a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">hip√≥tese de</a> que "a din√¢mica da aprendizagem por descida gradiente em redes multicamadas est√° sujeita √† auto-regula√ß√£o".  √â uma fortuna extrema, mas tamb√©m um fato bastante perturbador, pois n√£o entendemos por que isso acontece.  Enquanto isso, adotaremos uma abordagem pragm√°tica e usaremos a regulariza√ß√£o sempre que poss√≠vel.  Isso ser√° ben√©fico para nossa Assembl√©ia Nacional. <br><br>  Deixe-me terminar esta se√ß√£o, voltando ao que n√£o expliquei antes: que a regulariza√ß√£o de L2 n√£o limita os deslocamentos.  Naturalmente, seria f√°cil alterar o procedimento de regulariza√ß√£o para regularizar os deslocamentos.  Mas, empiricamente, isso geralmente n√£o altera os resultados de maneira percept√≠vel; portanto, at√© certo ponto, lidar com a regulariza√ß√£o de vieses, ou n√£o, √© uma quest√£o de concord√¢ncia.  No entanto, vale a pena notar que um grande deslocamento n√£o torna um neur√¥nio sens√≠vel a entradas como pesos grandes.  Portanto, n√£o precisamos nos preocupar com grandes compensa√ß√µes que permitam que nossas redes aprendam o ru√≠do nos dados de treinamento.  Ao mesmo tempo, ao permitir grandes deslocamentos, tornamos nossas redes mais flex√≠veis em seu comportamento - em particular, grandes deslocamentos facilitam a satura√ß√£o dos neur√¥nios, da qual gostar√≠amos.  Por esse motivo, geralmente n√£o inclu√≠mos compensa√ß√µes na regulariza√ß√£o. <br><br><h2>  Outras t√©cnicas de regulariza√ß√£o </h2><br>  Existem muitas t√©cnicas de regulariza√ß√£o al√©m de L2.  De fato, tantas t√©cnicas j√° foram desenvolvidas que, com todo o desejo, eu n√£o conseguia descrever brevemente todas elas.  Nesta se√ß√£o, descreverei brevemente tr√™s outras abordagens para reduzir a reciclagem: regularizar L1, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">desistir</a> e aumentar artificialmente o conjunto de treinamento.  N√£o os estudaremos t√£o profundamente quanto os t√≥picos anteriores.  Em vez disso, apenas os conhecemos e, ao mesmo tempo, apreciamos a variedade de t√©cnicas de regulariza√ß√£o existentes. <br><br><h3>  Regulariza√ß√£o L1 </h3><br>  Nesta abordagem, modificamos a fun√ß√£o de custo irregular adicionando a soma dos valores absolutos dos pesos: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>95</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="40.447ex" height="2.66ex" viewBox="0 -832 17414.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-43" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-3D" x="1038" y="0"></use><g transform="translate(2094,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-30" x="1011" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-2B" x="3486" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-66" x="4736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="5287" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="5738" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="6268" y="0"></use><g transform="translate(6701,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6E" x="10140" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-73" x="10991" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-75" x="11460" y="0"></use><g transform="translate(12033,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="1242" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-7C" x="13518" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="13797" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-7C" x="14513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="15042" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="15403" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="15933" y="0"></use><g transform="translate(16413,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-35" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>95</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> C = C_0 + \ frac {\ lambda} {n} \ sum_w | w | \ tag {95} </script></p><br><br>  Intuitivamente, isso √© semelhante √† regulariza√ß√£o de L2, que multas por grandes pesos e faz a rede preferir pesos baixos.  Obviamente, o termo de regulariza√ß√£o L1 n√£o √© como o termo de regulariza√ß√£o L2, portanto, voc√™ n√£o deve esperar exatamente o mesmo comportamento.  Vamos tentar entender como o comportamento de uma rede treinada com regulariza√ß√£o L1 difere de uma rede treinada com regulariza√ß√£o L2. <br><br>  Para fazer isso, observe as derivadas parciais da fun√ß√£o de custo.  Diferenciando (95), obtemos: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>C</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mspace width=&quot;thinmathspace&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>96</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="86.831ex" height="2.66ex" viewBox="0 -832 37385.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="1781" y="0"></use><g transform="translate(2215,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="2168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="3043" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-43" x="3341" y="0"></use></g><g transform="translate(6317,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="2168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="3043" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="3341" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-3D" x="10652" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-66" x="11959" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="12509" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="12961" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="13490" y="0"></use><g transform="translate(13924,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="2168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="3043" y="0"></use><g transform="translate(3341,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(18434,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="2168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="3043" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="3341" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-2B" x="22715" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-66" x="23965" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="24516" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="24967" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="25497" y="0"></use><g transform="translate(25930,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6E" x="29369" y="0"></use><g transform="translate(30137,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6D" x="701" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-73" x="1580" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="2049" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6E" x="2530" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-28" x="33267" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="33657" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-29" x="34373" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="35013" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="35374" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="35904" y="0"></use><g transform="translate(36384,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-36" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>C</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mspace width="thinmathspace"></mspace><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>96</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> \ frac {\ parcial C} {\ parcial w} = \ frac {\ parcial C_0} {\ parcial w} + \ frac {\ lambda} {n} \, {\ rm sgn} (w) \ tag {96 } </script></p><br><br>  onde sgn (w) √© o sinal de w, ou seja, +1 se w for positivo e -1 se w for negativo.  Usando esta express√£o, modificamos levemente a propaga√ß√£o reversa para que ela execute a descida do gradiente estoc√°stico usando a regulariza√ß√£o L1.  A regra de atualiza√ß√£o final para a rede regularizada L1: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><msub><mi>C</mi><mn>0</mn></msub><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>97</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="92.583ex" height="2.78ex" viewBox="0 -883.9 39862.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-2212" x="9100" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-66" x="10351" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="10901" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="11353" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="11882" y="0"></use><g transform="translate(12316,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6E" x="17362" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6D" x="18213" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-62" x="19091" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6F" x="19521" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-78" x="20006" y="0"></use><g transform="translate(20579,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-73" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="469" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6E" x="950" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-28" x="22129" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="22519" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-29" x="23235" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-2212" x="23847" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-65" x="25098" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="25564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="25926" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-66" x="26705" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="27256" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="27707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="28237" y="0"></use><g transform="translate(28670,0)"><g transform="translate(250,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-30" x="1011" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-70" x="1419" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="1922" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="2452" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="2903" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="3337" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="3682" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="4212" y="0"></use></g><g transform="translate(33181,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="2168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="3043" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="3341" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="37489" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="37851" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="38380" y="0"></use><g transform="translate(38861,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-37" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>w</mi><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><msub><mi>C</mi><mn>0</mn></msub><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>97</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> w \ rightarrow w '= w- \ frac {\ eta \ lambda} {n} \ mbox {sgn} (w) - \ eta \ frac {\ C_0 parcial} {\ parcial w} \ tag {97} </script></p><br><br>  onde, como sempre, ‚àÇC / ‚àÇw pode opcionalmente ser estimado usando o valor m√©dio do minipacote.  Compare isso com a regra de atualiza√ß√£o de regulariza√ß√£o L2 (93): <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><msub><mi>C</mi><mn>0</mn></msub><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>98</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="92.607ex" height="2.78ex" viewBox="0 -883.9 39872.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="9128" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-65" x="9426" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-66" x="9893" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="10443" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-28" x="10805" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-31" x="11194" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-2212" x="11917" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-66" x="13168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="13718" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="14170" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="14699" y="0"></use><g transform="translate(15133,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6E" x="20179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="21030" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="21481" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="21827" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-68" x="22307" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="22884" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-29" x="23245" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-2212" x="23857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-65" x="25108" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="25574" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="25936" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-66" x="26715" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="27266" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="27717" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="28247" y="0"></use><g transform="translate(28680,0)"><g transform="translate(250,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-30" x="1011" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-70" x="1419" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="1922" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="2452" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="2903" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="3337" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="3682" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="4212" y="0"></use></g><g transform="translate(33191,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-63" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-69" x="2168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="2513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="3043" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-77" x="3341" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="37499" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="37861" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="38390" y="0"></use><g transform="translate(38871,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-38" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>w</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy="false">(</mo><mn>1</mn><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><msub><mi>C</mi><mn>0</mn></msub><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>c</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>98</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> w \ rightarrow w '= w \ left (1 - \ frac {\ eta \ lambda} {n} \ right) - \ eta \ frac {\ C_0 parcial} {\ parcial w} \ tag {98} </script></p><br><br>  Nas duas express√µes, o efeito da regulariza√ß√£o √© reduzir pesos.  Isso coincide com a no√ß√£o intuitiva de que ambos os tipos de regulariza√ß√£o penalizam grandes pesos.  No entanto, os pesos s√£o reduzidos de maneiras diferentes.  Na regulariza√ß√£o de L1, os pesos diminuem em um valor constante, tendendo a 0. Na regulariza√ß√£o de L2, os pesos diminuem em um valor proporcional a w.  Portanto, quando algum peso tem um grande valor | w |, a regulariza√ß√£o de L1 reduz o peso n√£o tanto quanto L2.  E vice-versa, quando | w |  pequena, a regulariza√ß√£o de L1 reduz o peso muito mais do que a regulariza√ß√£o de L2.  Como resultado, a regulariza√ß√£o de L1 tende a concentrar os pesos da rede em um n√∫mero relativamente pequeno de v√≠nculos de alta import√¢ncia, enquanto outros pesos tendem a zero. <br><br>  Eu suavizei levemente um problema na discuss√£o anterior - a derivada parcial ‚àÇC / ‚àÇw n√£o √© definida quando w = 0.  Isso ocorre porque a fun√ß√£o | w |  existe uma ‚Äútor√ß√£o‚Äù aguda no ponto w = 0, portanto, n√£o pode ser diferenciada l√°.  Mas isso n√£o √© assustador.  Apenas aplicamos a regra irregular usual para a descida do gradiente estoc√°stico quando w = 0.  Intuitivamente, n√£o h√° nada de errado nisso - a regulariza√ß√£o deve reduzir pesos e, obviamente, n√£o pode reduzir pesos j√° iguais a 0. Mais precisamente, usaremos as equa√ß√µes (96) e (97) com a condi√ß√£o de que sgn (0) = 0  Isso nos dar√° uma regra conveniente e compacta para a descida do gradiente estoc√°stico com regulariza√ß√£o L1. <br><br><h3>  Exce√ß√£o [desist√™ncia] </h3><br>  Uma exce√ß√£o √© uma t√©cnica de regulariza√ß√£o completamente diferente.  Diferentemente da regulariza√ß√£o de L1 e L2, a exce√ß√£o n√£o trata de uma altera√ß√£o na fun√ß√£o de custo.  Em vez disso, estamos mudando a pr√≥pria rede.  Deixe-me explicar a mec√¢nica b√°sica da opera√ß√£o de uma exce√ß√£o antes de aprofundar o t√≥pico sobre por que ela funciona e com quais resultados. <br><br>  Suponha que estamos tentando treinar uma rede: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae5/671/838/ae5671838ebc48f82eb01c1a839b60a7.png"></div><br>  Em particular, digamos que tenhamos a entrada de treinamento xe a sa√≠da desejada correspondente y.  Normalmente, trein√°vamos isso distribuindo x diretamente pela rede e depois propagando novamente para determinar a contribui√ß√£o do gradiente.  Uma exce√ß√£o modifica esse processo.  Come√ßamos removendo aleatoriamente e temporariamente metade dos neur√¥nios ocultos na rede, deixando os neur√¥nios de entrada e sa√≠da inalterados.  Depois disso, teremos aproximadamente essa rede.  Observe que os neur√¥nios exclu√≠dos, aqueles que s√£o removidos temporariamente, ainda est√£o marcados no diagrama: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ec3/51c/b6a/ec351cb6a877c8f0688718e0d5980088.png"></div><br>  Passamos x pela distribui√ß√£o direta pela rede alterada e depois distribu√≠mos o resultado, tamb√©m pela rede alterada.  Depois de fazer isso com um mini-pacote de exemplos, atualizamos os pesos e compensa√ß√µes correspondentes.  Em seguida, repetimos esse processo, restaurando primeiro os neur√¥nios exclu√≠dos e, em seguida, escolhendo um novo subconjunto aleat√≥rio de neur√¥nios ocultos para remover, avaliar o gradiente para outro minipacote e atualizar os pesos e compensa√ß√µes da rede. <br><br>  Repetindo esse processo repetidamente, obtemos uma rede que aprendeu alguns pesos e deslocamentos.  Naturalmente, esses pesos e deslocamentos foram aprendidos sob condi√ß√µes nas quais metade dos neur√¥nios ocultos foram exclu√≠dos.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">E quando lan√ßarmos a rede por completo, teremos o dobro de neur√¥nios ocultos ativos. </font><font style="vertical-align: inherit;">Para compensar isso, reduzimos pela metade os pesos provenientes dos neur√¥nios ocultos.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O procedimento de exclus√£o pode parecer estranho e arbitr√°rio. Por que ela deveria ajudar na regulariza√ß√£o? Para explicar o que est√° acontecendo, quero que voc√™ esque√ßa a exce√ß√£o por um tempo e apresente o treinamento da Assembl√©ia Nacional de maneira padr√£o. Em particular, imagine que treinamos v√°rios NS diferentes usando os mesmos dados de treinamento. Obviamente, as redes podem variar no in√≠cio e, √†s vezes, o treinamento pode produzir resultados diferentes. Nesses casos, poder√≠amos aplicar algum tipo de esquema de m√©dia ou vota√ß√£o para decidir qual dos produtos aceitar. Por exemplo, se treinamos cinco redes, e tr√™s delas classificam o n√∫mero como "3", ent√£o provavelmente s√£o as tr√™s verdadeiras. E as outras duas redes provavelmente est√£o erradas. Esse esquema de m√©dia √© geralmente uma maneira √∫til (embora cara) de reduzir a reciclagem. A raz√£o √©que redes diferentes podem ser treinadas de maneiras diferentes, e a m√©dia pode ajudar a eliminar essa reciclagem.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como tudo isso se relaciona com a exce√ß√£o? Heuristicamente, quando exclu√≠mos diferentes conjuntos de n√™utrons, √© como se estiv√©ssemos treinando diferentes SNs. Portanto, o procedimento de exclus√£o √© semelhante √† m√©dia dos efeitos em um n√∫mero muito grande de redes diferentes. Redes diferentes s√£o treinadas de maneiras diferentes, portanto, espera-se que o efeito m√©dio da exclus√£o reduza a reciclagem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uma explica√ß√£o heur√≠stica relacionada dos benef√≠cios da exclus√£o √© apresentada em </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">um dos primeiros trabalhos</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">usando esta t√©cnica: ‚ÄúEssa t√©cnica reduz a complexa adapta√ß√£o articular dos neur√¥nios, porque o neur√¥nio n√£o pode contar com a presen√ßa de certos vizinhos. No final, ele precisa aprender caracter√≠sticas mais confi√°veis ‚Äã‚Äãque possam ser √∫teis no trabalho em conjunto com muitos subconjuntos aleat√≥rios diferentes de neur√¥nios. ‚Äù Em outras palavras, se imaginarmos nossa Assembl√©ia Nacional como um modelo que faz previs√µes, uma exce√ß√£o ser√° uma maneira de garantir a estabilidade do modelo √† perda de partes individuais da evid√™ncia. Nesse sentido, a t√©cnica se assemelha √†s regulariza√ß√µes de L1 e L2, que buscam reduzir pesos e, dessa forma, tornam a rede mais resistente √† perda de qualquer conex√£o individual na rede. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naturalmente, a verdadeira medida da utilidade da exclus√£o √© seu tremendo sucesso em melhorar a efici√™ncia das redes neurais. No </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trabalho original</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">onde esse m√©todo foi introduzido, foi aplicado a muitas tarefas diferentes. </font><font style="vertical-align: inherit;">Estamos particularmente interessados ‚Äã‚Äãno fato de os autores aplicarem a exce√ß√£o √† classifica√ß√£o dos n√∫meros do MNIST, usando uma rede de distribui√ß√£o direta simples semelhante √† que examinamos. </font><font style="vertical-align: inherit;">O artigo observa que, at√© ent√£o, o melhor resultado para essa arquitetura era 98,4% de precis√£o. </font><font style="vertical-align: inherit;">Eles melhoraram para 98,7% usando uma combina√ß√£o de exclus√£o e uma forma modificada de regulariza√ß√£o L2. </font><font style="vertical-align: inherit;">Resultados igualmente impressionantes foram obtidos para muitas outras tarefas, incluindo reconhecimento de padr√µes e fala e processamento de linguagem natural. </font><font style="vertical-align: inherit;">A exce√ß√£o foi especialmente √∫til no treinamento de grandes redes profundas, onde o problema de reciclagem geralmente surge.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Conjunto de dados de treinamento em expans√£o artificial </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vimos anteriormente que nossa precis√£o na classifica√ß√£o MNIST caiu para 80%, quando usamos apenas 1.000 imagens de treinamento. E n√£o √© de admirar - com menos dados, nossa rede encontrar√° menos op√ß√µes para escrever n√∫meros por pessoas. Vamos tentar treinar nossa rede de 30 neur√¥nios ocultos, usando diferentes volumes do conjunto de treinamento para observar a mudan√ßa na efici√™ncia. Treinamos usando o tamanho de minipacote 10, a velocidade de aprendizado Œ∑ = 0,5, o par√¢metro de regulariza√ß√£o Œª = 5,0 e a fun√ß√£o de custo com entropia cruzada. Treinaremos uma rede de 30 eras usando um conjunto completo de dados e aumentaremos o n√∫mero de eras proporcionalmente √† diminui√ß√£o no volume de dados de treinamento. Para garantir o mesmo fator de redu√ß√£o de peso para diferentes conjuntos de dados de treinamento, usaremos o par√¢metro de regulariza√ß√£o Œª = 5,0 com um conjunto de treinamento completo e reduza-o proporcionalmente com uma diminui√ß√£o nos volumes de dados.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d6/a9f/6db/7d6a9f6db97493f1d716450344dd877f.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pode-se observar que a precis√£o da classifica√ß√£o cresce significativamente com o aumento dos dados de treinamento. √â prov√°vel que esse crescimento continue com um aumento adicional nos volumes. Obviamente, a julgar pelo gr√°fico acima, estamos nos aproximando da satura√ß√£o. No entanto, suponha que refizemos este gr√°fico para uma depend√™ncia logar√≠tmica da quantidade de dados de treinamento:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">V√™-se que, no final, o gr√°fico ainda tende a subir. Isso sugere que, se coletarmos uma quantidade muito maior de dados - por exemplo, milh√µes ou mesmo bilh√µes de exemplos manuscritos, em vez de 50.000 -, provavelmente obteremos uma rede de trabalho muito melhor, mesmo de tamanho t√£o pequeno. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obter mais dados de treinamento √© uma √≥tima id√©ia. Infelizmente, isso pode ser caro, portanto, na pr√°tica, nem sempre √© poss√≠vel. No entanto, h√° outra id√©ia que pode funcionar quase t√£o bem - aumentar artificialmente o conjunto de dados. Por exemplo, suponha que tiremos uma imagem de cinco do MNIST e giremos um pouco, graus por 15:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bcf/555/69f/bcf55569f0ecfda9357d6c1ce1f3e9fb.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/408/bcc/32a/408bcc32a8b3b03094eb4f2b7fdea833.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esta √© claramente a mesma figura. Mas no n√≠vel de pixels, √© muito diferente das imagens dispon√≠veis no banco de dados MNIST. √â razo√°vel supor que a adi√ß√£o dessa imagem ao conjunto de dados de treinamento possa ajudar nossa rede a aprender mais sobre a classifica√ß√£o de imagens. Al√©m disso, obviamente n√£o estamos limitados √† capacidade de adicionar apenas uma imagem. Podemos expandir nossos dados de treinamento fazendo algumas pequenas reviravoltas em todas as imagens de treinamento do MNIST e, em seguida, usando o conjunto estendido de dados de treinamento para aumentar a efici√™ncia da rede. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Essa ideia √© muito poderosa e √© amplamente usada. Vejamos os resultados do </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trabalho cient√≠fico</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que aplicaram v√°rias varia√ß√µes dessa id√©ia ao MNIST. Uma das arquiteturas das redes consideradas por eles era semelhante √† que usamos - uma rede de distribui√ß√£o direta com 800 neur√¥nios ocultos, usando a fun√ß√£o de custo com entropia cruzada. Ao lan√ßar essa rede com o conjunto de treinamento padr√£o do MNIST, eles obtiveram uma precis√£o de classifica√ß√£o de 98,4%. Mas eles expandiram os dados de treinamento, usando n√£o apenas a rota√ß√£o que descrevi acima, mas tamb√©m a transfer√™ncia e distor√ß√£o de imagens. Depois de treinar a rede em dados avan√ßados, eles aumentaram sua precis√£o para 98,9%. Eles tamb√©m experimentaram o chamado "Distor√ß√£o el√°stica", um tipo especial de distor√ß√£o de imagem, projetado para eliminar as vibra√ß√µes aleat√≥rias dos m√∫sculos da m√£o. Usando distor√ß√µes el√°sticas para expandir os dados, eles alcan√ßaram precis√£o de 99,3%. Em ess√™ncia, eles expandiram sua experi√™ncia de rede,dando-lhe v√°rias varia√ß√µes manuscritas encontradas em caligrafia real.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Variantes dessa id√©ia podem ser usadas para melhorar o desempenho de muitas tarefas de aprendizado, n√£o apenas para reconhecimento de manuscrito. O princ√≠pio geral √© expandir os dados de treinamento aplicando opera√ß√µes a eles que refletem as varia√ß√µes encontradas na realidade. Tais varia√ß√µes s√£o f√°ceis de encontrar. Suponha que estamos criando o NS para reconhecimento de fala. As pessoas podem reconhecer a fala mesmo com distor√ß√µes, como ru√≠do de fundo. Portanto, voc√™ pode expandir os dados adicionando ru√≠do de fundo. Tamb√©m somos capazes de reconhecer a fala acelerada e lenta. Essa √© outra maneira de expandir os dados de treinamento. Essas t√©cnicas nem sempre s√£o usadas - por exemplo, em vez de expandir o conjunto de treinamento adicionando ru√≠do, pode ser mais eficiente limpar a entrada aplicando um filtro de ru√≠do a elas. E, no entanto, vale lembrar a ideia de expandir o conjunto de treinamento,e procure maneiras de us√°-lo.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exerc√≠cio </font></font></h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como discutimos acima, uma maneira de estender os dados de treinamento do MNIST √© usar pequenas rota√ß√µes das imagens de treinamento. </font><font style="vertical-align: inherit;">Que problema pode surgir se permitirmos a rota√ß√£o das imagens em qualquer √¢ngulo?</font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Digress√£o de big data e o significado da compara√ß√£o da precis√£o da classifica√ß√£o </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Vamos dar uma olhada novamente em como a precis√£o do nosso NS varia dependendo do tamanho do conjunto de treinamento: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suponha que, em vez de usar o NS, us√°ssemos outra tecnologia de aprendizado de m√°quina para classificar n√∫meros. Por exemplo, vamos tentar usar o m√©todo de m√°quina de vetores de suporte (SVM), que conhecemos brevemente no cap√≠tulo 1. Como ent√£o, n√£o se preocupe se voc√™ n√£o estiver familiarizado com o SVM, n√£o precisamos entender seus detalhes. Usaremos o SVM atrav√©s da biblioteca scikit-learn. Veja como a efic√°cia do SVM varia com o tamanho do conjunto de treinamento. Para compara√ß√£o, coloquei o cronograma e os resultados da Assembl√©ia Nacional.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a6/21b/a21/9a621ba21ebb087e64fc7c68d3238ef5.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Provavelmente a primeira coisa que chama a sua aten√ß√£o - o NS supera o SVM em qualquer tamanho do conjunto de treinamento. Isso √© bom, embora n√£o valha a pena tirar conclus√µes de longo alcance disso, pois usei as configura√ß√µes predefinidas de aprendizado de scikit e trabalh√°mos bastante seriamente em nosso NS. Um fato menos v√≠vido, mas mais interessante, que segue no gr√°fico, √© que, se treinarmos nosso SVM usando 50.000 imagens, ele funcionar√° melhor (precis√£o de 94,48%) do que nosso NS treinado com 5000 imagens ( 93,24%). Em outras palavras, um aumento no volume de dados de treinamento √†s vezes compensa a diferen√ßa nos algoritmos MO.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Algo mais interessante pode acontecer. Suponha que estamos tentando resolver um problema usando dois algoritmos MO, A e B. √Äs vezes acontece que o algoritmo A est√° √† frente do algoritmo B em um conjunto de dados de treinamento e o algoritmo B est√° √† frente do algoritmo A em outro conjunto de dados de treinamento. N√£o vimos isso acima - os gr√°ficos se cruzariam - mas </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">isso acontece</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . A resposta correta para a pergunta: "O algoritmo A √© superior ao algoritmo B?" de fato, este: "Que conjunto de dados de treinamento voc√™ est√° usando?"</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tudo isso deve ser levado em considera√ß√£o, tanto no desenvolvimento quanto na leitura de artigos cient√≠ficos. Muitos trabalhos concentram-se em encontrar novos truques para extrair melhores resultados em conjuntos de dados de medi√ß√£o padr√£o. ‚ÄúNossa tecnologia de superalimentos nos proporcionou uma melhoria de X% no conjunto comparativo padr√£o Y‚Äù - o formul√°rio de inscri√ß√£o can√¥nico nesse estudo. √Äs vezes, essas declara√ß√µes s√£o realmente interessantes, mas vale a pena entender que elas s√£o aplic√°veis ‚Äã‚Äãapenas no contexto de um conjunto de treinamento espec√≠fico. Imagine uma hist√≥ria alternativa na qual as pessoas que inicialmente criaram um conjunto comparativo receberam uma bolsa de pesquisa maior. Eles poderiam usar dinheiro extra para coletar dados adicionais. √â poss√≠vel que a ‚Äúmelhoria‚Äù da tecnologia super-duper desapare√ßa em um conjunto de dados maior. Em outras palavras,a ess√™ncia da melhoria pode ser apenas um acidente. A partir disso, a seguinte moralidade deve ser levada ao campo da aplica√ß√£o pr√°tica: precisamos de algoritmos aprimorados e dados de treinamento aprimorados. N√£o h√° nada de errado em procurar algoritmos aprimorados, mas certifique-se de n√£o se concentrar nisso, ignorando a maneira mais f√°cil de vencer aumentando o volume ou a qualidade dos dados de treinamento.</font></font><br><br><h3>  Desafio </h3><br><ul><li>  .              ?                 .        ‚Äì  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">   </a> ,  ,   ,      .   ,             .        -   ?   ,      . </li></ul><br><h3>  Sum√°rio </h3><br>  Conclu√≠mos nossa imers√£o em reciclagem e regulariza√ß√£o.  Obviamente, retornaremos a esses problemas.  Como mencionei v√°rias vezes, a reciclagem √© um grande problema no campo do NS, especialmente quando os computadores se tornam mais poderosos e podemos treinar redes maiores.  Como resultado, h√° uma necessidade urgente de desenvolver t√©cnicas eficazes de regulariza√ß√£o para reduzir a reciclagem, portanto essa √°rea √© muito ativa atualmente. <br><br><h2>  Inicializa√ß√£o de peso </h2><br>  Quando criamos nosso NS, precisamos fazer uma escolha dos valores iniciais de pesos e compensa√ß√µes.  At√© agora, n√≥s os escolhemos de acordo com as diretrizes descritas brevemente no Cap√≠tulo 1. Deixe-me lembr√°-lo de que escolhemos pesos e compensa√ß√µes com base em uma distribui√ß√£o gaussiana independente com uma expectativa matem√°tica de 0 e um desvio padr√£o de 1. Essa abordagem funcionou bem, mas parece bastante arbitr√°ria, portanto vale a pena. revise-o e pense se √© poss√≠vel encontrar uma maneira melhor de atribuir os pesos e deslocamentos iniciais e, talvez, ajudar nossos NSs a aprender mais rapidamente. <br><br>  Acontece que o processo de inicializa√ß√£o pode ser seriamente aprimorado em compara√ß√£o com a distribui√ß√£o gaussiana normalizada.  Para entender isso, digamos que trabalhamos com uma rede com um grande n√∫mero de neur√¥nios de entrada, digamos, de 1000. E digamos que usamos a distribui√ß√£o Gaussiana normalizada para inicializar pesos conectados √† primeira camada oculta.  At√© agora, focarei apenas nas escalas que conectam os neur√¥nios de entrada ao primeiro neur√¥nio na camada oculta e ignorarei o restante da rede: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ac3/c9d/62f/ac3c9d62f301e85234b7cd4bbcbd0e0d.png"></div><br>  Por simplicidade, vamos imaginar que estamos tentando treinar a rede com a entrada x, na qual metade dos neur√¥nios de entrada est√° ativada, ou seja, eles t√™m um valor de 1 e a metade est√° desativada, ou seja, eles t√™m um valor de 0. O pr√≥ximo argumento funciona em um caso mais geral, mas √© mais f√°cil para voc√™ vai entend√™-lo neste exemplo em particular.  Considere a soma ponderada z = w <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b de entradas para um neur√¥nio oculto.  500 membros da soma desaparecem porque o x <sub>j</sub> correspondente √© 0. Portanto, z √© a soma de 501 vari√°veis ‚Äã‚Äãaleat√≥rias Gaussianas normalizadas, 500 pesos e 1 deslocamento adicional.  Portanto, o pr√≥prio valor z tem uma distribui√ß√£o gaussiana com uma expectativa matem√°tica de 0 e um desvio padr√£o de ‚àö501 ‚âà 22,4.  Ou seja, z tem uma distribui√ß√£o gaussiana bastante ampla, sem picos acentuados: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/an/fj/_3/anfj_3qej8-fcxrnig7beewu3qm.png"></div><br>  Em particular, este gr√°fico mostra que | z | √© provavelmente muito grande, ou seja, z ‚â´ 1 ou z ‚â´ -1.  Nesse caso, a sa√≠da dos neur√¥nios ocultos œÉ (z) ser√° muito pr√≥xima de 1 ou 0. Isso significa que nosso neur√¥nio oculto ficar√° saturado.  E quando isso acontece, como j√° sabemos, pequenas altera√ß√µes nos pesos produzir√£o pequenas altera√ß√µes na ativa√ß√£o de um neur√¥nio oculto.  Essas pequenas mudan√ßas, por sua vez, praticamente n√£o afetar√£o os n√™utrons restantes na rede, e veremos as pequenas mudan√ßas correspondentes na fun√ß√£o de custo.  Como resultado, esses pesos ser√£o treinados muito lentamente quando usarmos o algoritmo de descida de gradiente.  Isso √© semelhante √† tarefa que j√° discutimos neste cap√≠tulo, na qual os neur√¥nios de sa√≠da saturados com valores incorretos fazem com que o aprendizado desacelere.  Costum√°vamos resolver esse problema escolhendo inteligentemente uma fun√ß√£o de custo.  Infelizmente, embora isso tenha ajudado com neur√¥nios de sa√≠da saturados, n√£o ajuda em nada com a satura√ß√£o de neur√¥nios ocultos. <br><br>  Agora eu falei sobre as escalas de entrada da primeira camada oculta.  Naturalmente, os mesmos argumentos se aplicam √†s seguintes camadas ocultas: se os pesos nas camadas ocultas posteriores forem inicializados usando distribui√ß√µes Gaussianas normalizadas, sua ativa√ß√£o ser√° frequentemente pr√≥xima de 0 ou 1 e o treinamento ser√° muito lento. <br><br>  Existe uma maneira de escolher as melhores op√ß√µes de inicializa√ß√£o para pesos e compensa√ß√µes, para n√£o obtermos essa satura√ß√£o e evitar atrasos na aprendizagem?  Suponha que temos um neur√¥nio com o n√∫mero de pesos recebidos n <sub>pol</sub> .  Ent√£o, precisamos inicializar esses pesos com distribui√ß√µes gaussianas aleat√≥rias com uma expectativa matem√°tica de 0 e um desvio padr√£o de 1 / ‚àön <sub>in</sub> .  Ou seja, comprimimos os gaussianos e reduzimos a probabilidade de satura√ß√£o do neur√¥nio.  Em seguida, escolheremos uma distribui√ß√£o gaussiana para deslocamentos com uma expectativa matem√°tica de 0 e um desvio padr√£o de 1, por raz√µes √†s quais voltarei um pouco mais tarde.  Tendo feito essa escolha, descobrimos novamente que z = w <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b ser√° uma vari√°vel aleat√≥ria com uma distribui√ß√£o gaussiana com uma expectativa matem√°tica de 0, mas com um pico muito mais pronunciado do que antes.  Suponha, como antes, que 500 entradas s√£o 0 e 500 s√£o 1. Ent√£o √© f√°cil mostrar (veja o exerc√≠cio abaixo) que z tem uma distribui√ß√£o gaussiana com uma expectativa matem√°tica de 0 e um desvio padr√£o de ‚àö (3/2) = 1,22 ... Este gr√°fico tem um pico muito mais n√≠tido, tanto que, mesmo na imagem abaixo, a situa√ß√£o √© um pouco subestimada, porque tive que alterar a escala do eixo vertical em compara√ß√£o com o gr√°fico anterior: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/ee/xk/5eeexk-kyxhmi3pdoslo9w_al44.png"></div><br>  Esse neur√¥nio ser√° saturado com uma probabilidade muito menor e, consequentemente, ser√° menos prov√°vel que ocorra uma desacelera√ß√£o no aprendizado. <br><br><h3>  Exerc√≠cio </h3><br><ul><li>  Confirme que o desvio padr√£o de z = w <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b do par√°grafo anterior √© ‚àö (3/2).  Considera√ß√µes a favor disso: a varia√ß√£o da soma das vari√°veis ‚Äã‚Äãaleat√≥rias independentes √© igual √† soma das varia√ß√µes das vari√°veis ‚Äã‚Äãaleat√≥rias individuais;  a varia√ß√£o √© igual ao quadrado do desvio padr√£o. </li></ul><br>  Mencionei acima que continuaremos inicializando os deslocamentos, como antes, com base em uma distribui√ß√£o gaussiana independente com uma expectativa matem√°tica de 0 e um desvio padr√£o de 1. E isso √© normal, porque n√£o aumenta muito a probabilidade de satura√ß√£o de nossos neur√¥nios.  Na verdade, a inicializa√ß√£o de compensa√ß√µes n√£o importa muito se conseguirmos evitar o problema de satura√ß√£o.  Alguns at√© tentam inicializar todas as compensa√ß√µes para zero e confiam no fato de que a descida do gradiente pode aprender as compensa√ß√µes apropriadas.  Mas como a probabilidade de isso afetar algo √© pequena, continuaremos usando o mesmo procedimento de inicializa√ß√£o de antes. <br><br>  Vamos comparar os resultados das abordagens antiga e nova para inicializar pesos usando a tarefa de classificar n√∫meros do MNIST.  Como antes, usaremos 30 neur√¥nios ocultos, um mini-pacote de tamanho 10, um par√¢metro de regulariza√ß√£o &amp; lambda = 5.0 e uma fun√ß√£o de custo com entropia cruzada.  Reduziremos gradualmente a velocidade de aprendizado de Œ∑ = 0,5 para 0,1, pois dessa forma os resultados ser√£o um pouco melhor vis√≠veis nos gr√°ficos.  Voc√™ pode aprender usando o m√©todo de inicializa√ß√£o de peso antigo: <br><br><pre><code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.large_weight_initializer() &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Voc√™ tamb√©m pode aprender usando a nova abordagem para inicializar pesos.  Isso √© ainda mais simples, porque, por padr√£o, a rede2 inicializa os pesos usando uma nova abordagem.  Isso significa que podemos omitir a chamada net.large_weight_initializer () anteriormente: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Tra√ßamos (usando o programa weight_initialization.py): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/59d/640/4ab/59d6404ab3c5e01946106b26743ae130.png"></div><br>  Nos dois casos, √© obtida uma precis√£o de classifica√ß√£o de 96%.  A precis√£o resultante √© quase a mesma nos dois casos.  Mas a nova t√©cnica de inicializa√ß√£o chega a esse ponto muito, muito mais rapidamente.  No final da √∫ltima era do treinamento, a abordagem antiga para inicializar pesos atinge uma precis√£o de 87%, e a nova abordagem j√° se aproxima de 93%.  Aparentemente, uma nova abordagem para inicializar pesos come√ßa de uma posi√ß√£o muito melhor, para obter bons resultados muito mais rapidamente.  O mesmo fen√¥meno √© observado se construirmos os resultados para uma rede com 100 neur√¥nios: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ccd/72d/8ef/ccd72d8effddc7815bd526dcb1434acd.png"></div><br>  Nesse caso, duas curvas n√£o ocorrem.  No entanto, minhas experi√™ncias dizem que, se voc√™ adicionar um pouco mais de √©pocas, a precis√£o come√ßar√° a quase coincidir.  Portanto, com base nessas experi√™ncias, podemos dizer que melhorar a inicializa√ß√£o dos pesos acelera apenas o treinamento, mas n√£o altera a efici√™ncia geral da rede.  No entanto, no cap√≠tulo 4, veremos exemplos de NSs nos quais a efici√™ncia a longo prazo √© significativamente aprimorada como resultado da inicializa√ß√£o dos pesos atrav√©s de 1 / ‚àön <sub>in</sub> .  Portanto, melhora n√£o apenas a velocidade da aprendizagem, mas √†s vezes a efic√°cia resultante. <br><br>  A abordagem para inicializar pesos por meio de 1 / ‚àön ajuda a melhorar o treinamento de redes neurais.  Outras t√©cnicas para inicializar pesos foram propostas, muitas das quais baseadas nesta id√©ia b√°sica.  N√£o os considerarei aqui, pois 1 / ‚àön funciona bem para nossos prop√≥sitos.  Se voc√™ estiver interessado, recomendo ler a discuss√£o nas p√°ginas 14 e 15 em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo de 2012 de</a> Yoshua Benggio. <br><br><h3>  Desafio </h3><br><ul><li>  A combina√ß√£o de regulariza√ß√£o e um m√©todo aprimorado de inicializa√ß√£o de peso.  √Äs vezes, a regulariza√ß√£o de L2 nos fornece resultados semelhantes a um novo m√©todo de inicializa√ß√£o de pesos.  Digamos que usamos a abordagem antiga para inicializar pesos.  Descreva um argumento heur√≠stico que prove que: (1) se Œª n√£o for muito pequeno, nas primeiras √©pocas do treinamento, o enfraquecimento dos pesos dominar√° quase completamente;  (2) se Œ∑Œª ‚â™ n, os pesos enfraquecer√£o e <sup>-Œ∑Œª / m</sup> vezes na √©poca;  (3) se Œª n√£o for muito grande, o enfraquecimento dos pesos diminuir√° quando os pesos diminuirem para cerca de 1 / ‚àön, onde n √© o n√∫mero total de pesos na rede.  Prove que essas condi√ß√µes s√£o atendidas nos exemplos para os quais os gr√°ficos s√£o constru√≠dos nesta se√ß√£o. </li></ul><br><br><h2>  Retornando ao reconhecimento de manuscrito: c√≥digo </h2><br>  Vamos implementar as id√©ias descritas neste cap√≠tulo.  Vamos desenvolver um novo programa, network2.py, uma vers√£o aprimorada do programa network.py que criamos no cap√≠tulo 1. Se voc√™ n√£o v√™ o c√≥digo h√° muito tempo, talvez seja necess√°rio analis√°-lo rapidamente.  Essas s√£o apenas 74 linhas de c√≥digo e √© f√°cil de entender. <br><br>  Assim como em network.py, a estrela de network2.py √© a classe Network, que usamos para representar nossos NSs.  Inicializamos a inst√¢ncia da classe com uma lista de tamanhos das camadas de rede correspondentes e, com a escolha da fun√ß√£o de custo, por padr√£o, ser√° entropia cruzada: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, sizes, cost=CrossEntropyCost)</span></span></span><span class="hljs-function">:</span></span> self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost</code> </pre> <br>  As primeiras duas linhas do m√©todo __init__ s√£o iguais a network.py e s√£o entendidas por elas mesmas.  As pr√≥ximas duas linhas s√£o novas e precisamos entender em detalhes o que est√£o fazendo. <br><br>  Vamos come√ßar com o m√©todo default_weight_initializer.  Ele usa uma abordagem nova e aprimorada para inicializar pesos.  Como vimos, nessa abordagem, os pesos que entram no neur√¥nio s√£o inicializados com base em uma distribui√ß√£o gaussiana independente com uma expectativa matem√°tica de 0 e um desvio padr√£o de 1 dividido pela raiz quadrada do n√∫mero de links de entrada para o neur√¥nio.  Al√©m disso, esse m√©todo inicializar√° as compensa√ß√µes usando a distribui√ß√£o Gaussiana com uma m√©dia de 0 e um desvio padr√£o de 1. Aqui est√° o c√≥digo: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">default_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Para entender, √© necess√°rio lembrar que np √© uma biblioteca Numpy que lida com √°lgebra linear.  N√≥s o importamos no in√≠cio do programa.  Observe tamb√©m que n√£o inicializamos deslocamentos na primeira camada de neur√¥nios.  A primeira camada √© de entrada, portanto, as compensa√ß√µes n√£o s√£o usadas.  O mesmo foi network.py. <br><br>  Al√©m do m√©todo default_weight_initializer, criaremos um m√©todo large_weight_initializer.  Inicializa pesos e compensa√ß√µes usando a abordagem antiga do Cap√≠tulo 1, onde pesos e compensa√ß√µes s√£o inicializados com base em uma distribui√ß√£o Gaussiana independente com uma expectativa matem√°tica de 0 e um desvio padr√£o de 1. Esse c√≥digo, √© claro, n√£o √© muito diferente do default_weight_initializer: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">large_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Inclu√≠ esse m√©todo principalmente porque era mais conveniente comparar os resultados deste cap√≠tulo e do cap√≠tulo 1. N√£o consigo imaginar op√ß√µes reais nas quais eu recomendaria us√°-lo! <br><br>  A segunda novidade do m√©todo __init__ ser√° a inicializa√ß√£o do atributo cost.  Para entender como isso funciona, vejamos a classe que usamos para representar a fun√ß√£o de custo entre entropia (a diretiva @staticmethod diz ao int√©rprete que esse m√©todo √© independente do objeto, para que o par√¢metro self n√£o seja passado para os m√©todos fn e delta). <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CrossEntropyCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.sum(np.nan_to_num(-y*np.log(a)-(<span class="hljs-number"><span class="hljs-number">1</span></span>-y)*np.log(<span class="hljs-number"><span class="hljs-number">1</span></span>-a))) @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay)</code> </pre> <br>  Vamos descobrir.  A primeira coisa que pode ser vista aqui √© que, embora a entropia cruzada seja uma fun√ß√£o do ponto de vista matem√°tico, n√≥s a implementamos como uma classe python, n√£o como uma fun√ß√£o python.  Por que eu decidi fazer isso?  Em nossa rede, o valor desempenha dois pap√©is diferentes.  √ìbvio - √© uma medida de qu√£o bem a ativa√ß√£o da sa√≠da a corresponde √† sa√≠da desejada y.  Esta fun√ß√£o √© fornecida pelo m√©todo CrossEntropyCost.fn.  (A prop√≥sito, observe que chamar np.nan_to_num dentro de CrossEntropyCost.fn garante que o Numpy processe corretamente o logaritmo dos n√∫meros pr√≥ximos a zero).  No entanto, a fun√ß√£o de custo √© usada em nossa rede da segunda maneira.  Recordamos no Cap√≠tulo 2 que, ao iniciar o algoritmo de retropropaga√ß√£o, precisamos considerar o erro de sa√≠da da rede Œ¥ <sup>L.</sup>  A forma do erro de sa√≠da depende da fun√ß√£o de custo: diferentes fun√ß√µes de custo ter√£o diferentes formas de erro de sa√≠da.  Para entropia cruzada, o erro de sa√≠da, como segue na equa√ß√£o (66), ser√° igual a: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>&amp;#x2212;</mo><mi>y</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>99</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.761ex" height="2.901ex" viewBox="0 -987.6 9799.8 1249" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-65" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-6C" x="1240" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="1538" y="0"></use><g transform="translate(1900,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-3D" x="3289" y="0"></use><g transform="translate(4345,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-2212" x="5679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-79" x="6679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-74" x="7427" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-61" x="7788" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMATHI-67" x="8318" y="0"></use><g transform="translate(8798,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhiL2SdqZ0tOGfEbL9DvzMC2C1FRSg#MJMAIN-39" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>‚àí</mo><mi>y</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>99</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> \ delta ^ L = a ^ L-y \ tag {99} </script></p><br><br>  Portanto, defino um segundo m√©todo, CrossEntropyCost.delta, cujo objetivo √© explicar √† rede como calcular o erro de sa√≠da.  E ent√£o combinamos esses dois m√©todos em uma classe que cont√©m tudo o que nossa rede precisa saber sobre a fun√ß√£o de custo. <br><br>  Por um motivo semelhante, network2.py cont√©m uma classe que representa uma fun√ß√£o de custo quadr√°tico.  Incluindo isso para compara√ß√£o com os resultados do Cap√≠tulo 1, j√° que no futuro usaremos principalmente entropia cruzada.  O c√≥digo est√° abaixo.  O m√©todo QuadraticCost.fn √© um c√°lculo simples do custo quadr√°tico associado √† sa√≠da a e √† sa√≠da desejada y.  O valor retornado por QuadraticCost.delta √© baseado na express√£o (30) para o erro de sa√≠da do valor quadr√°tico, que derivamos no cap√≠tulo 2. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">QuadraticCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>*np.linalg.norm(ay)**<span class="hljs-number"><span class="hljs-number">2</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay) * sigmoid_prime(z)</code> </pre> <br>  Agora descobrimos as principais diferen√ßas entre network2.py e network2.py.  Tudo √© muito simples.  Existem outras pequenas mudan√ßas que descreverei abaixo, incluindo a implementa√ß√£o da regulariza√ß√£o de L2.  Antes disso, vejamos o c√≥digo completo network2.py.  N√£o √© necess√°rio estud√°-lo em detalhes, mas vale a pena entender a estrutura b√°sica, em particular, ler os coment√°rios para entender o que cada uma das partes do programa faz.  √â claro que n√£o pro√≠bo de me aprofundar nessa quest√£o o quanto voc√™ quiser!  Se voc√™ se perder, tente ler o texto ap√≥s o programa e retorne ao c√≥digo novamente.  Em geral, aqui est√°: <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">"""network2.py ~~~~~~~~~~~~~~   network.py,            .   ‚Äì      , ,   .     ,    .   ,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import json import random import sys #  import numpy as np ####   ,      class QuadraticCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. """ return 0.5*np.linalg.norm(ay)**2 @staticmethod def delta(z, a, y): """  delta   .""" return (ay) * sigmoid_prime(z) class CrossEntropyCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. np.nan_to_num    .  ,   ``a``  ``y``      1.0,   (1-y)*np.log(1-a)  nan. np.nan_to_num ,       (0.0). """ return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) @staticmethod def delta(z, a, y): """  delta   .  ``z``    ,          delta     . """ return (ay) ####   Network class Network(object): def __init__(self, sizes, cost=CrossEntropyCost): """  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].       ,   ``self.default_weight_initializer`` (.  ). """ self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost def default_weight_initializer(self): """            0    1,       ,       .          0    1.    ,         ,           . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def large_weight_initializer(self): """          0    1.          0    1.    ,         ,           .         1,    .       . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def feedforward(self, a): """  ,  ``a``  .""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0, evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False, monitor_training_accuracy=False): """     -    . ``training_data`` ‚Äì   ``(x, y)``,       .       ,    ``lmbda``.    ``evaluation_data``,     ,   .         ,     ,   .      :   ,    ,   ,              .  ,      30 ,        30 ,        .     ,   . """ if evaluation_data: n_data = len(evaluation_data) n = len(training_data) evaluation_cost, evaluation_accuracy = [], [] training_cost, training_accuracy = [], [] for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch( mini_batch, eta, lmbda, len(training_data)) print "Epoch %s training complete" % j if monitor_training_cost: cost = self.total_cost(training_data, lmbda) training_cost.append(cost) print "Cost on training data: {}".format(cost) if monitor_training_accuracy: accuracy = self.accuracy(training_data, convert=True) training_accuracy.append(accuracy) print "Accuracy on training data: {} / {}".format( accuracy, n) if monitor_evaluation_cost: cost = self.total_cost(evaluation_data, lmbda, convert=True) evaluation_cost.append(cost) print "Cost on evaluation data: {}".format(cost) if monitor_evaluation_accuracy: accuracy = self.accuracy(evaluation_data) evaluation_accuracy.append(accuracy) print "Accuracy on evaluation data: {} / {}".format( self.accuracy(evaluation_data), n_data) print return evaluation_cost, evaluation_accuracy, \ training_cost, training_accuracy def update_mini_batch(self, mini_batch, eta, lmbda, n): """    ,          -. ``mini_batch`` ‚Äì    ``(x, y)``, ``eta`` ‚Äì  , ``lmbda`` -  , ``n`` -     .""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """  ``(nabla_b, nabla_w)``,      C_x. ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #   activation = x activations = [x] #      zs = [] #     z- for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = (self.cost).delta(zs[-1], activations[-1], y) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) """  l      ,      . l = 1    , l = 2 ‚Äì ,   .    ,   python      . """ for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def accuracy(self, data, convert=False): """    ``data``,      .   ‚Äì        .  ``convert``  False,    ‚Äì    ( )  True,   .    - ,  ``y`` -     .  ,       .           .          ?     ‚Äì       ,      .   ,      .       mnist_loader.load_data_wrapper. """ if convert: results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data] else: results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data] return sum(int(x == y) for (x, y) in results) def total_cost(self, data, lmbda, convert=False): """      ``data``.  ``convert``   False,   ‚Äì  (),   True,   ‚Äì   . .    ,      ``accuracy``, . """ cost = 0.0 for x, y in data: a = self.feedforward(x) if convert: y = vectorized_result(y) cost += self.cost.fn(a, y)/len(data) cost += 0.5*(lmbda/len(data))*sum( np.linalg.norm(w)**2 for w in self.weights) return cost def save(self, filename): """    ``filename``.""" data = {"sizes": self.sizes, "weights": [w.tolist() for w in self.weights], "biases": [b.tolist() for b in self.biases], "cost": str(self.cost.__name__)} f = open(filename, "w") json.dump(data, f) f.close() ####  Network def load(filename): """    ``filename``.    Network. """ f = open(filename, "r") data = json.load(f) f.close() cost = getattr(sys.modules[__name__], data["cost"]) net = Network(data["sizes"], cost=cost) net.weights = [np.array(w) for w in data["weights"]] net.biases = [np.array(b) for b in data["biases"]] return net ####   def vectorized_result(j): """  10-    1.0   j     .      (0..9)     . """ e = np.zeros((10, 1)) e[j] = 1.0 return e def sigmoid(z): """.""" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): """ .""" return sigmoid(z)*(1-sigmoid(z))</span></span></code> </pre> <br>  Entre as mudan√ßas mais interessantes est√° a inclus√£o da regulariza√ß√£o de L2.  Embora essa seja uma grande mudan√ßa conceitual, √© t√£o f√°cil de implementar que voc√™ pode n√£o perceber no c√≥digo.  Na maioria das vezes, isso √© simplesmente passar o par√¢metro lmbda para diferentes m√©todos, especialmente o Network.SGD.  Todo o trabalho √© realizado em uma linha do programa, a quarta do final no m√©todo Network.update_mini_batch.  L√°, alteramos a regra de atualiza√ß√£o da descida do gradiente para incluir a redu√ß√£o de peso.  A mudan√ßa √© pequena, mas afeta seriamente os resultados! <br><br>  A prop√≥sito, isso geralmente acontece ao implementar novas t√©cnicas em redes neurais.  Passamos milhares de palavras discutindo a regulariza√ß√£o.  Conceitualmente, essa √© uma coisa bastante sutil e dif√≠cil de entender.  No entanto, pode ser adicionado trivialmente ao programa!  Inesperadamente, t√©cnicas complexas podem ser implementadas com pequenas altera√ß√µes no c√≥digo. <br><br>  Outra altera√ß√£o pequena, mas importante no c√≥digo, √© a adi√ß√£o de v√°rios sinalizadores opcionais ao m√©todo de descida de gradiente estoc√°stico Network.SGD.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esses sinalizadores tornam poss√≠vel rastrear o custo e a precis√£o nos dados de treinamento ou nos dados de avalia√ß√£o, que podem ser transmitidos ao Network.SGD. </font><font style="vertical-align: inherit;">No in√≠cio do cap√≠tulo, costumamos usar esses sinalizadores, mas deixe-me dar um exemplo de seu uso, apenas como um lembrete:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Definimos dados de avalia√ß√£o por meio de dados de valida√ß√£o. No entanto, podemos acompanhar o desempenho em test_data e em qualquer outro conjunto de dados. Tamb√©m temos quatro sinalizadores que especificam a necessidade de acompanhar o custo e a precis√£o nos dados de avalia√ß√£o e nos dados de treinamento. Esses sinalizadores s√£o definidos como Falso por padr√£o, no entanto, s√£o inclu√≠dos aqui para rastrear a efic√°cia da rede. Al√©m disso, o m√©todo Network.SGD de network2.py retorna uma tupla de quatro elementos que representa os resultados do rastreamento. Voc√™ pode us√°-lo assim:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>evaluation_cost, evaluation_accuracy, ... training_cost, training_accuracy = net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Assim, por exemplo, assessment_cost ser√° uma lista de 30 elementos que cont√™m o custo dos dados estimados no final de cada era. Essas informa√ß√µes s√£o extremamente √∫teis para entender o comportamento de uma rede neural. Essas informa√ß√µes s√£o extremamente √∫teis para entender o comportamento da rede. Por exemplo, pode ser usado para desenhar gr√°ficos de aprendizado de rede ao longo do tempo. Foi assim que constru√≠ todos os gr√°ficos deste cap√≠tulo. No entanto, se um dos sinalizadores n√£o estiver definido, o elemento de tupla correspondente ser√° uma lista vazia.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Outras adi√ß√µes de c√≥digo incluem o m√©todo Network.save, que salva o objeto Rede no disco e a fun√ß√£o de carreg√°-lo na mem√≥ria. O salvamento e o carregamento s√£o feitos via JSON, n√£o os m√≥dulos pickle ou cPickle do Python, que geralmente s√£o usados ‚Äã‚Äãpara salvar no disco e carregar no python. O uso de JSON requer mais c√≥digo do que seria necess√°rio para pickle ou cPickle. Para entender por que escolhi o JSON, imagine que em algum momento no futuro decidimos mudar nossa classe de rede para que houvesse mais do que neur√¥nios sigm√≥ides. Para implementar essa altera√ß√£o, provavelmente alterar√≠amos os atributos definidos no m√©todo Network .__ init__. E se apenas usamos pickle para salvar, nossa fun√ß√£o de carregamento n√£o funcionaria. O uso de JSON com serializa√ß√£o expl√≠cita facilita a garantia deque vers√µes mais antigas do objeto Rede podem ser baixadas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Existem muitas pequenas altera√ß√µes no c√≥digo, mas essas s√£o apenas pequenas varia√ß√µes do network.py. </font><font style="vertical-align: inherit;">O resultado final √© uma extens√£o do nosso programa de 74 linhas para um programa muito mais funcional de 152 linhas.</font></font><br><br><h3>  Desafio </h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modifique o c√≥digo abaixo introduzindo a regulariza√ß√£o L1 e use-o para classificar os d√≠gitos MNIST por uma rede com 30 neur√¥nios ocultos. </font><font style="vertical-align: inherit;">Voc√™ pode escolher um par√¢metro de regulariza√ß√£o que permita melhorar o resultado comparado a uma rede sem regulariza√ß√£o?</font></font></li><li>    Network.cost_derivative method  network.py.      .        ?     ,       ?  network2.py      Network.cost_derivative,     CrossEntropyCost.delta.      ? </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt459816/">https://habr.com/ru/post/pt459816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt459802/index.html">Habr Weekly # 9 / Burnout na juventude, interfaces japonesas, rede neural Battle.net, jogos e crueldade</a></li>
<li><a href="../pt459804/index.html">Crie cart√µes de ajuda de crowdsourcing no WordPress + shMapper</a></li>
<li><a href="../pt459806/index.html">Como tratamos o gato Lapuna</a></li>
<li><a href="../pt459810/index.html">Microsservi√ßos ou mon√≥lito: procurando uma solu√ß√£o</a></li>
<li><a href="../pt459814/index.html">O que voc√™ √©, Rendering Engine? Ou como o m√≥dulo de exibi√ß√£o do navegador funciona</a></li>
<li><a href="../pt459820/index.html">Basta passar o cart√£o: como o OS / 2 √© usado no metr√¥ de Nova York</a></li>
<li><a href="../pt459822/index.html">Um exemplo de uma rede neural simples, como resultado, descobre o que √© o que</a></li>
<li><a href="../pt459824/index.html">Lista de verifica√ß√£o para escrever √≥timas extens√µes do Visual Studio</a></li>
<li><a href="../pt459828/index.html">Not√≠cias semanais: pre√ßo do bilhete Hyperloop na R√∫ssia, minera√ß√£o convencional de computadores Apollo, bot de IA no StarCraft II</a></li>
<li><a href="../pt459830/index.html">Claro, eles deram poder e uma linha de uma metralhadora. C√¢ncer e mais ... experi√™ncia com medicina</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>