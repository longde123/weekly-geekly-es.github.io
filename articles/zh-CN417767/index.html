<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤞🏻 👨🏻‍⚕️ 🐥 使用卷积神经网络进行文本情感分析 🗃️ 🧒 🤴🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="假设您有一段文字。 是否有可能理解这段文字所带来的情感：喜悦，悲伤，愤怒？ 可以的 我们简化了任务，并且将情感分类为正面或负面，没有具体说明。 解决此问题的方法有很多，其中一种是卷积神经网络 （Convolutional Neural Networks）。 CNN最初是为图像处理而开发的，但是它们成...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>使用卷积神经网络进行文本情感分析</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/417767/"><img src="https://habrastorage.org/webt/2u/l3/lw/2ul3lwsbyobovjnol2g_cbvrghi.gif"><br><br> 假设您有一段文字。 是否有可能理解这段文字所带来的情感：喜悦，悲伤，愤怒？ 可以的 我们简化了任务，并且将情感分类为正面或负面，没有具体说明。 解决此问题的方法有很多，其中一种是<b>卷积神经网络</b> （Convolutional Neural Networks）。  CNN最初是为图像处理而开发的，但是它们成功地处理了自动文字处理领域的任务。 我将向您介绍使用卷积神经网络对俄语语言的语调进行二进制分析的过程，其中，单词的矢量表示是在经过训练的<b>Word2Vec</b>模型的基础上形成的。 <br><br> 本文具有概述性，我强调了实际内容。 我想立即警告您，在每个阶段做出的决定可能都不是最佳选择。 在阅读之前，我建议您熟悉有关在自然语言处理任务中使用CNN的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">介绍性文章</a> ，并阅读有关单词矢量表示方法的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">材料</a> 。 <br><a name="habracut"></a><br><h2> 建筑学 </h2><br> 正在考虑的CNN架构基于方法[1]和[2]。 方法[1]使用卷积和递归网络的合奏，在计算机语言学的年度年度竞赛中，SemEval-2017在分析声调的任务5项提名中获得第一名[3]。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20a/058/4aa/20a0584aa0d0a5a6c8108af970c896fa.png"><br>  <i>图1. CNN架构[2]。</i> <br><br>  CNN输入（图1）是一个具有固定高度<i>n</i>的矩阵，其中每一行是令牌到维数<i>k</i>的特征空间中的矢量映射。 诸如Word2Vec，Glove，FastText等分布式语义工具通常用于形成特征空间。 <br><br> 在第一阶段，输入矩阵由卷积层处理。 通常，过滤器的宽度等于属性空间的尺寸，并且只有一个参数配置为过滤器尺寸-height <i>h</i> 。 事实证明， <i>h</i>是过滤器一起考虑的相邻线的高度。 因此，每个滤波器的输出特征矩阵的尺寸根据该滤波器的高度<i>h</i>和原始矩阵<i>n</i>的高度而变化。 <br><br> 接下来，在每个滤波器的输出处获得的特征图由具有特定压缩功能（图像中的1-max合并）的子采样层进行处理，即 减小生成的特征图的尺寸。 因此，无论每个卷积在文本中的位置如何，都将提取最重要的信息。 换句话说，对于所使用的矢量显示，卷积层和子采样层的组合使得可以从文本中提取最高有效的<i>n-</i> g。 <br><br> 此后，将在每个子采样层的输出处计算出的特征图组合为一个公共特征向量。 它被馈送到隐藏的，完全连接的层的输入，然后馈送到神经网络的输出层，在该层中计算最终的类别标签。 <br><br><h2> 训练数据 </h2><br> 为了进行培训，我选择了<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">尤莉亚·鲁布佐娃（Yulia Rubtsova）的短文本语料库，</a>该<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">语料库是</a>基于Twitter的俄语消息而形成的[4]。 它包含114 991条正面推文，111 923条负面推文以及未分配的推文库，其消息量为17 639 674条。 <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-comment"><span class="hljs-comment">#   n = ['id', 'date', 'name', 'text', 'typr', 'rep', 'rtw', 'faw', 'stcount', 'foll', 'frien', 'listcount'] data_positive = pd.read_csv('data/positive.csv', sep=';', error_bad_lines=False, names=n, usecols=['text']) data_negative = pd.read_csv('data/negative.csv', sep=';', error_bad_lines=False, names=n, usecols=['text']) #    sample_size = min(data_positive.shape[0], data_negative.shape[0]) raw_data = np.concatenate((data_positive['text'].values[:sample_size], data_negative['text'].values[:sample_size]), axis=0) labels = [1] * sample_size + [0] * sample_size</span></span></code> </pre> <br> 培训之前，课文通过了初步处理： <br><br><ul><li> 转换为小写； <br></li><li> 用“ e”代替“ e”； <br></li><li> 替换指向“ URL”令牌的链接； <br></li><li> 用USER令牌替换用户的提及； <br></li><li> 删除标点符号。 <br></li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = text.lower().replace(<span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>) text = re.sub(<span class="hljs-string"><span class="hljs-string">'((www\.[^\s]+)|(https?://[^\s]+))'</span></span>, <span class="hljs-string"><span class="hljs-string">'URL'</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">'@[^\s]+'</span></span>, <span class="hljs-string"><span class="hljs-string">'USER'</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() data = [preprocess_text(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> raw_data]</code> </pre> <br> 接下来，我将数据集以4：1的比例分为训练样本和测试样本。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><h2> 单词的矢量显示 </h2><br> 卷积神经网络的输入数据是高度固定为<i>n</i>的矩阵，其中每一行都是一个单词到维数为<i>k</i>的特征空间中的向量映射。 为了形成神经网络的嵌入层，我使用了Word2Vec [5]分布式语义实用程序，该实用程序旨在将单词的语义映射到向量空间中。  Word2Vec通过假设在相似的上下文中发现了语义相关的单词来查找单词之间的关系。 您可以在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">原始文章</a>以及<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此处</a>和<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此处</a>阅读有关Word2Vec的更多信息。 由于推文具有作者标点符号和表情符号的特征，因此定义句子的边界变得相当耗时。 在这项工作中，我假设每条推文仅包含一个句子。 <br><br> 未分配推文的基础以SQL格式存储，并包含超过1750万条记录。 为了方便起见，我使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此</a>脚本将其转换为SQLite。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sqlite3 <span class="hljs-comment"><span class="hljs-comment">#  SQLite   conn = sqlite3.connect('mysqlite3.db') c = conn.cursor() with open('data/tweets.txt', 'w', encoding='utf-8') as f: #    for row in c.execute('SELECT ttext FROM sentiment'): if row[0]: tweet = preprocess(row[0]) #      print(tweet, file=f)</span></span></code> </pre> <br> 然后，使用Gensim库，我使用以下参数训练了Word2Vec模型： <br><br><ul><li>  <i>size = 200-</i>属性空间的尺寸； <br></li><li>  <i>window = 5-</i>算法分析的上下文中的单词数； <br></li><li>  <i>min_count = 3-</i>该单词必须至少出现3次，以便模型<i>将</i>其考虑在内。 <br></li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec logging.basicConfig(format=<span class="hljs-string"><span class="hljs-string">'%(asctime)s : %(levelname)s : %(message)s'</span></span>, level=logging.INFO) <span class="hljs-comment"><span class="hljs-comment">#      data = gensim.models.word2vec.LineSentence('data/tweets.txt') #   model = Word2Vec(data, size=200, window=5, min_count=3, workers=multiprocessing.cpu_count()) model.save("models/w2v/model.w2v")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/uk/zh/h0/ukzhh0kwiptrhim7tygo1vh5vwq.png"><br>  <i>图2.使用t-SNE可视化相似词簇。</i> <br><br> 为了更详细地了解图2中Word2Vec的操作。  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">图</a> 2显示了来自训练模型的相似词的几个聚类的可视化，并使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">t-SNE可视化算法</a>映射到二维空间中。 <br><br><h2> 文本的矢量显示 </h2><br><img src="https://habrastorage.org/webt/er/de/wc/erdewcunafpymiafxeqgby-8-h8.png"><br>  <i>图3.文本长度的分布。</i> <br><br> 在下一步中，每个文本都映射到令牌标识符数组。 我选择文本向量<i>的</i>维数<i>s = 26</i> ，因为在该值下，成形主体中所有文本的99.71％被完全覆盖（图3）。 如果在分析过程中，推文中的单词数超过了矩阵的高度，则剩余的单词将被丢弃，并且在分类中不予考虑。 提案矩阵的最终尺寸为<i>s×d = 26×200</i> 。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.sequence <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pad_sequences <span class="hljs-comment"><span class="hljs-comment">#   (    ) SENTENCE_LENGTH = 26 #   NUM = 100000 def get_sequences(tokenizer, x): sequences = tokenizer.texts_to_sequences(x) return pad_sequences(sequences, maxlen=SENTENCE_LENGTH) # C    tokenizer = Tokenizer(num_words=NUM) tokenizer.fit_on_texts(x_train) #        x_train_seq = get_sequences(tokenizer, x_train) x_test_seq = get_sequences(tokenizer, x_test)</span></span></code> </pre> <br><h2> 卷积神经网络 </h2><br> 为了构建神经网络，我使用了Keras库，该库充当TensorFlow，CNTK和Theano的高级附加组件。  Keras提供了出色的文档，以及一个涵盖许多机器学习任务（例如<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">初始化嵌入层）</a>的博客。 在我们的案例中，嵌入层是通过学习Word2Vec获得的权重而启动的。 为了最大程度地减少嵌入层中的更改，我在训练的第一阶段将其冻结。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.embeddings <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Embedding tweet_input = Input(shape=(SENTENCE_LENGTH,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) tweet_encoder = Embedding(NUM, DIM, input_length=SENTENCE_LENGTH, weights=[embedding_matrix], trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)(tweet_input)</code> </pre> <br> 在已开发的体系结构中，使用了高度为<i>h =（</i> 2、3、4、5）的过滤器，它们分别用于并行处理双字母组，三字母组，4克和5克。 在每个神经网络中为每个滤波器高度添加10个卷积层，其激活函数为ReLU。 寻找最佳高度和过滤器数量的建议可以在[2]中找到。 <br><br> 经过卷积层处理后，属性映射被馈送到子采样层，在其中进行了1-max-pooling操作，从而从文本中提取了最重要的n-gram。 在下一阶段，它们合并为一个公共特征向量（合并层），该向量被馈送到具有30个神经元的隐藏的完全连接层中。 在最后阶段，将最终的特征图通过S型激活函数馈送到神经网络的输出层。 <br><br> 由于神经网络易于重新训练，因此在嵌入层之后和隐藏的完全连接层之前，我添加了一个辍学正则化，其顶点弹出概率为p = 0.2。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optimizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, concatenate, Activation, Dropout <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.convolutional <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Conv1D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.pooling <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GlobalMaxPooling1D branches = [] <span class="hljs-comment"><span class="hljs-comment">#  dropout- x = Dropout(0.2)(tweet_encoder) for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10)]: for i in range(filters_count): #    branch = Conv1D(filters=1, kernel_size=size, padding='valid', activation='relu')(x) #    branch = GlobalMaxPooling1D()(branch) branches.append(branch) #    x = concatenate(branches, axis=1) #  dropout- x = Dropout(0.2)(x) x = Dense(30, activation='relu')(x) x = Dense(1)(x) output = Activation('sigmoid')(x) model = Model(inputs=[tweet_input], outputs=[output])</span></span></code> </pre> <br> 我使用Adam优化函数（自适应矩估计）和作为误差函数的二进制交叉熵来配置最终模型。 分类器的质量根据宏观平均的准确性，完整性和f度量进行评估。 <br><br><pre> <code class="python hljs">model.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[precision, recall, f1]) model.summary()</code> </pre> <br> 在训练的第一阶段，将嵌入层冻结，将所有其他层训练10个时代： <br><br><ul><li> 用于训练的示例组的大小为32。 <br></li><li> 验证样本的大小：25％。 <br></li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint checkpoint = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">"models/cnn/cnn-frozen-embeddings-{epoch:02d}-{val_f1:.2f}.hdf5"</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_f1'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'max'</span></span>, period=<span class="hljs-number"><span class="hljs-number">1</span></span>) history = model.fit(x_train_seq, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">32</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>, validation_split=<span class="hljs-number"><span class="hljs-number">0.25</span></span>, callbacks = [checkpoint])</code> </pre> <br><br><div class="spoiler">  <b class="spoiler_title">日志</b> <div class="spoiler_text"> <code>Train on 134307 samples, validate on 44769 samples <br> Epoch 1/10 <br> 134307/134307 [==============================] - 221s 2ms/step - loss: 0.5703 - precision: 0.7006 - recall: 0.6854 - f1: 0.6839 - val_loss: 0.5014 - val_precision: 0.7538 - val_recall: 0.7493 - val_f1: 0.7452 <br> Epoch 2/10 <br> 134307/134307 [==============================] - 218s 2ms/step - loss: 0.5157 - precision: 0.7422 - recall: 0.7258 - f1: 0.7263 - val_loss: 0.4911 - val_precision: 0.7413 - val_recall: 0.7924 - val_f1: 0.7602 <br> Epoch 3/10 <br> 134307/134307 [==============================] - 213s 2ms/step - loss: 0.5023 - precision: 0.7502 - recall: 0.7337 - f1: 0.7346 - val_loss: 0.4825 - val_precision: 0.7750 - val_recall: 0.7411 - val_f1: 0.7512 <br> Epoch 4/10 <br> 134307/134307 [==============================] - 215s 2ms/step - loss: 0.4956 - precision: 0.7545 - recall: 0.7412 - f1: 0.7407 - val_loss: 0.4747 - val_precision: 0.7696 - val_recall: 0.7590 - val_f1: 0.7584 <br> Epoch 5/10 <br> 134307/134307 [==============================] - 229s 2ms/step - loss: 0.4891 - precision: 0.7587 - recall: 0.7492 - f1: 0.7473 - val_loss: 0.4781 - val_precision: 0.8014 - val_recall: 0.7004 - val_f1: 0.7409 <br> Epoch 6/10 <br> 134307/134307 [==============================] - 217s 2ms/step - loss: 0.4830 - precision: 0.7620 - recall: 0.7566 - f1: 0.7525 - val_loss: 0.4749 - val_precision: 0.7877 - val_recall: 0.7411 - val_f1: 0.7576 <br> Epoch 7/10 <br> 134307/134307 [==============================] - 219s 2ms/step - loss: 0.4802 - precision: 0.7632 - recall: 0.7568 - f1: 0.7532 - val_loss: 0.4730 - val_precision: 0.7969 - val_recall: 0.7241 - val_f1: 0.7522 <br> Epoch 8/10 <br> 134307/134307 [==============================] - 215s 2ms/step - loss: 0.4769 - precision: 0.7644 - recall: 0.7605 - f1: 0.7558 - val_loss: 0.4680 - val_precision: 0.7829 - val_recall: 0.7542 - val_f1: 0.7619 <br> Epoch 9/10 <br> 134307/134307 [==============================] - 227s 2ms/step - loss: 0.4741 - precision: 0.7657 - recall: 0.7663 - f1: 0.7598 - val_loss: 0.4672 - val_precision: 0.7695 - val_recall: 0.7784 - val_f1: 0.7682 <br> Epoch 10/10 <br> 134307/134307 [==============================] - 221s 2ms/step - loss: 0.4727 - precision: 0.7670 - recall: 0.7647 - f1: 0.7590 - val_loss: 0.4673 - val_precision: 0.7833 - val_recall: 0.7561 - val_f1: 0.7636</code> <br> </div></div><br><br> 然后他选择了验证数据集上F值最高的模型，即 在第八个教育阶段获得的模型（F <sub>1</sub> = 0.7791）。 该模型解冻了嵌入层，此后又启动了五个训练时代。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optimizers <span class="hljs-comment"><span class="hljs-comment">#    model.load_weights('models/cnn/cnn-frozen-embeddings-09-0.77.hdf5') #  embedding     model.layers[1].trainable = True #  learning rate adam = optimizers.Adam(lr=0.0001) model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[precision, recall, f1]) model.summary() checkpoint = ModelCheckpoint("models/cnn/cnn-trainable-{epoch:02d}-{val_f1:.2f}.hdf5", monitor='val_f1', save_best_only=True, mode='max', period=1) history_trainable = model.fit(x_train_seq, y_train, batch_size=32, epochs=5, validation_split=0.25, callbacks = [checkpoint])</span></span></code> </pre> <br><br><div class="spoiler">  <b class="spoiler_title">日志</b> <div class="spoiler_text"> <code>Train on 134307 samples, validate on 44769 samples <br> Epoch 1/5 <br> 134307/134307 [==============================] - 2042s 15ms/step - loss: 0.4495 - precision: 0.7806 - recall: 0.7797 - f1: 0.7743 - val_loss: 0.4560 - val_precision: 0.7858 - val_recall: 0.7671 - val_f1: 0.7705 <br> Epoch 2/5 <br> 134307/134307 [==============================] - 2253s 17ms/step - loss: 0.4432 - precision: 0.7857 - recall: 0.7842 - f1: 0.7794 - val_loss: 0.4543 - val_precision: 0.7923 - val_recall: 0.7572 - val_f1: 0.7683 <br> Epoch 3/5 <br> 134307/134307 [==============================] - 2018s 15ms/step - loss: 0.4372 - precision: 0.7899 - recall: 0.7879 - f1: 0.7832 - val_loss: 0.4519 - val_precision: 0.7805 - val_recall: 0.7838 - val_f1: 0.7767 <br> Epoch 4/5 <br> 134307/134307 [==============================] - 1901s 14ms/step - loss: 0.4324 - precision: 0.7943 - recall: 0.7904 - f1: 0.7869 - val_loss: 0.4504 - val_precision: 0.7825 - val_recall: 0.7808 - val_f1: 0.7762 <br> Epoch 5/5 <br> 134307/134307 [==============================] - 1924s 14ms/step - loss: 0.4256 - precision: 0.7986 - recall: 0.7947 - f1: 0.7913 - val_loss: 0.4497 - val_precision: 0.7989 - val_recall: 0.7549 - val_f1: 0.7703</code> <br> </div></div><br><br> 验证样本中的最高指标<i>F <sub>1</sub> = 76.80％</i>是在训练的第三个时代实现的。 测试数据上训练模型的质量为<i>F <sub>1</sub> = 78.1％</i> 。 <br><br> 表1.对测试数据的情感分析的质量。 <br><div class="scrollable-table"><table><tbody><tr><td> 类标签 <br></td><td> 准确度 <br></td><td> 完整性 <br></td><td>  <sub>1号</sub> <br></td><td> 物件数量 <br></td></tr><tr><td> 负数 <br></td><td>  0.78194 <br></td><td>  0.78243 <br></td><td>  0.78218 <br></td><td>  22457 <br></td></tr><tr><td> 正面的 <br></td><td>  0.78089 <br></td><td>  0.78040 <br></td><td>  0.78064 <br></td><td>  22313 <br></td></tr><tr><td> 平均/总计 <br></td><td>  0.78142 <br></td><td>  0.78142 <br></td><td>  0.78142 <br></td><td>  44770 <br></td></tr></tbody></table></div><br><h2> 结果 </h2><br> 作为基准解决方案，我用多项分布模型<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">训练了</a>朴素的贝叶斯分类器，比较结果列在表中。  2。 <br><br> 表2.音调分析质量的比较。 <br><div class="scrollable-table"><table><tbody><tr><td> 分类器 <br></td><td> 精密度 <br></td><td> 召回 <br></td><td>  <sub>1号</sub> <br></td></tr><tr><td> 锰 <br></td><td>  0.7577 <br></td><td>  0.7564 <br></td><td>  0.7560 <br></td></tr><tr><td>  CNN <br></td><td>  <b>0.78142</b> <br></td><td>  <b>0.78142</b> <br></td><td>  <b>0.78142</b> <br></td></tr></tbody></table></div><br> 如您所见，CNN分类的质量比MNB高出几个百分点。 如果您致力于优化超参数和网络体系结构，则可以进一步提高指标值。 例如，您可以更改训练时代的次数，检查使用单词及其组合的各种矢量表示的效果，选择过滤器的数量及其高度，实施更有效的文本预处理（错字校正，归一化，标记），调整其中隐藏的完全连接层和神经元的数量。 <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Github上提供了</a>源代码，可以在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此处</a>下载经过训练的CNN和Word2Vec模型。 <br><br><h2> 资料来源 </h2><br><ol><li>  Cliche M.BB_twtr在SemEval-2017上的任务4：Twitter与CNN和LSTM的情感分析//第11届国际语义评估研讨会（SemEval-2017）的会议记录。  -2017年-S.573-580。 <br></li><li>  Zhang Y.，Wallace B.对卷积神经网络进行句子分类的敏感性分析（和从业人员指南）// arXiv预印本arXiv：1510.03820。  -2015年 <br></li><li>  Rosenthal S.，Farra N.，Nakov P.SemEval-2017任务4：Twitter中的情感分析//第11届国际语义评估研讨会（SemEval-2017）的会议记录。  -2017年-S.502-518。 <br></li><li>  Yu。V. Rubtsova。 构建用于设置音调分类器的文本正文//软件产品和系统，2015年，第1号（109），-C.72-78。 <br></li><li>  Mikolov T.等。 单词和短语的分布式表示及其组成//神经信息处理系统的进展。  -2013 .-- S.3111-3119。 <br></li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN417767/">https://habr.com/ru/post/zh-CN417767/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN417755/index.html">研究：2017年ICO中有80％被认为具有欺诈性</a></li>
<li><a href="../zh-CN417757/index.html">创建一个机器人参加AI迷你杯。 GPU体验</a></li>
<li><a href="../zh-CN417759/index.html">做我的橡皮鸭</a></li>
<li><a href="../zh-CN417761/index.html">GitLab正在从Azure迁移到Google Cloud Platform。 搬迁新闻和维护日期</a></li>
<li><a href="../zh-CN417763/index.html">MVIDroid：新MVI库的回顾（模型-视图-意图）</a></li>
<li><a href="../zh-CN417769/index.html">用户内存设计：如何设计年龄</a></li>
<li><a href="../zh-CN417771/index.html">ICANN计划：公司提供新的DNS根服务器管理模型</a></li>
<li><a href="../zh-CN417773/index.html">自制OpenPnP组件安装程序</a></li>
<li><a href="../zh-CN417775/index.html">比特币佣金机制以及为什么与矿工成为朋友</a></li>
<li><a href="../zh-CN417777/index.html">周末读物：初学者乙烯基爱好者的25种材料</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>