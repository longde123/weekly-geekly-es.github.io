<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõ∑ üç≤ üë©üèø‚Äçüè≠ Jouez √† Mortal Kombat avec TensorFlow.js üë©üèæ‚Äçüéì üßìüèª üí∫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En exp√©rimentant des am√©liorations pour le mod√®le de pr√©vision de Guess.js , j'ai commenc√© √† examiner de pr√®s l'apprentissage profond: les r√©seaux de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Jouez √† Mortal Kombat avec TensorFlow.js</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428019/">  En exp√©rimentant des am√©liorations pour le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mod√®le de</a> pr√©vision de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Guess.js</a> , j'ai commenc√© √† examiner de pr√®s l'apprentissage profond: les r√©seaux de neurones r√©currents (RNN), en particulier les LSTM, en raison de leur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">¬´efficacit√© d√©raisonnable¬ª</a> dans le domaine o√π Guess.js travaille.  Dans le m√™me temps, j'ai commenc√© √† jouer avec les r√©seaux de neurones convolutifs (CNN), qui sont √©galement souvent utilis√©s pour les s√©ries chronologiques.  Les CNN sont couramment utilis√©s pour classer, reconna√Ætre et d√©tecter des images. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1fb/9be/edc/1fb9beedcad00d1c0dcdc7bbef67e6d9.png"><br>  <i><font color="gray">G√©rer <a href="">MK.js</a> avec TensorFlow.js</font></i> <br><br><blockquote>  Le code source de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cet article</a> et <a href="">MK.js</a> sont sur mon <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub</a> .  Je n'ai pas publi√© de jeu de donn√©es de formation, mais vous pouvez cr√©er le v√¥tre et former le mod√®le comme d√©crit ci-dessous! </blockquote><a name="habracut"></a><br>  Apr√®s avoir jou√© avec CNN, je me suis souvenu d'une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">exp√©rience que</a> j'avais men√©e il y a plusieurs ann√©es lorsque les d√©veloppeurs de navigateurs ont publi√© l'API <code>getUserMedia</code> .  Dans celui-ci, la cam√©ra de l'utilisateur a servi de contr√¥leur pour jouer au petit clone JavaScript de Mortal Kombat 3. Vous pouvez trouver ce jeu dans <a href="">le r√©f√©rentiel GitHub</a> .  Dans le cadre de l'exp√©rience, j'ai impl√©ment√© un algorithme de positionnement de base qui classe l'image dans les classes suivantes: <br><br><ul><li>  Poin√ßon gauche ou droit </li><li>  Coup de pied gauche ou droit </li><li>  Pas √† gauche et √† droite </li><li>  Squat </li><li>  Aucune de ces r√©ponses </li></ul><br>  L'algorithme est si simple que je peux l'expliquer en quelques phrases: <br><br><blockquote>  L'algorithme photographie l'arri√®re-plan.  D√®s que l'utilisateur appara√Æt dans le cadre, l'algorithme calcule la diff√©rence entre l'arri√®re-plan et le cadre actuel avec l'utilisateur.  Il d√©termine donc la position de la figure de l'utilisateur.  L'√©tape suivante consiste √† afficher le corps de l'utilisateur en blanc sur noir.  Apr√®s cela, des histogrammes verticaux et horizontaux sont construits, sommant les valeurs pour chaque pixel.  Sur la base de ce calcul, l'algorithme d√©termine la position actuelle du corps. </blockquote><br>  La vid√©o montre comment fonctionne le programme.  Code source de <a href="">GitHub</a> . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0_yfU_iNUYo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Bien que le petit clone MK ait fonctionn√© avec succ√®s, l'algorithme est loin d'√™tre parfait.  Un cadre avec un arri√®re-plan est requis.  Pour un fonctionnement correct, l'arri√®re-plan doit √™tre de la m√™me couleur tout au long de l'ex√©cution du programme.  Une telle limitation signifie que les changements de lumi√®re, d'ombre et d'autres choses interf√®rent et donnent un r√©sultat inexact.  Enfin, l'algorithme ne reconna√Æt pas l'action;  il classe uniquement le nouveau cadre comme la position du corps √† partir d'un ensemble pr√©d√©fini. <br><br>  Maintenant, gr√¢ce aux progr√®s de l'API Web, √† savoir WebGL, j'ai d√©cid√© de revenir √† cette t√¢che en appliquant TensorFlow.js. <br><br><h1>  Pr√©sentation </h1><br>  Dans cet article, je partagerai mon exp√©rience dans la cr√©ation d'un algorithme pour classer les positions du corps √† l'aide de TensorFlow.js et MobileNet.  Consid√©rez les sujets suivants: <br><br><ul><li>  Collecte de donn√©es d'entra√Ænement pour la classification d'images </li><li>  Augmentation des donn√©es avec <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">imgaug</a> </li><li>  Transfert d'apprentissage avec MobileNet </li><li>  Classification binaire et classification N-primaire </li><li>  Formation au mod√®le de classification d'images TensorFlow.js dans Node.js et utilisation dans un navigateur </li><li>  Quelques mots sur la classification des actions avec LSTM </li></ul><br>  Dans cet article, nous allons r√©duire le probl√®me de la d√©termination de la position du corps sur la base d'une image, contrairement √† la reconnaissance des actions par une s√©quence d'images.  Nous allons d√©velopper un mod√®le d'apprentissage en profondeur avec un enseignant qui, √† partir de l'image de la webcam de l'utilisateur, d√©termine les mouvements d'une personne: coup de pied, jambe ou rien de tout cela. <br><br>  √Ä la fin de l'article, nous serons en mesure de construire un mod√®le pour jouer √† <a href="">MK.js</a> : <br><br><img src="https://habrastorage.org/webt/2u/0e/g6/2u0eg6ng2p4kwxosmut1koa751g.gif"><br><br>  Pour une meilleure compr√©hension de l'article, le lecteur doit √™tre familiaris√© avec les concepts fondamentaux de la programmation et de JavaScript.  Une compr√©hension de base de l'apprentissage en profondeur est √©galement utile, mais pas n√©cessaire. <br><br><h1>  Collecte de donn√©es </h1><br>  La pr√©cision du mod√®le d'apprentissage en profondeur d√©pend fortement de la qualit√© des donn√©es.  Nous devons nous efforcer de collecter un vaste ensemble de donn√©es, comme en production. <br><br>  Notre mod√®le devrait √™tre capable de reconna√Ætre les coups de poing et les coups de pied.  Cela signifie que nous devons collecter des images de trois cat√©gories: <br><br><ul><li>  Coups de pied </li><li>  Coups de pied </li><li>  Autre </li></ul><br>  Dans cette exp√©rience, deux volontaires ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">@lili_vs</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">@gsamokovarov</a> ) m'ont aid√© √† collecter des photos.  Nous avons enregistr√© 5 vid√©os QuickTime sur mon MacBook Pro, chacune contenant 2-4 coups de pied et 2-4 coups de pied. <br><br>  Ensuite, nous utilisons ffmpeg pour extraire des images individuelles des vid√©os et les enregistrer sous forme d'images <code>jpg</code> : <br><br> <code>ffmpeg -i video.mov $filename%03d.jpg</code> <br> <br>  Pour ex√©cuter la commande ci-dessus, vous devez d'abord <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">installer</a> <code>ffmpeg</code> sur l'ordinateur. <br><br>  Si nous voulons former le mod√®le, nous devons fournir les donn√©es d'entr√©e et les donn√©es de sortie correspondantes, mais √† ce stade, nous n'avons qu'un tas d'images de trois personnes dans des poses diff√©rentes.  Pour structurer les donn√©es, vous devez classer les cadres en trois cat√©gories: coups de poing, coups de pied et autres.  Pour chaque cat√©gorie, un r√©pertoire s√©par√© est cr√©√© o√π toutes les images correspondantes sont d√©plac√©es. <br><br>  Ainsi, dans chaque r√©pertoire, il devrait y avoir environ 200 images similaires √† celles ci-dessous: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/798/e9a/908/798e9a9083a1f5dfa5811fbb7de3bcc9.jpg"><br><br>  Veuillez noter qu'il y aura beaucoup plus d'images dans le r√©pertoire Others, car relativement peu d'images contiennent des photos de coups de poing et de pied, et dans les images restantes, les gens marchent, se retournent ou contr√¥lent la vid√©o.  Si nous avons trop d'images d'une classe, nous courons le risque d'enseigner le mod√®le biais√© vers cette classe particuli√®re.  Dans ce cas, lors de la classification d'une image ayant un impact, le r√©seau neuronal peut toujours d√©terminer la classe ¬´Autre¬ª.  Pour r√©duire ce biais, vous pouvez supprimer certaines photos du r√©pertoire Others et entra√Æner le mod√®le sur un nombre √©gal d'images de chaque cat√©gorie. <br><br>  Pour plus de commodit√©, nous attribuons les num√©ros dans les catalogues de <code>1</code> √† <code>190</code> , donc la premi√®re image sera <code>1.jpg</code> , la seconde <code>2.jpg</code> , etc. <br><br>  Si nous formons le mod√®le √† seulement 600 photographies prises dans le m√™me environnement avec les m√™mes personnes, nous n'atteindrons pas un niveau de pr√©cision tr√®s √©lev√©.  Pour tirer le meilleur parti de nos donn√©es, il est pr√©f√©rable de g√©n√©rer quelques √©chantillons suppl√©mentaires √† l'aide de l'augmentation des donn√©es. <br><br><h1>  Augmentation des donn√©es </h1><br>  L'augmentation des donn√©es est une technique qui augmente le nombre de points de donn√©es en synth√©tisant de nouveaux points √† partir d'un ensemble existant.  En r√®gle g√©n√©rale, l'augmentation est utilis√©e pour augmenter la taille et la diversit√© de l'ensemble d'entra√Ænement.  Nous transf√©rons les images originales vers le pipeline de transformations qui cr√©ent de nouvelles images.  Vous ne pouvez pas aborder les transformations de mani√®re trop agressive: seuls les autres coups de poing doivent √™tre g√©n√©r√©s √† partir d'un coup de poing. <br><br>  Les transformations acceptables sont la rotation, l'inversion des couleurs, le flou, etc. Il existe d'excellents outils open source pour l'augmentation des donn√©es.  Au moment d'√©crire cet article en JavaScript, il n'y avait pas trop d'options, j'ai donc utilis√© la biblioth√®que impl√©ment√©e en Python - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">imgaug</a> .  Il dispose d'un ensemble d'agrandisseurs qui peuvent √™tre appliqu√©s de mani√®re probabiliste. <br><br>  Voici la logique d'augmentation des donn√©es pour cette exp√©rience: <br><br><pre> <code class="python hljs">np.random.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) ia.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-string"><span class="hljs-string">"others-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"hits"</span></span>, <span class="hljs-string"><span class="hljs-string">"hits-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"kicks"</span></span>, <span class="hljs-string"><span class="hljs-string">"kicks-aug"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">draw_single_sequential_images</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, path, aug_path)</span></span></span><span class="hljs-function">:</span></span> image = misc.imresize(ndimage.imread(path + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + filename + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span>), (<span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>)) sometimes = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> aug: iaa.Sometimes(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, aug) seq = iaa.Sequential( [ iaa.Fliplr(<span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-comment"><span class="hljs-comment"># horizontally flip 50% of all images # crop images by -5% to 10% of their height/width sometimes(iaa.CropAndPad( percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255) )), sometimes(iaa.Affine( scale={"x": (0.8, 1.2), "y": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis translate_percent={"x": (-0.1, 0.1), "y": (-0.1, 0.1)}, # translate by -10 to +10 percent (per axis) rotate=(-5, 5), shear=(-5, 5), # shear by -5 to +5 degrees order=[0, 1], # use nearest neighbour or bilinear interpolation (fast) cval=(0, 255), # if mode is constant, use a cval between 0 and 255 mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples) )), iaa.Grayscale(alpha=(0.0, 1.0)), iaa.Invert(0.05, per_channel=False), # invert color channels # execute 0 to 5 of the following (less important) augmenters per image # don't execute all of them, as that would often be way too strong iaa.SomeOf((0, 5), [ iaa.OneOf([ iaa.GaussianBlur((0, 2.0)), # blur images with a sigma between 0 and 2.0 iaa.AverageBlur(k=(2, 5)), # blur image using local means with kernel sizes between 2 and 5 iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 3 and 5 ]), iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value) iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation # either change the brightness of the whole image (sometimes # per channel) or change the brightness of subareas iaa.OneOf([ iaa.Multiply((0.9, 1.1), per_channel=0.5), iaa.FrequencyNoiseAlpha( exponent=(-2, 0), first=iaa.Multiply((0.9, 1.1), per_channel=True), second=iaa.ContrastNormalization((0.9, 1.1)) ) ]), iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast ], random_order=True ) ], random_order=True ) im = np.zeros((16, 56, 100, 3), dtype=np.uint8) for c in range(0, 16): im[c] = image for im in range(len(grid)): misc.imsave(aug_path + "/" + filename + "_" + str(im) + ".jpg", grid[im])</span></span></code> </pre> <br>  Ce script utilise la m√©thode <code>main</code> avec trois boucles <code>for</code> - une pour chaque cat√©gorie d'image.  Dans chaque it√©ration, dans chacune des boucles, nous appelons la m√©thode <code>draw_single_sequential_images</code> : le premier argument est le nom du fichier, le second est le chemin, le troisi√®me est le r√©pertoire o√π enregistrer le r√©sultat. <br><br>  Apr√®s cela, nous lisons l'image du disque et lui appliquons une s√©rie de transformations.  J'ai document√© la plupart des transformations dans l'extrait de code ci-dessus, nous ne le r√©p√©terons donc pas. <br><br>  Pour chaque image, 16 autres images sont cr√©√©es.  Voici un exemple de leur apparence: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/759/ad9/43d/759ad943d7aa07dbccee4a6f26a1d920.jpg"></a> <br><br>  Veuillez noter que dans le script ci-dessus, nous redimensionnons les images √† <code>100x56</code> pixels.  Nous le faisons pour r√©duire la quantit√© de donn√©es et, par cons√©quent, le nombre de calculs que notre mod√®le effectue pendant la formation et l'√©valuation. <br><br><h1>  Construction de mod√®les </h1><br>  Maintenant, construisez un mod√®le pour la classification! <br><br>  Puisque nous avons affaire √† des images, nous utilisons un r√©seau neuronal convolutif (CNN).  Cette architecture de r√©seau est connue pour convenir √† la reconnaissance d'images, √† la d√©tection d'objets et √† la classification. <br><br><h3>  Transfert d'apprentissage </h3><br>  L'image ci-dessous montre le populaire CNN VGG-16, utilis√© pour classer les images. <br><br><img src="https://habrastorage.org/webt/7t/0u/zk/7t0uzk4kdf4pbesgvlojn5nal18.png"><br><br>  Le r√©seau neuronal VGG-16 reconna√Æt 1 000 classes d'images.  Il a 16 couches (sans compter les couches de regroupement et de sortie).  Un tel r√©seau multicouche est difficile √† former en pratique.  Cela n√©cessitera un grand ensemble de donn√©es et de nombreuses heures de formation. <br><br>  Les couches masqu√©es de CNN form√©s reconnaissent divers √©l√©ments d'images de l'ensemble d'apprentissage, en commen√ßant par les bords, pour passer √† des √©l√©ments plus complexes, tels que des formes, des objets individuels, etc.  Un CNN form√© dans le style de VGG-16 pour reconna√Ætre un grand ensemble d'images doit avoir des couches cach√©es qui ont appris beaucoup de fonctionnalit√©s de l'ensemble d'entra√Ænement.  Ces fonctionnalit√©s seront communes √† la plupart des images et, par cons√©quent, r√©utilis√©es dans diff√©rentes t√¢ches. <br><br>  Le transfert d'apprentissage vous permet de r√©utiliser un r√©seau existant et form√©.  Nous pouvons prendre la sortie de n'importe laquelle des couches du r√©seau existant et la transf√©rer comme entr√©e vers le nouveau r√©seau neuronal.  Ainsi, en enseignant le r√©seau neuronal nouvellement cr√©√©, au fil du temps, il peut √™tre appris √† reconna√Ætre de nouvelles fonctionnalit√©s d'un niveau sup√©rieur et √† classer correctement les images de classes que le mod√®le d'origine n'avait jamais vues auparavant. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7n/cc/a7/7ncca7e5ne2ammearn2sqnk4by0.png"></div><br><br>  Pour nos besoins, prenez le r√©seau neuronal MobileNet du package <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">@ tensorflow-models / mobilenet</a> .  MobileNet est tout aussi puissant que VGG-16, mais il est beaucoup plus petit, ce qui acc√©l√®re la distribution directe, c'est-√†-dire la propagation r√©seau (propagation directe) et r√©duit le temps de t√©l√©chargement dans le navigateur.  MobileNet s'est form√© sur l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ensemble de donn√©es de</a> classification d'images <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ILSVRC-2012-CLS</a> . <br><br>  Lors du d√©veloppement d'un mod√®le avec un transfert d'apprentissage, nous avons deux choix: <br><br><ol><li>  Sortie √† partir de quelle couche du mod√®le source √† utiliser comme entr√©e pour le mod√®le cible. </li><li>  Combien de couches du mod√®le cible allons-nous former, le cas √©ch√©ant. </li></ol><br>  Le premier point est tr√®s significatif.  Selon la couche s√©lectionn√©e, nous obtiendrons des caract√©ristiques √† un niveau d'abstraction inf√©rieur ou sup√©rieur en entr√©e de notre r√©seau neuronal. <br><br>  Nous n'entra√Ænerons aucune couche de MobileNet.  Nous <code>global_average_pooling2d_1</code> sortie de <code>global_average_pooling2d_1</code> et la transmettons en entr√©e √† notre petit mod√®le.  Pourquoi ai-je choisi cette couche particuli√®re?  Empiriquement.  J'ai fait quelques tests, et cette couche fonctionne plut√¥t bien. <br><br><h3>  D√©finition du mod√®le </h3><br>  La t√¢che initiale consistait √† classer l'image en trois classes: main, pied et autres mouvements.  Tout d'abord, r√©solvons le plus petit probl√®me: nous d√©terminerons s'il y a ou non un coup de main dans le cadre.  Il s'agit d'un probl√®me de classification binaire typique.  Pour cela, nous pouvons d√©finir le mod√®le suivant: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'@tensorflow/tfjs'</span></span>; const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span> })); model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br>  Un tel code d√©finit un mod√®le simple, une couche de <code>1024</code> unit√©s et l'activation <code>ReLU</code> , ainsi qu'une unit√© de sortie qui passe par la <code>sigmoid</code> activation <code>sigmoid</code> .  Ce dernier donne un nombre de <code>0</code> √† <code>1</code> , selon la probabilit√© de coup de main dans ce cadre. <br><br>  Pourquoi ai-je choisi <code>1024</code> unit√©s pour le deuxi√®me niveau et une vitesse d'entra√Ænement de <code>1e-6</code> ?  Eh bien, j'ai essay√© plusieurs options diff√©rentes et j'ai vu que ces options fonctionnaient mieux.  La m√©thode Spear ne semble pas √™tre la meilleure approche, mais dans une large mesure, c'est ainsi que les param√®tres hyperparam√©triques dans le travail d'apprentissage en profondeur - sur la base de notre compr√©hension du mod√®le, nous utilisons l'intuition pour mettre √† jour les param√®tres orthogonaux et v√©rifier empiriquement le fonctionnement du mod√®le. <br><br>  La m√©thode de <code>compile</code> compile les couches ensemble, pr√©parant le mod√®le pour la formation et l'√©valuation.  Ici, nous annon√ßons que nous voulons utiliser l'algorithme d'optimisation d' <code>adam</code> .  Nous d√©clarons √©galement que nous allons calculer la perte (perte) de l'entropie crois√©e et indiquer que nous voulons √©valuer la pr√©cision du mod√®le.  TensorFlow.js calcule ensuite la pr√©cision √† l'aide de la formule: <br><br> <code>Accuracy = (True Positives + True Negatives) / (Positives + Negatives)</code> <br> <br>  Si vous transf√©rez la formation √† partir du mod√®le MobileNet d'origine, vous devez d'abord la t√©l√©charger.  Comme il n'est pas pratique de former notre mod√®le sur plus de 3 000 images dans un navigateur, nous utiliserons Node.js et chargerons le r√©seau neuronal √† partir du fichier. <br><br>  T√©l√©chargez MobileNet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  Le catalogue contient le fichier <code>model.json</code> , qui contient l'architecture du mod√®le - couches, activations, etc.  Les fichiers restants contiennent des param√®tres de mod√®le.  Vous pouvez charger le mod√®le √† partir d'un fichier √† l'aide de ce code: <br><br><pre> <code class="python hljs">export const loadModel = <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> () =&gt; { const mn = new mobilenet.MobileNet(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); mn.path = `file://PATH/TO/model.json`; <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> mn.load(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (input): tf.Tensor1D =&gt; mn.infer(input, <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>) .reshape([<span class="hljs-number"><span class="hljs-number">1024</span></span>]); };</code> </pre> <br>  Notez que dans la m√©thode <code>loadModel</code> nous retournons une fonction qui accepte un tenseur unidimensionnel en entr√©e et retourne <code>mn.infer(input, Layer)</code> .  La m√©thode d' <code>infer</code> prend un tenseur et une couche comme arguments.  Le calque d√©termine le calque cach√© dont nous voulons la sortie.  Si vous ouvrez <a href="">model.json</a> et <code>global_average_pooling2d_1</code> , vous trouverez un tel nom sur l'une des couches. <br><br>  Vous devez maintenant cr√©er un ensemble de donn√©es pour l'apprentissage du mod√®le.  Pour ce faire, nous devons passer toutes les images via la m√©thode d'inf√©rence dans MobileNet et leur attribuer des √©tiquettes: <code>1</code> pour les images avec traits et <code>0</code> pour les images sans traits: <br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor1d( new Array(punches.length).fill(<span class="hljs-number"><span class="hljs-number">1</span></span>) .concat(new Array(others.length).fill(<span class="hljs-number"><span class="hljs-number">0</span></span>))); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br>  Dans le code ci-dessus, nous lisons d'abord les fichiers dans des r√©pertoires avec et sans hits.  Ensuite, nous d√©terminons le tenseur unidimensionnel contenant les √©tiquettes de sortie.  Si nous avons <code>n</code> images avec traits et <code>m</code> autres images, le tenseur aura <code>n</code> √©l√©ments avec une valeur de 1 et <code>m</code> √©l√©ments avec une valeur de 0. <br><br>  Dans <code>xs</code> nous <code>infer</code> r√©sultats de l'appel de la m√©thode d' <code>infer</code> pour des images individuelles.  Notez que pour chaque image, nous appelons la m√©thode <code>readInput</code> .  Voici sa mise en ≈ìuvre: <br><br><pre> <code class="python hljs">export const readInput = img =&gt; imageToInput(readImage(img), TotalChannels); const readImage = path =&gt; jpeg.decode(fs.readFileSync(path), true); const imageToInput = image =&gt; { const values = serializeImage(image); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.tensor3d(values, [image.height, image.width, <span class="hljs-number"><span class="hljs-number">3</span></span>], <span class="hljs-string"><span class="hljs-string">'int32'</span></span>); }; const serializeImage = image =&gt; { const totalPixels = image.width * image.height; const result = new Int32Array(totalPixels * <span class="hljs-number"><span class="hljs-number">3</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; totalPixels; i++) { result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>]; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result; };</code> </pre> <br>  <code>readInput</code> appelle d'abord la fonction <code>readImage</code> , puis d√©l√®gue son appel √† <code>imageToInput</code> .  La fonction <code>readImage</code> lit une image √† partir du disque, puis d√©code jpg √† partir du tampon √† l'aide du package <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">jpeg-js</a> .  Dans <code>imageToInput</code> nous convertissons l'image en un tenseur tridimensionnel. <br><br>  Par cons√©quent, pour chaque <code>i</code> de <code>0</code> √† <code>TotalImages</code> doit √™tre <code>ys[i]</code> √©gal √† <code>1</code> si <code>xs[i]</code> correspond √† l'image avec un hit, et <code>0</code> sinon. <br><br><h1>  Formation mod√®le </h1><br>  Maintenant, le mod√®le est pr√™t pour la formation!  Appelez la m√©thode d' <code>fit</code> : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.fit(xs, ys, { epochs: Epochs, batchSize: parseInt(((punches.length + others.length) * BatchSize).toFixed(<span class="hljs-number"><span class="hljs-number">0</span></span>)), callbacks: { onBatchEnd: <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> (_, logs) =&gt; { console.log(<span class="hljs-string"><span class="hljs-string">'Cost: %s, accuracy: %s'</span></span>, logs.loss.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>), logs.acc.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> tf.nextFrame(); } } });</code> </pre> <br>  Les appels de code ci-dessus <code>fit</code> √† trois arguments: <code>xs</code> , ys et l'objet de configuration.  Dans l'objet de configuration, nous d√©finissons le nombre d'√®res que le mod√®le, la taille du paquet et le rappel que TensorFlow.js g√©n√©rera apr√®s le traitement de chaque paquet seront form√©s. <br><br>  La taille du paquet d√©termine <code>xs</code> et <code>ys</code> pour former le mod√®le √† une √©poque.  Pour chaque √©poque, TensorFlow.js s√©lectionnera un sous-ensemble de <code>xs</code> et les √©l√©ments correspondants de <code>ys</code> , effectuera une distribution directe, recevra la sortie de la couche avec activation <code>sigmoid</code> , puis, en fonction de la perte, effectuera l'optimisation √† l'aide de l'algorithme <code>adam</code> . <br><br>  Apr√®s avoir d√©marr√© le script de formation, vous verrez un r√©sultat similaire √† celui ci-dessous: <br><br><pre>  Co√ªt: 0,84212, pr√©cision: 1,00000
 eta = 0,3&gt; ---------- acc = 1,00 perte = 0,84 Co√ªt: 0,79740, pr√©cision: 1,00000
 eta = 0,2 =&gt; --------- acc = 1,00 perte = 0,80 Co√ªt: 0,81533, pr√©cision: 1,00000
 eta = 0,2 ==&gt; -------- acc = 1,00 perte = 0,82 Co√ªt: 0,64303, pr√©cision: 0,50000
 eta = 0,2 ===&gt; ------- acc = 0,50 perte = 0,64 Co√ªt: 0,51377, pr√©cision: 0,00000
 eta = 0,2 ====&gt; ------ acc = 0,00 perte = 0,51 Co√ªt: 0,46473, pr√©cision: 0,50000
 eta = 0,1 =====&gt; ----- acc = 0,50 perte = 0,46 Co√ªt: 0,50872, pr√©cision: 0,00000
 eta = 0,1 ======&gt; ---- acc = 0,00 perte = 0,51 Co√ªt: 0,62556, pr√©cision: 1,00000
 eta = 0,1 =======&gt; --- acc = 1,00 perte = 0,63 Co√ªt: 0,65133, pr√©cision: 0,50000
 eta = 0,1 ========&gt; - acc = 0,50 perte = 0,65 Co√ªt: 0,63824, pr√©cision: 0,50000
 eta = 0.0 ===========&gt;
 293ms 14675us / step - acc = 0,60 perte = 0,65
 √âpoque 3/50
 Co√ªt: 0,44661, pr√©cision: 1,00000
 eta = 0,3&gt; ---------- acc = 1,00 perte = 0,45 Co√ªt: 0,78060, pr√©cision: 1,00000
 eta = 0,3 =&gt; --------- acc = 1,00 perte = 0,78 Co√ªt: 0,79208, pr√©cision: 1,00000
 eta = 0,3 ==&gt; -------- acc = 1,00 perte = 0,79 Co√ªt: 0,49072, pr√©cision: 0,50000
 eta = 0,2 ===&gt; ------- acc = 0,50 perte = 0,49 Co√ªt: 0,62232, pr√©cision: 1,00000
 eta = 0,2 ====&gt; ------ acc = 1,00 perte = 0,62 Co√ªt: 0,82899, pr√©cision: 1,00000
 eta = 0,2 =====&gt; ----- acc = 1,00 perte = 0,83 Co√ªt: 0,67629, pr√©cision: 0,50000
 eta = 0,1 ======&gt; ---- acc = 0,50 perte = 0,68 Co√ªt: 0,62621, pr√©cision: 0,50000
 eta = 0,1 =======&gt; --- acc = 0,50 perte = 0,63 Co√ªt: 0,46077, pr√©cision: 1,00000
 eta = 0,1 ========&gt; - acc = 1,00 perte = 0,46 Co√ªt: 0,62076, pr√©cision: 1,00000
 eta = 0.0 ===========&gt;
 304ms 15221us / step - acc = 0,85 perte = 0,63 </pre><br>  Remarquez comment la pr√©cision augmente avec le temps et la perte diminue. <br><br>  Sur mon jeu de donn√©es, le mod√®le apr√®s l'entra√Ænement a montr√© une pr√©cision de 92%.  Gardez √† l'esprit que la pr√©cision peut ne pas √™tre tr√®s √©lev√©e en raison du petit ensemble de donn√©es d'entra√Ænement. <br><br><h1>  Ex√©cution du mod√®le dans un navigateur </h1><br>  Dans la section pr√©c√©dente, nous avons form√© le mod√®le de classification binaire.  Maintenant, ex√©cutez-le dans un navigateur et connectez-vous √† <a href="">MK.js</a> ! <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> video = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'cam'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> Layer = <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> mobilenetInfer = <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">m</span></span></span><span class="hljs-function"> =&gt;</span></span> (p): tf.Tensor&lt;tf.Rank&gt; =&gt; m.infer(p, Layer); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> canvas = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'canvas'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> scale = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'crop'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> ImageSize = { <span class="hljs-attr"><span class="hljs-attr">Width</span></span>: <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-attr"><span class="hljs-attr">Height</span></span>: <span class="hljs-number"><span class="hljs-number">56</span></span> }; navigator.mediaDevices .getUserMedia({ <span class="hljs-attr"><span class="hljs-attr">video</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">audio</span></span>: <span class="hljs-literal"><span class="hljs-literal">false</span></span> }) .then(<span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">stream</span></span></span><span class="hljs-function"> =&gt;</span></span> { video.srcObject = stream; });</code> </pre> <br>  Il y a plusieurs d√©clarations dans le code ci-dessus: <br><br><ul><li> <code>video</code>     <code>HTML5 video</code>   </li><li> <code>Layer</code>     MobileNet,                  </li><li> <code>mobilenetInfer</code> ‚Äî ,    MobileNet    .              MobileNet </li><li> <code>canvas</code>    <code>HTML5 canvas</code> ,          </li><li> <code>scale</code> ‚Äî   <code>canvas</code> ,       </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apr√®s cela, nous obtenons le flux vid√©o de la cam√©ra de l'utilisateur et le d√©finissons comme source pour l'√©l√©ment </font></font><code>video</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'√©tape suivante consiste √† impl√©menter un filtre en niveaux de gris qui accepte </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et convertit son contenu:</font></font><br><br><pre> <code class="python hljs">const grayscale = (canvas: HTMLCanvasElement) =&gt; { const imageData = canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).getImageData(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.height); const data = imageData.data; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; data.length; i += <span class="hljs-number"><span class="hljs-number">4</span></span>) { const avg = (data[i] + data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] + data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>]) / <span class="hljs-number"><span class="hljs-number">3</span></span>; data[i] = avg; data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] = avg; data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>] = avg; } canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).putImageData(imageData, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Dans la prochaine √©tape, nous connecterons le mod√®le avec MK.js: </font></font><br><br><pre> <code class="python hljs">let mobilenet: (p: any) =&gt; tf.Tensor&lt;tf.Rank&gt;; tf.loadModel(<span class="hljs-string"><span class="hljs-string">'http://localhost:5000/model.json'</span></span>).then(model =&gt; { mobileNet .load() .then((mn: any) =&gt; mobilenet = mobilenetInfer(mn)) .then(startInterval(mobilenet, model)); });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans le code ci-dessus, nous chargeons d'abord le mod√®le que nous avons form√© ci-dessus, puis t√©l√©chargeons MobileNet. </font><font style="vertical-align: inherit;">Nous passons MobileNet dans la m√©thode </font></font><code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pour obtenir le moyen de calculer la sortie de la couche r√©seau cach√©e. </font><font style="vertical-align: inherit;">Apr√®s cela, nous appelons la m√©thode </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">avec deux r√©seaux comme arguments.</font></font><br><br><pre> <code class="python hljs">const startInterval = (mobilenet, model) =&gt; () =&gt; { setInterval(() =&gt; { canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).drawImage(video, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); grayscale(scale .getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>) .drawImage( canvas, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.width / (ImageSize.Width / ImageSize.Height), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, ImageSize.Width, ImageSize.Height )); const [punching] = Array.<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>(( model.predict(mobilenet(tf.fromPixels(scale))) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D) .dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Float32Array); const detect = (window <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punching &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) detect &amp;&amp; detect.onPunch(); }, <span class="hljs-number"><span class="hljs-number">100</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La partie la plus int√©ressante commence dans la m√©thode </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">! Tout d'abord, nous ex√©cutons un intervalle o√π tout le monde </font></font><code>100ms</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">appelle une fonction anonyme. Dans celui-ci, la </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vid√©o avec l'image actuelle est rendue en </font><font style="vertical-align: inherit;">premier dessus </font><font style="vertical-align: inherit;">. Ensuite, nous r√©duisons la taille du cadre </font></font><code>100x56</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et lui appliquons un filtre en niveaux de gris. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'√©tape suivante consiste √† transf√©rer la trame vers MobileNet, √† obtenir la sortie de la couche cach√©e souhait√©e et √† la transf√©rer comme entr√©e dans la m√©thode de </font></font><code>predict</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">notre mod√®le. Cela renvoie un tenseur avec un √©l√©ment. En utilisant, </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nous obtenons la valeur du tenseur et l'assignons √† une constante </font></font><code>punching</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enfin, nous v√©rifions: si la probabilit√© d'un coup de main d√©passe </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, alors nous appelons la m√©thode </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">globale de l'objet </font></font><code>Detect</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. MK.js fournit un objet global avec trois m√©thodes:</font></font><code>onKick</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et </font></font><code>onStand</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que nous pouvons utiliser pour contr√¥ler l'un des personnages.</font></font><br><br>  C'est fait!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Voici le r√©sultat! </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/83e/05c/e0e/83e05ce0e9304865bb6aee072204902b.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Reconnaissance des coups de pied et des bras avec classification N </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans la section suivante, nous cr√©erons un mod√®le plus intelligent: un r√©seau neuronal qui reconna√Æt les coups de poing, les coups de pied et d'autres images. </font><font style="vertical-align: inherit;">Cette fois, commen√ßons par pr√©parer l'ensemble de formation:</font></font><br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const kicks = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Kicks) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Kicks}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor2d( new Array(punches.length) .fill([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]) .concat(new Array(kicks.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>])) .concat(new Array(others.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])), [punches.length + kicks.length + others.length, <span class="hljs-number"><span class="hljs-number">3</span></span>] ); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(kicks.map((path: string) =&gt; mobileNet(readInput(path)))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme pr√©c√©demment, nous avons d'abord lu les catalogues avec des images de coups de poing √† la main, au pied et d'autres images. Apr√®s cela, contrairement √† la derni√®re fois, nous formons le r√©sultat attendu sous la forme d'un tenseur bidimensionnel et non unidimensionnel. Si nous avons </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> images avec un coup de pied, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> images avec un coup de pied et </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> autres images, alors le tenseur </font></font><code>ys</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aura des </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√©l√©ments avec une valeur </font></font><code>[1, 0, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, des </font></font><code>m</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√©l√©ments avec une valeur </font></font><code>[0, 1, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et des </font></font><code>k</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√©l√©ments avec une valeur </font></font><code>[0, 0, 1]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un vecteur d' </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√©l√©ments dans lequel il y a des </font></font><code>n - 1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√©l√©ments avec une valeur </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et un √©l√©ment avec une valeur </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nous appelons un vecteur unitaire (un vecteur chaud). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apr√®s cela, nous formons le tenseur d'entr√©e</font></font><code>xs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">empiler la sortie de chaque image de MobileNet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ici, vous devez mettre √† jour la d√©finition du mod√®le:</font></font><br><br><pre> <code class="python hljs">const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">3</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'softmax'</span></span> })); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Les deux seules diff√©rences par rapport au mod√®le pr√©c√©dent sont: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Le nombre d'unit√©s dans la couche de sortie </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Activations dans la couche de sortie </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Il y a trois unit√©s dans la couche de sortie, car nous avons trois cat√©gories d'images diff√©rentes: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Coup de main </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Coup de pied </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Autre </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'activation est d√©clench√©e sur ces trois unit√©s </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, qui convertit leurs param√®tres en un tenseur √† trois valeurs. Pourquoi trois unit√©s pour la couche de sortie? Chacune des trois valeurs pour trois classes peut √™tre repr√©sent√© par deux bits: </font></font><code>00</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>01</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>10</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. La somme des valeurs du tenseur cr√©√© </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">est 1, c'est-√†-dire que nous n'obtiendrons jamais 00, nous ne pourrons donc pas classer les images d'une des classes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apr√®s avoir entra√Æn√© le mod√®le au fil des </font></font><code>500</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√¢ges, j'ai atteint une pr√©cision d'environ 92%! Ce n'est pas mal, mais n'oubliez pas que la formation s'est d√©roul√©e sur un petit ensemble de donn√©es. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La prochaine √©tape consiste √† ex√©cuter le mod√®le dans un navigateur! √âtant donn√© que la logique est tr√®s similaire √† l'ex√©cution du mod√®le pour la classification binaire, jetez un ≈ìil √† la derni√®re √©tape, o√π l'action est s√©lectionn√©e en fonction de la sortie du mod√®le:</font></font><br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [punch, kick, nothing] = <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span>.from((model.predict( mobilenet(tf.fromPixels(scaled)) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D).dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-built_in"><span class="hljs-built_in">Float32Array</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> detect = (<span class="hljs-built_in"><span class="hljs-built_in">window</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (nothing &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (kick &gt; punch &amp;&amp; kick &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) { detect.onKick(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punch &gt; kick &amp;&amp; punch &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) detect.onPunch();</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous appelons d'abord MobileNet avec un cadre r√©duit dans des tons de gris, puis nous transf√©rons le r√©sultat de notre mod√®le form√©. </font><font style="vertical-align: inherit;">Le mod√®le renvoie un tenseur unidimensionnel, que nous convertissons en </font></font><code>Float32Array</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Dans l'√©tape suivante, nous utilisons </font></font><code>Array.from</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pour convertir un tableau typ√© en un tableau JavaScript. </font><font style="vertical-align: inherit;">Ensuite, nous extrayons les probabilit√©s qu'un tir avec une main, un coup de pied ou rien soit pr√©sent sur le cadre. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si la probabilit√© du troisi√®me r√©sultat d√©passe </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nous revenons. </font><font style="vertical-align: inherit;">Sinon, si la probabilit√© d'un coup de pied est plus √©lev√©e </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nous envoyons une commande de coup de pied √† MK.js. </font><font style="vertical-align: inherit;">Si la probabilit√© d'un coup de pied est plus √©lev√©e </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et plus √©lev√©e que la probabilit√© d'un coup de pied, alors envoyez l'action d'un coup de pied. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En g√©n√©ral, c'est tout! </font><font style="vertical-align: inherit;">Le r√©sultat est pr√©sent√© ci-dessous:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/168/f71/f3d/168f71f3df8d267bec3e0791d5857c64.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Reconnaissance de l'action </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si vous collectez un ensemble de donn√©es important et vari√© sur les personnes qui battent avec leurs mains et leurs pieds, vous pouvez cr√©er un mod√®le qui fonctionne parfaitement sur des cadres individuels. Mais est-ce suffisant? Et si nous voulons aller encore plus loin et distinguer deux types de coups de pied diff√©rents: d'un virage et d'un dos (coup de pied arri√®re). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme on peut le voir dans les images ci-dessous, √† un certain moment sous un certain angle, les deux traits se ressemblent: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c1/567/5bf/6c15675bf7b8c238e7ce9d5aaefeea80.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a60/e3c/dba/a60e3cdba0eb3ecbc8730c39bc6c95b2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mais si vous regardez la performance, les mouvements sont compl√®tement diff√©rents: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e72/28b/fe8/e7228bfe8cfe9bbe73f9011d94778a7a.gif"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">comment pouvez-vous former un r√©seau neuronal √† analyser une s√©quence d'images, et pas seulement une image? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√Ä cette fin, nous pouvons explorer une autre classe de r√©seaux de neurones, appel√©s r√©seaux de neurones r√©currents (RNN). Par exemple, les RNN sont parfaits pour travailler avec des s√©ries chronologiques:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Traitement du langage naturel (NLP), o√π chaque mot d√©pend du pr√©c√©dent et du suivant </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Pr√©dire la page suivante en fonction de votre historique de navigation </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Reconnaissance de trame </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> L'impl√©mentation d'un tel mod√®le d√©passe le cadre de cet article, mais regardons un exemple d'architecture pour avoir une id√©e de la fa√ßon dont tout cela fonctionnera ensemble. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La puissance de RNN </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le sch√©ma ci-dessous montre le mod√®le de reconnaissance des actions: </font></font><br><br><img src="https://habrastorage.org/webt/kz/oq/ie/kzoqieod8t9nhs_taapnhpr_y0c.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous prenons les derni√®res </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">images de la vid√©o et les transf√©rons sur CNN. </font><font style="vertical-align: inherit;">La sortie CNN pour chaque trame est transmise comme entr√©e RNN. </font><font style="vertical-align: inherit;">Un r√©seau de neurones r√©current d√©terminera les relations entre les cadres individuels et reconna√Ætra √† quelle action ils correspondent.</font></font><br><br><h1>  Conclusion </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dans cet article, nous avons d√©velopp√© un mod√®le de classification d'images. √Ä cet effet, nous avons collect√© un ensemble de donn√©es: nous avons extrait des images vid√©o et les avons divis√©es manuellement en trois cat√©gories. Ensuite, les donn√©es ont √©t√© </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">augment√©es en</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ajoutant des images √† l'aide d' </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">imgaug</font></a><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apr√®s cela, nous avons expliqu√© ce qu'est le transfert d'apprentissage et utilis√© le mod√®le MobileNet form√© √† partir du </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">package @ tensorflow-models / mobilenet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √† nos propres fins </font><font style="vertical-align: inherit;">. Nous avons charg√© MobileNet √† partir d'un fichier dans le processus Node.js et form√© une couche dense suppl√©mentaire o√π les donn√©es ont √©t√© aliment√©es √† partir de la couche MobileNet cach√©e. Apr√®s l'entra√Ænement, nous avons atteint une pr√©cision de plus de 90%! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour utiliser ce mod√®le dans un navigateur, nous l'avons t√©l√©charg√© avec MobileNet et commenc√© √† cat√©goriser les images de la webcam de l'utilisateur toutes les 100 ms. Nous avons connect√© le mod√®le avec le jeu</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MK.js</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> et utilis√© la sortie du mod√®le pour contr√¥ler l'un des caract√®res. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enfin, nous avons examin√© comment am√©liorer le mod√®le en le combinant avec un r√©seau neuronal r√©current pour reconna√Ætre les actions. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">J'esp√®re que vous n'avez pas moins appr√©ci√© ce petit projet que moi! </font><font style="vertical-align: inherit;">‚Äç</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr428019/">https://habr.com/ru/post/fr428019/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr428003/index.html">Conception r√©active: maintien de la forme des √©l√©ments de balisage</a></li>
<li><a href="../fr428005/index.html">Trois fa√ßons efficaces d'aggraver une catastrophe de relations publiques</a></li>
<li><a href="../fr428007/index.html">D√©j√† pas un pc luggable, pas encore un notebook: Laptop TOSHIBA T3100 / 20</a></li>
<li><a href="../fr428009/index.html">Equifax: un an apr√®s la plus grosse fuite de donn√©es</a></li>
<li><a href="../fr428011/index.html">Chansons de zombies spatiaux</a></li>
<li><a href="../fr428021/index.html">Scelle contre le r√©seau neuronal. Ou s√©lectionnez et ex√©cutez un r√©seau de neurones pour reconna√Ætre les objets sur le Raspberry Zero</a></li>
<li><a href="../fr428023/index.html">Bases de la s√©curit√© √©lectrique dans la conception d'appareils √©lectroniques</a></li>
<li><a href="../fr428025/index.html">Connexion d'un fichier d'√©change (SWAP) sous MAC OS X lors de l'utilisation d'un SSD externe comme syst√®me</a></li>
<li><a href="../fr428027/index.html">Comment j'ai essay√© de faire un analyseur statique GLSL (et ce qui n'a pas fonctionn√©)</a></li>
<li><a href="../fr428029/index.html">√âv√©nements num√©riques √† Moscou du 29 octobre au 4 novembre</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>