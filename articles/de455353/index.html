<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõå üßìüèª üîá Hardwarebeschleunigung von tiefen neuronalen Netzen: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP und andere Buchstaben üëÜüèΩ üë¥üèª üéÉ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Am 14. Mai, als Trump sich darauf vorbereitete, alle Hunde auf Huawei zu starten, sa√ü ich friedlich in Shenzhen auf der Huawei STW 2019 - einer gro√üen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Hardwarebeschleunigung von tiefen neuronalen Netzen: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP und andere Buchstaben</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455353/"><img src="https://habrastorage.org/getpro/habr/post_images/1d8/7a7/de6/1d87a7de6c72f1f712049ce550978d7c.png"><br><br>  Am 14. Mai, als Trump sich darauf vorbereitete, alle Hunde auf Huawei zu starten, sa√ü ich friedlich in Shenzhen auf der Huawei STW 2019 - einer gro√üen Konferenz f√ºr 1000 Teilnehmer -, die Berichte von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Philip Wong</a> , Vizepr√§sident der TSMC-Forschung √ºber die Aussichten von Nicht-von-Neumann-Computern, enthielt Architekturen und Heng Liao, Huawei Fellow, Chefwissenschaftler Huawei 2012 Lab, √ºber die Entwicklung einer neuen Architektur von Tensorprozessoren und Neuroprozessoren.  Wenn Sie wissen, stellt TSMC neuronale Beschleuniger f√ºr Apple und Huawei mithilfe der 7-nm-Technologie her (die nur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wenige Menschen besitzen</a> ), und Huawei ist bereit, in Bezug auf Neuroprozessoren mit Google und NVIDIA zu konkurrieren. <br><br>  Google in China ist verboten. Ich habe mir nicht die M√ºhe gemacht, ein VPN auf dem Tablet zu <s>installieren.</s> Deshalb habe ich Yandex <s>patriotisch</s> verwendet, um zu sehen, wie die Situation bei anderen Herstellern von √§hnlichem Eisen ist und was im Allgemeinen passiert.  Im Allgemeinen habe ich die Situation beobachtet, aber erst nach diesen Berichten wurde mir klar, wie gro√ü die Revolution in den Eingeweiden der Unternehmen und in der Stille der wissenschaftlichen R√§ume vorbereitet wurde. <br><br>  Allein im letzten Jahr wurden mehr als 3 Milliarden US-Dollar in das Thema investiert.  Google hat die neuronalen Netze seit langem zu einem strategischen Bereich erkl√§rt und baut seinen Hardware- und Software-Support aktiv auf.  NVIDIA sp√ºrt, dass der Thron schwankt, und unternimmt fantastische Anstrengungen in Bibliotheken zur Beschleunigung neuronaler Netze und neuer Hardware.  Intel gab 2016 0,8 Milliarden aus, um zwei Unternehmen zu kaufen, die an der Hardwarebeschleunigung neuronaler Netze beteiligt sind.  Und dies trotz der Tatsache, dass die Hauptk√§ufe noch nicht begonnen haben und die Anzahl der Spieler f√ºnfzig √ºberschritten hat und schnell w√§chst. <br><br><div style="text-align:center;"><img width="66%" src="https://habrastorage.org/getpro/habr/post_images/74c/308/a37/74c308a372700574cc0e29f13347ede5.png"></div><br>  TPU, VPU, IPU, DPU, NPU, RPU, NNP - was bedeutet das alles und wer wird gewinnen?  Versuchen wir es herauszufinden.  Wen k√ºmmert es - Willkommen bei Katze! <br><a name="habracut"></a><br><hr>  <b><font color="#ff0000">Haftungsausschluss: Der</font></b> Autor musste die Videoverarbeitungsalgorithmen f√ºr eine effektive Implementierung auf ASIC komplett neu schreiben, und die Kunden haben Prototypen auf FPGA erstellt, sodass eine Vorstellung von der Tiefe der Unterschiede in den Architekturen besteht.  Der Autor hat jedoch in letzter Zeit nicht direkt mit Eisen gearbeitet.  Aber er rechnet damit, dass er sich damit befassen muss. <br><br><h2>  Hintergrund der Probleme </h2><br>  Die Anzahl der erforderlichen Berechnungen w√§chst schnell, die Leute w√ºrden gerne mehr Ebenen und Architekturoptionen verwenden und aktiver mit Hyperparametern spielen, aber ... das h√§ngt von der Leistung ab.  Gleichzeitig zum Beispiel mit dem Produktivit√§tswachstum der guten alten Prozessoren - gro√üe Probleme.  Alle guten Dinge gehen zu Ende: Wie Sie wissen, l√§uft das Moore-Gesetz aus und die Wachstumsrate der Prozessorleistung sinkt: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/704/8e8/bab/7048e8bab326e1f4f62058e5f3a925f4.png"></div><br>  <i>Berechnungen der tats√§chlichen Leistung von Ganzzahloperationen auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SPECint im</a> Vergleich zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VAX11-780</a> , im Folgenden h√§ufig eine logarithmische Skala</i> <br><br>  Wenn von Mitte der 80er bis Mitte der 2000er Jahre - in den gesegneten Jahren der Bl√ºtezeit der Computer - das Wachstum durchschnittlich 52% pro Jahr betrug, ist es in den letzten Jahren auf 3% pro Jahr zur√ºckgegangen.  Und dies ist ein Problem (eine √úbersetzung eines k√ºrzlich erschienenen Artikels von Patriarch John Hennessey √ºber die Probleme und Perspektiven der modernen Architektur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">war auf Habr√©</a> ). <br><br>  Es gibt viele Gr√ºnde, zum Beispiel, dass die H√§ufigkeit von Prozessoren nicht mehr w√§chst: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/a49/5d6/be5/a495d6be5c00ce436bb4c2f1f7f356fc.png"></div><br>  Es wurde schwieriger, die Gr√∂√üe von Transistoren zu reduzieren.  Das letzte Ungl√ºck, das die Produktivit√§t drastisch reduziert (einschlie√ülich der Leistung bereits freigegebener CPUs), ist (Trommelwirbel) ... richtig, Sicherheit.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Meltdown</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spectre</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">andere</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sicherheitsl√ºcken</a> verursachen enorme Sch√§den an der Wachstumsrate der CPU-Verarbeitungsleistung ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein Beispiel f√ºr das Deaktivieren von Hyperthreading</a> (!)).  Das Thema ist popul√§r geworden und neue Schwachstellen dieser Art werden <i>fast monatlich gefunden</i> .  Und das ist eine Art Albtraum, weil es in Bezug auf die Leistung weh tut. <br><br>  Gleichzeitig ist die Entwicklung vieler Algorithmen fest mit dem bekannten Wachstum der Prozessorleistung verbunden.  Zum Beispiel sind viele Forscher heutzutage nicht besorgt √ºber die Geschwindigkeit von Algorithmen - sie werden sich etwas einfallen lassen.  Und es w√§re sch√∂n, wenn man lernt - Netzwerke werden gro√ü und "schwierig" zu benutzen.  Dies zeigt sich insbesondere im Video, f√ºr das die meisten Ans√§tze im Prinzip nicht mit hoher Geschwindigkeit anwendbar sind.  Und sie machen oft nur in Echtzeit Sinn.  Dies ist auch ein Problem. <br><br>  Ebenso werden neue Komprimierungsstandards entwickelt, die eine Erh√∂hung der Decoderleistung implizieren.  Und wenn die Prozessorleistung nicht w√§chst?  Die √§ltere Generation erinnert sich, wie es in den 2000er Jahren Probleme gab, hochaufl√∂sende Videos im damals frischen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">H.264</a> auf √§lteren Computern <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">abzuspielen</a> .  Ja, die Qualit√§t war bei einer kleineren Gr√∂√üe besser, aber bei schnellen Szenen hing das Bild oder der Ton war zerrissen.  Ich muss mit den Entwicklern des neuen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VVC / H.266 kommunizieren</a> (eine Ver√∂ffentlichung ist f√ºr n√§chstes Jahr geplant).  Sie werden sie nicht beneiden. <br><br>  Was bereitet uns das kommende Jahrhundert angesichts des R√ºckgangs der Wachstumsrate der Prozessorleistung in Bezug auf neuronale Netze vor? <br><br><h2>  CPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/f1e/b44/44b/f1eb4444b3e5e320447c6f65fa4ef14c.png"><br><br>  Eine normale <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CPU</a> ist ein gro√üer Brecher, der seit Jahrzehnten perfektioniert wird.  Leider f√ºr andere Aufgaben. <br><br>  Wenn wir mit neuronalen Netzen arbeiten, insbesondere mit tiefen, kann unser Netzwerk selbst Hunderte von Megabyte belegen.  Beispielsweise sind die Speicheranforderungen von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Objekterkennungsnetzwerken</a> wie folgt: <br><div class="scrollable-table"><table><tbody><tr><td>  Modell <br></td><td>  Eingabegr√∂√üe <br></td><td>  param Speicher <br></td><td>  Funktionsspeicher <br></td></tr><tr><td>  <a href="">rfcn-res50-pascal</a> <br></td><td>  600 x 850 <br></td><td>  122 MB <br></td><td>  1 GB <br></td></tr><tr><td>  <a href="">rfcn-res101-pascal</a> <br></td><td>  600 x 850 <br></td><td>  194 MB <br></td><td>  2 GB <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-300</a> <br></td><td>  300 x 300 <br></td><td>  100 MB <br></td><td>  116 MB <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-512</a> <br></td><td>  512 x 512 <br></td><td>  104 MB <br></td><td>  337 MB <br></td></tr><tr><td>  <a href="">ssd-pascal-mobilet-ft</a> <br></td><td>  300 x 300 <br></td><td>  22 MB <br></td><td>  37 MB <br></td></tr><tr><td>  <a href="">schneller-rcnn-vggvd-pascal</a> <br></td><td>  600 x 850 <br></td><td>  523 MB <br></td><td>  600 MB <br></td></tr></tbody></table></div><br><br>  Nach unserer Erfahrung k√∂nnen die Koeffizienten eines tiefen neuronalen Netzwerks zur Verarbeitung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">durchscheinender Grenzen</a> 150 bis 200 MB einnehmen.  Kollegen im neuronalen Netzwerk bestimmen das Alter und Geschlecht der Gr√∂√üe der Koeffizienten in der Gr√∂√üenordnung von 50 MB.  Und w√§hrend der Optimierung f√ºr die mobile Version mit reduzierter Genauigkeit - ca. 25 MB (float32‚áífloat16). <br><br>  Gleichzeitig wird der Verz√∂gerungsgraph beim Zugriff auf den Speicher in Abh√§ngigkeit von der Gr√∂√üe der Daten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ungef√§hr so</a> verteilt (horizontale Skala ist logarithmisch): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1f4/2aa/134/1f42aa13427972cd1762383b33cfe974.png"></div><br><br>  Das hei√üt,  Bei einer Zunahme des Datenvolumens um mehr als 16 MB erh√∂ht sich die Verz√∂gerung um das 50-fache oder mehr, was sich negativ auf die Leistung auswirkt.  Tats√§chlich wartet die CPU die meiste Zeit, wenn sie mit tiefen neuronalen Netzen arbeitet, <s>dumm</s> auf Daten.  Interessant sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Daten</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intel</a> zur Beschleunigung verschiedener Netzwerke, bei denen die Beschleunigung tats√§chlich nur dann erfolgt, wenn das Netzwerk klein wird (z. B. aufgrund der Quantisierung von Gewichten), um zusammen mit den verarbeiteten Daten zumindest teilweise in den Cache zu gelangen.  Beachten Sie, dass der Cache einer modernen CPU bis zur H√§lfte der Prozessorenergie verbraucht.  Im Fall von schweren neuronalen Netzen ist es unwirksam und arbeitet unangemessen teure Heizung. <br><br><div class="spoiler">  <b class="spoiler_title">F√ºr Anh√§nger neuronaler Netze auf der CPU</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Laut</a> unseren internen Tests verliert sogar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intel OpenVINO</a> die Implementierung des Matrix-Multiplikations- + NNPACK-Frameworks auf vielen Netzwerkarchitekturen (insbesondere auf einfachen Architekturen, bei denen die Bandbreite f√ºr die Echtzeitdatenverarbeitung im Single-Threaded-Modus wichtig ist).  Ein solches Szenario ist f√ºr verschiedene Klassifizierer von Objekten im Bild relevant (bei denen das neuronale Netzwerk mehrmals ausgef√ºhrt werden muss - 50 bis 100 in Bezug auf die Anzahl der Objekte im Bild), und der Aufwand f√ºr das Starten von OpenVINO wird unangemessen hoch. <br></div></div><br>  <b>Vorteile:</b> <br><br><ul><li>  "Jeder hat es" und ist normalerweise unt√§tig, d.h.  relativ niedriger <i>Einstiegspreis</i> f√ºr Abrechnung und Implementierung. <br></li><li>  Es gibt separate Nicht-CV-Netzwerke, die gut in die CPU passen. Kollegen nennen sie beispielsweise Wide &amp; Deep und GNMT. <br></li></ul><br>  <b>Minus:</b> <br><ul><li>  Die CPU ist ineffizient, wenn mit tiefen neuronalen Netzen gearbeitet wird (wenn die Anzahl der Netzwerkschichten und die Gr√∂√üe der Eingabedaten gro√ü sind), funktioniert alles schmerzhaft langsam. <br></li></ul><br><h2>  GPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/aee/06a/d5b/aee06ad5beab0896005e75769b3ba910.png"><br><br>  Das Thema ist bekannt, daher skizzieren wir kurz die Hauptsache.  Bei neuronalen Netzen hat die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GPU</a> bei massiv parallelen Aufgaben einen erheblichen Leistungsvorteil: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/717/03d/d9b/71703dd9bce82918d1e0c7138a09adf0.png"></div><br>  Achten Sie darauf, wie das 72-Kern- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Xeon Phi 7290</a> gegl√ºht wird, w√§hrend das "Blau" auch das Server-Xeon ist, d. H.  Intel gibt nicht so einfach auf, was weiter unten erl√§utert wird.  Noch wichtiger ist jedoch, dass der Speicher von Grafikkarten urspr√ºnglich f√ºr eine etwa f√ºnfmal h√∂here Leistung ausgelegt war.  In neuronalen Netzen ist das Rechnen mit Daten √§u√üerst einfach.  Ein paar elementare Aktionen, und wir brauchen neue Daten.  Infolgedessen ist die Datenzugriffsgeschwindigkeit f√ºr den effizienten Betrieb eines neuronalen Netzwerks entscheidend.  Ein Hochgeschwindigkeitsspeicher ‚Äûan Bord‚Äú der GPU und ein flexibleres Cache-Verwaltungssystem als auf der CPU k√∂nnen dieses Problem l√∂sen: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/30a/d1b/07e/30ad1b07e7b22d97dc3372e4351c2e3c.png"></div><br><br>  Tim Detmers unterst√ºtzt seit mehreren Jahren die interessante Rezension <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûWelche GPUs f√ºr Deep Learning: Meine Erfahrungen und Ratschl√§ge f√ºr die Verwendung von GPUs beim Deep Learning‚Äú</a> .  Es ist klar, dass Tesla und Titans f√ºr das Training regieren, obwohl der Unterschied in den Architekturen interessante Ausbr√ºche verursachen kann, zum Beispiel bei wiederkehrenden neuronalen Netzen (und der Marktf√ºhrer im Allgemeinen ist TPU, Hinweis f√ºr die Zukunft): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6e6/bb0/43a/6e6bb043aab85679d1ddf01028e19728.png"></div><br>  Es gibt jedoch ein √§u√üerst n√ºtzliches Leistungsdiagramm f√ºr den Dollar, wo auf dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RTX-</a> Pferd (h√∂chstwahrscheinlich aufgrund <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">seiner</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorkerne</a> ), wenn Sie gen√ºgend Speicher daf√ºr haben, nat√ºrlich: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f13/338/032/f13338032ca812397cbbe0228bda9ed5.png"></div><br>  Nat√ºrlich sind die Rechenkosten wichtig.  Der zweite Platz der ersten Bewertung und der letzte der zweiten - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tesla V100</a> wird f√ºr 700.000 Rubel verkauft, wie 10 ‚Äûnormale‚Äú Computer (+ der teure Infiniband-Switch, wenn Sie auf mehreren Knoten trainieren m√∂chten).  Echte V100 und funktioniert f√ºr zehn.  Die Menschen sind bereit, f√ºr eine sp√ºrbare Beschleunigung des Lernens zu viel zu bezahlen. <br><br>  Insgesamt zusammenfassen! <br><br>  <b>Vorteile:</b> <br><ul><li>  Kardinal - 10-100 mal - Beschleunigung im Vergleich zur CPU. <br></li><li>  Extrem effektiv f√ºr das Training (und etwas weniger effektiv f√ºr den Gebrauch). <br></li></ul><br>  <b>Minus:</b> <br><ul><li>  Die Kosten f√ºr Top-End-Grafikkarten (die √ºber gen√ºgend Speicher verf√ºgen, um gro√üe Netzwerke zu trainieren) √ºbersteigen die Kosten f√ºr den Rest des Computers ... <br></li></ul><br><h2>  FPGA </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/0d5/554/90b/0d555490b015730051341cc857d14606.png"><br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FPGA</a> ist schon interessanter.  Dies ist ein Netzwerk von mehreren Millionen programmierbaren Bl√∂cken, die wir auch programmgesteuert miteinander verbinden k√∂nnen.  Das Netzwerk und die Bl√∂cke <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sehen</a> ungef√§hr so ‚Äã‚Äãaus (der Engpass ist der Engpass, achten Sie darauf, noch einmal vor dem Chipspeicher, aber es ist einfacher, was weiter unten beschrieben wird): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0d8/8f9/789/0d88f9789f60720da892b7acae7ab8ec.png"><br>  Nat√ºrlich ist es sinnvoll, FPGA bereits in der Phase der Verwendung eines neuronalen Netzwerks zu verwenden (in den meisten F√§llen ist nicht gen√ºgend Speicher f√ºr das Training vorhanden).  Dar√ºber hinaus hat das Thema der Ausf√ºhrung auf FPGA nun begonnen, sich aktiv zu entwickeln.  Hier ist zum Beispiel das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fpgaConvNet-Framework</a> , das die Verwendung von CNN auf FPGAs erheblich beschleunigen und den Stromverbrauch senken kann. <br><br>  Das Hauptvorteil von FPGA ist, dass wir das Netzwerk direkt in den Zellen speichern k√∂nnen, d. H.  Ein d√ºnner Fleck in Form von Hunderten von Megabyte derselben Daten, die 25 Mal pro Sekunde (f√ºr Videos) in dieselbe Richtung √ºbertragen werden, verschwindet auf magische Weise.  Dies erm√∂glicht eine niedrigere Taktrate und das Fehlen von Caches anstelle einer geringeren Leistung, um eine sp√ºrbare Steigerung zu erzielen.  Ja, und den Energieverbrauch f√ºr die <s>globale Erw√§rmung</s> pro Berechnungseinheit drastisch reduzieren. <br><br>  Intel beteiligte sich aktiv an dem Prozess und ver√∂ffentlichte letztes Jahr das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenVINO Toolkit</a> im Open Source- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Format</a> , das das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Learning Deployment Toolkit</a> (Teil von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenCV</a> ) enth√§lt.  Dar√ºber hinaus sieht die Leistung von FPGAs in verschiedenen Grids sehr interessant aus, und der Vorteil von FPGAs im Vergleich zu GPUs (obwohl integrierte Intel-GPUs) ist ziemlich bedeutend: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/8ba/a66/37e/8baa6637e904732e4e883a196b8d803d.png"></div><br>  Was die Seele des Autors besonders w√§rmt - FPS werden verglichen, d.h.  Bilder pro Sekunde ist die praktischste Metrik f√ºr Videos.  Angesichts der Tatsache, dass Intel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Altera</a> , den zweitgr√∂√üten Player auf dem FPGA-Markt, im Jahr 2015 gekauft hat, bietet die Grafik gute Denkanst√∂√üe. <br><br>  Und offensichtlich ist die Eintrittsbarriere f√ºr solche Architekturen h√∂her, so dass einige Zeit vergehen muss, bis praktische Tools erscheinen, die die grundlegend andere FPGA-Architektur effektiv ber√ºcksichtigen.  Aber das Potenzial der Technologie zu untersch√§tzen, lohnt sich nicht.  Es sind schmerzhaft viele d√ºnne Stellen, die sie stickt. <br><br>  Schlie√ülich betonen wir, dass das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Programmieren von FPGAs</a> eine eigenst√§ndige Kunst ist.  Daher wird das Programm dort nicht ausgef√ºhrt, und alle Berechnungen werden in Bezug auf Datenstr√∂me, Stream-Verz√∂gerungen (die die Leistung beeintr√§chtigen) und verwendete Gates (die immer fehlen) durchgef√ºhrt.  Um effektiv programmieren zu k√∂nnen, m√ºssen Sie daher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ihre eigene Firmware</a> (im neuronalen Netzwerk zwischen Ihren Ohren) gr√ºndlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√§ndern</a> .  Bei guter Effizienz wird dies √ºberhaupt nicht erreicht.  Die neuen Frameworks werden jedoch bald den externen Unterschied vor den Forschern verbergen. <br><br>  <b>Vorteile:</b> <br><br><ul><li>  Potenziell schnellere Netzwerkausf√ºhrung. <br></li><li>  Deutlich geringerer Stromverbrauch im Vergleich zu CPU und GPU (dies ist besonders wichtig f√ºr mobile L√∂sungen). <br></li></ul><br>  <b>Nachteile:</b> <br><br><ul><li>  Meistens helfen sie dabei, die Ausf√ºhrung zu beschleunigen, und das Training auf ihnen ist im Gegensatz zur GPU merklich weniger bequem. <br></li><li>  Komplexere Programmierung im Vergleich zu fr√ºheren Optionen. <br></li><li>  Deutlich weniger Spezialisten. <br></li></ul><br><h2>  ASIC </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/dee/5e4/059/dee5e40597454e07651718c325f2ac3f.png"><br><br>  Als n√§chstes kommt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ASIC</a> , kurz f√ºr Application-Specific Integrated Circuit, d. H.  integrierte Schaltung f√ºr unsere Aufgabe.  Zum Beispiel die Realisierung eines in Eisen gelegten neuronalen Netzwerks.  Die meisten Rechenknoten k√∂nnen jedoch parallel arbeiten.  Tats√§chlich k√∂nnen nur Datenabh√§ngigkeiten und ungleichm√§√üiges Rechnen auf verschiedenen Ebenen des Netzwerks verhindern, dass wir st√§ndig alle funktionierenden ALUs verwenden. <br><br>  Vielleicht hat das Cryptocurrency Mining in den letzten Jahren die gr√∂√üte ASIC-Werbung in der √ñffentlichkeit gemacht.  Am Anfang war das Mining auf der CPU ziemlich profitabel, sp√§ter musste ich eine GPU, dann ein FPGA und dann spezialisierte ASICs kaufen, da die Leute (lesen - der Markt) f√ºr Auftr√§ge reiften, bei denen ihre Produktion rentabel wurde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de1/fda/062/de1fda062c797262a51047f57eac2f11.png"><br>  In unserer Region sind auch (nat√ºrlich!) Bereits <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dienste erschienen,</a> die dazu beitragen, ein neuronales Netz auf Eisen mit den notwendigen Eigenschaften f√ºr Energieverbrauch, FPS und Preis zu setzen.  Magisch zustimmen! <br><br>  ABER!  Wir verlieren die Netzwerkanpassung.  Und nat√ºrlich denken die Leute auch dar√ºber nach.  Hier ist zum Beispiel ein Artikel mit dem Spruch: " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kann eine rekonfigurierbare Architektur ASIC als CNN-Beschleuniger schlagen?</a> " ("Kann eine konfigurierbare Architektur ASIC wie einen CNN-Beschleuniger schlagen?").  Es gibt genug Arbeit zu diesem Thema, da die Frage nicht unt√§tig ist.  Der Hauptnachteil von ASIC besteht darin, dass es f√ºr uns schwierig wird, das Netzwerk zu √§ndern, nachdem wir es in Hardware umgewandelt haben.  Sie sind am vorteilhaftesten f√ºr F√§lle, in denen wir bereits ein gut funktionierendes Netzwerk mit Millionen von Chips mit geringem Stromverbrauch und hoher Leistung ben√∂tigen.  Und diese Situation entwickelt sich beispielsweise auf dem Markt f√ºr Autopilotautos allm√§hlich.  Oder in √úberwachungskameras.  Oder in den Kammern von Roboterstaubsaugern.  Oder in den Kammern eines Haushaltsk√ºhlschranks.  Oder in einer Kaffeemaschinenkammer.  <s>Oder in der Eisenkammer.</s>  Nun, Sie verstehen die Idee, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kurz gesagt</a> ! <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51a/b5e/949/51ab5e9497fee71cf9f23606d3de4fb6.png"></div><br><br>  Es ist wichtig, dass der Chip in der Massenproduktion billig ist, schnell arbeitet und ein Minimum an Energie verbraucht. <br><br>  <b>Vorteile:</b> <br><br><ul><li>  Die niedrigsten Chipkosten im Vergleich zu allen vorherigen L√∂sungen. <br></li><li>  Niedrigster Stromverbrauch pro Betriebseinheit. <br></li><li>  Ziemlich hohe Geschwindigkeit (auf Wunsch auch eine Aufzeichnung). <br></li></ul><br>  <b>Nachteile:</b> <br><br><ul><li>  Sehr eingeschr√§nkte F√§higkeit, das Netzwerk und die Logik zu aktualisieren. <br></li><li>  H√∂chste Entwicklungskosten im Vergleich zu allen bisherigen L√∂sungen. <br></li><li>  Die Verwendung von ASIC ist vor allem bei gro√üen Auflagen kosteng√ºnstig. <br></li></ul><br><h2>  TPU </h2><br>  Denken Sie daran, dass es bei der Arbeit mit Netzwerken zwei Aufgaben gibt - Training und Ausf√ºhrung (Inferenz).  Wenn FPGA / ASICs in erster Linie darauf abzielen, die Ausf√ºhrung zu beschleunigen (einschlie√ülich einiger fester Netzwerke), ist TPU (Tensor Processing Unit oder Tensor Processors) entweder eine hardwarebasierte Lernbeschleunigung oder eine relativ universelle Beschleunigung eines beliebigen Netzwerks.  Der Name ist wundersch√∂n, stimme zu, obwohl tats√§chlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensoren vom</a> Rang 2 mit einer Mixed Multiply Unit (MXU), die an einen High-Bandwidth Memory (HBM) angeschlossen ist, immer noch verwendet werden.  Unten sehen Sie das Architekturdiagramm der 2. und 3. Version von TPU Google: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f5e/29c/696/f5e29c696e265435fadbc2e56f67e12e.png"></div><br><h2>  TPU Google </h2><br>  Im Allgemeinen machte Google eine Werbung f√ºr den TPU-Namen und enth√ºllte interne Entwicklungen im Jahr 2017: <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4af/ea1/4ca/4afea14cad5dc0a7d941ae6b7963c171.png"></div><br>  Sie begannen 2006 mit ihren Worten mit den Vorarbeiten an spezialisierten Prozessoren f√ºr neuronale Netze, 2013 schufen sie ein Projekt mit guter Finanzierung und 2015 begannen sie mit den ersten Chips zu arbeiten, die bei neuronalen Netzen f√ºr den Google Translate Cloud-Dienst und mehr sehr hilfreich waren.  Wir betonen, dass dies die Beschleunigung des Netzwerks war.  Ein wichtiger Vorteil f√ºr Rechenzentren ist die um zwei Gr√∂√üenordnungen h√∂here TPU-Energieeffizienz im Vergleich zu CPUs (Grafik f√ºr TPU v1): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/877/a91/f61/877a91f61ce45f2d04c7c3fe9df55348.png"></div><br>  Au√üerdem ist die <i>Leistung des</i> Netzwerks im Vergleich zur GPU in der Regel 10 bis 30 Mal besser: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/10a/302/83b/10a30283b9c0fe8f0c7c1993f0bc68aa.png"></div><br>  Der Unterschied ist sogar zehnmal signifikant.  Es ist klar, dass der Unterschied zur GPU in 20 bis 30-mal die Entwicklung dieser Richtung bestimmt. <br><br>  Und zum Gl√ºck ist Google nicht allein. <br><br><h2>  TPU Huawei </h2><br>  Heute begann das langm√ºtige Huawei vor einigen Jahren auch mit der Entwicklung von TPU unter dem Namen Huawei Ascend und in zwei Versionen gleichzeitig - f√ºr Rechenzentren (wie Google) und f√ºr mobile Ger√§te (mit denen Google k√ºrzlich ebenfalls begonnen hat).  Wenn Sie den Materialien von Huawei glauben, haben sie das frische Google TPU v3 2,5-mal durch FP16 und 2-mal NVIDIA V100 √ºberholt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb8/692/66a/bb869266a2cfa5e28c2f086026309b01.png"><br><br>  Wie immer eine gute Frage: Wie wird sich dieser Chip bei realen Aufgaben verhalten?  Wie Sie sehen k√∂nnen, wird in der Grafik die Spitzenleistung angezeigt.  Dar√ºber hinaus ist Google TPU v3 in vielerlei Hinsicht gut, da es in Clustern von 1024 Prozessoren effektiv arbeiten kann.  Huawei k√ºndigte auch Server-Cluster f√ºr das Ascend 910 an, aber es gibt keine Details.  Im Allgemeinen haben sich die Ingenieure von Huawei in den letzten 10 Jahren als √§u√üerst kompetent erwiesen, und es besteht jede Chance, dass in diesem Fall eine 2,8-mal h√∂here Spitzenleistung im Vergleich zu Google TPU v3 in Verbindung mit der neuesten 7-nm-Prozesstechnologie verwendet wird. <br><br>  Der Speicher und der Datenbus sind f√ºr die Leistung von entscheidender Bedeutung, und die Folie zeigt, dass diesen Komponenten erhebliche Aufmerksamkeit geschenkt wurde (einschlie√ülich der Geschwindigkeit der Kommunikation mit dem Speicher, die viel schneller ist als die der GPU): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/98e/7bd/ab1/98e7bdab1c9e1a81e891bb72db3976c2.png"><br><br>  Der Chip verwendet auch einen etwas anderen Ansatz - keine zweidimensionalen MXU 128x128-Skalen, sondern Berechnungen in einem dreidimensionalen W√ºrfel kleinerer Gr√∂√üe 16x16xN, wobei N = {16,8,4,2,1}.  Die Schl√ºsselfrage ist daher, wie gut es auf der tats√§chlichen Beschleunigung bestimmter Netzwerke liegt (z. B. sind Berechnungen in einem W√ºrfel f√ºr Bilder praktisch).  Eine sorgf√§ltige Untersuchung der Folie zeigt auch, dass der Chip im Gegensatz zu Google sofort Arbeiten mit komprimiertem FullHD-Video enth√§lt.  F√ºr den Autor klingt das <b>sehr</b> ermutigend! <br><br>  Wie oben erw√§hnt, werden in derselben Linie Prozessoren f√ºr mobile Ger√§te entwickelt, f√ºr die Energieeffizienz entscheidend ist und auf denen das Netzwerk haupts√§chlich ausgef√ºhrt wird (d. H. Separat Prozessoren f√ºr das Cloud-Lernen und separat f√ºr die Ausf√ºhrung): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ea0/a3c/7cb/ea0a3c7cb72def7195afa2011ba02907.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Und mit diesem Parameter sieht zumindest im Vergleich zu NVIDIA alles gut aus (beachten Sie, dass kein Vergleich mit Google durchgef√ºhrt wurde, Google jedoch keine Cloud-TPUs an die H√§nde gibt). </font><font style="vertical-align: inherit;">Und ihre mobilen Chips werden mit Prozessoren von Apple, Google und anderen Unternehmen konkurrieren, aber es ist noch zu fr√ºh, um hier Bilanz zu ziehen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist deutlich zu sehen, dass die neuen Nano-, Tiny- und Lite-Chips noch besser sein sollten. </font><font style="vertical-align: inherit;">Es wird deutlich, </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">warum Trump Angst hatte,</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> warum viele Hersteller die Erfolge von Huawei (das alle Eisenhersteller in den USA, einschlie√ülich Intel im Jahr 2018, √ºbertraf) sorgf√§ltig pr√ºfen.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Analoge tiefe Netzwerke </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie Sie wissen, entwickelt sich Technologie oft spiralf√∂rmig, wenn alte und vergessene Ans√§tze in einer neuen Runde relevant werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√Ñhnliches k√∂nnte sehr gut mit neuronalen Netzen passieren. Sie haben vielleicht geh√∂rt, dass die Multiplikations- und Additionsoperationen einmal von Elektronenr√∂hren und Transistoren ausgef√ºhrt wurden (zum Beispiel die Umwandlung von Farbr√§umen - eine typische Multiplikation von Matrizen - in jedem Farbfernseher bis Mitte der 90er Jahre)? Es stellte sich eine gute Frage: Wenn unser neuronales Netzwerk relativ unempfindlich gegen ungenaue Berechnungen im Inneren ist, was ist, wenn wir diese Berechnungen in analoge Form umwandeln? Wir erhalten sofort eine sp√ºrbare Beschleunigung der Berechnungen und eine m√∂glicherweise dramatische Reduzierung des Energieverbrauchs f√ºr einen Vorgang:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/6b2/e08/aff/6b2e08afff1639668e02a548ed663fba.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mit diesem Ansatz wird DNN (Deep Neural Network) schnell und energieeffizient berechnet. Es gibt jedoch ein Problem - dies sind DAC / ADCs (DAC / ADC) - Konverter von digital zu analog und umgekehrt, die sowohl die Energieeffizienz als auch die Prozessgenauigkeit verringern. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bereits 2017 schlug IBM Research </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ein analoges CMOS</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> f√ºr RPU ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resistive Processing Units</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) vor, mit dem Sie verarbeitete Daten auch in analoger Form speichern und die Gesamteffizienz des Ansatzes erheblich steigern k√∂nnen:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/0d3/720/3a8/0d37203a85a89791e8701098583e01ea.png"></div><br> ,          ‚Äî     RPU,  ,       .   IBM   ,           2-          (   ),     100  (!)       GPU: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1c4/1eb/c09/1c41ebc09f5e53d206534eefbb105eac.png"></div><br>       ,         : <br><br><div style="text-align:center;"><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/498/f6c/bdb/498f6cbdb53f20806cf41a78b7110e8b.png"></div><br>  Die m√∂gliche Richtung des analogen Rechnens sieht jedoch <b>√§u√üerst</b> interessant aus. <br><br>  Das einzige, was verwirrt, ist, dass es IBM ist, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die bereits Dutzende von Patenten zu diesem Thema angemeldet hat</a> .  Erfahrungsgem√§√ü kooperieren sie aufgrund der Besonderheiten der Unternehmenskultur relativ schwach mit anderen Unternehmen und verlangsamen, da sie einige Technologien besitzen, ihre Entwicklung unter anderem eher, als sie effektiv zu teilen.  Beispielsweise lehnte IBM es einmal ab, die arithmetische Komprimierung f√ºr JPEG an das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ISO-</a> Komitee zu lizenzieren, obwohl der Standardentwurf eine Option mit arithmetischer Komprimierung war.  Infolgedessen wurde JPEG mit Huffman-Komprimierung zum Leben erweckt und stach 10-15% schlechter als es konnte.  Die gleiche Situation war bei Videokomprimierungsstandards.  Und die Branche hat erst nach Ablauf von 5 IBM-Patenten 12 Jahre sp√§ter massiv auf arithmetische Komprimierung in Codecs umgestellt ... Hoffen wir, dass IBM diesmal eher zur Zusammenarbeit neigt, und <b>w√ºnschen</b> dementsprechend <b>allen, die nicht mit IBM verbunden sind, maximalen Erfolg auf diesem Gebiet</b> Der Nutzen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">solcher Menschen und Unternehmen ist gro√ü</a> . <br><br>  Wenn es klappt, <b>wird es eine Revolution in der Nutzung neuronaler Netze und eine Revolution in vielen Bereichen der Informatik sein.</b> <br><br><h2>  Verschiedenes Andere Briefe </h2><br>  Im Allgemeinen ist das Thema der Beschleunigung neuronaler Netze in Mode gekommen, alle gro√üen Unternehmen und Dutzende von Startups sind daran beteiligt, und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mindestens f√ºnf von ihnen haben</a> bis Anfang 2018 Investitionen in H√∂he von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mehr als 100 Millionen US-Dollar angezogen</a> .  Insgesamt wurden im Jahr 2017 1,5 Milliarden US-Dollar in Startups im Zusammenhang mit der Entwicklung von Chips investiert.  Trotz der Tatsache, dass die Anleger die Chiphersteller seit gut 15 Jahren nicht mehr bemerkten (weil es dort vor dem Hintergrund der Giganten nichts zu fangen gab).  Im Allgemeinen - jetzt gibt es eine echte Chance f√ºr eine kleine Eisenrevolution.  Dar√ºber hinaus ist es √§u√üerst schwierig vorherzusagen, welche Architektur gewinnen wird, der Bedarf an Revolution ist gereift und die M√∂glichkeiten zur Steigerung der Produktivit√§t sind gro√ü.  Die klassische revolution√§re Situation ist gereift: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Moore</a> kann nicht mehr und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dean</a> ist noch nicht bereit. <br><br>  Nun, da das wichtigste Marktrecht - anders sein - gibt es viele neue Buchstaben, zum Beispiel: <br><br><ul><li>  <b>Neural Processing Unit ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NPU</a> )</b> - Ein Neuroprozessor, manchmal wundersch√∂n - ein neuromorpher Chip - im Allgemeinen der allgemeine Name f√ºr einen Beschleuniger neuronaler Netze, die als Chips <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Samsung</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Huawei</a> und weiter auf der Liste bezeichnet werden ... <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/d28/900/72e/d2890072e95d7e63f70543b09ba759bb.png"></div>  <i>Im Folgenden werden in diesem Abschnitt haupts√§chlich Folien von Unternehmenspr√§sentationen als Beispiele f√ºr Technologie <b>-Selbstnamen aufgef√ºhrt</b></i> <br><br>  Es ist klar, dass ein direkter Vergleich problematisch ist, aber hier sind einige interessante Daten zum Vergleich von Chips mit Neuroprozessoren von Apple und Huawei, die von TSMC hergestellt wurden und von eingangs erw√§hnt wurden.  Es zeigt sich, dass der Wettbewerb hart ist, die neue Generation eine 2-8-fache Produktivit√§tssteigerung und die Komplexit√§t technologischer Prozesse aufweist: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ee/ga/iu/eegaiu5u_rw5trv0-exwjngc7sw.png"></div><br></li><li>  <b>Neural Network Processor (NNP)</b> - Prozessor f√ºr neuronale Netze. <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/917/1bb/ad2/9171bbad26941f97399ce80a56373e51.png"></div><br>  Dies ist der Name seiner Chipfamilie, zum Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intel</a> (urspr√ºnglich war es die Firma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nervana Systems</a> , die Intel 2016 f√ºr √ºber 400 Millionen US-Dollar gekauft hat).  In <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikeln</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">B√ºchern ist der</a> Name NNP jedoch auch weit verbreitet. <br></li><li>  <b>Intelligence Processing Unit (IPU)</b> - ein intelligenter Prozessor - der Name der von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Graphcore</a> beworbenen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Chips</a> (die √ºbrigens bereits eine Investition von 310 Millionen US-Dollar erhalten haben). <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e93/18f/8e7/e9318f8e7711ea5b3a669946484c72bf.png"></div><br>  Es werden spezielle Karten f√ºr Computer hergestellt, die jedoch auf das Training neuronaler Netze ausgerichtet sind. Die RNN-Trainingsleistung ist 180- bis 240-mal h√∂her als die des NVIDIA P100. <br></li><li>  <b>Dataflow Processing Unit (DPU)</b> - Datenverarbeitungsprozessor - der Name wird von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WAVE Computing</a> beworben, das bereits eine Investition von 203 Millionen US-Dollar erhalten hat.  Es produziert ungef√§hr die gleichen Beschleuniger wie Graphcore: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f53/e5e/25a/f53e5e25abb2fc2fee636d1949242be8.png"></div><br>  Da sie 100 Millionen weniger erhalten haben, erkl√§ren sie, dass das Training nur mehr als 25 Mal schneller ist als auf der GPU (obwohl sie versprechen, dass es bald 1000 Mal sein wird).  Mal sehen ... <br></li><li>  <b>Vision Processing Unit ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VPU</a> )</b> - Computer Vision-Prozessor: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/80e/756/110/80e7561100a794bb005635899b06ec40.png"></div><br>  Der Begriff wird in Produkten mehrerer Unternehmen verwendet, z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Myriad X VPU</a> von Movidius (ebenfalls 2016 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von Intel gekauft</a> ). <br></li><li>  Einer der Konkurrenten von IBM (der, wie wir uns erinnern, den Begriff <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RPU verwendet</a> ) - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mythic</a> - bewegt <b>Analog DNN</b> , das auch das Netzwerk im Chip speichert und relativ schnell ausf√ºhrt.  Bisher haben sie nur Versprechen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wenn auch ernst</a> : <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/01e/ae6/253/01eae6253a1d7ef3e2dec9401857a33a.png"></div><br></li></ul><br>  Und dies listet nur die gr√∂√üten Gebiete auf, in deren Entwicklung Hunderte Millionen investiert wurden (dies ist wichtig f√ºr die Entwicklung von Eisen). <br><br>  Wie wir sehen, bl√ºhen im Allgemeinen alle Blumen schnell.  Allm√§hlich werden Unternehmen Investitionen in Milliardenh√∂he verdauen (normalerweise dauert die Herstellung von Chips 1,5 bis 3 Jahre), Staub wird sich absetzen, der Marktf√ºhrer wird klar, die Gewinner werden wie √ºblich eine Geschichte schreiben und der Name der erfolgreichsten Technologie auf dem Markt wird allgemein akzeptiert.  Dies ist bereits mehrmals vorgekommen (‚ÄûIBM PC‚Äú, ‚ÄûSmartphone‚Äú, ‚ÄûXerox‚Äú usw.). <br><br><h2>  Ein paar Worte zum richtigen Vergleich </h2><br>  Wie bereits oben erw√§hnt, ist es nicht einfach, die Leistung neuronaler Netze richtig zu vergleichen.  Genau aus diesem Grund ver√∂ffentlicht Google ein Diagramm, in dem TPU v1 den NVIDIA V100 herstellt.  NVIDIA ver√∂ffentlicht eine solche Schande und ver√∂ffentlicht einen Zeitplan, in dem Google TPU v1 die V100 verliert.  (Also!) Google ver√∂ffentlicht die folgende Tabelle, in der der V100 auf Google TPU v2 &amp; v3 verliert.  Und schlie√ülich ist Huawei der Zeitplan, in dem jeder beim Huawei Ascend verliert, aber das V100 ist besser als das TPU v3.  Kurz gesagt, Zirkus.  Was ist charakteristisch - <i>jedes</i> Diagramm <i>hat seine eigene</i> Wahrheit! <br><br>  Die Hauptursachen der Situation sind klar: <br><br><ul><li>  Sie k√∂nnen die Lerngeschwindigkeit oder die Ausf√ºhrungsgeschwindigkeit messen (je nachdem, was bequemer ist). <br></li><li>  Es ist m√∂glich, verschiedene neuronale Netze zu messen, da sich die Geschwindigkeit der Ausf√ºhrung / des Trainings verschiedener neuronaler Netze auf bestimmten Architekturen aufgrund der Netzwerkarchitektur und der erforderlichen Datenmenge erheblich unterscheiden kann. <br></li><li>  Und Sie k√∂nnen die Spitzenleistung des Beschleunigers messen (vielleicht die abstrakteste von allen oben genannten). <br></li></ul><br>  Als Versuch, die Dinge in diesem Zoo in Ordnung zu bringen, erschien der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MLPerf-</a> Test, f√ºr den jetzt Version 0.5 verf√ºgbar ist, d. H.  Derzeit entwickelt er eine Vergleichsmethode, die im <a href="">3. Quartal dieses Jahres erstmals ver√∂ffentlicht werden soll</a> : <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/d14/e35/575/d14e3557543c05b83b99f12dc9e572ce.png"></div><br>  Da die Autoren dort einen der Hauptverantwortlichen f√ºr TensorFlow sind, besteht jede M√∂glichkeit herauszufinden, wie man am besten trainiert und m√∂glicherweise verwendet (da die mobile Version von TF im Laufe der Zeit h√∂chstwahrscheinlich auch in diesen Test einbezogen wird). <br><br>  K√ºrzlich hat die internationale Organisation <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">IEEE</a> , die den dritten Teil der weltweiten Fachliteratur zu Funkelektronik, Computern und Elektrotechnik ver√∂ffentlicht, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Huawei aus</a> dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gesicht eines</a> Kindes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verbannt</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">das</a> Verbot jedoch bald <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aufgehoben</a> .  Huawei ist noch nicht im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aktuellen</a> MLPerf-Ranking, w√§hrend Huawei TPU ein ernstzunehmender Konkurrent von Google TPUs und NVIDIA-Karten ist (d. H. Neben politischen gibt es wirtschaftliche Gr√ºnde, Huawei offen zu ignorieren).  Mit unverhohlenem Interesse werden wir die Entwicklung der Ereignisse verfolgen! <br><br><h2>  Alles in den Himmel!  N√§her an den Wolken! </h2><br>  Und da es um Training ging, lohnt es sich, ein paar Worte zu seinen Besonderheiten zu sagen: <br><br><ul><li>  Mit der weit verbreiteten Abkehr von der Erforschung tiefer neuronaler Netze (mit Dutzenden und Hunderten von Schichten, die wirklich alle zerrei√üen) war es notwendig, Hunderte von Megabyte an Koeffizienten zu mahlen, was alle Prozessor-Caches fr√ºherer Generationen sofort unwirksam machte.  Gleichzeitig <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diskutiert</a> das klassische ImageNet eine strikte Korrelation zwischen der Gr√∂√üe des Netzwerks und seiner Genauigkeit (je h√∂her desto besser, rechts, desto gr√∂√üer das Netzwerk, ist die horizontale Achse logarithmisch): <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/5fa/788/84f/5fa78884f561b6f93dfa126be53376f4.png"></div><br></li><li>  Der Berechnungsprozess innerhalb des neuronalen Netzwerks folgt einem festen Schema, d.h.  Wo in den allermeisten F√§llen alle ‚ÄûVerzweigungen‚Äú und ‚Äû√úberg√§nge‚Äú (im Hinblick auf das letzte Jahrhundert) stattfinden werden, ist im Voraus genau bekannt, so dass die spekulative Ausf√ºhrung von Anweisungen ohne Arbeit bleibt, was zuvor die Produktivit√§t erheblich steigert: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ss/-6/9n/ss-69n-vr5c3rszuvmhkaomtct0.png"></div><br>  Dies macht die angeh√§uften <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">superskalaren</a> Vorhersagemechanismen f√ºr Verzweigungen und Vorberechnungen fr√ºherer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jahrzehnte der</a> Prozessorverbesserung unwirksam (dieser Teil des Chips tr√§gt leider auch zur globalen Erw√§rmung bei, √§hnlich wie DNN im DNN-Cache). <br></li><li>  Dar√ºber hinaus ist das neuronale Netzwerktraining <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">horizontal</a> relativ schwach <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">skaliert</a> .  Das hei√üt,  Wir k√∂nnen nicht 1000 leistungsf√§hige Computer nehmen und 1000-mal Lernbeschleunigung erhalten.  Und selbst bei 100 k√∂nnen wir nicht (zumindest bis das theoretische Problem der Verschlechterung der Trainingsqualit√§t bei einer gro√üen Gr√∂√üe der Charge gel√∂st ist).  Im Allgemeinen ist es f√ºr uns ziemlich schwierig, etwas auf mehrere Computer zu verteilen, da die Geschwindigkeit des Lernens katastrophal abnimmt, sobald die Zugriffsgeschwindigkeit auf den einheitlichen Speicher, in dem sich das Netzwerk befindet, abnimmt.  Wenn ein Forscher <s>kostenlos</s> auf 1000 leistungsstarke Computer zugreifen kann, wird er sie sicherlich bald alle nutzen, aber h√∂chstwahrscheinlich (wenn es kein Infiniband + RDMA gibt) wird es viele neuronale Netze mit unterschiedlichen Hyperparametern geben.  Das hei√üt,  Die gesamte Trainingszeit ist nur um ein Vielfaches k√ºrzer als bei einem Computer.  Dort ist es m√∂glich, mit der Gr√∂√üe des Stapels, der Weiterbildung und anderen neuen modischen Technologien zu spielen, aber die Hauptschlussfolgerung lautet: Ja, mit zunehmender Anzahl von Computern steigen die Arbeitseffizienz und die Wahrscheinlichkeit, ein Ergebnis zu erzielen, jedoch nicht linear.  Und heute ist die Zeit eines Data Science-Forschers teuer und oft, wenn Sie viele Autos ausgeben k√∂nnen (wenn auch unvern√ºnftig), aber eine Beschleunigung erhalten - dies geschieht (siehe das Beispiel mit 1, 2 und 4 teuren V100 in den Wolken unten). <br></li></ul><br>  Genau diese Punkte erkl√§ren, warum so viele Menschen zur Entwicklung von Spezialeisen f√ºr tiefe neuronale Netze eilten.  Und warum haben sie ihre Milliarden bekommen?  Am Ende des Tunnels ist wirklich sichtbares Licht und nicht nur Graphcore (das, wie man sich erinnert, das 240-fache des RNN-Trainings beschleunigte). <br><br>  Zum Beispiel sind die Herren von IBM Research <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">optimistisch,</a> dass die Entwicklung spezieller Chips, die die Recheneffizienz in 5 Jahren um eine Gr√∂√üenordnung steigern (und in 10 Jahren um 2 Gr√∂√üenordnungen, die in diesem Diagramm eine 1000-fache Steigerung gegen√ºber 2016 erreichen), wahr ist , in der Effizienz pro Watt, aber die Kernleistung wird auch zunehmen): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51c/81f/226/51c81f2267b99ba00231532e606cf7fd.png"></div><br>  All dies bedeutet das Auftreten von Eisenst√ºcken, deren Training relativ schnell, aber teuer sein wird, was nat√ºrlich zu der Idee f√ºhrt, die Zeit f√ºr die Verwendung dieses teuren Eisenst√ºcks zwischen den Forschern zu teilen.  Und diese Idee f√ºhrt uns heute nicht weniger nat√ºrlich zum Cloud Computing.  Und der √úbergang des Lernens in die Wolken ist seit langem aktiv. <br><br>  Beachten Sie, dass sich das Training derselben Modelle jetzt zeitlich um eine Gr√∂√üenordnung verschiedener Cloud-Dienste unterscheiden kann.  Amazon liegt an der Spitze, und Googles kostenloses Colab steht an letzter Stelle.  Bitte beachten Sie, wie sich das Ergebnis der Anzahl der V100 unter den F√ºhrenden √§ndert - eine Erh√∂hung der Anzahl der Karten um das Vierfache (!) Erh√∂ht die Produktivit√§t um weniger als ein Drittel (!!!) von Blau auf Lila, und Google hat noch weniger: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/78d/892/d83/78d892d833c0858572548aee7fca2697.png"></div><br>  Es scheint, dass der Unterschied in den kommenden Jahren auf zwei Gr√∂√üenordnungen anwachsen wird.  Herr!  Geld kochen!  Wir werden den erfolgreichsten Investoren einvernehmlich mehrere Milliarden Investitionen zur√ºckgeben ... <br><br><h2>  Kurz gesagt </h2><br>  Versuchen wir, die wichtigsten Punkte des Tablets zusammenzufassen: <br><div class="scrollable-table"><table><tbody><tr><td>  Typ <br></td><td>  Was beschleunigt <br></td><td>  Kommentar <br></td></tr><tr><td>  CPU <br></td><td>  Grunds√§tzlich tun <br></td><td>  Normalerweise die schlechteste Geschwindigkeit und Energieeffizienz, aber gut geeignet f√ºr die Ausf√ºhrung kleiner neuronaler Netze <br></td></tr><tr><td>  GPU <br></td><td>  Ausf√ºhrung + <br>  Ausbildung <br></td><td>  Die universellste L√∂sung, aber ziemlich teuer, sowohl hinsichtlich der Berechnungskosten als auch der Energieeffizienz <br></td></tr><tr><td>  FPGA <br></td><td>  Erf√ºllung <br></td><td>  Eine relativ universelle L√∂sung f√ºr die Ausf√ºhrung von Netzwerken kann in einigen F√§llen die Implementierung erheblich beschleunigen <br></td></tr><tr><td>  ASIC <br></td><td>  Erf√ºllung <br></td><td>  Die billigste, schnellste und energieeffizienteste Version des Netzwerks, aber gro√üe Auflagen sind erforderlich <br></td></tr><tr><td>  TPU <br></td><td>  Ausf√ºhrung + <br>  Ausbildung <br></td><td>  Die ersten Versionen wurden verwendet, um die Ausf√ºhrung zu beschleunigen. Jetzt werden sie verwendet, um die Ausf√ºhrung und das Training sehr schnell zu beschleunigen <br></td></tr><tr><td>  IPU, DPU ... NNP <br></td><td>  Meistens Training <br></td><td>  Viele Marketingbriefe, die in den kommenden Jahren sicher vergessen werden.  Das Hauptvorteil dieses Zoos ist die √úberpr√ºfung verschiedener Richtungen der DNN-Beschleunigung <br></td></tr><tr><td>  Analoge DNN / RPU <br></td><td>  Ausf√ºhrung + <br>  Ausbildung <br></td><td>  Potenziell analoge Beschleuniger k√∂nnen die Geschwindigkeit und Energieeffizienz beim Durchf√ºhren und Trainieren neuronaler Netze revolutionieren <br></td></tr></tbody></table></div><br><h2>  Ein paar Worte zur Softwarebeschleunigung </h2><br>  Fairerweise erw√§hnen wir, dass heute das gro√üe Thema die Softwarebeschleunigung der Ausf√ºhrung und des Trainings tiefer neuronaler Netze ist.  Die Ausf√ºhrung kann vor allem durch die sogenannte Quantisierung des Netzwerks erheblich beschleunigt werden.  Vielleicht liegt dies zum einen daran, dass der Bereich der verwendeten Gewichte nicht so gro√ü ist und es h√§ufig m√∂glich ist, Gewichte von einem 4-Byte-Gleitkommawert auf eine 1-Byte-Ganzzahl zu vergr√∂bern (und, wenn man sich an die Erfolge von IBM erinnert, noch st√§rker).  Zweitens ist das trainierte Netzwerk insgesamt ziemlich widerstandsf√§hig gegen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rechenrauschen</a> und die Genauigkeit des √úbergangs zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">int8</a> sinkt leicht.  Gleichzeitig erh√∂ht die Tatsache, dass das Netzwerk um das Vierfache verkleinert wird und als schnelle Vektoroperationen betrachtet werden kann, die Gesamtausf√ºhrungsgeschwindigkeit erheblich, obwohl die Anzahl der Operationen (aufgrund der Skalierung bei der Berechnung) sogar zunehmen kann.  Dies ist besonders wichtig f√ºr mobile Anwendungen, funktioniert aber auch in den Clouds (ein Beispiel f√ºr eine beschleunigte Ausf√ºhrung in Amazon Clouds): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e52/bbd/2cc/e52bbd2cc36e24eff9a788c2d8371c83.png"></div><br>  Es gibt andere M√∂glichkeiten, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Netzwerkausf√ºhrung</a> algorithmisch zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">beschleunigen,</a> und noch mehr M√∂glichkeiten, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">das Lernen</a> zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">beschleunigen</a> .  Dies sind jedoch separate gro√üe Themen, √ºber die diesmal nicht gesprochen wird. <br><br><h2>  Anstelle einer Schlussfolgerung </h2><br>  In seinen Vortr√§gen gibt der Investor und Autor <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tony Ceba</a> ein gro√üartiges Beispiel: Im Jahr 2000 nahm der Supercomputer Nr. 1 mit einer Kapazit√§t von 1 Teraflops 150 Quadratmeter ein, kostete 46 Millionen US-Dollar und verbrauchte 850 kW: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/2v/6y/xe/2v6yxeo59aewcqngaybonbftsae.png"></div><br>  15 Jahre sp√§ter passte die NVIDIA-GPU mit einer Leistung von 2,3 Teraflops (2-mal mehr) in eine Hand, kostete 59 USD (eine Verbesserung um das Millionenfache) und verbrauchte 15 Watt (eine Verbesserung um das 56.000-fache): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/gx/wz/qq/gxwzqqj1si3e3ho33nnypq_au3w.png"></div><br>  Im M√§rz dieses Jahres stellte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google TPU-Pods vor</a> , bei denen es sich tats√§chlich um fl√ºssigkeitsgek√ºhlte Supercomputer handelt, die auf TPU v3 basieren. Das Hauptmerkmal besteht darin, dass sie in Systemen mit 1024 TPU zusammenarbeiten k√∂nnen.  Sie sehen ziemlich beeindruckend aus: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/37c/849/2ec/37c8492ec7e380c5a26f8a420fc591d9.png"></div><br>  Die genauen Daten werden nicht angegeben, aber es wird gesagt, dass das System mit den Top-5-Supercomputern der Welt vergleichbar ist.  TPU Pod kann die Geschwindigkeit des Lernens neuronaler Netze drastisch erh√∂hen.  Um die Interaktionsgeschwindigkeit zu erh√∂hen, werden TPUs durch Hochgeschwindigkeitsleitungen zu einer Ringstruktur verbunden: <br><br><img width="25%" src="https://habrastorage.org/getpro/habr/post_images/0f0/e26/532/0f0e2653272b633b8af1b6103540e95c.gif"><br>  Es scheint, dass dieser doppelt so leistungsstarke Neuroprozessor nach 15 Jahren auch in Ihre Hand passen wird, wie der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Skynet-Prozessor</a> (Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">m√ºssen</a> zugeben, es ist etwas √Ñhnliches): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/cf2/5da/db6/cf25dadb60a350332cfef6bd147ec91b.png"></div>  <i>Aufnahme aus der Regieversion des Films <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Terminator 2"</a></i> <br><br>  Angesichts der aktuellen Verbesserungsrate von Hardwarebeschleunigern in tiefen neuronalen Netzen und des obigen Beispiels ist dies v√∂llig real.  In ein paar Jahren gibt es jede Chance, einen Chip mit einer Leistung wie dem heutigen TPU Pod zu kaufen. <br><br>  √úbrigens ist es lustig, dass die Chiphersteller im Film (anscheinend vorstellend, wohin das Selbsttrainingsnetzwerk f√ºhren k√∂nnte) die Umschulung standardm√§√üig deaktiviert haben.  Charakteristischerweise konnte der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">T-800</a> selbst den Trainingsmodus nicht aktivieren und arbeitete im Inferenzmodus (siehe die l√§ngere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Regieversion</a> ).  Dar√ºber hinaus wurde der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neuronale Netzprozessor</a> weiterentwickelt und konnte beim Einschalten der Umschulung die zuvor gesammelten Daten zur Aktualisierung des Modells verwenden.  Nicht schlecht f√ºr 1991. <br><br>  Dieser Text wurde im hei√üen 13-millionsten Shenzhen begonnen.  Ich sa√ü in einem der 27.000 elektrischen Taxis der Stadt und schaute mit gro√üem Interesse auf die 4 Fl√ºssigkristallschirme des Autos.  Ein kleines - unter den Ger√§ten vor dem Fahrer, zwei - in der Mitte im Armaturenbrett und das letzte - durchscheinend - im R√ºckspiegel, kombiniert mit einem DVR, einer Video√ºberwachungskamera und einem Android an Bord (gemessen an der obersten Zeile mit dem Ladezustand und der Kommunikation mit dem Netzwerk).  Es wurden Fahrerdaten (√ºber die man sich beschweren sollte, wenn dies der Fall war), eine frische Wettervorhersage angezeigt, und es schien eine Verbindung zur Taxiflotte zu bestehen.  Der Fahrer konnte kein Englisch und konnte ihn nicht nach seinen Eindr√ºcken von der elektrischen Maschine fragen.  Deshalb trat er tr√§ge aufs Pedal und bewegte das Auto leicht im Stau.  Und ich beobachtete das Fenster mit einem futuristischen Blick mit Interesse - die Chinesen in ihren Jacken fuhren von der Arbeit an Elektrorollern und Monor√§dern ... und fragten mich, wie das alles in 15 Jahren aussehen w√ºrde ... <br><br>  Tats√§chlich kann der R√ºckspiegel bereits heute mithilfe der Daten der Kamera des DVR und der <i>Hardwarebeschleunigung der neuronalen Netze</i> das Auto im Verkehr steuern und die Route festlegen.  Zumindest nachmittags).  Nach 15 Jahren wird das System eindeutig nicht nur in der Lage sein, ein Auto zu fahren, sondern wird mich auch gerne mit den Eigenschaften frischer chinesischer Elektrofahrzeuge versorgen.  Nat√ºrlich auf Russisch (optional: Englisch, Chinesisch ... endlich Albanisch).  Der Fahrer hier ist √ºberfl√ºssig, schlecht ausgebildet, eine Verbindung. <br><br>  Herr!  <b>EXTREM INTERESSANT</b> 15 Jahre warten auf uns! <br><br>  Bleib dran! <br><br>  Ich komme wieder!  ))) <br><br><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/3e8/caf/2cd/3e8caf2cde12f7b9bce6cd64de106357.png"><br><br>  <b>UPD:</b> Die interessantesten Kommentare: <br>  √úber die Quantisierung und Beschleunigung von Berechnungen auf FPGA <br><div class="spoiler">  <b class="spoiler_title">Kommentare @Mirn</b> <div class="spoiler_text"><br>  Auf dem FPGA steht nicht nur eine Arithmetik mit beliebiger Genauigkeit zur Verf√ºgung, sondern auch die verdammt wichtige F√§higkeit, Daten beliebiger Bits zu speichern und zu verarbeiten.  Zum Beispiel enth√§lt das nervige MobileNetV2 W und B zu viele Koeffizienten, und Sie k√∂nnen sie ohne gro√üen Genauigkeitsverlust auf nur 16 Bit quantisieren, oder Sie m√ºssen neu trainieren.  Wenn Sie jedoch nach innen schauen und Statistiken √ºber Kan√§le und Ebenen sammeln, k√∂nnen Sie sehen, dass alle 16 Bits nur am Eingang der ersten 1000-W-Koeffizienten verwendet werden. Der Rest hat punktuell 8 bis 11 Bits, von denen nur 2-3 h√∂chstwertige Bits und Vorzeichen wirklich wichtig sind. und Statistiken √ºber die Verwendung von Kan√§len, so dass es viele Kan√§le gibt, in denen im Allgemeinen Nullen oder kleine Werte vorhanden sind, oder Kan√§le, in denen fast alle Werte 8 bis 11 Bit sind, d.h.  Es ist m√∂glich, den Aussteller in der Kompilierungszeit in N√§gel zu nageln und nicht zu lagern, d.h.  Tats√§chlich ist es m√∂glich, nicht 16-Bit-, sondern 4-Bit-Werte im ROM-Speicher zu speichern, und Sie k√∂nnen sogar das gesamte neuronale Netzwerk auf billigen FPGAs ohne gro√üen Genauigkeitsverlust (weniger als 1%) speichern und mit einer Latenz von bis zu Zehntausenden von FPS verarbeiten, sodass wir sofort eine Antwort auf das neuronale Netzwerk erhalten Wie endet der Empfang des Frames? <br><br>  √úber die Quantisierung: Meine Idee ist, dass, wenn sich in einer Reihe von Berechnungsstufen W die Koeffizienten f√ºr Kanal Nr. 0 nur von +50 auf -50 √§ndern, es sinnvoll ist, die Bitterkeit auf 7 zu komprimieren, und wenn beispielsweise von -123 auf +124, dann auf 8 (einschlie√ülich des Vorzeichens) )    FPGA      ,      7, 8         ROM        .                 ,      . <br><br>           (,  , ),  RTL            ,          ,      .        GCC  AVX256    bitperfect (    FPGA  )           FPS      (       W  B,         ). <br><br>            W  fc   , ..      -100  +100   +10000      255      9        ( ). <br><br>                  !  weil dephwise    . <br><br>        u-law       (  !                ). <br><br>  ,          ,   6,      ,       . <br><br>                          (         ).  ‚Äî   ,    FixedPoint  dot product  ‚Äî     Fractional part,         ‚Äî       ,    ,      fc             . </div></div><br>       GPU, FPGA, ASIC    <br><div class="spoiler"> <b class="spoiler_title"> @BigPack</b> <div class="spoiler_text"><br> -        TVM   ( tvm.ai/about),      (   Keras)    .   ,      ‚Äî  ¬´¬ª-  (bare metal,    ISA, FPGA  .)       edge computing.       TVM   HLS  TVM    FPGA.  HLS     FPGA ¬´¬ª    ,  ( )      FPGA    ,    GPU/TPU . <br><br> PS  FPGA    transparent hardware (  ‚Äî open-source hardware),        ,          (    ¬´¬ª )    .     -.  , FPGA        ‚Äî         </div></div><br>      FPGA,  FPGA Microsoft     <br><div class="spoiler"> <b class="spoiler_title">  @Brak0del</b> <div class="spoiler_text"><br>    FPGA,   2019       ,      .       ‚Äî   .   /   dsp-  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Xilinx</a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Achronix</a> ,       DDR. <br><br>   , ,    , FPGA    ASIC-.   FPGA     :        ,       ASIC     ,  FPGA     -    .  Das hei√üt,     -    .            , ASIC-, ,     .  ,        FPGA  ,   ASIC. <br><br> ,    CPU, FPGA           ,      ,        . <br> ,    GPU      ,   FPGA    ,     :  , - ,    GPU      ,       ,     , -   (     ,   ,  ,   ,  FPGA   ,   GPU     , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> ).  , FPGA       ,   ,   ,    ASIC-. <br><br>       Microsoft ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> Catapult v.2</a> ),       FPGA-.  ,        FPGA.      ()    . <br><br>       FPGA         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ristretto</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deephi</a> ,      ,  Deephi       FPGA.   ,     ,         ,  . <br>   FPGA              . </div></div><br>    FPGA    ASIC <br><div class="spoiler"> <b class="spoiler_title">  @Mirn</b> <div class="spoiler_text"><br>  ,  FPGA   : <br>     ,            ASIC. <br><br>  : <br> <b>FPGA</b> <br>     (  ),     (   , ,   IP      30-50   5     ). <br>   ,     10       (    ),    5*(N+1) <br>   , ,     ‚Äî    10     ,          ,   120*N <br>        (    ,     ‚Äî   ) <br>   : (120+50+5)*N,  5   880  <br>          <br><br> <b>ASIC</b> <br>              (    2 ) <br>             <br>           (3-4 ) <br>  ASIC      ¬´ ¬ª       ‚Äî    :     ,     <br>  ,      (             ), ,     ‚Äî         ,       . <br>  :          ‚Äî     ,    ,        . <br><br>                    (     MiT ‚Äî   ,          ,           ,    ) <br><br>     ,      ,    10              3-5 ,      (  ‚Äî   ,     ,   ‚Äî      ,   ‚Äî       )          ,    :      . <br>     ! !    .  NEC  SONY (c      ,        10-15        ,    ) <br><br> : FPGA             ASIC. </div></div><br><div class="spoiler"> <b class="spoiler_title"></b> <div class="spoiler_text">    : <br><br><ul><li>      . ..           , <br></li><li>  ,     ,    , <br></li><li>   ,      ,       , <br></li><li> , ,    ,  ,  ,  ,  ,  ,  ,  ,  ,            ,     ! <br></li></ul><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de455353/">https://habr.com/ru/post/de455353/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de455341/index.html">Was ist √ºber die ITIL 4-Zertifizierung bekannt?</a></li>
<li><a href="../de455343/index.html">Schulung Cisco 200-125 CCNA v3.0. Tag 9. Die physische Welt der Schalter. Teil 2</a></li>
<li><a href="../de455345/index.html">Vorsicht Doktor</a></li>
<li><a href="../de455347/index.html">Funktionsschnittstellen ... in VBA</a></li>
<li><a href="../de455351/index.html">VMware EMPOWER 2019 - die wichtigsten Ank√ºndigungen und Schlussfolgerungen der Konferenz</a></li>
<li><a href="../de455355/index.html">Kabelfernsehnetze f√ºr die Kleinsten. Teil 8: Optisches Backbone-Netzwerk</a></li>
<li><a href="../de455361/index.html">Wir machen eine Browser-Erweiterung, die die Ergebnisse der Pr√ºfung √ºberpr√ºft</a></li>
<li><a href="../de455367/index.html">VueJs + MVC minimaler Code maximale Funktionalit√§t</a></li>
<li><a href="../de455369/index.html">Zertifizierung von Datenbankadministratoren und vielem mehr zum Jubil√§um von DevConfX (21.-22. Juni in Moskau)</a></li>
<li><a href="../de455371/index.html">Stabile Stromquelle von 5 ŒºA bis 20 mA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>