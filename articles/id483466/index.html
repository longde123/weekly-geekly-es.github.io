<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👈 🖐🏿 🧦 Memperkenalkan metode backpropagation 🚵🏽 🤱🏽 🙏🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo semuanya! Liburan Tahun Baru telah berakhir, yang berarti bahwa kami siap kembali untuk membagikan materi yang bermanfaat dengan Anda. Terjemahan...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Memperkenalkan metode backpropagation</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/483466/">  <i>Halo semuanya!</i>  <i>Liburan Tahun Baru telah berakhir, yang berarti bahwa kami siap kembali untuk membagikan materi yang bermanfaat dengan Anda.</i>  <i>Terjemahan artikel ini disiapkan untuk mengantisipasi peluncuran aliran baru pada kursus <a href="https://otus.pw/h0mh/">"Algoritma untuk Pengembang"</a> .</i> <i><br><br></i>  <i>Ayo pergi!</i> <br><br><img src="https://habrastorage.org/webt/kl/6d/cd/kl6dcdek8egee8jyp_p0_7hcz30.png"><br><br><hr><br>  Metode back propagation of error mungkin merupakan komponen paling mendasar dari jaringan saraf.  Ini pertama kali dideskripsikan pada 1960-an dan hampir 30 tahun kemudian dipopulerkan oleh Rumelhart, Hinton dan Williams dalam sebuah artikel berjudul <a href="https://www.nature.com/articles/323533a0">"Mempelajari representasi dengan kesalahan perbanyakan kembali"</a> . <a name="habracut"></a><br><br>  Metode ini digunakan untuk secara efektif melatih jaringan saraf menggunakan aturan rantai yang disebut (aturan diferensiasi fungsi kompleks).  Sederhananya, setelah setiap melewati jaringan, propagasi kembali melakukan oper pada arah yang berlawanan dan menyesuaikan parameter model (bobot dan perpindahan). <br><br>  Pada artikel ini, saya ingin mempertimbangkan secara rinci dari sudut pandang matematika proses pembelajaran dan mengoptimalkan jaringan saraf 4-layer yang sederhana.  Saya percaya ini akan membantu pembaca memahami cara kerja backpropagation, serta menyadari signifikansinya. <br><br><h3>  Mendefinisikan model jaringan saraf </h3><br>  Jaringan saraf empat-lapisan terdiri dari empat neuron di lapisan input, empat neuron di lapisan tersembunyi dan 1 neuron di lapisan output. <br><br><img src="https://habrastorage.org/webt/d3/1z/7q/d31z7q7wxug2d-435f_t1fi19ki.png"><br>  <i>Gambar sederhana dari jaringan saraf empat lapis.</i> <br><br><h3>  Lapisan input </h3><br>  Dalam gambar, neuron ungu mewakili input.  Mereka dapat jumlah skalar sederhana atau lebih kompleks - vektor atau matriks multidimensi. <br><br><img src="https://habrastorage.org/webt/su/ba/e1/subae1x6bn1yju51obgv3qaephm.png"><br>  <i>Persamaan yang menjelaskan input xi.</i> <br><br>  Set aktivasi pertama (a) sama dengan nilai input.  "Aktivasi" adalah nilai neuron setelah menerapkan fungsi aktivasi.  Lihat di bawah untuk detail lebih lanjut. <br><br><h3>  Lapisan tersembunyi </h3><br>  Nilai akhir dalam neuron tersembunyi (dalam gambar hijau) dihitung menggunakan input berbobot z <sup>l</sup> pada lapisan I dan aktivasi <sup>I</sup> pada lapisan L. Untuk lapisan 2 dan 3, persamaannya adalah sebagai berikut: <br><br>  Untuk l = 2: <br><br><img src="https://habrastorage.org/webt/wh/ix/ho/whixhogzr32hedjvb-rackpht1c.png"><br><br>  Untuk l = 3: <br><br><img src="https://habrastorage.org/webt/yv/eb/qq/yvebqquzvpxg3iu3xwqbhli-fpu.png"><br><br>  W <sup>2</sup> dan W <sup>3</sup> adalah bobot pada layer 2 dan 3, dan b <sup>2</sup> dan b <sup>3</sup> adalah offset pada lapisan ini. <br><br>  Aktivasi <sup>2</sup> dan <sup>3</sup> dihitung menggunakan fungsi aktivasi f.  Sebagai contoh, fungsi ini f adalah non-linear (seperti <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> , <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> dan <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">hiperbolik tangen</a> ) dan memungkinkan jaringan untuk mempelajari pola kompleks dalam data.  Kami tidak akan membahas tentang bagaimana fungsi aktivasi bekerja, tetapi jika Anda tertarik, saya sangat merekomendasikan membaca <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">artikel yang</a> luar biasa <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">ini</a> . <br><br>  Jika Anda melihat lebih dekat, Anda akan melihat bahwa semua x, z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> , a <sup>3</sup> , W <sup>1</sup> , W <sup>2</sup> , b <sup>1</sup> dan b <sup>2</sup> tidak memiliki indeks yang lebih rendah seperti pada gambar jaringan saraf empat lapis.  Faktanya adalah bahwa kami menggabungkan semua nilai parameter ke dalam matriks yang dikelompokkan berdasarkan layer.  Ini adalah cara standar untuk bekerja dengan jaringan saraf, dan cukup nyaman.  Namun, saya akan pergi melalui persamaan sehingga tidak ada kebingungan. <br><br>  Mari kita ambil layer 2 dan parameternya sebagai contoh.  Operasi yang sama dapat diterapkan ke semua lapisan jaringan saraf. <br>  W <sup>1</sup> adalah matriks bobot dimensi <i>(n, m)</i> , di mana <i>n</i> adalah jumlah neuron keluaran (neuron pada lapisan berikutnya), dan <i>m</i> adalah jumlah neuron input (neuron pada lapisan sebelumnya).  Dalam kasus kami, <i>n = 2</i> dan <i>m = 4</i> . <br><br><img src="https://habrastorage.org/webt/ez/pw/6j/ezpw6j_huyb2cr5zkxinl9ylku8.png"><br><br>  Di sini, angka pertama dalam subskrip dari setiap bobot sesuai dengan indeks neuron pada lapisan berikutnya (dalam kasus kami, ini adalah lapisan tersembunyi kedua), dan angka kedua sesuai dengan indeks neuron pada lapisan sebelumnya (dalam kasus kami, ini adalah lapisan input). <br><br>  <i>x</i> adalah vektor input dimensi ( <i>m</i> , 1), di mana <i>m</i> adalah jumlah neuron input.  Dalam kasus kami, <i>m</i> = 4. <br><br><img src="https://habrastorage.org/webt/5a/by/8a/5aby8acxjiohf0-f5jrmgbfbxsi.png"><br><br>  b <sup>1</sup> adalah vektor perpindahan dimensi ( <i>n</i> , 1), di mana <i>n</i> adalah jumlah neuron dalam lapisan saat ini.  Dalam kasus kami, <i>n</i> = 2. <br><br><img src="https://habrastorage.org/webt/2u/5t/9v/2u5t9vhiftmq9fou4khqqvkynhc.png"><br><br>  Dengan mengikuti persamaan untuk z2 <sup>,</sup> kita dapat menggunakan definisi W1, x dan b1 di <sup>atas</sup> untuk mendapatkan persamaan z2: <br><br><img src="https://habrastorage.org/webt/-5/bz/kz/-5bzkzalwngzrkhbuwq52fugpkc.png"><br><br>  Sekarang perhatikan dengan seksama ilustrasi jaringan saraf di atas: <br><br><img src="https://habrastorage.org/webt/_e/sj/ld/_esjld_rfdemfxpuztceadijwns.png"><br><br>  Seperti yang Anda lihat, z <sup>2</sup> dapat diekspresikan dalam bentuk z <sub>1</sub> <sup>2</sup> dan z <sub>2</sub> <sup>2</sup> , di mana z <sub>1</sub> <sup>2</sup> dan z <sub>2</sub> <sup>2</sup> adalah jumlah dari produk dari masing-masing nilai input x <sup>i</sup> dengan bobot yang sesuai W <sub>ij</sub> <sup>1</sup> . <br><br>  Ini mengarah ke persamaan yang sama untuk z <sup>2</sup> dan membuktikan bahwa representasi matriks z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> dan a <sup>3</sup> adalah benar. <br><br><h3>  Lapisan output </h3><br>  Bagian terakhir dari jaringan saraf adalah lapisan keluaran, yang memberikan nilai prediksi.  Dalam contoh sederhana kami, itu disajikan dalam bentuk neuron tunggal bernoda biru dan dihitung sebagai berikut: <br><br><img src="https://habrastorage.org/webt/fy/vh/05/fyvh05jvkxbosdqhaqak-vbzn0k.png"><br><br>  Sekali lagi, kami menggunakan representasi matriks untuk menyederhanakan persamaan.  Anda dapat menggunakan metode di atas untuk memahami logika yang mendasarinya. <br><br><h3>  Distribusi dan evaluasi langsung </h3><br>  Persamaan di atas membentuk distribusi langsung melalui jaringan saraf.  Berikut ini gambaran singkatnya: <br><br><img src="https://habrastorage.org/webt/pe/ya/fr/peyafrffaxqvrnjeito3i-j-gpk.png"><br><br>  <i>(1) - lapisan input</i> <i><br></i>  <i>(2) - nilai neuron di lapisan tersembunyi pertama</i> <i><br></i>  <i>(3) - nilai aktivasi pada lapisan tersembunyi pertama</i> <i><br></i>  <i>(4) - nilai neuron di lapisan kedua yang tersembunyi</i> <i><br></i>  <i>(5) - nilai aktivasi di tingkat tersembunyi kedua</i> <i><br></i>  <i>(6) - lapisan keluaran</i> <br><br>  Langkah terakhir dalam operan langsung adalah mengevaluasi nilai output yang diprediksi relatif terhadap nilai output yang diharapkan <i>y</i> . <br><br>  Output y adalah bagian dari set data pelatihan (x, y), di mana <i>x</i> adalah input (seperti yang kita ingat dari bagian sebelumnya). <br><br>  Estimasi antara <i>s</i> dan <i>y</i> terjadi melalui fungsi kerugian.  Ini bisa sederhana sebagai <a href="https://en.wikipedia.org/wiki/Mean_squared_error">kesalahan standar</a> atau lebih kompleks sebagai <a href="http://neuralnetworksanddeeplearning.com/chap3.html">cross entropy</a> . <br><br>  Kami menyebut fungsi kerugian ini C dan menyatakannya sebagai berikut: <br><br><img src="https://habrastorage.org/webt/eg/s7/wh/egs7whz63c-ryaazd_r-vvuxbae.png"><br><br>  Di mana <i>biaya</i> dapat sama dengan kesalahan standar, entropi silang, atau fungsi kerugian lainnya. <br><br>  Berdasarkan nilai C, model “tahu” berapa parameternya perlu disesuaikan untuk mendekati nilai output <i>y yang diharapkan</i> .  Ini terjadi menggunakan metode backpropagation. <br><br><h3>  Kembali propagasi kesalahan dan perhitungan gradien </h3><br>  Berdasarkan artikel 1989, metode backpropagation: <br><br>  <i>Secara konstan menyesuaikan bobot koneksi dalam jaringan untuk meminimalkan ukuran perbedaan antara vektor keluaran aktual jaringan dan vektor keluaran yang diinginkan</i> . <br>  dan <br>  <i>... memungkinkan untuk membuat fungsi-fungsi baru yang berguna yang membedakan backpropagation dari metode sebelumnya dan lebih sederhana ...</i> <br><br>  Dengan kata lain, propagasi balik ditujukan untuk meminimalkan fungsi kerugian dengan menyesuaikan bobot dan offset jaringan.  Tingkat penyesuaian ditentukan oleh gradien dari fungsi kerugian sehubungan dengan parameter ini. <br><br>  Satu pertanyaan muncul: <i>Mengapa menghitung gradien</i> ? <br><br>  Untuk menjawab pertanyaan ini, pertama-tama kita perlu merevisi beberapa konsep komputasi: <br><br>  Gradien fungsi C (x <sup>1</sup> , x <sup>2</sup> , ..., x <sup>m</sup> ) pada x adalah <a href="https://en.wikipedia.org/wiki/Partial_derivative">vektor turunan parsial</a> C <i>sehubungan</i> dengan <i>x</i> . <br><br><img src="https://habrastorage.org/webt/km/eo/zl/kmeozlylfgdy7nknsa0cq6ytaei.png"><br><br>  Turunan dari fungsi C mencerminkan sensitivitas terhadap perubahan nilai fungsi (nilai output) relatif terhadap perubahan dalam argumen <i>x</i> ( <a href="https://en.wikipedia.org/wiki/Derivative">nilai input</a> ).  Dengan kata lain, turunan memberi tahu kita ke arah mana C. bergerak. <br><br>  Gradien menunjukkan betapa perlu untuk mengubah parameter <i>x</i> (dalam arah positif atau negatif) untuk meminimalkan C. <br><br>  Gradien ini dihitung menggunakan metode yang disebut <a href="https://en.wikipedia.org/wiki/Chain_rule">aturan</a> rantai. <br>  Untuk satu berat (w <sup>jk</sup> ) <sub>l,</sub> gradiennya adalah: <br><br><img src="https://habrastorage.org/webt/y7/97/mu/y797mumguvia31hytpq6gzq3gvy.png"><br><br>  <i>(1) Aturan rantai</i> <i><br></i>  <i>(2) Menurut definisi, m adalah jumlah neuron per l - 1 lapisan</i> <i><br></i>  <i>(3) Perhitungan derivatif</i> <i><br></i>  <i>(4) Nilai akhir</i> <i><br></i>  <i>Serangkaian persamaan yang serupa dapat diterapkan ke (b <sup>j</sup> ) <sub>l</sub></i> : <br><br><img src="https://habrastorage.org/webt/oo/7_/gz/oo7_gzmr5wpgql73bxefnlfob5u.png"><br><br>  <i>(1) Aturan rantai</i> <i><br></i>  <i>(2) Perhitungan derivatif</i> <i><br></i>  <i>(3) Nilai akhir</i> <br>  Bagian umum dalam kedua persamaan sering disebut "gradien lokal" dan dinyatakan sebagai berikut: <br><br><img src="https://habrastorage.org/webt/k9/4a/nc/k94anc1xfk3sjjgk08qf9_48fam.png"><br><br>  "Gradien lokal" dapat dengan mudah ditentukan menggunakan aturan rantai.  Saya tidak akan melukis proses ini sekarang. <br><br>  Gradien memungkinkan pengoptimalan parameter model: <br><br>  Sampai kriteria berhenti tercapai, berikut ini dilakukan: <br><br><img src="https://habrastorage.org/webt/xw/31/1s/xw311s5zex1_sdlnvvucd9qqubk.png"><br><br>  <i>Algoritma untuk mengoptimalkan bobot dan offset</i> (juga disebut gradient descent) <br><ul><li>  Nilai awal <i>w</i> dan <i>b</i> dipilih secara acak. </li><li>  Epsilon (e) adalah kecepatan belajar.  Ini menentukan efek gradien. </li><li>  <i>w</i> dan <i>b</i> adalah representasi matriks dari bobot dan offset. </li><li>  Turunan C sehubungan dengan <i>w</i> atau <i>b</i> dapat dihitung dengan menggunakan turunan sebagian C sehubungan dengan bobot atau offset individu. </li><li>  Kondisi terminasi dipenuhi segera setelah fungsi kerugian diminimalkan. </li></ul><br><br>  Saya ingin mengabdikan bagian terakhir dari bagian ini untuk contoh sederhana di mana kita menghitung gradien C sehubungan dengan satu berat (w <sup>22</sup> ) <sub>2</sub> . <br><br>  Mari memperbesar bagian bawah jaringan saraf yang disebutkan di atas: <br><br><img src="https://habrastorage.org/webt/l7/0w/6d/l70w6d7hhxqjm0wqxtwoj8y8nxq.png"><br><br>  <i>Representasi visual backpropagation dalam jaringan saraf</i> <br>  Berat (w <sup>22</sup> ) <sub>2</sub> menghubungkan (a <sup>2</sup> ) <sub>2</sub> dan (z <sup>2</sup> ) <sub>2</sub> , jadi menghitung gradien membutuhkan penerapan aturan rantai pada (z <sup>2</sup> ) <sub>3</sub> dan (a <sup>2</sup> ) <sub>3</sub> : <br><br><img src="https://habrastorage.org/webt/n_/mz/nm/n_mznmzn_dt1lqe-nyx7qoplcsa.png"><br><br>  Perhitungan nilai akhir dari turunan C dari (a <sup>2</sup> ) <sub>3</sub> membutuhkan pengetahuan tentang fungsi C. Karena C tergantung pada (a <sup>2</sup> ) <sub>3</sub> , perhitungan derivatif harus sederhana. <br><br>  Saya harap contoh ini telah berhasil menjelaskan matematika di balik menghitung gradien.  Jika Anda ingin tahu lebih banyak, saya sangat menyarankan agar Anda memeriksa serangkaian artikel NLP Stanford, di mana Richard Socher memberikan 4 penjelasan bagus untuk propaganda balik. <br><br><h3>  Komentar penutup </h3><br>  Dalam artikel ini, saya menjelaskan secara rinci bagaimana perbanyakan kembali kesalahan bekerja di bawah tenda menggunakan metode matematika seperti menghitung gradien, aturan rantai, dll.  Mengetahui mekanisme algoritme ini akan memperkuat pengetahuan Anda tentang jaringan saraf dan memungkinkan Anda merasa nyaman saat bekerja dengan model yang lebih kompleks.  Semoga sukses dalam perjalanan belajar Anda yang dalam! <br><br>  <b><i>Itu saja.</i></b>  <b><i>Kami mengundang semua orang ke webinar gratis dengan tema <a href="https://otus.pw/h0mh/">"Pohon segmen: sederhana dan cepat</a> . <a href="https://otus.pw/h0mh/">"</a></i></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id483466/">https://habr.com/ru/post/id483466/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id483448/index.html">Disney - Dua Arah Terhebat dalam Sejarah Manusia</a></li>
<li><a href="../id483454/index.html">Beralih dari Mercurial ke GIT di Atlassian Bitbucket dengan menyimpan file dalam Cyrillic</a></li>
<li><a href="../id483458/index.html">Asisten Basis Data GreenPig</a></li>
<li><a href="../id483460/index.html">SQL HowTo: membangun rantai menggunakan fungsi jendela</a></li>
<li><a href="../id483462/index.html">Diam dan ambil uangku</a></li>
<li><a href="../id483468/index.html">Tes Integrasi Flutter - Mudah</a></li>
<li><a href="../id483470/index.html">Lay tile secara efisien (Pro CSS, SVG, pola, dan lainnya)</a></li>
<li><a href="../id483472/index.html">Hapus semua: cara menghapus data dan mengembalikan SSD NVMe ke pengaturan pabrik</a></li>
<li><a href="../id483476/index.html">Moral transportasi robot: masalah troli, risiko dan konsekuensi</a></li>
<li><a href="../id483478/index.html">Matahari, angin, dan air ver 0.1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>