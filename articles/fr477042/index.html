<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📴 💓 👙 Optimisation de la stratégie de blackjack de Monte Carlo 👩🏽‍🤝‍👨🏾 🖐🏼 🅰️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La traduction de l'article a été préparée spécialement pour les étudiants du cours Machine Learning . 



 La formation renforcée a pris le monde de l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Optimisation de la stratégie de blackjack de Monte Carlo</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477042/">  <i>La traduction de l'article a été préparée spécialement pour les étudiants du <a href="https://otus.pw/Zkti/">cours Machine Learning</a> .</i> <br><hr><br><img src="https://habrastorage.org/webt/dj/mh/7h/djmh7hubq1mecnsb-gilezo8qyi.png"><br><br>  La formation renforcée a pris le monde de l'intelligence artificielle.  À partir d'AlphaGo et d' <a href="https://www.space.com/alphastar-artificial-intelligence-starcraft-2-grandmaster.html">AlphaStar</a> , un nombre croissant d'activités qui étaient auparavant dominées par les humains sont désormais conquises par des agents de l'IA basés sur la formation de renforcement.  En bref, ces réalisations dépendent de l'optimisation des actions de l'agent dans un environnement particulier pour obtenir une récompense maximale.  Dans les derniers articles de <a href="https://medium.com/gradientcrescent">GradientCrescent,</a> nous avons examiné divers aspects fondamentaux de l'apprentissage renforcé, depuis les bases des systèmes de <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-the-k-bandit-problem-illustrat-940eea430296">bandits</a> et <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">des</a> approches basées sur les <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">politiques</a> jusqu'à l'optimisation du comportement basé sur <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82">les</a> récompenses dans <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82">les environnements Markov</a> .  Toutes ces approches nécessitaient une connaissance complète de notre environnement.  <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310">La programmation dynamique</a> , par exemple, nécessite que nous ayons une distribution de probabilité complète de toutes les transitions d'état possibles.  Cependant, en réalité, nous constatons que la plupart des systèmes ne peuvent pas être entièrement interprétés et que les distributions de probabilité ne peuvent pas être obtenues explicitement en raison de la complexité, de l'incertitude inhérente ou des limitations des capacités de calcul.  Par analogie, considérons la tâche du météorologue - le nombre de facteurs impliqués dans les prévisions météorologiques peut être si grand qu'il est impossible de calculer avec précision la probabilité. <a name="habracut"></a><br><br>  Dans de tels cas, des méthodes d'enseignement telles que Monte Carlo sont la solution.  Le terme Monte Carlo est couramment utilisé pour décrire toute approche d'estimation d'échantillonnage aléatoire.  En d'autres termes, nous ne prédisons pas les connaissances sur notre environnement, mais apprenons de l'expérience en passant par des séquences exemplaires d'états, d'actions et de récompenses obtenues à la suite d'une interaction avec l'environnement.  Ces méthodes fonctionnent en observant directement les récompenses retournées par le modèle en fonctionnement normal afin de juger de la valeur moyenne de ses conditions.  Fait intéressant, même sans aucune connaissance de la dynamique de l'environnement (qui devrait être considérée comme la distribution de probabilité des transitions d'état), nous pouvons toujours obtenir un comportement optimal pour maximiser les récompenses. <br><br>  À titre d'exemple, considérons le résultat d'un lancer de 12 dés.  En considérant ces lancers comme un seul état, nous pouvons faire la moyenne de ces résultats afin de nous rapprocher du vrai résultat prédit.  Plus l'échantillon est grand, plus nous nous rapprocherons avec précision du résultat réel attendu. <br><br><img src="https://habrastorage.org/webt/xr/85/d6/xr85d6ugn6dbszaqh5b-nbq-usq.png"><br>  <i>Le montant moyen attendu sur 12 dés pour 60 tirs est de 41,57</i> <br><br>  Ce type d'évaluation basée sur l'échantillonnage peut sembler familier au lecteur, car un tel échantillonnage est également effectué pour les systèmes à k-bandit.  Au lieu de comparer différents bandits, les méthodes de Monte Carlo sont utilisées pour comparer différentes politiques dans les environnements Markov, en déterminant la valeur de l'état lorsqu'une certaine politique est suivie jusqu'à ce que le travail soit terminé. <br><br><h3>  Estimation de Monte Carlo de la valeur d'état </h3><br>  Dans le contexte de l'apprentissage par renforcement, les méthodes de Monte Carlo sont un moyen d'évaluer la signification de l'état d'un modèle en faisant la moyenne des résultats des échantillons.  En raison de la nécessité d'un état terminal, les méthodes de Monte Carlo sont intrinsèquement applicables aux environnements épisodiques.  En raison de cette limitation, les méthodes Monte Carlo sont généralement considérées comme «autonomes» dans lesquelles toutes les mises à jour sont effectuées après avoir atteint l'état terminal.  Une analogie simple avec la recherche d'un moyen de sortir d'un labyrinthe peut être donnée - une approche autonome obligerait l'agent à atteindre la fin avant d'utiliser l'expérience intermédiaire acquise pour essayer de réduire le temps qu'il faut pour traverser le labyrinthe.  D'un autre côté, avec l'approche en ligne, l'agent changera constamment son comportement déjà lors du passage du labyrinthe, peut-être qu'il remarquera que les couloirs verts mènent à des impasses et décidera de les éviter, par exemple.  Nous discuterons des approches en ligne dans l'un des articles suivants. <br><br>  La méthode de Monte Carlo peut être formulée comme suit: <br><br><img src="https://habrastorage.org/webt/u4/kd/o_/u4kdo_uc3dj64rihmdjhnyj1r7y.png"><br><br>  Pour mieux comprendre le fonctionnement de la méthode Monte Carlo, considérez le diagramme de transition d'état ci-dessous.  La récompense pour chaque transition d'état est affichée en noir, un facteur d'actualisation de 0,5 lui est appliqué.  Mettons de côté la valeur réelle de l'état et concentrons-nous sur le calcul des résultats d'un lancer. <br><br><img src="https://habrastorage.org/webt/oc/he/n9/ochen91kvgbix5pgltzijpbozgg.png"><br>  <i>Diagramme de transition d'état.</i>  <i>Le numéro d'état est affiché en rouge, le résultat est noir.</i> <br>  Étant donné que l'état terminal renvoie un résultat égal à 0, calculons le résultat de chaque état, en commençant par l'état terminal (G5).  Veuillez noter que nous avons fixé le facteur d'actualisation à 0,5, ce qui entraînera une pondération vers les États ultérieurs. <br><br><img src="https://habrastorage.org/webt/uw/y6/qq/uwy6qq_amgfxpsyj2_3gpij498o.png"><br><br>  Ou plus généralement: <br><br><img src="https://habrastorage.org/webt/k7/i4/ua/k7i4ua-g8vaph8w62-rfpc3r8sk.png"><br><br>  Pour éviter de stocker tous les résultats dans la liste, nous pouvons effectuer la procédure de mise à jour progressive de la valeur d'état dans la méthode de Monte Carlo, en utilisant une équation qui présente quelques similitudes avec la descente de gradient traditionnelle: <br><br><img src="https://habrastorage.org/webt/lv/tm/nk/lvtmnki_ff4csblmytygnnxjn6u.png"><br>  <i>Procédure de mise à jour incrémentielle de Monte Carlo.</i>  <i>S est l'état, V est sa valeur, G est son résultat et A est le paramètre de valeur de pas.</i> <br><br>  Dans le cadre de la formation de renforcement, les méthodes de Monte Carlo peuvent même être classées en première visite ou à chaque visite.  En bref, la différence entre les deux est le nombre de fois qu'un état peut être visité par un passage avant la mise à jour de Monte Carlo.  La méthode Monte Carlo First-visit estime la valeur de tous les états comme la valeur moyenne des résultats après des visites uniques dans chaque état avant la fin, tandis que la méthode Monte Carlo Every visit fait la moyenne des résultats après n visites jusqu'à la fin.  Nous utiliserons la première visite de Monte Carlo tout au long de cet article en raison de sa relative simplicité. <br><br><h3>  Monte Carlo Policy Management </h3><br>  Si le modèle ne peut pas fournir la politique, Monte Carlo peut être utilisé pour évaluer les valeurs d'état-action.  Ceci est plus utile que la simple signification des états, car l'idée de la signification de chaque action <i>(q)</i> dans un état donné permet à l'agent de formuler automatiquement une politique à partir d'observations dans un environnement inconnu. <br><br>  Plus formellement, nous pouvons utiliser Monte Carlo pour estimer <i>q (s, a, pi)</i> , le résultat attendu en partant de l'état s, de l'action a et de la politique <i>Pi suivante</i> .  Les méthodes de Monte-Carlo restent les mêmes, sauf qu'il existe une dimension supplémentaire des actions entreprises pour un certain état.  On pense qu'une paire état-action <i>(s, a)</i> est visitée pendant le passage si l'état <i>s est</i> jamais visité et que l'action <i>a y</i> est effectuée.  De même, l'évaluation des actions de valeur peut être réalisée en utilisant les approches «Première visite» et «Chaque visite». <br><br>  Comme dans la programmation dynamique, nous pouvons utiliser une politique d'itération généralisée (GPI) pour former une politique à partir de l'observation des valeurs d'état-action. <br><br><img src="https://habrastorage.org/webt/wi/pm/p5/wipmp58itbgocsyruulw0ccliay.png"><br><br>  En alternant les étapes de l'évaluation et de l'amélioration des politiques, et en incluant des recherches pour s'assurer que toutes les actions possibles sont visitées, nous pouvons atteindre la politique optimale pour chaque condition.  Pour le Monte Carlo GPI, cette rotation se fait généralement après la fin de chaque passage. <br><br><img src="https://habrastorage.org/webt/nf/b4/nw/nfb4nwijndtfex4ytwhdn2ujcyw.png"><br>  <i>Monte Carlo GPI</i> <br><br><h3>  Stratégie de Blackjack </h3><br>  Pour mieux comprendre comment la méthode de Monte Carlo fonctionne dans la pratique pour évaluer différentes valeurs d'état, prenons une démonstration étape par étape de l'exemple d'un jeu de blackjack.  Pour commencer, déterminons les règles et conditions de notre jeu: <br><br><ul><li>  Nous jouerons uniquement contre le croupier, il n'y aura pas d'autres joueurs.  Cela nous permettra de considérer les mains du concessionnaire comme faisant partie de l'environnement. </li><li>  La valeur des cartes avec des nombres égaux aux valeurs nominales.  La valeur des cartes illustrées: Jack, King and Queen est de 10. La valeur de l'as peut être de 1 ou 11 selon le choix du joueur. </li><li>  Les deux camps reçoivent deux cartes.  Deux cartes de joueur sont face visible, une des cartes du croupier est également face visible. </li><li>  Le but du jeu est que le nombre de cartes en main soit &lt;= 21.  Une valeur supérieure à 21 est un buste, si les deux côtés ont une valeur de 21, alors le jeu se joue en match nul. </li><li>  Une fois que le joueur a vu ses cartes et la première carte du croupier, le joueur peut choisir de lui prendre une nouvelle carte ("encore") ou non ("assez") jusqu'à ce qu'il soit satisfait de la somme des valeurs de carte dans sa main. </li><li>  Ensuite, le croupier montre sa deuxième carte - si le montant résultant est inférieur à 17, il est obligé de prendre des cartes jusqu'à ce qu'il atteigne 17 points, après quoi il ne prend plus la carte. </li></ul><br>  Voyons comment la méthode Monte Carlo fonctionne avec ces règles. <br><br><h4>  Tour 1. </h4><br>  Vous gagnez un total de 19. Mais vous essayez d'attraper la chance par la queue, de prendre une chance, d'en obtenir 3 et de vous ruiner.  Lorsque vous avez fait faillite, le croupier n'avait qu'une seule carte ouverte avec une somme de 10. Cela peut être représenté comme suit: <br><br><img src="https://habrastorage.org/webt/4o/di/sl/4odisljnoapgkqblzsixkr2a4_4.png"><br><br>  Si nous faisons faillite, notre récompense pour la manche est de -1.  Définissons cette valeur comme le résultat de retour de l'avant-dernier état, en utilisant le format suivant [Montant de l'agent, Montant du concessionnaire, as?]: <br><br><img src="https://habrastorage.org/webt/2q/er/g4/2qerg4mooln9blj9sfqo1ndzxbo.png"><br><br>  Eh bien, maintenant nous n'avons plus de chance.  Passons à un autre tour. <br><br><h4>  Tour 2. </h4><br>  Vous tapez un total de 19. Cette fois, vous décidez d'arrêter.  Le croupier compose le 13, prend une carte et fait faillite.  L'avant-dernier état peut être décrit comme suit. <br><br><img src="https://habrastorage.org/webt/al/th/dv/althdvyqwkhwqd5cj2vvzcjbiam.png"><br><br>  Décrivons les conditions et les récompenses que nous avons reçues lors de ce tour: <br><br><img src="https://habrastorage.org/webt/rm/mv/n8/rmmvn88mg7za9gyrtvrvohywmwc.png"><br><br>  Avec la fin du passage, nous pouvons maintenant mettre à jour les valeurs de tous nos états dans ce tour en utilisant les résultats calculés.  En prenant un facteur de remise de 1, nous répartissons simplement notre nouvelle récompense de main, comme cela a été fait avec les transitions d'état plus tôt.  Puisque l'état <i>V (19, 10, non) a</i> précédemment renvoyé -1, nous calculons la valeur de retour attendue et l'affectons à notre état: <br><br><img src="https://habrastorage.org/webt/u0/um/q9/u0umq9dpg6qw420ecz44cqfp9mk.png"><br>  <i>Valeurs d'état final pour la démonstration en utilisant le blackjack comme exemple</i> . <br><br><h3>  Implémentation </h3><br>  Écrivons un jeu de blackjack en utilisant la méthode Monte Carlo de la première visite pour découvrir toutes les valeurs d'état possibles (ou diverses combinaisons disponibles) dans le jeu en utilisant Python.  Notre approche sera basée sur l' <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">approche de Sudharsan et.</a>  <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">al.</a>  .  Comme d'habitude, vous pouvez retrouver tout le code de l'article sur <a href="https://github.com/EXJUSTICE/GradientCrescent)">notre GitHub</a> . <br><br>  Pour simplifier la mise en œuvre, nous utiliserons gym d'OpenAI.  Considérez l'environnement comme une interface pour démarrer le blackjack avec une quantité minimale de code, cela nous permettra de nous concentrer sur la mise en œuvre d'un apprentissage renforcé.  Idéalement, toutes les informations collectées sur les états, les actions et les récompenses sont stockées dans les variables <i>«observation»</i> , qui sont accumulées pendant les sessions de jeu en cours. <br><br>  Commençons par importer toutes les bibliothèques dont nous avons besoin pour obtenir et collecter nos résultats. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyplot <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> defaultdict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> functools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> partial %matplotlib inline plt.style.use(<span class="hljs-string"><span class="hljs-string">'ggplot'</span></span>)</code> </pre> <br>  Ensuite, initialisons notre environnement de <i>gym</i> et définissons une politique qui coordonnera les actions de notre agent.  En fait, nous continuerons à prendre la carte jusqu'à ce que le montant en main atteigne 19 ou plus, après quoi nous nous arrêterons. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Observation here encompassess all data about state that we need, as well as reactions to it env = gym.make('Blackjack-v0') #Define a policy where we hit until we reach 19. # actions here are 0-stand, 1-hit def sample_policy(observation): score, dealer_score, usable_ace = observation return 0 if score &gt;= 19 else 1</span></span></code> </pre> <br>  Définissons une méthode pour générer des données de passe à l'aide de notre politique.  Nous conserverons des informations sur l'état, les actions entreprises et les récompenses de l'action. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate_episode</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># we initialize the list for storing states, actions, and rewards states, actions, rewards = [], [], [] # Initialize the gym environment observation = env.reset() while True: # append the states to the states list states.append(observation) # now, we select an action using our sample_policy function and append the action to actions list action = sample_policy(observation) actions.append(action) # We perform the action in the environment according to our sample_policy, move to the next state observation, reward, done, info = env.step(action) rewards.append(reward) # Break if the state is a terminal state (ie done) if done: break return states, actions, rewards</span></span></code> </pre> <br>  Enfin, définissons la première visite de la fonction de prédiction Monte Carlo.  Tout d'abord, nous initialisons un dictionnaire vide pour stocker les valeurs de l'état actuel et un dictionnaire qui stocke le nombre d'enregistrements pour chaque état dans différentes passes. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">first_visit_mc_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env, n_episodes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, we initialize the empty value table as a dictionary for storing the values of each state value_table = defaultdict(float) N = defaultdict(int)</span></span></code> </pre> <br>  Pour chaque passage, nous appelons notre méthode <i>generate_episode</i> pour obtenir des informations sur les valeurs d'état et les récompenses reçues après que l'état se soit produit.  Nous initialisons également la variable pour stocker nos résultats incrémentiels.  Ensuite, nous obtenons la récompense et la valeur de l'état actuel pour chaque état visité pendant le passage, et augmentons nos rendements variables de la valeur de la récompense par étape. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_episodes): <span class="hljs-comment"><span class="hljs-comment"># Next, we generate the epsiode and store the states and rewards states, _, rewards = generate_episode(policy, env) returns = 0 # Then for each step, we store the rewards to a variable R and states to S, and we calculate for t in range(len(states) — 1, -1, -1): R = rewards[t] S = states[t] returns += R # Now to perform first visit MC, we check if the episode is visited for the first time, if yes, #This is the standard Monte Carlo Incremental equation. # NewEstimate = OldEstimate+StepSize(Target-OldEstimate) if S not in states[:t]: N[S] += 1 value_table[S] += (returns — value_table[S]) / N[S] return value_table</span></span></code> </pre> <br>  Permettez-moi de vous rappeler que puisque nous mettons en œuvre la première visite de Monte-Carlo, nous visitons un État en un seul passage.  Par conséquent, nous effectuons une vérification conditionnelle du dictionnaire d'état pour voir si l'état a été visité.  Si cette condition est remplie, nous sommes en mesure de calculer la nouvelle valeur en utilisant la procédure précédemment définie pour mettre à jour les valeurs d'état en utilisant la méthode de Monte Carlo et d'augmenter le nombre d'observations pour cet état de 1. Ensuite, nous répétons le processus pour la prochaine passe afin d'obtenir finalement la valeur moyenne du résultat . <br><br>  Lançons ce que nous avons obtenu et regardons les résultats! <br><br><pre> <code class="python hljs">value = first_visit_mc_prediction(sample_policy, env, n_episodes=<span class="hljs-number"><span class="hljs-number">500000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): print(value.popitem())</code> </pre> <br><img src="https://habrastorage.org/webt/xc/ow/al/xcowalpr-34tgxtf0gq8zlfllgg.png"><br>  <i>Conclusion d'un échantillon montrant les valeurs d'état de diverses combinaisons sur les mains au blackjack.</i> <br><br>  Nous pouvons continuer à faire des observations de Monte Carlo pour 5000 passes et construire une distribution des valeurs d'état qui décrit les valeurs de n'importe quelle combinaison entre les mains du joueur et du croupier. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_blackjack</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(V, ax1, ax2)</span></span></span><span class="hljs-function">:</span></span> player_sum = np.arange(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">21</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) dealer_show = np.arange(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) usable_ace = np.array([<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>]) state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, player <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(player_sum): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j, dealer <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(dealer_show): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k, ace <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(usable_ace): state_values[i, j, k] = V[player, dealer, ace] X, Y = np.meshgrid(player_sum, dealer_show) ax1.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>]) ax2.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ax1, ax2: ax.set_zlim(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) ax.set_ylabel(<span class="hljs-string"><span class="hljs-string">'player sum'</span></span>) ax.set_xlabel(<span class="hljs-string"><span class="hljs-string">'dealer sum'</span></span>) ax.set_zlabel(<span class="hljs-string"><span class="hljs-string">'state-value'</span></span>) fig, axes = pyplot.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>),subplot_kw={<span class="hljs-string"><span class="hljs-string">'projection'</span></span>: <span class="hljs-string"><span class="hljs-string">'3d'</span></span>}) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/o usable ace'</span></span>) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/ usable ace'</span></span>) plot_blackjack(value, axes[<span class="hljs-number"><span class="hljs-number">0</span></span>], axes[<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/db/7t/he/db7thebzqawmtv_jemoprsaeqvg.png"><br>  <i>Visualisation des valeurs d'état de diverses combinaisons au blackjack.</i> <br><br>  Résumons donc ce que nous avons appris. <br><br><ul><li>  Les méthodes d'apprentissage basées sur l'échantillonnage nous permettent d'évaluer les valeurs d'état et d'action-état sans aucune dynamique de transition, simplement par échantillonnage. </li><li>  Les approches de Monte Carlo sont basées sur un échantillonnage aléatoire du modèle, en observant les récompenses retournées par le modèle et en collectant des informations pendant le fonctionnement normal pour déterminer la valeur moyenne de ses états. </li><li>  En utilisant les méthodes de Monte Carlo, une politique d'itération généralisée est possible. </li><li>  La valeur de toutes les combinaisons possibles entre les mains du joueur et du croupier au blackjack peut être estimée en utilisant plusieurs simulations de Monte Carlo, ouvrant la voie à des stratégies optimisées. </li></ul><br>  Ceci conclut l'introduction à la méthode de Monte Carlo.  Dans notre prochain article, nous passerons aux méthodes d'enseignement sous la forme d'apprentissage de la différence temporelle. <br><br><h3>  Sources: </h3><br>  Sutton et.  al, Renforcement Learning <br>  White et.  al, Fundamentals of Reinforcement Learning, Université de l'Alberta <br>  Silva et.  al, apprentissage par renforcement, UCL <br>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Platt et.</a>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Al, Université de Northeaster</a> <br><br>  C’est tout.  Rendez-vous sur le <a href="https://otus.pw/Zkti/">parcours</a> ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr477042/">https://habr.com/ru/post/fr477042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr477022/index.html">Comment pouvons-nous vous aider? Comment pouvez-vous nous aider?</a></li>
<li><a href="../fr477026/index.html">Septième hackathon annuel JetBrains</a></li>
<li><a href="../fr477032/index.html">De la blockchain au DAG: se débarrasser des intermédiaires</a></li>
<li><a href="../fr477038/index.html">Le meilleur langage de programmation pour les débutants</a></li>
<li><a href="../fr477040/index.html">Graphique de Gartner 2019: de quoi parlent tous ces mots à la mode?</a></li>
<li><a href="../fr477044/index.html">Automatisation des tests End-2-End d'un système d'information intégré. Partie 2. Technique</a></li>
<li><a href="../fr477046/index.html">Meetup .Net à Raiffeisenbank 28/11 + Broadcast</a></li>
<li><a href="../fr477048/index.html">Pourquoi une entreprise avec une capitalisation de 55 milliards de dollars a-t-elle pensé quitter la bourse</a></li>
<li><a href="../fr477050/index.html">Black Friday 2019 pour la vidéosurveillance et les nuages.</a></li>
<li><a href="../fr477052/index.html">Reactor, WebFlux, Kotlin Coroutines ou Asynchrony avec un exemple simple</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>