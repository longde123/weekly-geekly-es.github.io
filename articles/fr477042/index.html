<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì¥ üíì üëô Optimisation de la strat√©gie de blackjack de Monte Carlo üë©üèΩ‚Äçü§ù‚Äçüë®üèæ üñêüèº üÖ∞Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La traduction de l'article a √©t√© pr√©par√©e sp√©cialement pour les √©tudiants du cours Machine Learning . 



 La formation renforc√©e a pris le monde de l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Optimisation de la strat√©gie de blackjack de Monte Carlo</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477042/">  <i>La traduction de l'article a √©t√© pr√©par√©e sp√©cialement pour les √©tudiants du <a href="https://otus.pw/Zkti/">cours Machine Learning</a> .</i> <br><hr><br><img src="https://habrastorage.org/webt/dj/mh/7h/djmh7hubq1mecnsb-gilezo8qyi.png"><br><br>  La formation renforc√©e a pris le monde de l'intelligence artificielle.  √Ä partir d'AlphaGo et d' <a href="https://www.space.com/alphastar-artificial-intelligence-starcraft-2-grandmaster.html">AlphaStar</a> , un nombre croissant d'activit√©s qui √©taient auparavant domin√©es par les humains sont d√©sormais conquises par des agents de l'IA bas√©s sur la formation de renforcement.  En bref, ces r√©alisations d√©pendent de l'optimisation des actions de l'agent dans un environnement particulier pour obtenir une r√©compense maximale.  Dans les derniers articles de <a href="https://medium.com/gradientcrescent">GradientCrescent,</a> nous avons examin√© divers aspects fondamentaux de l'apprentissage renforc√©, depuis les bases des syst√®mes de <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-the-k-bandit-problem-illustrat-940eea430296">bandits</a> et <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">des</a> approches bas√©es sur les <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">politiques</a> jusqu'√† l'optimisation du comportement bas√© sur <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82">les</a> r√©compenses dans <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82">les environnements Markov</a> .  Toutes ces approches n√©cessitaient une connaissance compl√®te de notre environnement.  <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310">La programmation dynamique</a> , par exemple, n√©cessite que nous ayons une distribution de probabilit√© compl√®te de toutes les transitions d'√©tat possibles.  Cependant, en r√©alit√©, nous constatons que la plupart des syst√®mes ne peuvent pas √™tre enti√®rement interpr√©t√©s et que les distributions de probabilit√© ne peuvent pas √™tre obtenues explicitement en raison de la complexit√©, de l'incertitude inh√©rente ou des limitations des capacit√©s de calcul.  Par analogie, consid√©rons la t√¢che du m√©t√©orologue - le nombre de facteurs impliqu√©s dans les pr√©visions m√©t√©orologiques peut √™tre si grand qu'il est impossible de calculer avec pr√©cision la probabilit√©. <a name="habracut"></a><br><br>  Dans de tels cas, des m√©thodes d'enseignement telles que Monte Carlo sont la solution.  Le terme Monte Carlo est couramment utilis√© pour d√©crire toute approche d'estimation d'√©chantillonnage al√©atoire.  En d'autres termes, nous ne pr√©disons pas les connaissances sur notre environnement, mais apprenons de l'exp√©rience en passant par des s√©quences exemplaires d'√©tats, d'actions et de r√©compenses obtenues √† la suite d'une interaction avec l'environnement.  Ces m√©thodes fonctionnent en observant directement les r√©compenses retourn√©es par le mod√®le en fonctionnement normal afin de juger de la valeur moyenne de ses conditions.  Fait int√©ressant, m√™me sans aucune connaissance de la dynamique de l'environnement (qui devrait √™tre consid√©r√©e comme la distribution de probabilit√© des transitions d'√©tat), nous pouvons toujours obtenir un comportement optimal pour maximiser les r√©compenses. <br><br>  √Ä titre d'exemple, consid√©rons le r√©sultat d'un lancer de 12 d√©s.  En consid√©rant ces lancers comme un seul √©tat, nous pouvons faire la moyenne de ces r√©sultats afin de nous rapprocher du vrai r√©sultat pr√©dit.  Plus l'√©chantillon est grand, plus nous nous rapprocherons avec pr√©cision du r√©sultat r√©el attendu. <br><br><img src="https://habrastorage.org/webt/xr/85/d6/xr85d6ugn6dbszaqh5b-nbq-usq.png"><br>  <i>Le montant moyen attendu sur 12 d√©s pour 60 tirs est de 41,57</i> <br><br>  Ce type d'√©valuation bas√©e sur l'√©chantillonnage peut sembler familier au lecteur, car un tel √©chantillonnage est √©galement effectu√© pour les syst√®mes √† k-bandit.  Au lieu de comparer diff√©rents bandits, les m√©thodes de Monte Carlo sont utilis√©es pour comparer diff√©rentes politiques dans les environnements Markov, en d√©terminant la valeur de l'√©tat lorsqu'une certaine politique est suivie jusqu'√† ce que le travail soit termin√©. <br><br><h3>  Estimation de Monte Carlo de la valeur d'√©tat </h3><br>  Dans le contexte de l'apprentissage par renforcement, les m√©thodes de Monte Carlo sont un moyen d'√©valuer la signification de l'√©tat d'un mod√®le en faisant la moyenne des r√©sultats des √©chantillons.  En raison de la n√©cessit√© d'un √©tat terminal, les m√©thodes de Monte Carlo sont intrins√®quement applicables aux environnements √©pisodiques.  En raison de cette limitation, les m√©thodes Monte Carlo sont g√©n√©ralement consid√©r√©es comme ¬´autonomes¬ª dans lesquelles toutes les mises √† jour sont effectu√©es apr√®s avoir atteint l'√©tat terminal.  Une analogie simple avec la recherche d'un moyen de sortir d'un labyrinthe peut √™tre donn√©e - une approche autonome obligerait l'agent √† atteindre la fin avant d'utiliser l'exp√©rience interm√©diaire acquise pour essayer de r√©duire le temps qu'il faut pour traverser le labyrinthe.  D'un autre c√¥t√©, avec l'approche en ligne, l'agent changera constamment son comportement d√©j√† lors du passage du labyrinthe, peut-√™tre qu'il remarquera que les couloirs verts m√®nent √† des impasses et d√©cidera de les √©viter, par exemple.  Nous discuterons des approches en ligne dans l'un des articles suivants. <br><br>  La m√©thode de Monte Carlo peut √™tre formul√©e comme suit: <br><br><img src="https://habrastorage.org/webt/u4/kd/o_/u4kdo_uc3dj64rihmdjhnyj1r7y.png"><br><br>  Pour mieux comprendre le fonctionnement de la m√©thode Monte Carlo, consid√©rez le diagramme de transition d'√©tat ci-dessous.  La r√©compense pour chaque transition d'√©tat est affich√©e en noir, un facteur d'actualisation de 0,5 lui est appliqu√©.  Mettons de c√¥t√© la valeur r√©elle de l'√©tat et concentrons-nous sur le calcul des r√©sultats d'un lancer. <br><br><img src="https://habrastorage.org/webt/oc/he/n9/ochen91kvgbix5pgltzijpbozgg.png"><br>  <i>Diagramme de transition d'√©tat.</i>  <i>Le num√©ro d'√©tat est affich√© en rouge, le r√©sultat est noir.</i> <br>  √âtant donn√© que l'√©tat terminal renvoie un r√©sultat √©gal √† 0, calculons le r√©sultat de chaque √©tat, en commen√ßant par l'√©tat terminal (G5).  Veuillez noter que nous avons fix√© le facteur d'actualisation √† 0,5, ce qui entra√Ænera une pond√©ration vers les √âtats ult√©rieurs. <br><br><img src="https://habrastorage.org/webt/uw/y6/qq/uwy6qq_amgfxpsyj2_3gpij498o.png"><br><br>  Ou plus g√©n√©ralement: <br><br><img src="https://habrastorage.org/webt/k7/i4/ua/k7i4ua-g8vaph8w62-rfpc3r8sk.png"><br><br>  Pour √©viter de stocker tous les r√©sultats dans la liste, nous pouvons effectuer la proc√©dure de mise √† jour progressive de la valeur d'√©tat dans la m√©thode de Monte Carlo, en utilisant une √©quation qui pr√©sente quelques similitudes avec la descente de gradient traditionnelle: <br><br><img src="https://habrastorage.org/webt/lv/tm/nk/lvtmnki_ff4csblmytygnnxjn6u.png"><br>  <i>Proc√©dure de mise √† jour incr√©mentielle de Monte Carlo.</i>  <i>S est l'√©tat, V est sa valeur, G est son r√©sultat et A est le param√®tre de valeur de pas.</i> <br><br>  Dans le cadre de la formation de renforcement, les m√©thodes de Monte Carlo peuvent m√™me √™tre class√©es en premi√®re visite ou √† chaque visite.  En bref, la diff√©rence entre les deux est le nombre de fois qu'un √©tat peut √™tre visit√© par un passage avant la mise √† jour de Monte Carlo.  La m√©thode Monte Carlo First-visit estime la valeur de tous les √©tats comme la valeur moyenne des r√©sultats apr√®s des visites uniques dans chaque √©tat avant la fin, tandis que la m√©thode Monte Carlo Every visit fait la moyenne des r√©sultats apr√®s n visites jusqu'√† la fin.  Nous utiliserons la premi√®re visite de Monte Carlo tout au long de cet article en raison de sa relative simplicit√©. <br><br><h3>  Monte Carlo Policy Management </h3><br>  Si le mod√®le ne peut pas fournir la politique, Monte Carlo peut √™tre utilis√© pour √©valuer les valeurs d'√©tat-action.  Ceci est plus utile que la simple signification des √©tats, car l'id√©e de la signification de chaque action <i>(q)</i> dans un √©tat donn√© permet √† l'agent de formuler automatiquement une politique √† partir d'observations dans un environnement inconnu. <br><br>  Plus formellement, nous pouvons utiliser Monte Carlo pour estimer <i>q (s, a, pi)</i> , le r√©sultat attendu en partant de l'√©tat s, de l'action a et de la politique <i>Pi suivante</i> .  Les m√©thodes de Monte-Carlo restent les m√™mes, sauf qu'il existe une dimension suppl√©mentaire des actions entreprises pour un certain √©tat.  On pense qu'une paire √©tat-action <i>(s, a)</i> est visit√©e pendant le passage si l'√©tat <i>s est</i> jamais visit√© et que l'action <i>a y</i> est effectu√©e.  De m√™me, l'√©valuation des actions de valeur peut √™tre r√©alis√©e en utilisant les approches ¬´Premi√®re visite¬ª et ¬´Chaque visite¬ª. <br><br>  Comme dans la programmation dynamique, nous pouvons utiliser une politique d'it√©ration g√©n√©ralis√©e (GPI) pour former une politique √† partir de l'observation des valeurs d'√©tat-action. <br><br><img src="https://habrastorage.org/webt/wi/pm/p5/wipmp58itbgocsyruulw0ccliay.png"><br><br>  En alternant les √©tapes de l'√©valuation et de l'am√©lioration des politiques, et en incluant des recherches pour s'assurer que toutes les actions possibles sont visit√©es, nous pouvons atteindre la politique optimale pour chaque condition.  Pour le Monte Carlo GPI, cette rotation se fait g√©n√©ralement apr√®s la fin de chaque passage. <br><br><img src="https://habrastorage.org/webt/nf/b4/nw/nfb4nwijndtfex4ytwhdn2ujcyw.png"><br>  <i>Monte Carlo GPI</i> <br><br><h3>  Strat√©gie de Blackjack </h3><br>  Pour mieux comprendre comment la m√©thode de Monte Carlo fonctionne dans la pratique pour √©valuer diff√©rentes valeurs d'√©tat, prenons une d√©monstration √©tape par √©tape de l'exemple d'un jeu de blackjack.  Pour commencer, d√©terminons les r√®gles et conditions de notre jeu: <br><br><ul><li>  Nous jouerons uniquement contre le croupier, il n'y aura pas d'autres joueurs.  Cela nous permettra de consid√©rer les mains du concessionnaire comme faisant partie de l'environnement. </li><li>  La valeur des cartes avec des nombres √©gaux aux valeurs nominales.  La valeur des cartes illustr√©es: Jack, King and Queen est de 10. La valeur de l'as peut √™tre de 1 ou 11 selon le choix du joueur. </li><li>  Les deux camps re√ßoivent deux cartes.  Deux cartes de joueur sont face visible, une des cartes du croupier est √©galement face visible. </li><li>  Le but du jeu est que le nombre de cartes en main soit &lt;= 21.  Une valeur sup√©rieure √† 21 est un buste, si les deux c√¥t√©s ont une valeur de 21, alors le jeu se joue en match nul. </li><li>  Une fois que le joueur a vu ses cartes et la premi√®re carte du croupier, le joueur peut choisir de lui prendre une nouvelle carte ("encore") ou non ("assez") jusqu'√† ce qu'il soit satisfait de la somme des valeurs de carte dans sa main. </li><li>  Ensuite, le croupier montre sa deuxi√®me carte - si le montant r√©sultant est inf√©rieur √† 17, il est oblig√© de prendre des cartes jusqu'√† ce qu'il atteigne 17 points, apr√®s quoi il ne prend plus la carte. </li></ul><br>  Voyons comment la m√©thode Monte Carlo fonctionne avec ces r√®gles. <br><br><h4>  Tour 1. </h4><br>  Vous gagnez un total de 19. Mais vous essayez d'attraper la chance par la queue, de prendre une chance, d'en obtenir 3 et de vous ruiner.  Lorsque vous avez fait faillite, le croupier n'avait qu'une seule carte ouverte avec une somme de 10. Cela peut √™tre repr√©sent√© comme suit: <br><br><img src="https://habrastorage.org/webt/4o/di/sl/4odisljnoapgkqblzsixkr2a4_4.png"><br><br>  Si nous faisons faillite, notre r√©compense pour la manche est de -1.  D√©finissons cette valeur comme le r√©sultat de retour de l'avant-dernier √©tat, en utilisant le format suivant [Montant de l'agent, Montant du concessionnaire, as?]: <br><br><img src="https://habrastorage.org/webt/2q/er/g4/2qerg4mooln9blj9sfqo1ndzxbo.png"><br><br>  Eh bien, maintenant nous n'avons plus de chance.  Passons √† un autre tour. <br><br><h4>  Tour 2. </h4><br>  Vous tapez un total de 19. Cette fois, vous d√©cidez d'arr√™ter.  Le croupier compose le 13, prend une carte et fait faillite.  L'avant-dernier √©tat peut √™tre d√©crit comme suit. <br><br><img src="https://habrastorage.org/webt/al/th/dv/althdvyqwkhwqd5cj2vvzcjbiam.png"><br><br>  D√©crivons les conditions et les r√©compenses que nous avons re√ßues lors de ce tour: <br><br><img src="https://habrastorage.org/webt/rm/mv/n8/rmmvn88mg7za9gyrtvrvohywmwc.png"><br><br>  Avec la fin du passage, nous pouvons maintenant mettre √† jour les valeurs de tous nos √©tats dans ce tour en utilisant les r√©sultats calcul√©s.  En prenant un facteur de remise de 1, nous r√©partissons simplement notre nouvelle r√©compense de main, comme cela a √©t√© fait avec les transitions d'√©tat plus t√¥t.  Puisque l'√©tat <i>V (19, 10, non) a</i> pr√©c√©demment renvoy√© -1, nous calculons la valeur de retour attendue et l'affectons √† notre √©tat: <br><br><img src="https://habrastorage.org/webt/u0/um/q9/u0umq9dpg6qw420ecz44cqfp9mk.png"><br>  <i>Valeurs d'√©tat final pour la d√©monstration en utilisant le blackjack comme exemple</i> . <br><br><h3>  Impl√©mentation </h3><br>  √âcrivons un jeu de blackjack en utilisant la m√©thode Monte Carlo de la premi√®re visite pour d√©couvrir toutes les valeurs d'√©tat possibles (ou diverses combinaisons disponibles) dans le jeu en utilisant Python.  Notre approche sera bas√©e sur l' <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">approche de Sudharsan et.</a>  <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">al.</a>  .  Comme d'habitude, vous pouvez retrouver tout le code de l'article sur <a href="https://github.com/EXJUSTICE/GradientCrescent)">notre GitHub</a> . <br><br>  Pour simplifier la mise en ≈ìuvre, nous utiliserons gym d'OpenAI.  Consid√©rez l'environnement comme une interface pour d√©marrer le blackjack avec une quantit√© minimale de code, cela nous permettra de nous concentrer sur la mise en ≈ìuvre d'un apprentissage renforc√©.  Id√©alement, toutes les informations collect√©es sur les √©tats, les actions et les r√©compenses sont stock√©es dans les variables <i>¬´observation¬ª</i> , qui sont accumul√©es pendant les sessions de jeu en cours. <br><br>  Commen√ßons par importer toutes les biblioth√®ques dont nous avons besoin pour obtenir et collecter nos r√©sultats. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyplot <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> defaultdict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> functools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> partial %matplotlib inline plt.style.use(<span class="hljs-string"><span class="hljs-string">'ggplot'</span></span>)</code> </pre> <br>  Ensuite, initialisons notre environnement de <i>gym</i> et d√©finissons une politique qui coordonnera les actions de notre agent.  En fait, nous continuerons √† prendre la carte jusqu'√† ce que le montant en main atteigne 19 ou plus, apr√®s quoi nous nous arr√™terons. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Observation here encompassess all data about state that we need, as well as reactions to it env = gym.make('Blackjack-v0') #Define a policy where we hit until we reach 19. # actions here are 0-stand, 1-hit def sample_policy(observation): score, dealer_score, usable_ace = observation return 0 if score &gt;= 19 else 1</span></span></code> </pre> <br>  D√©finissons une m√©thode pour g√©n√©rer des donn√©es de passe √† l'aide de notre politique.  Nous conserverons des informations sur l'√©tat, les actions entreprises et les r√©compenses de l'action. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate_episode</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># we initialize the list for storing states, actions, and rewards states, actions, rewards = [], [], [] # Initialize the gym environment observation = env.reset() while True: # append the states to the states list states.append(observation) # now, we select an action using our sample_policy function and append the action to actions list action = sample_policy(observation) actions.append(action) # We perform the action in the environment according to our sample_policy, move to the next state observation, reward, done, info = env.step(action) rewards.append(reward) # Break if the state is a terminal state (ie done) if done: break return states, actions, rewards</span></span></code> </pre> <br>  Enfin, d√©finissons la premi√®re visite de la fonction de pr√©diction Monte Carlo.  Tout d'abord, nous initialisons un dictionnaire vide pour stocker les valeurs de l'√©tat actuel et un dictionnaire qui stocke le nombre d'enregistrements pour chaque √©tat dans diff√©rentes passes. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">first_visit_mc_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env, n_episodes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, we initialize the empty value table as a dictionary for storing the values of each state value_table = defaultdict(float) N = defaultdict(int)</span></span></code> </pre> <br>  Pour chaque passage, nous appelons notre m√©thode <i>generate_episode</i> pour obtenir des informations sur les valeurs d'√©tat et les r√©compenses re√ßues apr√®s que l'√©tat se soit produit.  Nous initialisons √©galement la variable pour stocker nos r√©sultats incr√©mentiels.  Ensuite, nous obtenons la r√©compense et la valeur de l'√©tat actuel pour chaque √©tat visit√© pendant le passage, et augmentons nos rendements variables de la valeur de la r√©compense par √©tape. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_episodes): <span class="hljs-comment"><span class="hljs-comment"># Next, we generate the epsiode and store the states and rewards states, _, rewards = generate_episode(policy, env) returns = 0 # Then for each step, we store the rewards to a variable R and states to S, and we calculate for t in range(len(states) ‚Äî 1, -1, -1): R = rewards[t] S = states[t] returns += R # Now to perform first visit MC, we check if the episode is visited for the first time, if yes, #This is the standard Monte Carlo Incremental equation. # NewEstimate = OldEstimate+StepSize(Target-OldEstimate) if S not in states[:t]: N[S] += 1 value_table[S] += (returns ‚Äî value_table[S]) / N[S] return value_table</span></span></code> </pre> <br>  Permettez-moi de vous rappeler que puisque nous mettons en ≈ìuvre la premi√®re visite de Monte-Carlo, nous visitons un √âtat en un seul passage.  Par cons√©quent, nous effectuons une v√©rification conditionnelle du dictionnaire d'√©tat pour voir si l'√©tat a √©t√© visit√©.  Si cette condition est remplie, nous sommes en mesure de calculer la nouvelle valeur en utilisant la proc√©dure pr√©c√©demment d√©finie pour mettre √† jour les valeurs d'√©tat en utilisant la m√©thode de Monte Carlo et d'augmenter le nombre d'observations pour cet √©tat de 1. Ensuite, nous r√©p√©tons le processus pour la prochaine passe afin d'obtenir finalement la valeur moyenne du r√©sultat . <br><br>  Lan√ßons ce que nous avons obtenu et regardons les r√©sultats! <br><br><pre> <code class="python hljs">value = first_visit_mc_prediction(sample_policy, env, n_episodes=<span class="hljs-number"><span class="hljs-number">500000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): print(value.popitem())</code> </pre> <br><img src="https://habrastorage.org/webt/xc/ow/al/xcowalpr-34tgxtf0gq8zlfllgg.png"><br>  <i>Conclusion d'un √©chantillon montrant les valeurs d'√©tat de diverses combinaisons sur les mains au blackjack.</i> <br><br>  Nous pouvons continuer √† faire des observations de Monte Carlo pour 5000 passes et construire une distribution des valeurs d'√©tat qui d√©crit les valeurs de n'importe quelle combinaison entre les mains du joueur et du croupier. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_blackjack</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(V, ax1, ax2)</span></span></span><span class="hljs-function">:</span></span> player_sum = np.arange(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">21</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) dealer_show = np.arange(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) usable_ace = np.array([<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>]) state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, player <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(player_sum): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j, dealer <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(dealer_show): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k, ace <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(usable_ace): state_values[i, j, k] = V[player, dealer, ace] X, Y = np.meshgrid(player_sum, dealer_show) ax1.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>]) ax2.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ax1, ax2: ax.set_zlim(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) ax.set_ylabel(<span class="hljs-string"><span class="hljs-string">'player sum'</span></span>) ax.set_xlabel(<span class="hljs-string"><span class="hljs-string">'dealer sum'</span></span>) ax.set_zlabel(<span class="hljs-string"><span class="hljs-string">'state-value'</span></span>) fig, axes = pyplot.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>),subplot_kw={<span class="hljs-string"><span class="hljs-string">'projection'</span></span>: <span class="hljs-string"><span class="hljs-string">'3d'</span></span>}) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/o usable ace'</span></span>) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/ usable ace'</span></span>) plot_blackjack(value, axes[<span class="hljs-number"><span class="hljs-number">0</span></span>], axes[<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/db/7t/he/db7thebzqawmtv_jemoprsaeqvg.png"><br>  <i>Visualisation des valeurs d'√©tat de diverses combinaisons au blackjack.</i> <br><br>  R√©sumons donc ce que nous avons appris. <br><br><ul><li>  Les m√©thodes d'apprentissage bas√©es sur l'√©chantillonnage nous permettent d'√©valuer les valeurs d'√©tat et d'action-√©tat sans aucune dynamique de transition, simplement par √©chantillonnage. </li><li>  Les approches de Monte Carlo sont bas√©es sur un √©chantillonnage al√©atoire du mod√®le, en observant les r√©compenses retourn√©es par le mod√®le et en collectant des informations pendant le fonctionnement normal pour d√©terminer la valeur moyenne de ses √©tats. </li><li>  En utilisant les m√©thodes de Monte Carlo, une politique d'it√©ration g√©n√©ralis√©e est possible. </li><li>  La valeur de toutes les combinaisons possibles entre les mains du joueur et du croupier au blackjack peut √™tre estim√©e en utilisant plusieurs simulations de Monte Carlo, ouvrant la voie √† des strat√©gies optimis√©es. </li></ul><br>  Ceci conclut l'introduction √† la m√©thode de Monte Carlo.  Dans notre prochain article, nous passerons aux m√©thodes d'enseignement sous la forme d'apprentissage de la diff√©rence temporelle. <br><br><h3>  Sources: </h3><br>  Sutton et.  al, Renforcement Learning <br>  White et.  al, Fundamentals of Reinforcement Learning, Universit√© de l'Alberta <br>  Silva et.  al, apprentissage par renforcement, UCL <br>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Platt et.</a>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Al, Universit√© de Northeaster</a> <br><br>  C‚Äôest tout.  Rendez-vous sur le <a href="https://otus.pw/Zkti/">parcours</a> ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr477042/">https://habr.com/ru/post/fr477042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr477022/index.html">Comment pouvons-nous vous aider? Comment pouvez-vous nous aider?</a></li>
<li><a href="../fr477026/index.html">Septi√®me hackathon annuel JetBrains</a></li>
<li><a href="../fr477032/index.html">De la blockchain au DAG: se d√©barrasser des interm√©diaires</a></li>
<li><a href="../fr477038/index.html">Le meilleur langage de programmation pour les d√©butants</a></li>
<li><a href="../fr477040/index.html">Graphique de Gartner 2019: de quoi parlent tous ces mots √† la mode?</a></li>
<li><a href="../fr477044/index.html">Automatisation des tests End-2-End d'un syst√®me d'information int√©gr√©. Partie 2. Technique</a></li>
<li><a href="../fr477046/index.html">Meetup .Net √† Raiffeisenbank 28/11 + Broadcast</a></li>
<li><a href="../fr477048/index.html">Pourquoi une entreprise avec une capitalisation de 55 milliards de dollars a-t-elle pens√© quitter la bourse</a></li>
<li><a href="../fr477050/index.html">Black Friday 2019 pour la vid√©osurveillance et les nuages.</a></li>
<li><a href="../fr477052/index.html">Reactor, WebFlux, Kotlin Coroutines ou Asynchrony avec un exemple simple</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>