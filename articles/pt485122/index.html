<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíó ü§¶ üñ®Ô∏è O cabe√ßalho "Leia artigos para voc√™". Outubro - dezembro de 2019 üî± üòô ‚úçÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! Continuamos a publicar resenhas de artigos cient√≠ficos de membros da comunidade Open Data Science no canal #article_essense. Se voc√™ deseja ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O cabe√ßalho "Leia artigos para voc√™". Outubro - dezembro de 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/485122/"><img src="https://habrastorage.org/webt/gx/-y/xl/gx-yxlo7xiz-5y8krpyoj3rgswq.png"><br><p><br>  Ol√° Habr!  Continuamos a publicar resenhas de artigos cient√≠ficos de membros da comunidade Open Data Science no canal #article_essense.  Se voc√™ deseja receb√™-los antes de todos os outros - participe da <a href="http://ods.ai/">comunidade</a> ! </p><br><p>  Artigos para hoje: </p><br><ol><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Poly-encoders: arquiteturas de transformadores e estrat√©gias de pr√©-treinamento para pontua√ß√£o r√°pida e precisa em v√°rias frases (Facebook, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Discriminador impl√≠cito no autoencoder variacional (Instituto indiano de tecnologia Ropar, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">O autotreinamento com o Noisy Student melhora a classifica√ß√£o do ImageNet (Google Research, Carnegie Mellon University, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Momentum Contraste para aprendizado n√£o supervisionado de representa√ß√£o visual (Facebook, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">An√°lise comparativa da robustez da rede neural a rupturas e perturba√ß√µes comuns (University of California, Oregon State University, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">DistilBERT, uma vers√£o destilada do BERT: menor, mais r√°pida, mais barata e mais leve (Hugging Face, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Modelos de linguagem Plug and Play: uma abordagem simples para a gera√ß√£o controlada de texto (Uber AI, Caltech, HKUST, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Representa√ß√£o de Deep Salience para estimativa F0 em m√∫sica polif√¥nica (New York University, EUA, 2017)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Analisando e melhorando a qualidade da imagem do StyleGAN (NVIDIA, 2019)</a> </li></ol><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Links para cole√ß√µes anteriores da s√©rie:</b> <div class="spoiler_text"><ul><li>  <a href="https://habr.com/ru/company/ods/blog/472672/">Julho - setembro 2019</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/471514/">Janeiro - junho 2019</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/352518/">Fevereiro - mar√ßo de 2018</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/352508/">Dezembro de 2017 - janeiro de 2018</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/343822/">Outubro - novembro de 2017</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/339094/">Setembro 2017</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/336624/">Agosto de 2017</a> </li></ul></div></div><br><h3 id="1-poly-encoders-transformer-architectures-and-pre-training-strategies-for-fast-and-accurate-multi-sentence-scoring">  1. Poly-encoders: arquiteturas de transformadores e estrat√©gias de pr√©-treinamento para pontua√ß√£o r√°pida e precisa em v√°rias frases </h3><br><p>  Autores: Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston (Facebook, 2019) <br>  <a href="https://arxiv.org/abs/1905.01969">‚Üí Artigo original</a> <br>  Autor do coment√°rio: Alexey (em slack zhirzemli) </p><br><p>  <strong>TLDR</strong> </p><br><p>  O artigo prop√µe uma nova abordagem para a pontua√ß√£o de pares de senten√ßas (declara√ß√µes).  Esse procedimento √© relevante nas tarefas de prever a correspond√™ncia de uma resposta a um contexto condicional, bem como em tarefas como a previs√£o do pr√≥ximo sentido.  O m√©todo Poly-Encoder proposto √© comparado com as estrat√©gias Bi-Encoder e Cross-Encoder.  O m√©todo combina a vantagem do Bi-Encoder (a capacidade de armazenar em cache a apresenta√ß√£o de respostas) e o Cross-Encoder (treinamento n√£o incondicional dos codificadores de contexto e resposta) </p><br><img src="https://habrastorage.org/webt/ax/qd/nl/axqdnlibzffxcyjtbfzeguhsdja.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Pontua√ß√£o em v√°rias frases</strong> </p><br><p>  (Um pequeno lembrete sobre as abordagens do Bi e Cross Encoder. Para aqueles que est√£o familiarizados, voc√™ pode pular) </p><br><p>  A tarefa de determinar a correspond√™ncia do contexto (solicita√ß√£o ou declara√ß√£o do usu√°rio) para o conjunto de respostas existentes √© principalmente relevante nos sistemas de di√°logo e recupera√ß√£o de informa√ß√µes.  Isso √© resolvido encontrando uma certa velocidade (produto escalar) entre as representa√ß√µes codificadas do contexto e a resposta, ou codificando conjuntamente o contexto e a resposta em um vetor com subsequente transforma√ß√£o linear em escalar. </p><br><p>  A primeira abordagem √© chamada Bi-Encoder e a vantagem √≥bvia desse m√©todo √© a capacidade de contar offline as representa√ß√µes de todas as respostas dispon√≠veis.  Essas visualiza√ß√µes s√£o armazenadas em cache e, durante a infer√™ncia, voc√™ s√≥ precisa encontrar o vetor de consulta, criar um produto de ponto com vetores de resposta e organizar o resultado.  Al√©m disso, essa abordagem permite uma amostragem negativa mais eficiente na fase de treinamento.  Ou seja, dentro de cada lote, s√£o consideradas representa√ß√µes para amostras positivas e exemplos negativos podem ser obtidos diretamente do mesmo lote.  Em ess√™ncia, reutilize o encaminhamento para exemplos positivos e negativos.  A desvantagem da abordagem Bi-Encoder √© o fato de que as representa√ß√µes de contexto e resposta aprendem quase de forma independente.  O √∫nico ponto em que pelo menos algum tipo de fluxo de informa√ß√µes √© poss√≠vel entre as visualiza√ß√µes de solicita√ß√£o e resposta √© a botnet na forma do produto final.  No n√≠vel de qualquer recurso textual, a informa√ß√£o n√£o √© atrapalhada. </p><br><p>  A segunda abordagem √© o Cross-Encoder.  Envolve uma intera√ß√£o mais poderosa de contexto e resposta no processo de aprendizagem e infer√™ncia.  Aqui, as seq√º√™ncias de token de solicita√ß√£o e resposta s√£o concatenadas em uma.  Um token separador especial √© colocado entre eles e uma incorpora√ß√£o especial √© adicionada a cada parte (solicita√ß√£o, resposta).  De fato, essa incorpora√ß√£o altera as representa√ß√µes de entrada dos tokens de resposta por alguma constante, para que o modelo possa distingui-los mais facilmente dos tokens de solicita√ß√£o.  Como resultado, o modelo aprende a encontrar uma representa√ß√£o conjunta da solicita√ß√£o e resposta, de modo que a camada linear final (vetor -&gt; escalar) retorne um grande valor de logits para pares de frases correspondentes entre si e um valor pequeno caso contr√°rio.  A desvantagem dessa abordagem √© a impossibilidade de contar offline as representa√ß√µes das respostas: elas devem ser avaliadas no est√°gio de infer√™ncia, junto com um conjunto condicional de tokens de solicita√ß√£o.  Al√©m disso, o truque para reutilizar as id√©ias de exemplos negativos e positivos na fase de treinamento n√£o funcionar√° mais aqui.  Voc√™ ter√° que coletar amostras negativas antes da forma√ß√£o do lote. </p><br><p>  <strong>Motiva√ß√£o</strong> <br>  A seguir, √© apresentada uma solu√ß√£o que permite mitigar as defici√™ncias e combinar as vantagens das abordagens do Bi e Cross Encoder.  A id√©ia √© que queremos treinar um codificador que, por um lado, leve em considera√ß√£o a depend√™ncia condicional dos tokens de resposta nos tokens de solicita√ß√£o e, por outro lado, a utiliza√ß√£o dessa depend√™ncia deve ocorrer em representa√ß√µes pr√©-avaliadas da resposta e solicita√ß√£o.  Geometricamente, eu pessoalmente imagino algo assim: mova a botnet (o produto final de pontos das duas apresenta√ß√µes) um pouco mais para a rede.  Crie alguma intera√ß√£o entre as visualiza√ß√µes de solicita√ß√£o e resposta.  Ao mesmo tempo, implementar essa intera√ß√£o n√£o est√° muito longe da camada final, de modo que a parte principal do codificador de solicita√ß√£o permanece independente do codificador de resposta. </p><br><p>  <strong>Implementa√ß√£o</strong> <br>  A implementa√ß√£o dessa id√©ia √© bastante simples: o codificador candidato funciona como no caso do Bi-Encoder: obtemos a representa√ß√£o de sequ√™ncia no formato vetorial (token [CLS]) usando o modelo baseado em transformador (BERT).  Armazenamos em cache essas representa√ß√µes ap√≥s o treinamento do modelo. </p><br><p>  O codificador de contexto, por sua vez, n√£o compacta a representa√ß√£o da sequ√™ncia de entrada em um √∫nico vetor.  Aqui deixamos todos os vetores de sequ√™ncia codificados pelo modelo. </p><br><p>  Para obter uma avalia√ß√£o da conformidade do contexto (um conjunto de vetores) e do candidato (um vetor), o mecanismo de aten√ß√£o √© usado.  O vetor candidato, neste caso, √© uma solicita√ß√£o e o vetor de contexto s√£o as chaves.  √â considerado produto escalar e, al√©m disso - softmax de acordo com os valores resultantes.  Os vetores de contexto s√£o ponderados pela distribui√ß√£o resultante e somados.  Como resultado, obtemos a representa√ß√£o do contexto na forma de um √∫nico vetor.  Al√©m disso, como no habitual Bi-Encoder, consideramos o produto escalar do contexto e o candidato. </p><br><p>  Al√©m disso, o artigo prop√¥s v√°rias maneiras de acelerar a pondera√ß√£o dos vetores de contexto.  A op√ß√£o mais √∫til foi um processo de contagem de aten√ß√£o, no qual apenas os primeiros m vetores da sequ√™ncia de contexto foram utilizados. </p><br><p>  <strong>Resultados</strong> <br>  Como resultado, descobriu-se que o Cross-Encoder ainda funciona melhor.  Mas o Poly-Encoder n√£o est√° muito atr√°s em termos de m√©tricas de qualidade e, em termos de velocidade de infer√™ncia, ele funciona centenas de vezes mais r√°pido. </p><br><h3 id="2-implicit-discriminator-in-variational-autoencoder">  2. Discriminador impl√≠cito no autoencoder variacional </h3><br><p>  Autores: Prateek Munjal, Akanksha Paul, Narayanan C. Krishnan (Instituto Indiano de Tecnologia Ropar, 2019) <br>  <a href="https://arxiv.org/abs/1909.13062">‚Üí Artigo original</a> <br>  Autor do coment√°rio: Alex Chiron (em sliron shiron8bit) </p><br><p>  No artigo, os autores propuseram uma arquitetura que tenta combinar as vantagens das abordagens VAE e GAN para gera√ß√£o de imagens, ignorando as desvantagens inerentes a cada abordagem: desfocagem no caso de autoencodificadores, colapso / falta de modo no caso de treinamento advers√°rio.  Eles conseguem isso devido aos pesos totais entre o codificador e o discriminador e o gerador / decodificador comum, o que, em primeiro lugar, reduz o n√∫mero de pesos da rede e, em segundo lugar, permite obter informa√ß√µes √∫teis do discriminador por meio de gradientes, se o gerador / decodificador n√£o cair na distribui√ß√£o de dados real. </p><br><p>  <strong>1. Introdu√ß√£o</strong> <br>  Nos problemas de gera√ß√£o, um papel importante √© desempenhado pela coincid√™ncia da distribui√ß√£o dos dados gerados Q com a distribui√ß√£o dos dados reais P, medidos atrav√©s da diverg√™ncia de Kullback-Leibler.  Uma caracter√≠stica distintiva dessa medida do afastamento das distribui√ß√µes √© que ela √© assim√©trica.  Conseq√ºentemente, obteremos imagens diferentes, dependendo de considerarmos Div_KL (P || Q) ou Div_KL (Q || P).  Se considerarmos duas op√ß√µes para comparar distribui√ß√µes (na imagem abaixo), com Div_KL (P || Q) (tamb√©m conhecido como forward-KL, conhecido como zero evitando), a segunda op√ß√£o fornecer√° um valor menor e para Div_KL (Q || P) (√© KL para tr√°s, tamb√©m √© zero for√ßando) as distribui√ß√µes da primeira op√ß√£o ser√£o consideradas distribui√ß√µes mais pr√≥ximas.  Na verdade, os resultados do VAE e GAN s√£o muito diferentes: a perda de reconstru√ß√£o (L2) ajuda a minimizar a diverg√™ncia de KL para a frente (e, portanto, preservamos todos os modos, mas obtemos imagens borradas), e o treinamento com um discriminador ajuda a minimizar a diverg√™ncia de KL para tr√°s (as imagens s√£o obtidas mais claro, mas existe o risco de pular o mod) </p><br><img src="https://habrastorage.org/webt/y9/7k/cd/y97kcdipocff08h4dsoaqly3udq.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Arquitetura, Perdas e Treinamento</strong> <br>  Como mencionado anteriormente, os autores prop√µem levar em conta as defici√™ncias de ambos os modos e combinar ambas as minimiza√ß√µes devido √† arquitetura da rede (na figura abaixo), na qual a maioria dos pesos do codificador e do discriminador s√£o comuns (apenas cabe√ßas totalmente conectadas que prev√™em a 'realidade' da imagem e par√¢metros s√£o separadas mu, sigma da camada latente do VAE) e tamb√©m devido ao modo de treinamento.  O codificador e o gerador s√£o os mesmos.A maioria das perdas usadas √© bastante padr√£o: nos codificadores L_enc, o erro L2 de recupera√ß√£o e a diverg√™ncia de Kullback-Leibler para N (0,1) (L_prior) s√£o usados, o resto √© um treinamento advers√°rio (minimizamos a sa√≠da do discriminador ao treinar o discriminador, maximiz√°-lo ao aprender um decodificador / gerador), mas existem 2 recursos distintos: </p><br><ul><li><p>  Nas quest√µes relacionadas ao treinamento advers√°rio, dois tipos diferentes de dados gerados s√£o fornecidos ao discriminador: recuperados por meio de um codificador / decodificador e gerados por um gerador / decodificador a partir de amostras de N (0,1) </p><br></li><li><p>  No decodificador Loss of the L_dec, h√° um membro no qual os recursos da pen√∫ltima camada do discriminador (novamente, essa √© a √∫ltima camada comum entre o discriminador e o codificador) s√£o comparados para imagens reais e restauradas. </p><br></li></ul><br><img src="https://habrastorage.org/webt/-d/n5/jh/-dn5jh_obvrbb4am3ujm37hd9qs.png" width="500" height="250"><br><p>  <strong>Resultados</strong> <br>  Os autores compararam os resultados com o VAE e outros trabalhos, de uma forma ou de outra tentando combinar VAE e GANs (VAE-GAN, alpha-GAN e AGE de Dmitry Ulyanov e Victor Lempitsky) em conjuntos de dados celeba e cifar10 (obrigado por n√£o mnist), recebeu quase os melhores indicadores em rela√ß√£o ao erro de reconstru√ß√£o e √† m√©trica Frechet Inception Distance (compara as estat√≠sticas de ativa√ß√£o da malha pr√©-treinada para imagens reais e geradas).  Observou-se separadamente que a classifica√ß√£o do FID depende fortemente da arquitetura escolhida, portanto, o resultado √© melhor verificar o conjunto de 'especialistas' (diferentes arquiteturas). </p><br><h3 id="3-self-training-with-noisy-student-improves-imagenet-classification">  3. O autotreinamento com o Noisy Student melhora a classifica√ß√£o do ImageNet </h3><br><p>  Autores: Qizhe Xie, Eduard Hovy, Minh-Thang Luong, Quoc V. Le (Pesquisa do Google, Universidade Carnegie Mellon, 2019) <br>  <a href="https://arxiv.org/abs/1911.04252">‚Üí Artigo original</a> <br>  Autor do coment√°rio: Alexander Belsky (em slack belskikh) </p><br><p>  O Google recebeu 87,4% dos top1 e 98,2% dos top5 de precis√£o na imagem.  Zayuzali obscurece pseudo-escurecimento e redes muito ousadas.  A abordagem foi chamada Noisy Student. </p><br><img src="https://habrastorage.org/webt/es/s8/tm/ess8tmsezy4cjwydsqqfhjxclcu.png"><br><p><br></p><br><p>  <strong>O algoritmo √©</strong> algo como isto: </p><br><ol><li>  Adotamos um modelo de professor, ensinamos uma imagem normal. </li><li>  Geramos r√≥tulos psudo suaves em imagens do conjunto de dados JFT. </li><li>  Ensinamos o modelo do aluno com pseudo-r√≥tulos suaves e interferimos o mais r√°pido poss√≠vel: ajuda forte, desist√™ncia e profundidade estoc√°stica </li><li>  Pegue o modelo do aluno, use-o como professor na etapa 2. Repita o processo.O conjunto de dados √© equilibrado de acordo com as aulas da seguinte forma.  Para come√ßar, pegamos o EfficientNet-B0, treinado na imagem, divulgamos suas previs√µes no conjunto de dados JFT.  Ent√£o eles pegaram os exemplos para os quais a confian√ßa m√°xima est√° acima de 0,3.  Para cada classe, foram tiradas 130 mil imagens (se, depois de filtrar por 0,3 lixeira, elas fossem menos - duplicadas, se mais - tiradas de acordo com os escopos de predicado mais altos).  Recebeu 130 milh√µes de imagens, emiss√µes duplicadas, 81 milh√µes restantes </li></ol><br><p>  <strong>Arquitetura:</strong> <br>  EfficeintNet, al√©m disso, o modelo do aluno adota um modelo de professor muito mais gordo.  Eles tamb√©m digitalizaram o pr√≥prio EfficientNet para EfficientNet-L0 / L1 / L2, resultando em um modelo L2 com par√¢metros de 480M (Resnet50 possui 26M par√¢metros, para compara√ß√£o) </p><br><p>  <strong>Processo de aprendizagem:</strong> <br>  Butchesize 2048. O modelo Sota L2 ensinou 350 √©pocas.  O maior modelo L2 estudado nesse modo por 3,5 dias no Cloud TPU v3 Pod com 2048 n√∫cleos. </p><br><p>  <strong>Procedimento de aprendizagem iterativa:</strong> <br>  No in√≠cio, eles ensinaram B7 tanto como estudante quanto como professor.  Ent√£o, usando B7 como professores, eles ensinaram L0 mais gordo como aluno.  Ent√£o, mudando de lugar dessa maneira, chegamos ao modelo L2, que no final usamos como professor para o mesmo modelo L2. Resultado :: sota: com 2 vezes menos par√¢metros de modelo em compara√ß√£o com a c√©lula anterior (FixRes ResNeXt-101 WSL 829M par√¢metros) </p><br><p>  Tamb√©m obteve <strong>resultados</strong> muito bons no ImageNet-A / C / P </p><br><img src="https://habrastorage.org/webt/ht/me/ce/htmeceti9jluqoyj84uqcy2fibo.png"><br><p><br></p><br><h3 id="4-momentum-contrast-for-unsupervised-visual-representation-learning">  4. Contraste moment√¢neo para o aprendizado n√£o supervisionado de representa√ß√£o visual </h3><br><p>  Autores do artigo: Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick (Facebook, 2019) <br>  <a href="https://arxiv.org/abs/1911.05722">‚Üí Artigo original</a> <br>  Autor do coment√°rio: Arseny Kravchenko (em slack arsenyinfo) </p><br><p>  O SotA √© um pr√©-treino n√£o supervisionado para v√°rias tarefas de vis√£o computacional (da classifica√ß√£o √† estimativa de pose densa), testado em diferentes conjuntos de dados (imagenet, instagram) e tarefas principais (imagenet, COCO, paisagens da cidade, LVIS etc.). </p><br><img src="https://habrastorage.org/webt/li/5p/bn/li5pbnzez-zowzce2movvxthfea.png"><br><p><br></p><br><p>  Como √© feito o pr√©-treinamento n√£o supervisionado?  Criamos algum tipo de tarefa para a qual as etiquetas n√£o s√£o necess√°rias, aprendemos o codificador, congelamos e resolvemos o principal problema adicionando as camadas ausentes (linear para classifica√ß√£o, decodificadores para segmenta√ß√£o etc.).  Uma das tarefas mais populares nesse nicho √© a discrimina√ß√£o de casos, com base na perda contrastiva, ou seja,  queremos que os recursos de diferentes amplia√ß√µes da mesma imagem estejam pr√≥ximos um do outro (por exemplo, em termos de dist√¢ncia do cosseno), e os recursos de diferentes est√£o distantes. </p><br><p>  Voc√™ pode tentar ensinar essa tarefa de ponta a ponta, mas depende muito do tamanho do lote: a qualidade depende muito da variedade de exemplos dentro do lote.  As experi√™ncias mostram que, com o aumento do tamanho do lote, a qualidade final melhora.  Mas o lote √© um pouco semelhante a Moscou: n√£o √© de borracha, n√£o funcionar√° por muito tempo para aument√°-lo na testa. </p><br><p>  Os caras anteriores de c√©lulas pr√≥ximas estragaram um banco de mem√≥ria: os recursos dos lotes anteriores foram armazenados separadamente na mem√≥ria e tamb√©m foram usados ‚Äã‚Äãpara gerar negativos, ou seja,  amostras diferentes.  Isso ajudou em parte, mas tamb√©m de maneira imperfeita: durante o treinamento, os pesos do codificador mudam e os recursos antigos ficam ruins. </p><br><p>  Por fim, a ideia do artigo: </p><br><ol><li>  Vamos substituir um banco de mem√≥ria simples por uma fila em que recursos bastante novos estar√£o; </li><li>  Manteremos duas vers√µes do codificador: uma √© usada para o lote atual e √© treinada, e a outra √© mais est√°vel, seus pesos s√£o atualizados a partir da primeira vers√£o, mas com um grande impulso; </li><li>  Os recursos do lote s√£o considerados o primeiro codificador, os recursos na fila s√£o contados pelo segundo codificador. </li></ol><br><p>  Essa abordagem torna poss√≠vel aproximar-se da qualidade do treinamento de ponta a ponta, mas, gra√ßas √† longa programa√ß√£o, ela alcan√ßa os resultados potenciais de um lote irrealisticamente grande.  Dessa forma, voc√™ obt√©m m√©tricas interessantes para diferentes tarefas, incluindo  em alguns lugares, √© um pouco melhor que a imagem supervisionada tradicional no imaginet. </p><br><h3 id="5-benchmarking-neural-network-robustness-to-common-corruptions-and-perturbations">  5. Benchmarking da robustez da rede neural para corrup√ß√£o e perturba√ß√µes comuns </h3><br><p>  Autores: Dan Hendrycks, Thomas Dietterich (Universidade da Calif√≥rnia, Oregon State University, 2019) <br>  <a href="https://arxiv.org/abs/1903.12261">‚Üí Artigo original</a> <br>  Autor do artigo: Vladimir Iglovikov (in ternaus slack) </p><br><img src="https://habrastorage.org/webt/fy/p3/zn/fyp3znumddg9tstty7aukiiuvwg.png" width="500" height="250"><br><p><br></p><br><p>  Foi aceito no ICLR 2019 e, pelo que entendi, este √© um daqueles trabalhos de DL que n√£o foram treinados em nenhuma rede. </p><br><p>  A tarefa foi assim - mas vamos tentar o aprimoramento para valida√ß√£o do ImageNet, mas vamos treinar no ininterrupto.  Al√©m disso, diferentemente do adevrsarial, n√£o temos a tarefa de tornar as transforma√ß√µes pequenas e invis√≠veis aos olhos. </p><br><p>  <strong>O que foi feito:</strong> </p><br><ol><li>  Um conjunto de aprimoramentos foi selecionado.  Os autores dizem que isso √© o mais comum, mas, na minha opini√£o, eles mentem. <br>  Eles usaram: GaussianNoise, ISONoise, Downscale, Defocus, MotionBlur, ZoomBlur, FrostedGlassBlur, JPEGCompression, Snow, Nevoeiro, Chuva, Transoform Elastic, etc. </li><li>  Todas essas transforma√ß√µes foram aplicadas √† valida√ß√£o do ImageNet.  O conjunto de dados resultante foi denominado ImageNet-C </li><li>  Tamb√©m foi proposta uma varia√ß√£o chamada ImageNet-P, na qual conjuntos de transforma√ß√µes de diferentes for√ßas foram aplicados a cada figura. </li><li>  Uma m√©trica foi proposta para avaliar a estabilidade do modelo. </li><li>  V√°rios modelos foram avaliados no contexto dessa m√©trica: AlexNet, VGG-11, VGG-19, Resnet-50, Resnet-18, VGG-19 + BN, etc. </li></ol><br><p>  <strong>Conclus√µes:</strong> </p><br><ol><li>  Quanto mais forte o aumento, mais a precis√£o do modelo sofre.  : capitan_obvious: </li><li>  Quanto mais complexo o modelo, mais est√°vel. </li><li>  A aplica√ß√£o de CLAHE nas figuras antes da infer√™ncia ajuda um pouco. </li><li>  blocos de agrega√ß√£o de recursos, como a ajuda do DenseNet ou Resnext. </li><li>  Redes com v√°rias escalas s√£o mais est√°veis.  Um exemplo dessas redes √© MSDNet, Multigrid (eu n√£o ouvi falar de tais redes) </li></ol><br><p>  <a href="https://github.com/hendrycks/robustness">C√≥digo</a> </p><br><h3 id="6-distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter">  6. DistilBERT, uma vers√£o destilada do BERT: menor, mais r√°pida, mais barata e mais leve </h3><br><p>  Autores: Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf (Hugging Face, 2019) <br>  <a href="https://arxiv.org/abs/1910.01108">‚Üí Artigo original</a> <br>  Autor do coment√°rio: Yuri Kashnitsky (in yorko slack) </p><br><p>  O artigo √© curto, √© muito f√°cil de ler.  No come√ßo, algumas palavras gerais sobre a corrida armamentista na PNL e a pegada ambiental.  Al√©m disso, a id√©ia de destila√ß√£o (e Hinton fez aqui tamb√©m) Na tarefa de modelagem de linguagem, prevemos de maneira padr√£o a pr√≥xima palavra no contexto.  Normalmente, a perda de entropia cruzada compara o vetor de probabilidades previstas (o comprimento de todo o dicion√°rio) com um vetor bin√°rio, onde h√° apenas uma unidade indicando a palavra real em um determinado local no conjunto de treinamento.  Ou seja, o segundo, terceiro, etc.  a palavra que o modelo considera apropriado √© ignorada pela perda.  Um exemplo √© dado no artigo: "Eu acho que isso √© o come√ßo de uma [M√ÅSCARA] linda", em vez de [M√ÅSCARA] BERT quer substituir primeiro o dia ou a vida, mas as palavras previstas pela probabilidade futura de futuro, hist√≥ria e mundo tamb√©m s√£o boas.  De alguma forma, podemos levar em considera√ß√£o o fato de o modelo produzir uma boa distribui√ß√£o de probabilidade?  Grosso modo, premiar o modelo pelo fato de n√£o haver Murdock, toler√¢ncia, maternidade e outras poucas palavras adequadas no topo. </p><br><img src="https://habrastorage.org/webt/wg/xd/rx/wgxdrxth-vykjuhkxszaxakxdke.png" width="500" height="250"><br><p><br></p><br><p>  <strong>A ideia de destila√ß√£o</strong> <br>  A id√©ia de um esquema espec√≠fico de professor-aluno √© que tenhamos um modelo de professor grande ( <strong>professor</strong> , BERT) e um modelo menor ( <strong>aluno</strong> , DistilBERT), que transmitir√° "conhecimento" do modelo de professor.  O modelo do aluno otimizar√° a perda de destila√ß√£o, ou seja, a perda de entropia cruzada, definida para as distribui√ß√µes de probabilidade do professor e do aluno: L = Œ£ t_i * log (s_i).  Ou seja, para uma palavra espec√≠fica apagada pelo s√≠mbolo [MASK] e que deve ser prevista pelo contexto, comparamos duas distribui√ß√µes de probabilidade da apar√™ncia de cada palavra do dicion√°rio: {t_i} e {s_i} - preditas, respectivamente, pelo modelo e modelo do professor aluno.  Assim, √© obtido um sinal de treinamento rico - o modelo do aluno em cada palavra recebe um sinal calculado n√£o apenas comparando seu vetor de previs√£o com a palavra real na amostra de treinamento, mas comparando-o com o vetor de proje√ß√£o do modelo de professor. </p><br><p>  <strong>DistilBERT Model</strong> <br>    ,   ‚Äî   ,  .   DistilBERT ‚Äî      BERT,    .   token-type embeddings  pooler, ,    .  ,  DistilBERT  40%  ‚Äî 66 .   110   BERT </p><br><p> <strong> DistilBERT</strong> <br>  DistilBERT  distillation loss     ‚Äî   masked language modeling loss,    BERT   cosine embedding loss ‚Äî           ( ,  ,      "" -   ,  "" ). :   ablation studies, ,   masked language modeling loss,    , ..    distillation loss  cosine embedding loss.   ,    RoBERTa   next sentence prediction   dynamic masking. </p><br><p>      ,  BERT (eng. wiki + Toronto Book Corpus) 90   8 V100 (16 GB).   RoBERTa    1024 V100 (32 GB). </p><br><p> <strong></strong> <br>     BERT ‚Äî "it performed surprisingly well",        DistilBERT ‚Äî  GLUE  surprisingly well ‚Äî     5  9   ,  BERT ,     SQuAD  IMDb ‚Äî  .   ,    DistilBERT   60% ‚Äî  . </p><br><p> <strong> </strong> <br>   DistilBERT  iPhone 7 Plus.   70% ,  BERT-base (  ),     200 .  ablation studies:     ,      ‚Äî distillation loss  cosine embedding loss. </p><br><p>      3          ,  DistilBERT ‚Äî     BERT,   40%  ,   60%    "97%   "    BERT (        ML). </p><br><p> -,      BERT,     . </p><br><p> <strong> :</strong> <br> <a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"> Jay Alammar</a> <br> <a href="https://www.kaggle.com/kashnitsky/distillbert-catalyst-amazon-product-reviews">  , DistilBERT + Catalyst:   </a> </p><br><h3 id="7-plug-and-play-language-models-a-simple-approach-to-controlled-text-generation"> 7. Plug and Play Language Models: A Simple Approach To Controlled Text Generation </h3><br><p>  : Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu (Uber AI, Caltech, HKUST, 2019) <br> <a href="https://arxiv.org/abs/1912.02164">‚Üí  </a> <br>  :   (  Egor Timofeev) </p><br><p>               . ,           / /      (, .  <a href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a> ).     ,         ,     , ,      . </p><br><p> <strong></strong> <br>       (   x_prev    ),        p(x),      conditional LM (,    ‚Äî CTRL)    p(x|a). </p><br><p>       : p(x|a) ‚àù p(x)p(a|x),  p(x)  ,    (, GPT2),  p(a|x) ‚Äî     .       ‚Äî       ,   /.     ,       ,    . </p><br><p>   <strong></strong> : </p><br><ol><li>    ,  log(p(a|x)) ( ).     hidden state  . </li><li>      ,  hidden state      log(p(a|x)).   H_new. </li><li>   :           p(x).    ,    : -,        KL(H, H_new),  -,  .. post-norm fusion ( <a href="https://arxiv.org/pdf/1809.00125.pdf">https://arxiv.org/pdf/1809.00125.pdf</a> ),   p(x)   non conditional LM  ,     . </li><li>      . </li></ol><br><p>           ,  p(a|x). </p><br><p> <strong></strong> <br>       ,   -            topic relevance.    :  (GPT2) &lt;  +    &lt;&lt;       &lt;    + . </p><br><img src="https://habrastorage.org/webt/jx/zm/ye/jxzmyeaubsu6wtcp2np1zda32tk.png" width="500" height="250"><br><p><br></p><br><h3 id="8-deep-salience-representation-for-f0-estimation-in-polyphonic-music"> 8. Deep Salience Representation for F0 Estimation in Polyphonic Music </h3><br><p>  : Rachel M. Bittner, Brian McFee, Justin Salamon, Peter Li, Juan Pablo Bello ( New York University, USA, 2017) <br> <a href="https://bmcfee.github.io/papers/ismir2017_salience.pdf">‚Üí  </a> <br>  :   (  nglaz) </p><br><p>    .  ,                .        ,     ‚Äì    .       ,   -   .   constant-Q ,          (      )          . </p><br><img src="https://habrastorage.org/webt/7l/6y/c0/7l6yc0irzsti2kbks7avmvrb7w4.png" width="500" height="250"><br><p>     .  constant-Q     -   f_min  -    F.    f_min   f_min * h,      ,    ,     .    h   {0.5, 1, 2, 3, 4, 5},        .   ,          3- ,        2-  3-    (, ,  ). ,    ,     ,    ,   (0.5f, f, 2f, 3f, 4f, 5f),    .     ( 55)      .         ,           ,  dilated-. </p><br><p>  , ,     constant-Q       F,           . </p><br><p>    F0 estimation,    ,          .  2017 ,   ,   state-of-the-art.           ,      . </p><br><h3 id="9-analyzing-and-improving-the-image-quality-of-stylegan"> 9. Analyzing and Improving the Image Quality of StyleGAN </h3><br><p>  : Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila (NVIDIA, 2019) <br> <a href="http://arxiv.org/abs/1912.04958">‚Üí  </a> <br>  :   (  shiron8bit) </p><br><p> GAN-      ,     ,         .     ,   ,      ,   ,    ( FID)   : </p><br><ul><li>   droplet-like  (    / ),  AdaIN. </li><li>   ,   ProGAN-    /       end-to-end     MSG-GAN.     ,        /,            . </li><li>  Path Length Regularization. </li><li>     :       W,      ,       stylegan2. </li></ul><br><img src="https://habrastorage.org/webt/f6/v3/7p/f6v37pcy3wcpw0epu5rz1-r24qk.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Artefatos de got√≠culas e AdaIN</strong> <br>  Os autores do artigo apresentam o seguinte argumento contra o uso da camada AdaIN: o adain normaliza cada mapa de recursos, destruindo informa√ß√µes sobre os valores de magnitude relativos entre si e droplet √© uma tentativa do gerador de empurrar essas informa√ß√µes de uma maneira diferente.  Como op√ß√£o para enfraquecer o AdaIN, foi proposto o seguinte: faremos todo o dimensionamento (modula√ß√£o / desmodula√ß√£o) diretamente na convolu√ß√£o, com base no estilo proveniente do bloco A e no deslocamento do sinal de sa√≠da (em vez de mu (y) / y_ {b, i} no AdaIN) deixe o bloco B transformar o ru√≠do.  Essa inova√ß√£o, ao mesmo tempo, permitiu acelerar o treinamento nas mesmas condi√ß√µes. </p><br><p>  <strong>Falha no ProGAN</strong> <br>  No artigo sobre MSG-GAN, foi proposto o uso de conex√µes puladas, conectando blocos geradores correspondentes e blocos discriminadores por resolu√ß√£o.  Os autores da Stylegan desenvolveram essa id√©ia resumindo as sa√≠das dos blocos geradores de todas as resolu√ß√µes (com upsampling) e alimentando a vers√£o downsampled correspondente da imagem na entrada de cada bloco discriminador.  Sugeriu-se que blocos residuais fossem usados ‚Äã‚Äãcomo segunda op√ß√£o, enquanto as conex√µes puladas no gerador e blocos residuais no discriminador apresentaram os melhores resultados (o discriminador √© semelhante ao LAPGAN, mas sem discriminadores para cada resolu√ß√£o, os mapas de recursos s√£o encaminhados ainda mais). como no caso do ProGAN, nas itera√ß√µes iniciais, as partes da grade respons√°veis ‚Äã‚Äãpor resolu√ß√µes mais baixas e o quadro geral d√£o uma contribui√ß√£o maior e, em seguida, a √™nfase √© transferida para pequenos detalhes. </p><br><p>  <strong>Regulariza√ß√£o do comprimento do caminho</strong> <br>  Observando que os baixos valores de FID nem sempre fornecem imagens de alta qualidade, e tamb√©m observando uma correla√ß√£o entre a qualidade da imagem e a m√©trica PPL (Perceptual Path Length - inicialmente a diferen√ßa entre os recursos vgg das imagens com pequenos passos em Z, mas a diferen√ßa foi substitu√≠da pelo LPIPS), os autores propuseram o Path Regulariza√ß√£o de comprimento, para minimizar a funcionalidade </p><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>J</mi><mi>w</mi><mi>T</mi></msubsup><mi>y</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><msub><mi>a</mi><mi>w</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.258ex" height="2.78ex" viewBox="0 -883.9 9583.3 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-4A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-54" x="929" y="488"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-77" x="785" y="-212"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-79" x="1255" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMAIN-3D" x="2030" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-6E" x="3336" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-61" x="3937" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-62" x="4466" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-6C" x="4896" y="0"></use><g transform="translate(5194,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-77" x="748" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMAIN-28" x="6330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-67" x="6720" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMAIN-28" x="7200" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-77" x="7590" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMAIN-29" x="8306" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMATHI-79" x="8696" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=17259,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjPQcyEM99SSELLX9xZFTFK5YipWw#MJMAIN-29" x="9193" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>J</mi><mi>w</mi><mi>T</mi></msubsup><mi>y</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><msub><mi>a</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mi>y</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> J ^ T_w y = \ nabla_w (g (w) y) </script></p><br>  onde g √© o pr√≥prio gerador, J_w √© o jacobiano em vari√°veis ‚Äã‚Äãde espa√ßo latente.  Ao mesmo tempo, os c√°lculos jacobianos podem ser feitos por meio de backprop, e tamb√©m √© dito que, para facilitar os c√°lculos, o regularizador pode ser contado apenas para cada 16 lotes.  O n√∫mero a √© calculado como a m√©dia m√≥vel exponencial da norma jacobiana. O uso da Regulariza√ß√£o de Comprimento do Caminho permite uma interpola√ß√£o mais 'suave' do espa√ßo oculto W, que, al√©m de melhorar a qualidade da imagem, pode melhorar a reversibilidade (ou seja, encontrar w que fornece uma determinada imagem ap√≥s a execu√ß√£o no gerador) e tamb√©m abre perspectivas em termos de anima√ß√£o e interpola√ß√£o entre quadros-chave (na nova arquitetura, entre proje√ß√µes de imagens semelhantes, deve haver pontos respons√°veis ‚Äã‚Äãpor imagens pr√≥ximas  I).  A introdu√ß√£o dessa regulariza√ß√£o tamb√©m desempenhou um papel na simplifica√ß√£o da detec√ß√£o de imagens geradas por essa arquitetura. <br><p>  O tempo de treinamento para 8 GPUs com uma resolu√ß√£o de 1024 * 1024 foi de 2 a 9 dias para diferentes configura√ß√µes. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt485122/">https://habr.com/ru/post/pt485122/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt485104/index.html">Criando uma chave RFID universal para interfones</a></li>
<li><a href="../pt485108/index.html">Estat√≠sticas de especialistas certificados em PMI na R√∫ssia em 10/01/2020</a></li>
<li><a href="../pt485110/index.html">Minha experi√™ncia de trabalho remoto eficaz</a></li>
<li><a href="../pt485118/index.html">C√≥digo Limpo de Robert Martin. Resumo. Como escrever um c√≥digo claro e bonito?</a></li>
<li><a href="../pt485120/index.html">Adicione uma API JSON muito r√°pida ao nosso aplicativo.</a></li>
<li><a href="../pt485124/index.html">Testes puros em PHP e PHPUnit</a></li>
<li><a href="../pt485126/index.html">Mu-mu, woof-woof, quack-quack: evolu√ß√£o da comunica√ß√£o ac√∫stica</a></li>
<li><a href="../pt485128/index.html">Economize nas licen√ßas Mikrotik CHR</a></li>
<li><a href="../pt485132/index.html">Participe do Festival de jogos independentes do Google Play</a></li>
<li><a href="../pt485136/index.html">Rastreamento e monitoramento Istio: microsservi√ßos e o princ√≠pio da incerteza</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>