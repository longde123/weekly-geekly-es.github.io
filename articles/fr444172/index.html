<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÉüèª ü•¢ üèá Sept mythes dans la recherche sur l'apprentissage automatique üßëüèæ‚Äçü§ù‚Äçüßëüèº üêÑ üë©üèª‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pour ceux qui sont trop paresseux pour tout lire: une r√©futation de sept mythes populaires est sugg√©r√©e, ce qui dans le domaine de la recherche en mac...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sept mythes dans la recherche sur l'apprentissage automatique</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444172/">  Pour ceux qui sont trop paresseux pour tout lire: une r√©futation de sept mythes populaires est sugg√©r√©e, ce qui dans le domaine de la recherche en machine learning est souvent consid√©r√© comme vrai, √† partir de f√©vrier 2019. Cet article est disponible sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site ArXiv au format pdf</a> [en anglais]. <br><br>  Mythe 1: TensorFlow est une biblioth√®que de tenseurs. <br>  Mythe 2: Les bases de donn√©es d'images refl√®tent de vraies photos trouv√©es dans la nature. <br>  Mythe 3: Les chercheurs en MO n'utilisent pas de kits de test pour les tests. <br>  Mythe 4: la formation au r√©seau neuronal utilise toutes les donn√©es d'entr√©e. <br>  Mythe 5: La normalisation des lots est n√©cessaire pour entra√Æner des r√©seaux r√©siduels tr√®s profonds. <br>  Mythe 6: Les r√©seaux avec attention valent mieux que la convolution. <br>  Mythe 7: Les cartes de signification sont un moyen fiable d'interpr√©ter les r√©seaux de neurones. <br><br>  Et maintenant pour les d√©tails. <br><a name="habracut"></a><br><h2>  Mythe 1: TensorFlow est une biblioth√®que de tenseurs </h2><br>  En fait, c'est une biblioth√®que pour travailler avec des matrices, et cette diff√©rence est tr√®s importante. <br><br>  Dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">calcul de d√©riv√©s d'ordre sup√©rieur d'expressions matricielles et tensorielles.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Laue et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Les</a> auteurs de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">NeurIPS 2018</a> d√©montrent que leur biblioth√®que de diff√©renciation automatique, bas√©e sur un calcul tensoriel r√©el, a des arbres d'expression beaucoup plus compacts.  Le fait est que le calcul tensoriel utilise la notation d'index, ce qui vous permet de travailler √©galement avec les modes direct et inverse. <br><br>  La num√©rotation matricielle masque les indices pour faciliter la notation, c'est pourquoi les arbres d'expression de diff√©renciation automatique deviennent souvent trop complexes. <br><br>  Consid√©rons la multiplication matricielle C = AB.  Nous avons <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>C</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>A</mi></mrow><mi>B</mi><mo>+</mo><mi>A</mi><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>B</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="26.011ex" height="2.178ex" viewBox="0 -780.1 11199 937.7" role="img" focusable="false" style="vertical-align: -0.366ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-6F" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-74" x="1259" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-43" x="1620" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-3D" x="2658" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-64" x="3965" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-6F" x="4488" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-74" x="4974" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-41" x="5335" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-42" x="6086" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-2B" x="7067" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-41" x="8068" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-64" x="9069" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-6F" x="9592" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-74" x="10078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-42" x="10439" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>C</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>A</mi></mrow><mi>B</mi><mo>+</mo><mi>A</mi><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>B</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-1"> \ dot {C} = \ dot {A} B + A \ dot {B} </script>  pour le mode direct et <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>C</mi></mrow><msup><mi>B</mi><mi>T</mi></msup><mo>,</mo><mi>B</mi><mo>=</mo><msup><mi>A</mi><mi>T</mi></msup><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>C</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="28.27ex" height="2.78ex" viewBox="0 -935.7 12171.6 1197.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-41" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-3D" x="1028" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-62" x="2334" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-61" x="2764" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-72" x="3293" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-43" x="3745" y="0"></use><g transform="translate(4505,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-42" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-54" x="1074" y="513"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-2C" x="5863" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-42" x="6308" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-3D" x="7345" y="0"></use><g transform="translate(8401,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-41" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-54" x="1061" y="513"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-62" x="10000" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-61" x="10430" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-72" x="10959" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-43" x="11411" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>C</mi></mrow><msup><mi>B</mi><mi>T</mi></msup><mo>,</mo><mi>B</mi><mo>=</mo><msup><mi>A</mi><mi>T</mi></msup><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>C</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-2"> A = \ bar {C} B ^ T, B = A ^ T \ bar {C} </script>  pour le contraire.  Pour effectuer correctement la multiplication, vous devez respecter strictement l'ordre et l'utilisation de la c√©sure.  Du point de vue de l'enregistrement, cela semble d√©routant pour une personne impliqu√©e dans MO, mais du point de vue des calculs, c'est une charge suppl√©mentaire pour le programme. <br><br>  Un autre exemple, moins trivial: c = det (A).  Nous avons <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>c</mi></mrow><mo>=</mo><mi>t</mi><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>A</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.951ex" height="2.66ex" viewBox="0 -832 10312.1 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-6F" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-74" x="1259" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-63" x="1620" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-3D" x="2331" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-74" x="3388" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-72" x="3749" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-28" x="4201" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-69" x="4590" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-6E" x="4936" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-76" x="5536" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-28" x="6022" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-41" x="6411" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-29" x="7162" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-64" x="7801" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-6F" x="8325" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-74" x="8810" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-41" x="9172" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-29" x="9922" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>c</mi></mrow><mo>=</mo><mi>t</mi><mi>r</mi><mo stretchy="false">(</mo><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>A</mi></mrow><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ dot {c} = tr (inv (A) \ dot {A}) </script>  pour le mode direct et <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>A</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>c</mi></mrow><mi>c</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><msup><mo stretchy=&quot;false&quot;>)</mo><mi>T</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.835ex" height="2.901ex" viewBox="0 -935.7 9831.7 1249" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-62" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-61" x="679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-72" x="1209" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-41" x="1660" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-3D" x="2688" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-62" x="3995" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-61" x="4424" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-72" x="4954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-63" x="5405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-63" x="5839" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-69" x="6272" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-6E" x="6618" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-76" x="7218" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-28" x="7704" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-41" x="8093" y="0"></use><g transform="translate(8844,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMAIN-29" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhieY6YBGN-gxLecFdUN8qd0eCTLEA#MJMATHI-54" x="550" y="513"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>A</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>c</mi></mrow><mi>c</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mi>A</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ bar {A} = \ bar {c} cinv (A) ^ T </script>  pour le contraire.  Dans ce cas, il est √©videmment impossible d'utiliser l'arbre d'expression pour les deux modes, √©tant donn√© qu'ils sont constitu√©s d'op√©rateurs diff√©rents. <br><br>  En g√©n√©ral, la fa√ßon dont TensorFlow et d'autres biblioth√®ques (par exemple, Mathematica, Maple, Sage, SimPy, ADOL-C, TAPENADE, TensorFlow, Theano, PyTorch, HIPS autograd) ont impl√©ment√© la diff√©renciation automatique, ce qui conduit au fait que pour le direct et le reverse Des arborescences d'expression diff√©rentes et inefficaces sont construites dans le mode.  La num√©rotation des tenseurs contourne ces probl√®mes en raison de la commutativit√© de la multiplication due √† la notation d'index.  Pour plus de d√©tails sur la fa√ßon dont cela fonctionne, voir l'article scientifique. <br><br>  Les auteurs ont test√© leur m√©thode en effectuant une diff√©renciation automatique du r√©gime inverse, √©galement connu sous le nom de r√©tropropagation, dans trois t√¢ches diff√©rentes, et ont mesur√© le temps n√©cessaire pour calculer les Hessians. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/923/263/331/923263331347f83e3bac0fe7a9477e8b.png"><br><br>  Dans le premier probl√®me, la fonction quadratique x <sup>T</sup> Ax a √©t√© optimis√©e.  Dans la seconde, la r√©gression logistique a √©t√© calcul√©e, dans la troisi√®me - factorisation matricielle. <br><br>  Sur le CPU, leur m√©thode s'est av√©r√©e √™tre deux ordres de grandeur plus rapide que des biblioth√®ques populaires telles que TensorFlow, Theano, PyTorch et HIPS autograd. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/184/395/7dd/1843957dde52a4722da309adcdd6ea92.png"><br><br>  Sur le GPU, ils ont observ√© une acc√©l√©ration encore plus grande, jusqu'√† trois ordres de grandeur. <br><br>  <b>Les cons√©quences:</b> <br><br>  Calculer des d√©riv√©es pour des fonctions du second ordre ou plus √† l'aide des biblioth√®ques d'apprentissage en profondeur actuelles est trop cher d'un point de vue informatique.  Cela inclut le calcul des tenseurs g√©n√©raux du quatri√®me ordre tels que les Hessians (par exemple, dans MAML et l'optimisation du second ordre de Newton).  Heureusement, les formules quadratiques sont rares en apprentissage profond.  Cependant, ils se trouvent souvent dans l'apprentissage automatique ¬´classique¬ª - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SVM</a> , m√©thode des moindres carr√©s, LASSO, processus gaussiens, etc. <br><br><h2>  Mythe 2: les bases de donn√©es d'images refl√®tent des photos du monde r√©el </h2><br>  Beaucoup de gens aiment penser que les r√©seaux de neurones ont appris √† mieux reconna√Ætre les objets que les gens.  Ce n'est pas le cas.  Ils peuvent √™tre en avance sur les personnes sur la base d'images s√©lectionn√©es, par exemple ImageNet, mais dans le cas de la reconnaissance d'objets √† partir de vraies photos de la vie ordinaire, ils ne pourront certainement pas d√©passer un adulte ordinaire.  En effet, la s√©lection d'images dans les ensembles de donn√©es actuels ne co√Øncide pas avec la s√©lection de toutes les images possibles rencontr√©es naturellement dans la r√©alit√©. <br><br>  Dans un travail assez ancien, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Unbias Look at Dataset Bias.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Torralba et Efros.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CVPR 2011.</a> , Les auteurs ont propos√© d'√©tudier les distorsions associ√©es √† un ensemble d'images dans douze bases de donn√©es populaires, afin de d√©terminer s'il est possible de former le classificateur pour d√©terminer l'ensemble de donn√©es √† partir duquel cette image a √©t√© prise. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/400/31a/f1a/40031af1a6a5bef3c9314ecb2373926a.png"><br><br>  Les chances de deviner accidentellement l'ensemble de donn√©es correctes sont de 1/12 √† 8%, tandis que les scientifiques eux-m√™mes ont fait face √† la t√¢che avec un taux de r√©ussite&gt; 75%. <br><br>  Ils ont form√© SVM sur un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">histogramme de gradient directionnel</a> (HOG) et ont constat√© que le classificateur a termin√© la t√¢che dans 39% des cas, ce qui d√©passe consid√©rablement les r√©sultats al√©atoires.  Si nous r√©p√©tions cette exp√©rience aujourd'hui, avec les r√©seaux de neurones les plus avanc√©s, nous verrions s√ªrement une augmentation de la pr√©cision du classificateur. <br><br>  Si les bases de donn√©es d'images affichaient correctement les vraies images du monde r√©el, nous n'aurions pas √† √™tre en mesure de d√©terminer de quel ensemble de donn√©es provient une image particuli√®re. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/879/7f7/4ad/8797f74ad14a0a4e28959ff0c67faca8.png"><br><br>  Cependant, il y a des traits dans les donn√©es qui rendent chaque ensemble d'images diff√©rent des autres.  ImageNet poss√®de de nombreuses voitures de course qui sont peu susceptibles de d√©crire la voiture moyenne ¬´th√©orique¬ª dans son ensemble. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/614/416/401/6144164018d5608bc5c2dc55e6e981f6.png"><br><br>  Les auteurs ont √©galement d√©termin√© la valeur de chaque ensemble de donn√©es en mesurant dans quelle mesure un classificateur form√© sur un ensemble fonctionne avec des images d'autres ensembles.  Selon cette m√©trique, les bases de donn√©es LabelMe et ImageNet se sont av√©r√©es √™tre les moins biais√©es, ayant re√ßu une note de 0,58 en utilisant la m√©thode du ¬´panier de devises¬ª.  Toutes les valeurs se sont av√©r√©es √™tre inf√©rieures √† l'unit√©, ce qui signifie que la formation sur un ensemble de donn√©es diff√©rent entra√Æne toujours de mauvaises performances.  Dans un monde id√©al sans ensembles biais√©s, certains chiffres auraient d√ª d√©passer un. <br><br>  Les auteurs ont conclu avec pessimisme: <br><blockquote>  Quelle est donc la valeur des ensembles de donn√©es existants pour les algorithmes de formation con√ßus pour le monde r√©el?  La r√©ponse qui en r√©sulte peut √™tre d√©crite comme ¬´meilleure que rien mais pas beaucoup¬ª. </blockquote><br><br><h2>  Mythe 3: les chercheurs de MO n'utilisent pas de kits de test pour les tests </h2><br>  Dans le manuel sur l'apprentissage automatique, nous apprenons √† diviser l'ensemble de donn√©es en formation, √©valuation et v√©rification.  L'efficacit√© du mod√®le, form√© sur l'ensemble de formation et √©valu√© sur l'√©valuation aide la personne impliqu√©e dans l'OM √† affiner le mod√®le pour maximiser l'efficacit√© dans son utilisation r√©elle.  L'ensemble de test n'a pas besoin d'√™tre touch√© jusqu'√† ce que la personne ait fini de s'ajuster pour fournir une √©valuation impartiale de l'efficacit√© r√©elle du mod√®le dans le monde r√©el.  Si une personne triche √† l'aide d'un ensemble de tests aux stades de la formation ou de l'√©valuation, le mod√®le risque de devenir trop adapt√© √† un ensemble de donn√©es particulier. <br><br>  Dans le monde hyper-comp√©titif de la recherche en MO, les nouveaux algorithmes et mod√®les sont souvent jug√©s par l'efficacit√© de leur travail avec les donn√©es de v√©rification.  Par cons√©quent, cela n'a aucun sens pour les chercheurs d'√©crire ou de publier des articles d√©crivant des m√©thodes qui fonctionnent mal avec des ensembles de donn√©es de test.  Et cela signifie, en substance, que la communaut√© de la r√©gion de Moscou dans son ensemble utilise un ensemble de tests pour l'√©valuation. <br><br>  Quelles sont les cons√©quences de cette arnaque? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2d4/f64/749/2d4f647491071cd0b77eb199ef563275.png"><br><br>  Les auteurs des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">classificateurs CIFAR-10 se g√©n√©ralisent-ils en CIFAR-10?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Recht et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ArXiv 2018 a</a> √©tudi√© ce probl√®me en cr√©ant une nouvelle suite de tests pour CIFAR-10.  Pour ce faire, ils ont fait une s√©lection d'images √† partir de Tiny Images. <br><br>  Ils ont choisi CIFAR-10 car il s'agit de l'un des ensembles de donn√©es les plus couramment utilis√©s dans le MO, le deuxi√®me ensemble le plus populaire de NeurIPS 2017 (apr√®s le MNIST).  Le processus de cr√©ation d'un ensemble de donn√©es pour CIFAR-10 est √©galement bien d√©crit et transparent, dans la grande base de donn√©es Tiny Images, il y a beaucoup d'√©tiquettes d√©taill√©es, de sorte que vous pouvez reproduire un nouvel ensemble de test, minimisant le d√©calage de distribution. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/71a/cd9/091/71acd909148795f73b49819b39946a5a.png"><br><br>  Ils ont constat√© qu'un grand nombre de mod√®les diff√©rents de r√©seaux de neurones sur le nouvel ensemble de tests ont montr√© une baisse significative de la pr√©cision (4% - 15%).  Cependant, le classement relatif des performances de chaque mod√®le est rest√© relativement stable. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6b/5aa/cfa/b6b5aacfab0c2628ea2ffe28babae533.png"><br><br>  En g√©n√©ral, les mod√®les les plus performants ont montr√© une baisse de pr√©cision plus faible que les mod√®les les moins performants.  C'est bien car il s'ensuit que la perte de g√©n√©ralisation du mod√®le due √† la triche, au moins dans le cas de CIFAR-10, diminue √† mesure que la communaut√© invente des m√©thodes et des mod√®les am√©lior√©s de MO. <br><br><h2>  Mythe 4: la formation au r√©seau neuronal utilise toutes les entr√©es </h2><br>  Il est g√©n√©ralement admis que les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">donn√©es sont une nouvelle huile</a> , et que plus nous avons de donn√©es, mieux nous pouvons former des mod√®les d'apprentissage en profondeur qui sont maintenant inefficaces sur l'√©chantillon et sur-param√©tris√©s. <br><br>  Dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une √©tude empirique d'exemples d'oubli pendant l'apprentissage d'un r√©seau neuronal profond.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Toneva et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Les</a> auteurs de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ICLR 2019</a> d√©montrent une redondance significative dans plusieurs ensembles courants de petites images.  √âtonnamment, 30% des donn√©es de CIFAR-10 peuvent simplement √™tre supprim√©es sans modifier consid√©rablement l'exactitude du contr√¥le. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/418/39b/03a/41839b03afd94771e940b60e2d7f905d.png"><br>  <i>Histoires d'oubli de (gauche √† droite) MNIST, permutedMNIST et CIFAR-10.</i> <br><br>  L'oubli se produit lorsqu'un r√©seau de neurones classe incorrectement une image au temps t + 1, alors qu'au temps t il a pu classer correctement une image.  Le flux de temps est mesur√© par les mises √† jour SGD.  Pour suivre l'oubli, les auteurs ont lanc√© leur r√©seau neuronal sur un petit ensemble de donn√©es apr√®s chaque mise √† jour SGD, et non sur tous les exemples disponibles dans la base de donn√©es.  Les exemples qui ne doivent pas √™tre oubli√©s sont appel√©s exemples inoubliables. <br><br>  Ils ont constat√© que 91,7% MNIST, 75,3% permut√©MNIST, 31,3% CIFAR-10 et 7,62% CIFAR-100 sont des exemples inoubliables.  Ceci est intuitivement compr√©hensible, car l'augmentation de la diversit√© et de la complexit√© de l'ensemble de donn√©es devrait faire oublier au r√©seau neuronal plus d'exemples. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdb/16b/ff9/fdb16bff98521d9d764588a679e51a59.png"><br><br>  Les exemples oubliables semblent pr√©senter des caract√©ristiques plus rares et √©tranges par rapport √† celles inoubliables.  Les auteurs les comparent aux vecteurs de support dans SVM, car ils semblent dessiner les contours des limites de d√©cision. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/c74/969/a75c74969991b75f621d340952a5cde3.png"><br><br>  Des exemples inoubliables, √† leur tour, codent pour la plupart des informations redondantes.  Si nous trions les exemples selon le degr√© d'inoubliable, nous pouvons compresser l'ensemble de donn√©es en supprimant les plus inoubliables. <br><br>  30% des donn√©es CIFAR-10 peuvent √™tre supprim√©es sans affecter l'exactitude des contr√¥les, et la suppression de 35% des donn√©es entra√Æne une l√©g√®re baisse de l'exactitude des contr√¥les de 0,2%.  Si vous s√©lectionnez au hasard 30% des donn√©es, leur suppression entra√Ænera une perte significative de la pr√©cision de la v√©rification de 1%. <br><br>  De m√™me, 8% des donn√©es peuvent √™tre supprim√©es du CIFAR-100 sans perte de pr√©cision de validation. <br><br>  Ces r√©sultats montrent qu'il existe une redondance significative dans les donn√©es pour la formation des r√©seaux de neurones, similaire √† la formation SVM, o√π les vecteurs non-support peuvent √™tre supprim√©s sans affecter la d√©cision du mod√®le. <br><br>  <b>Les cons√©quences:</b> <br><br>  Si nous pouvons d√©terminer lesquelles des donn√©es sont inoubliables avant de commencer la formation, nous pouvons √©conomiser de l'espace en les supprimant et en les temps sans les utiliser lors de la formation d'un r√©seau de neurones. <br><br><h2>  Mythe 5: La normalisation des lots est n√©cessaire pour entra√Æner des r√©seaux r√©siduels tr√®s profonds. </h2><br>  Pendant longtemps, on a cru que ¬´la formation d'un r√©seau neuronal profond pour une optimisation directe uniquement dans un but contr√¥l√© (par exemple, la probabilit√© logarithmique d'une classification correcte) en utilisant la descente de gradient, en commen√ßant par des param√®tres al√©atoires, ne fonctionne pas bien¬ª. <br><br>  Le tas de m√©thodes ing√©nieuses d'initialisation al√©atoire, de fonctions d'activation, de techniques d'optimisation et d'autres innovations, telles que les connexions r√©siduelles, qui est apparu depuis lors a facilit√© la formation de r√©seaux de neurones profonds en utilisant la m√©thode de descente en gradient. <br><br>  Mais une v√©ritable perc√©e s'est produite apr√®s l'introduction de la normalisation par lots (et d'autres techniques de normalisation s√©quentielle), limitant la taille des activations pour chaque couche du r√©seau afin d'√©liminer le probl√®me de la disparition et des gradients explosifs. <br><br>  Dans un ouvrage r√©cent, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Fixup Initialization: Residual Learning Without Normalization.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Zhang et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'ICLR 2019</a> a montr√© qu'il est possible de former un r√©seau √† 10000 couches en utilisant du SGD pur sans appliquer de normalisation. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d69/a29/cbb/d69a29cbb7bfce9750c9aa3ee6d7d659.png"><br><br>  Les auteurs ont compar√© la formation de r√©seaux neuronaux r√©siduels pour diff√©rentes profondeurs sur CIFAR-10 et ont constat√© que m√™me si les m√©thodes d'initialisation standard ne fonctionnaient pas pour 100 couches, les m√©thodes de normalisation Fixup et batch ont r√©ussi avec 10 000 couches. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bc9/23f/26a/bc923f26a4c39fb67663383716bc10d2.png" alt="image"><br><br>  Ils ont effectu√© une analyse th√©orique et ont montr√© que ¬´la normalisation du gradient de certaines couches est limit√©e par le nombre croissant √† l'infini d'un r√©seau profond¬ª, ce qui est un probl√®me de gradients explosifs.  Pour √©viter cela, Foxup est utilis√©, dont l'id√©e cl√© est de mettre √† l'√©chelle les poids en m couches pour chacune des branches r√©siduelles L par le nombre de fois en fonction de m et L. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/07f/a8f/39a/07fa8f39ad814861fd0cba6c936a2e18.png"><br><br>  Fixup a aid√© √† former un r√©seau r√©siduel profond avec 110 couches sur le CIFAR-10 avec une vitesse d'apprentissage √©lev√©e comparable au comportement d'un r√©seau d'architecture similaire form√© √† la normalisation par lots. <br><br>  Les auteurs ont en outre montr√© des r√©sultats de test similaires en utilisant Fixup sur le r√©seau sans aucune normalisation, en travaillant avec la base de donn√©es ImageNet et avec des traductions de l'anglais vers l'allemand. <br><br><h2>  Mythe 6: Les r√©seaux avec attention sont meilleurs que les r√©seaux convolutifs. </h2><br>  L'id√©e que les m√©canismes de ¬´l'attention¬ª sont sup√©rieurs aux r√©seaux de neurones convolutifs gagne en popularit√© dans la communaut√© des chercheurs de MO.  Dans le travail de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/(">Vaswani et ses coll√®gues</a> , il a √©t√© not√© que "le co√ªt de calcul des convolutions d√©tachables est √©gal √† la combinaison d'une couche d'auto-attention et d'une couche de r√©troaction ponctuelle". <br><br>  M√™me les r√©seaux avanc√©s comp√©titifs g√©n√©ratifs montrent l'avantage de l'attention personnelle sur la convolution standard lors de la mod√©lisation des d√©pendances √† longue port√©e. <br><br>  Les contributeurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">accordent moins d'attention aux convolutions l√©g√®res et dynamiques.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Wu et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'ICLR 2019</a> jette un doute sur l'efficacit√© param√©trique et l'efficacit√© de l'auto-attention lors de la mod√©lisation des d√©pendances √† long terme, et offre de nouvelles options de convolution, partiellement inspir√©es par l'auto-attention, plus efficaces en termes de param√®tres. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b40/f9a/a15/b40f9aa151eb4bcc3ebf224483ff8028.png"><br><br>  Les convolutions ¬´l√©g√®res¬ª sont s√©parables en profondeur, softmax normalis√©es en dimension temporelle, s√©par√©es en poids en dimension canal, et r√©utilisent les m√™mes poids √† chaque pas de temps (en tant que r√©seaux neuronaux r√©currents).  Les convolutions dynamiques sont des convolutions l√©g√®res qui utilisent des poids diff√©rents √† chaque pas de temps. <br><br>  De telles astuces rendent les convolutions l√©g√®res et dynamiques de plusieurs ordres de grandeur plus efficaces que les convolutions indivisibles standard. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/87d/d51/da0/87dd51da07637c42c4e4c2811b08b241.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c6a/469/ae5/c6a469ae5432cf31761e7663449497fd.png"><br><br>  Les auteurs montrent que ces nouvelles circonvolutions correspondent ou d√©passent les r√©seaux auto-absorbants en traduction automatique, mod√©lisation de langage, probl√®mes de sommation abstraite, utilisant des param√®tres identiques ou inf√©rieurs. <br><br><h2>  Mythe 7: Cartes de signification - un moyen fiable d'interpr√©ter les r√©seaux de neurones </h2><br>  Bien qu'il existe une opinion selon laquelle les r√©seaux de neurones sont des bo√Ætes noires, il y a eu de nombreuses tentatives pour les interpr√©ter.  Les plus populaires d'entre elles sont les cartes de signification ou d'autres m√©thodes similaires qui attribuent des √©valuations d'importance √† des fonctionnalit√©s ou √† des exemples de formation. <br><br>  Il est tentant de pouvoir conclure qu'une image donn√©e a √©t√© class√©e d'une certaine mani√®re en raison de certaines parties de l'image qui sont importantes pour le r√©seau neuronal.  Pour calculer les cartes de signification, il existe plusieurs m√©thodes qui utilisent souvent l'activation des r√©seaux de neurones dans une image donn√©e et les gradients passant par le r√©seau. <br><br>  Dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">interpr√©tation des r√©seaux de neurones est fragile.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ghorbani et al.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Les</a> auteurs de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AAAI 2019</a> montrent qu'ils peuvent introduire un changement insaisissable dans l'image, qui, cependant, faussera sa carte de signification. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/077/b89/e92/077b89e923e2936769b2f99662171a94.png"><br><br>  Le r√©seau neuronal d√©termine le papillon monarque non pas par le motif sur ses ailes, mais en raison de la pr√©sence de feuilles vertes sans importance sur le fond de la photo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1d2/314/9fa/1d23149fa91caab32dc2ea16a8593379.png"><br><br>  Les images multidimensionnelles sont souvent plus proches des limites de d√©cision prises par les r√©seaux de neurones profonds, d'o√π leur sensibilit√© aux attaques adversaires.  Et si les attaques concurrentielles d√©placent les images au-del√† des limites de la solution, les attaques interpr√©tatives comp√©titives les d√©placent le long des limites de la solution sans quitter le territoire de la m√™me solution. <br><br>  La m√©thode de base d√©velopp√©e par les auteurs est une modification de la m√©thode Goodfello de marquage √† gradient rapide, qui fut l'une des premi√®res m√©thodes r√©ussies d'attaques comp√©titives.  On peut supposer que d'autres attaques plus r√©centes et plus complexes peuvent √©galement √™tre utilis√©es pour des attaques sur l'interpr√©tation des r√©seaux de neurones. <br><br>  <b>Les cons√©quences:</b> <br><br>  En raison de la propagation croissante de l'apprentissage en profondeur dans des domaines d'application critiques tels que l'imagerie m√©dicale, il est important d'approcher attentivement l'interpr√©tation des d√©cisions prises par les r√©seaux de neurones.  Par exemple, m√™me s'il serait formidable que le r√©seau de neurones convolutifs puisse reconna√Ætre la tache sur l'image IRM comme une tumeur maligne, ces r√©sultats ne devraient pas √™tre fiables s'ils sont bas√©s sur des m√©thodes d'interpr√©tation peu fiables. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr444172/">https://habr.com/ru/post/fr444172/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr444162/index.html">Courte critique du t√©l√©phone IP Snom D120</a></li>
<li><a href="../fr444164/index.html">Pr√©sentation du syst√®me d'alerte Snom PA1</a></li>
<li><a href="../fr444166/index.html">Pavel Finkelstein √† propos de Kotlin en production sur jug.msk.ru</a></li>
<li><a href="../fr444168/index.html">Comment transf√©rer Windows 10 sous licence vers un autre ordinateur</a></li>
<li><a href="../fr444170/index.html">Publier des applications iOS sur l'App Store avec GitLab et fastlane</a></li>
<li><a href="../fr444174/index.html">GeekBrains invite les d√©butants √† un jeu √©ducatif</a></li>
<li><a href="../fr444176/index.html">Chiffres √©l√©mentaires en langage simple</a></li>
<li><a href="../fr444178/index.html">9 conseils pour cr√©er des jeux ind√©pendants √† partir d'un seul d√©veloppeur</a></li>
<li><a href="../fr444182/index.html">Go conditions et leurs bizarreries</a></li>
<li><a href="../fr444184/index.html">√Ä propos des perspectives des centres de donn√©es pr√©-assembl√©s</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>